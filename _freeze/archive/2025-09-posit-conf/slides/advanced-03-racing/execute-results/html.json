{
  "hash": "9d02b10378fe86b86a206f37380ef538",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"3 - Racing\"\nsubtitle: \"Getting More Out of Feature Engineering and Tuning for Machine Learning\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://workshops.tidymodels.org>\n    include-before-body: header.html\n    include-after-body: footer-annotations.html\n    theme: [default, tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n\n\n\n## Startup!  ![](hexes/tidymodels.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/finetune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/bonsai.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(finetune)\nlibrary(bonsai)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\n```\n:::\n\n\n\n## More startup!   ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load our example data for this section\n\"https://raw.githubusercontent.com/tidymodels/\" |> \n  paste0(\"workshops/main/slides/class_data.RData\") |> \n  url() |> \n  load()\n\nset.seed(429)\nsim_split <- initial_split(class_data, prop = 0.75, strata = class)\nsim_train <- training(sim_split)\nsim_test  <- testing(sim_split)\n\nset.seed(523)\nsim_rs <- vfold_cv(sim_train, v = 10, strata = class)\n```\n:::\n\n\n\n## First, a shameless promotion\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/finetune-toot.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n# Efficient Grid Search  {background-color=\"#5E9546FF\"}\n\n## Making Grid Search More Efficient\n\nPreviously, we evaluated 250 models (25 candidates times 10 resamples).\n\nWe can make this go faster using parallel processing. \n\n<br> \n\nAlso, for some models, we can _fit_ far fewer models than the number being evaluated. \n \n * For example, with boosted trees, a model with `X` trees can often predict on candidates with fewer than `X` trees (i.e., no retraining). \n \nThese strategies can lead to enormous speed-ups. \n\n## Model Racing \n\n[_Racing_](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=+Hoeffding+racing) is an old tool that we can use to go even faster. \n\n1. Evaluate all of the candidate models, but only for a few resamples. \n1. Determine which candidates have a low probability of being selected (_cough_, _cough_, `tanh` activation, _cough_).\n1. Eliminate poor candidates.\n1. Repeat with next resample (until no more resamples remain).\n\nThis can result in fitting a small number of models. \n\nIt is *not* an iterative search; it is an adaptive grid search. \n\n::: {.absolute bottom=0 right=0}\n[TMwR](https://www.tmwr.org/grid-search#racing), [TMwR example](https://www.tmwr.org/workflow-sets#racing-example), [AML4TD](https://aml4td.org/chapters/grid-search.html#sec-racing)\n:::\n\n\n## Discarding Candidates\n\nHow do we eliminate tuning parameter combinations? \n\nThere are a few methods to do so. We'll use one based on analysis of variance (ANOVA). \n\n_However_... there is typically a large resampling effect in the results. \n\n## Resampling Results (Non-Racing)\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\nHere are some realistic (but simulated) examples of two candidate models. \n\nAn error estimate is measured for each of 10 resamples. \n\n - The lines connect resamples. \n\nThere is usually a significant resample-to-resample effect (rank corr: 0.83). \n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](advanced-03-racing_files/figure-revealjs/race-data-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n:::\n\n::::\n\n\n## Are Candidates Different?\n\nOne way to evaluate these models is to do a paired t-test\n \n - or a t-test on their differences matched by resamples\n\nWith $n = 10$ resamples, the confidence interval for the difference in the model error is (0.99, 2.8), indicating that candidate number 2 has a smaller error. \n\n## Evaluating Differences in Candidates\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\nWhat if we were to have compared the candidates while we sequentially evaluated each resample? \n\nðŸ‘‰\n\n<be>\n\nOne candidate shows superiority when 5 resamples have been evaluated.\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](advanced-03-racing_files/figure-revealjs/race-ci-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n## Interim Analysis of Results\n\nOne version of racing uses a _mixed model ANOVA_ to construct one-sided confidence intervals for each candidate versus the current best. \n\nAny candidates whose bound does not include zero are discarded.  [Here](https://www.tmwr.org/race_results.mp4) is an animation.\n\nThe resamples are analyzed in a random order (so set the seed).\n\n<br>\n\n[Kuhn (2014)](https://arxiv.org/abs/1405.6974) has examples and simulations to show that the method works. \n\nThe [finetune](https://finetune.tidymodels.org/) package has functions `tune_race_anova()` and `tune_race_win_loss()`. \n\n# Boosted Trees  {background-color=\"#70B7E1FF\"}\n\n\n## Boosted Trees\n\nThese are popular ensemble methods that build a _sequence_ of tree models. \n\n<br>\n\nEach tree uses the results of the previous tree to better predict samples, especially those that have been poorly predicted. \n\n<br>\n\nEach tree in the ensemble is saved, and new samples are predicted using a weighted average of its votes. \n\n<br>\n\nWe'll focus on the popular lightgbm implementation. \n\n## Boosted Tree Tuning Parameters\n\nSome _possible_ parameters: \n\n* `mtry`: The number of predictors randomly sampled at each split (in $[1, ncol(x)]$ or $(0, 1]$).\n* `trees`: The number of trees ($[1, \\infty]$, but usually up to thousands).\n* `min_n`: The number of samples needed to further split ($[1, n]$).\n* `learn_rate`: The rate that each tree adapts from previous iterations ($(0, \\infty]$, usual maximum is 0.1).\n* `stop_iter`: The number of iterations of boosting where _no improvement_ was shown before stopping ($[1, trees]$).\n\n## Boosted Tree Tuning Parameters\n\nTBH, it is usually not difficult to optimize these models. \n\n<br>\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\nOften, there are multiple _candidate_ tuning parameter regions with very good results. \n\nFor example: ðŸ‘‰\n\n<br>\n\nTo demonstrate, we'll look at optimizing five of the tuning parameters.\n:::\n\n::: {.column width=\"50%\"}\n![](images/small_init.svg){fig-align=\"center\" width=80%}\n:::\n\n::::\n\n\n## Boosted Tree Tuning Parameters ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/parsnip.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/bonsai.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"}\n\nWe'll need to load the bonsai package. This has the information needed to use lightgbm\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|5-9|12-14|16-17|\"}\nlibrary(bonsai)\n\nlgbm_spec <-\n  boost_tree(\n    trees = tune(),\n    learn_rate = tune(),\n    mtry = tune(),\n    min_n = tune(),\n    stop_iter = tune()\n  ) |>\n  set_mode(\"classification\") |>\n  # Turn off within-tree parallel processing; it's faster to run \n  # the resamples/configurations in parallel\n  set_engine(\"lightgbm\", num_threads = 1) \n\n# No preprocessing required:\nlgbm_wflow <- workflow(class ~ ., lgbm_spec)\n```\n:::\n\n\n\n# Racing our boosted trees  {background-color=\"#3381A8FF\"}\n\n## Racing ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/finetune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/bonsai.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|3-4|6-7|12|14-15|\"}\nlibrary(finetune)\n\n# Set this to true to demo\nctrl <- control_race(verbose_elim = FALSE)\n\n# Optimizes on the first metric in the set\ncls_mtr <- metric_set(brier_class, roc_auc, sensitivity, specificity)\n\nmirai::daemons(parallel::detectCores() - 1)\n#> [1] 13\n\nset.seed(321)\nlgbm_res <-\n  lgbm_wflow |>\n  tune_race_anova(              # <- very similar syntax to tune_grid()\n    resamples = sim_rs,\n    # Let's use a larger grid\n    grid = 50,\n    control = ctrl,\n    metrics = cls_mtr\n  )\n```\n:::\n\n\n\n## Racing Results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1|5|\"}\nshow_best(lgbm_res, metric = \"brier_class\")\n#> # A tibble: 1 Ã— 11\n#>    mtry trees min_n learn_rate stop_iter .metric .estimator   mean     n std_err\n#>   <int> <int> <int>      <dbl>     <int> <chr>   <chr>       <dbl> <int>   <dbl>\n#> 1     9  1836     9    0.00222         6 brier_â€¦ binary     0.0379    10 0.00238\n#> # â„¹ 1 more variable: .config <chr>\n```\n:::\n\n\n\n<br> \n\n. . .\n\nTimes using 10 cores: sequential: 605s, parallel: 92s, and parallel racing: 50s. \n\n<br> \n\n. . .\n\nParallel was 6.6-fold faster, and racing in parallel was 12.3-fold faster.\n\n## Racing Results ![](hexes/finetune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\nOnly 378 models were fit (out of 500). \n\n`select_best()` never considers candidate models that did not get to the end of the race. \n\nThere is a helper function to see how candidate models were removed from consideration. \n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_race(lgbm_res)\n```\n\n::: {.cell-output-display}\n![](advanced-03-racing_files/figure-revealjs/plot-race-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n:::\n\n::::\n\n\n## Your turn {transition=\"slide-in\"}\n\n- *Run `tune_race_anova()` with a different seed and/or a different metric.*\n- *Did you get the same or similar results?*\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"racing-repeat\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">08</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}