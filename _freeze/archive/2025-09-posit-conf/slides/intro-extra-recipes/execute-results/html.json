{
  "hash": "38dece9012d5361dee5540a0696ceb39",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Extras - Recipes\"\nsubtitle: \"Introduction to Machine Learning in R with tidymodels\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://workshops.tidymodels.org>\n    include-before-body: header.html\n    include-after-body: footer-annotations.html\n    theme: [default, tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n    fig.path: \"figures/\"\n---\n\n\n\n\n\n## Looking at the predictors\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_train\n#> # A tibble: 8,749 × 19\n#>    forested  year elevation eastness roughness tree_no_tree dew_temp precip_annual temp_annual_mean temp_annual_min temp_annual_max temp_january_min vapor_min vapor_max canopy_cover   lon   lat\n#>    <fct>    <dbl>     <dbl>    <dbl>     <dbl> <fct>           <dbl>         <dbl>            <dbl>           <dbl>           <dbl>            <dbl>     <dbl>     <dbl>        <dbl> <dbl> <dbl>\n#>  1 Yes       1997        66       82        10 Tree            12.2           1315             18.4            1.88            24.9            11.8        121      1888           66 -84.9  32.4\n#>  2 No        1997       284      -99        58 Tree            10.3           1236             16.1           -0.26            22.3             9.92        68      1586           80 -85.0  34.1\n#>  3 Yes       2022       130       86        15 Tree            11.8           1194             17.6            1.3             24.1            11.2         61      1753           96 -83.0  33.1\n#>  4 Yes       2021       202      -55         3 Tree            10.7           1235             16.6            0.05            23.0            10.2         72      1682           65 -83.0  33.9\n#>  5 Yes       1995        75      -89         1 Tree            13.8           1256             19.2            3.63            25.5            12.9         57      1796           88 -83.4  31.3\n#>  6 No        1995       110      -53         5 Tree            12.4           1236             18.6            2.53            24.8            12.4        102      1835           51 -83.9  32.2\n#>  7 Yes       2022       111       73        12 Tree            11.5           1168             17.4            1               24.0            10.9         67      1772           84 -82.2  33.6\n#>  8 Yes       1997       230       96        14 Tree             9.98          1373             15.4           -1.35            21.8             9.03        46      1552           68 -85.3  34.8\n#>  9 Yes       2002       160      -88        13 Tree            11.1           1219             16.9            0.07            23.6            10.2         53      1731           95 -83.2  33.6\n#> 10 Yes       2020        39        9         6 Tree            13.9           1237             19.2            3.25            25.6            12.8         58      1812           86 -82.0  31.7\n#> # ℹ 8,739 more rows\n#> # ℹ 2 more variables: land_type <fct>, county <fct>\n```\n:::\n\n\n\n## Working with other models\n\nSome models can't handle non-numeric data\n\n-   Linear Regression\n-   K Nearest Neighbors\n\n<br>\n\n::: fragment\nSome models struggle if numeric predictors aren't scaled\n\n-   K Nearest Neighbors\n-   Anything using gradient descent\n:::\n\n## Types of needed preprocessing\n\n-   Do qualitative predictors require a numeric encoding?\n\n-   Should columns with a single unique value be removed?\n\n-   Does the model struggle with missing data?\n\n-   Does the model struggle with correlated predictors?\n\n-   Should predictors be centered and scaled?\n\n-   Is it helpful to transform predictors to be more symmetric?\n\n::: footer\n<https://www.tmwr.org/pre-proc-table.html>\n:::\n\n## Two types of preprocessing\n\n![](images/fe_venn.svg){fig-align=\"center\"}\n\n## Two types of preprocessing\n\n![](images/fe_venn_info.svg){fig-align=\"center\"}\n\n## General definitions\n\n* _Data preprocessing_ is what you do to make your model **successful**.\n* _Feature engineering_ is what you do to the original predictors to make the model do the **least work** to perform great.\n\n## Working with dates\n\nDatetime variables are automatically converted to an integer if given as a raw predictor. To avoid this, it can be re-encoded as:\n\n* Days since a reference date\n* Day of the week\n* Month\n* Year\n* Leap year\n* Indicators for holidays\n\n## Two types of transformations\n\n<br>\n\n::: columns\n::: {.column width=\"50%\"}\n\n### Static\n\n- Square root, log, inverse\n- Dummies for known levels\n- Date time extractions\n\n:::\n\n::: {.column width=\"50%\"}\n\n### Trained\n\n- Centering & scaling\n- Imputation\n- PCA\n- Anything for unknown factor levels\n\n:::\n\n:::\n\n::: fragment\nTrained methods need to calculate **sufficient information** to be applied again.\n:::\n\n## The recipes package\n\n::: {.incremental .highlight-last}\n- Modular + extensible\n- Works well with pipes ,`|>` and `%>%`\n- Deferred evaluation\n- Isolates test data from training data\n- Can do things formulas can't\n:::\n\n\n## How to write a recipe\n\n:::{style=\"font-family: 'Source Code Pro', monospace; font-size: 0.8em;\"}\nforested_rec <- recipe(forested ~ ., data = forested_train) |>  \n\\ \\ step_dummy(all_nominal_predictors()) |>  \n\\ \\ step_zv(all_predictors()) |>  \n\\ \\ step_log(canopy_cover, offset = 0.5) |>  \n\\ \\ step_normalize(all_numeric_predictors())\n:::\n\n## How to write a recipe\n\n:::{style=\"font-family: 'Source Code Pro', monospace; font-size: 0.8em;\"}\nforested_rec <- [recipe(forested ~ ., data = forested_train)]{style=\"color: #CA225E;\"} |>  \n\\ \\ step_dummy(all_nominal_predictors()) |>  \n\\ \\ step_zv(all_predictors()) |>  \n\\ \\ step_log(canopy_cover, offset = 0.5) |>  \n\\ \\ step_normalize(all_numeric_predictors())\n:::\n\n<br>\n\nStart by calling `recipe()` to denote the data source and variables used.\n\n## How to write a recipe\n\n:::{style=\"font-family: 'Source Code Pro', monospace; font-size: 0.8em;\"}\nforested_rec <- recipe(forested ~ ., data = forested_train) |>  \n\\ \\ [step_dummy]{style=\"color: #CA225E;\"}(all_nominal_predictors()) |>  \n\\ \\ [step_zv]{style=\"color: #CA225E;\"}(all_predictors()) |>  \n\\ \\ [step_log]{style=\"color: #CA225E;\"}(canopy_cover, offset = 0.5) |>  \n\\ \\ [step_normalize]{style=\"color: #CA225E;\"}(all_numeric_predictors())\n:::\n\n<br>\n\nSpecify what actions to take by adding `step_*()`s.\n\n## How to write a recipe\n\n:::{style=\"font-family: 'Source Code Pro', monospace; font-size: 0.8em;\"}\nforested_rec <- recipe(forested ~ ., data = forested_train) |>  \n\\ \\ step_dummy([all_nominal_predictors()]{style=\"color: #CA225E;\"}) |>  \n\\ \\ step_zv([all_predictors()]{style=\"color: #CA225E;\"}) |>  \n\\ \\ step_log([canopy_cover]{style=\"color: #CA225E;\"}, offset = 0.5) |> \n\\ \\ step_normalize([all_numeric_predictors()]{style=\"color: #CA225E;\"})\n:::\n<br>\n\nUse {tidyselect} and recipes-specific selectors to denote affected variables.\n\n## Using a recipe\n\n:::{style=\"font-family: 'Source Code Pro', monospace; font-size: 0.8em;\"}\nforested_rec <- recipe(forested ~ ., data = forested_train) |>  \n\\ \\ step_dummy(all_nominal_predictors()) |>  \n\\ \\ step_zv(all_predictors()) |>  \n\\ \\ step_log(canopy_cover, offset = 0.5) |> \n\\ \\ step_normalize(all_numeric_predictors())\n:::\n\n<br>\n\nSave the recipe we like so that we can use it in various places, e.g., with different models.\n\n<br>\n\n## Using a recipe with workflows\n\nRecipes are typically combined with a model in a `workflow()` object:\n\n<br>\n\n:::{style=\"font-family: 'Source Code Pro', monospace; font-size: 0.8em;\"}\nforested_wflow <- workflow() |>  \n\\ \\ [add_recipe(forested_rec)]{style=\"color: #CA225E;\"} |>  \n\\ \\ add_model(linear_reg())\n:::\n\n## Recipes are estimated\n\nEvery preprocessing step in a recipe that involved calculations uses the *training* set. For example:\n\n- Levels of a factor\n- Determination of zero-variance\n- Normalization\n- Feature extraction\n\nOnce a recipe is added to a workflow, this occurs when `fit()` is called.\n\n\n## Debugging a recipe\n\n- Typically, you will want to use a workflow to estimate and apply a recipe.\n\n. . .\n\n- If you have an error and need to debug your recipe, the original recipe object (e.g. `forested_rec`) can be estimated manually with a function called `prep()`. It is analogous to `fit()`. See [TMwR section 16.4](https://www.tmwr.org/dimensionality.html#recipe-functions).\n\n. . .\n\n- Another function, `bake()`, is analogous to `predict()`, and gives you the processed data back.\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n<br>\n\n*Take the recipe and `prep()` then `bake()` it to see what the resulting data set looks like.*\n\n*Try removing steps to see how the result changes.*\n\n<br>\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"recipes-prep\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n## Printing a recipe\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_rec\n#> \n#> ── Recipe ────────────────────────────────────────────────────────────\n#> \n#> ── Inputs\n#> Number of variables by role\n#> outcome:    1\n#> predictor: 18\n#> \n#> ── Operations\n#> • Dummy variables from: all_nominal_predictors()\n#> • Zero variance filter on: all_predictors()\n#> • Log transformation on: canopy_cover\n#> • Centering and scaling for: all_numeric_predictors()\n```\n:::\n\n\n\n## Prepping a recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprep(forested_rec)\n#> \n#> ── Recipe ────────────────────────────────────────────────────────────\n#> \n#> ── Inputs\n#> Number of variables by role\n#> outcome:    1\n#> predictor: 18\n#> \n#> ── Training information\n#> Training data contained 8749 data points and no incomplete rows.\n#> \n#> ── Operations\n#> • Dummy variables from: tree_no_tree, land_type, county | Trained\n#> • Zero variance filter removed: <none> | Trained\n#> • Log transformation on: canopy_cover | Trained\n#> • Centering and scaling for: year elevation, ... | Trained\n```\n:::\n\n\n\n## Baking a recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprep(forested_rec) |>\n  bake(new_data = forested_train)\n#> # A tibble: 8,749 × 177\n#>      year elevation eastness roughness dew_temp precip_annual temp_annual_mean temp_annual_min temp_annual_max temp_january_min vapor_min vapor_max canopy_cover     lon     lat forested\n#>     <dbl>     <dbl>    <dbl>     <dbl>    <dbl>         <dbl>            <dbl>           <dbl>           <dbl>            <dbl>     <dbl>     <dbl>        <dbl>   <dbl>   <dbl> <fct>   \n#>  1 -1.15    -0.521    1.13      0.0463  -0.232          0.178            0.172          -0.107           0.338         -0.00909    3.10     1.01          0.407  -1.44   -0.0184 Yes     \n#>  2 -1.15     1.10    -1.50      3.91    -1.39          -0.568           -1.31           -1.33           -1.28          -1.28       0.0408  -0.941         0.652  -1.51    1.36   No      \n#>  3  0.892   -0.0440   1.19      0.449   -0.422         -0.965           -0.305          -0.438          -0.170         -0.438     -0.363    0.140         0.884   0.331   0.556  Yes     \n#>  4  0.810    0.493   -0.858    -0.518   -1.11          -0.577           -1.01           -1.15           -0.878         -1.10       0.272   -0.320         0.388   0.331   1.26   Yes     \n#>  5 -1.31    -0.454   -1.35     -0.679    0.748         -0.379            0.701           0.893           0.683          0.702     -0.594    0.418         0.773  -0.0643 -0.992  Yes     \n#>  6 -1.31    -0.193   -0.829    -0.357   -0.0546        -0.568            0.331           0.264           0.281          0.366      2.00     0.670         0.0802 -0.468  -0.237  No      \n#>  7  0.892   -0.186    1.00      0.207   -0.631         -1.21            -0.444          -0.609          -0.264         -0.619     -0.0169   0.262         0.714   1.01    0.929  Yes     \n#>  8 -1.15     0.702    1.34      0.369   -1.56           0.726           -1.77           -1.95           -1.60          -1.87      -1.23    -1.16          0.445  -1.77    2.03   Yes     \n#>  9 -0.740    0.180   -1.34      0.288   -0.858         -0.729           -0.801          -1.14           -0.515         -1.08      -0.825   -0.00273       0.870   0.122   0.988  Yes     \n#> 10  0.729   -0.722    0.0719   -0.276    0.816         -0.559            0.708           0.676           0.745          0.635     -0.536    0.521         0.744   1.22   -0.653  Yes     \n#> # ℹ 8,739 more rows\n#> # ℹ 161 more variables: tree_no_tree_No.tree <dbl>, land_type_Non.tree.vegetation <dbl>, land_type_Tree <dbl>, county_Atkinson <dbl>, county_Bacon <dbl>, county_Baker <dbl>, county_Baldwin <dbl>,\n#> #   county_Banks <dbl>, county_Barrow <dbl>, county_Bartow <dbl>, county_Ben.Hill <dbl>, county_Berrien <dbl>, county_Bibb <dbl>, county_Bleckley <dbl>, county_Brantley <dbl>, county_Brooks <dbl>,\n#> #   county_Bryan <dbl>, county_Bulloch <dbl>, county_Burke <dbl>, county_Butts <dbl>, county_Calhoun <dbl>, county_Camden <dbl>, county_Candler <dbl>, county_Carroll <dbl>, county_Catoosa <dbl>,\n#> #   county_Charlton <dbl>, county_Chatham <dbl>, county_Chattahoochee <dbl>, county_Chattooga <dbl>, county_Cherokee <dbl>, county_Clarke <dbl>, county_Clay <dbl>, county_Clayton <dbl>,\n#> #   county_Clinch <dbl>, county_Cobb <dbl>, county_Coffee <dbl>, county_Colquitt <dbl>, county_Columbia <dbl>, county_Cook <dbl>, county_Coweta <dbl>, county_Crawford <dbl>, county_Crisp <dbl>,\n#> #   county_Dade <dbl>, county_Dawson <dbl>, county_Decatur <dbl>, county_DeKalb <dbl>, county_Dodge <dbl>, county_Dooly <dbl>, county_Dougherty <dbl>, county_Douglas <dbl>, county_Early <dbl>, …\n```\n:::\n\n\n\n## Tidying a recipe\n\nOnce a recipe as been estimated, there are various bits of information saved in it.\n\n- The `tidy()` function can be used to get specific results from the recipe.\n\n## Your turn {transition=\"slide-in\"}\n\n![](images/parsnip-flagger.jpg){.absolute top=\"0\" right=\"0\" width=\"150\" height=\"150\"}\n\n*Take a prepped recipe and use the `tidy()` function on it.*\n\n*Use the `number` argument to inspect different steps.*\n\n<br>\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"recipes-tidy\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n## Tidying a recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprep(forested_rec) |>\n  tidy()\n#> # A tibble: 4 × 6\n#>   number operation type      trained skip  id             \n#>    <int> <chr>     <chr>     <lgl>   <lgl> <chr>          \n#> 1      1 step      dummy     TRUE    FALSE dummy_hIEnQ    \n#> 2      2 step      zv        TRUE    FALSE zv_ZrQBx       \n#> 3      3 step      log       TRUE    FALSE log_6es7X      \n#> 4      4 step      normalize TRUE    FALSE normalize_XsIxb\n```\n:::\n\n\n\n## Tidying a recipe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprep(forested_rec) |>\n  tidy(number = 1)\n#> # A tibble: 161 × 3\n#>    terms        columns             id         \n#>    <chr>        <chr>               <chr>      \n#>  1 tree_no_tree No tree             dummy_hIEnQ\n#>  2 land_type    Non-tree vegetation dummy_hIEnQ\n#>  3 land_type    Tree                dummy_hIEnQ\n#>  4 county       Atkinson            dummy_hIEnQ\n#>  5 county       Bacon               dummy_hIEnQ\n#>  6 county       Baker               dummy_hIEnQ\n#>  7 county       Baldwin             dummy_hIEnQ\n#>  8 county       Banks               dummy_hIEnQ\n#>  9 county       Barrow              dummy_hIEnQ\n#> 10 county       Bartow              dummy_hIEnQ\n#> # ℹ 151 more rows\n```\n:::\n\n\n\n\n## Using a recipe in tidymodels\n\nThe recommended way to use a recipe in tidymodels is to use it as part of a `workflow()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_wflow <- workflow() |>  \n  add_recipe(forested_rec) |>  \n  add_model(linear_reg())\n```\n:::\n\n\n\nWhen used in this way, you don't need to worry about `prep()` and `bake()` as it is handled for you.\n\n## More information\n\n- <https://recipes.tidymodels.org/>\n- <https://www.tmwr.org/recipes.html>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}