---
title: "1 - Introduction - Classwork"
subtitle: "Getting More Out of Feature Engineering and Tuning for Machine Learning"
editor_options: 
  chunk_output_type: console
---

We recommend restarting R between each slide deck!

## Setup

```{r}
library(tidymodels)

# Max's usual settings:
tidymodels_prefer()
theme_set(theme_bw())
options(
  pillar.advice = FALSE,
  pillar.min_title_chars = Inf
)
```

## Imbalanced data

```{r}
"https://raw.githubusercontent.com/tidymodels/" |>
  paste0("workshops/main/slides/class_data.RData") |>
  url() |>
  load()
```

## Your turn

Let's warm up by taking 8 minutes to explore the data. 

We'll ask you to tell us something about these data that might be interesting for modeling. 

```{r}
# Your code here!

```

## Data splitting 

```{r}
set.seed(429)
sim_split <- initial_split(class_data, prop = 0.75, strata = class)
sim_split

sim_train <- training(sim_split)
sim_test <- testing(sim_split)
```

## Resampling

```{r}
set.seed(523)
sim_rs <- vfold_cv(sim_train, v = 10, strata = class)
sim_rs

model_data_1 <- sim_rs |> get_rsplit(1) |> analysis()
model_data_1 |> count(class)

perf_data_1 <- sim_rs |> get_rsplit(1) |> assessment()
perf_data_1 |> count(class)
```

## Models via parsnip

```{r}
# Specify what you want
tree_spec <- decision_tree(mode = "classification")

# Then train:
tree_fit <- tree_spec |> fit(class ~ ., data = model_data_1)
tree_fit
```  

## Predicting and augmenting

```{r}
predict(tree_fit, new_data = head(perf_data_1, 4))

predict(tree_fit, new_data = head(perf_data_1, 4), type = "prob")

tree_pred <- augment(tree_fit, new_data = perf_data_1)
tree_pred |> slice(1:5)
```  

## Performance metrics

```{r}
cls_metrics <- metric_set(brier_class, roc_auc, sensitivity, specificity)
tree_pred |> cls_metrics(truth = class, estimate = .pred_class, .pred_event)
```

## Recipes and workflows 

```{r}
rec <-
  recipe(class ~ ., data = sim_train) |>
  step_normalize(all_numeric_predictors())

tree_wflow <- workflow(rec, tree_spec)
```

## Your turn

Fit a type different decision tree, this time:

- Using the `C5.0` engine
- Change the minimum number of samples required for splitting to 10. 

Did performance change much? 

```{r}
# Your code here!

```
