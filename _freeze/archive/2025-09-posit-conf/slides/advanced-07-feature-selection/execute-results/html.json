{
  "hash": "5c2af275c33d920c6558f409d752e7e5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"7 - Feature selection\"\nsubtitle: \"Getting More Out of Feature Engineering and Tuning for Machine Learning\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://workshops.tidymodels.org>\n    include-before-body: header.html\n    include-after-body: footer-annotations.html\n    theme: [default, tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n\n## Startup!  ![](hexes/tidymodels.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/probably.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(important)\nlibrary(probably)\nlibrary(mirai)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\ndaemons(parallel::detectCores())\n```\n:::\n\n\n\n## More startup! ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load our example data for this section\n\"https://raw.githubusercontent.com/tidymodels/\" |> \n  paste0(\"workshops/main/slides/class_data.RData\") |> \n  url() |> \n  load()\n\nset.seed(429)\nsim_split <- initial_split(class_data, prop = 0.75, strata = class)\nsim_train <- training(sim_split)\nsim_test  <- testing(sim_split)\n\nset.seed(523)\nsim_rs <- vfold_cv(sim_train, v = 10, strata = class)\n```\n:::\n\n\n\n\n\n\n# Why remove features/predictors?  {background-color=\"#E0D890FF\"}\n\n## Models and feature selection\n\nSome models _automatically_ remove predictors by never using them in the model: \n\n - tree- and rule-based models\n - some regularized models (e.g., `glmnet`)\n - multivariate adaptive regression splines (MARS)\n - RuleFit\n - _not really_ ensembles though\n \nSometimes using irrelevant predictors *hurts* model performance.\n\n## Effects of extra predictors\n\n![](images/fig-irrelevant-predictors-1.svg){fig-align='center' width=60%}\n\n::: {.absolute bottom=0 right=0}\n[AML4TD](https://aml4td.org/chapters/feature-selection.html#sec-irrelevant-predictors)\n:::\n\n## General selection methods\n\n-  **wrappers**: a sequential algorithm proposes feature subsets, fits the model with these subsets, and then determines a better subset from the results.\n\n-  **filters**: screen predictors before adding them to the model. \n\ntidymodels doesn't have any wrappers (but see the [caret documentation for them](https://topepo.github.io/caret/recursive-feature-elimination.html))\n\n<br> \n\nThe new important package does have filters via recipes. \n\n## Be careful!!!\n\ntidymodels has always contained some \"hidden guardrails\" that should prevent practitioners from making subtle (but consequential) methodological mistakes. \n\n<br>\n\nFeature selection is a good example. Based on the literature, it is _easily done wrong_.\n\n<br>\n\nThe selection process should take place _inside_ a resampling loop so that the workflow does not overfit the predictors. \n\n::: {.absolute bottom=0 right=0}\n[A&M (2002)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=Selection+bias+in+gene+extraction+on+the+basis+of+microarray+gene+expression+data&as_ylo=2002&as_yhi=2002&btnG=), [AML4TD](https://aml4td.org/chapters/feature-selection.html#sec-selection-overfitting), [FES](https://bookdown.org/max/FES/selection-overfitting.html)\n:::\n\n# Imbalanced example (again) {background-color=\"#EDD6D5FF\"}\n\n## IMPORTANT ![](hexes/filtro.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nWe released two packages this year that enable supervised feature selection: \n\n- filtro: low-level _scoring_ methods for predictors (e.g., importance).\n- important: tools for permutation importance and recipes steps for supervised feature selection. \n\n<br>\n\nLet's look at the help page for [`important::step_predictor_best()`](https://important.tidymodels.org/reference/step_predictor_best.html).\n\n\n\n## K-nearest neighbors ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tailor.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/recipes.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/parsnip.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"} ![](hexes/filtro.png){.absolute top=-20 right=256 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|3-8|5|6|7|17|\"}\nrec <-\n  recipe(class ~ ., data = sim_train) |>\n  step_predictor_best(\n    all_predictors(),\n    score = \"imp_rf\",\n    prop_terms = tune(),\n    id = \"filter\"\n  ) |>\n  step_normalize(all_numeric_predictors())\n  \nknn_spec <- \n  nearest_neighbor(neighbors = tune(), weight_func = tune()) |> \n  set_mode(\"classification\")\n  \nthrsh_tlr <-\n  tailor() |>\n  adjust_probability_threshold(threshold = tune()) \n```\n:::\n\n\n\n## Setup the workflow  ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|7|8|17|\"}\nknn_wflow <- workflow(rec, knn_spec, thrsh_tlr)\n\nknn_param <-\n  knn_wflow |>\n  extract_parameter_set_dials() |>\n  update(\n    threshold = threshold(c(0.001, 0.1)),\n    neighbors = neighbors(c(1, 50))\n  )\n```\n:::\n\n\n\n## Tuning results  ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncls_mtr <- metric_set(brier_class, roc_auc, sensitivity, specificity)\nctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(12)\nknn_res <-\n  knn_wflow |>\n  tune_grid(\n    resamples = sim_rs,\n    grid = 50,\n    control = ctrl,\n    metrics = cls_mtr,\n    param_info = knn_param\n  )\n```\n:::\n\n\n\n## Grid results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(knn_res)\n```\n\n::: {.cell-output-display}\n![](advanced-07-feature-selection_files/figure-revealjs/sim-knn-all-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Brier results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(knn_res, metric = \"brier_class\") + \n  facet_grid(. ~ name, scale = \"free_x\") \n```\n\n::: {.cell-output-display}\n![](advanced-07-feature-selection_files/figure-revealjs/sim-knn-brier-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## ROC curve results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(knn_res, metric = \"roc_auc\") + \n  facet_grid(. ~ name, scale = \"free_x\") \n```\n\n::: {.cell-output-display}\n![](advanced-07-feature-selection_files/figure-revealjs/sim-knn-roc-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Sensitivity/Specificity results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(knn_res, metric = c(\"sensitivity\", \"specificity\"))\n```\n\n::: {.cell-output-display}\n![](advanced-07-feature-selection_files/figure-revealjs/sim-knn-two-class-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## Fit the model and get filter information  ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/broom.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1|5|\"}\nknn_fit <- fit_best(knn_res, metric = \"brier_class\")\nfilter_info <-\n\tknn_fit |>\n\textract_recipe() |>\n\ttidy(id = \"filter\")\n\nfilter_info\n#> # A tibble: 30 × 4\n#>    terms        removed      score id    \n#>    <chr>        <lgl>        <dbl> <chr> \n#>  1 predictor_01 TRUE     0.00152   filter\n#>  2 predictor_02 TRUE     0.00170   filter\n#>  3 predictor_03 TRUE    -0.0000460 filter\n#>  4 predictor_04 TRUE     0.000968  filter\n#>  5 predictor_05 TRUE    -0.0000626 filter\n#>  6 predictor_06 TRUE     0.000126  filter\n#>  7 predictor_07 TRUE     0.000779  filter\n#>  8 predictor_08 TRUE     0.00160   filter\n#>  9 predictor_09 TRUE     0.00218   filter\n#> 10 predictor_10 TRUE    -0.000302  filter\n#> # ℹ 20 more rows\n```\n:::\n\n\n\n## The truth about our data\n\n\n\n\n\n\n\nThe data were simulated and 15 out of 30 predictors were uninformative (and highly correlated). How did we do? \n\n. . .\n\n<br> \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|        | noise| real|\n|:-------|-----:|----:|\n|kept    |     0|    2|\n|removed |    15|   13|\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n- selection sensitivity: 13.3%\n- selection specificity: 100%\n\nIt was good at removing noise but not keeping the real predictors. \n:::\n\n::::\n\n## Random forest importance scores\n\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\n# A \"truth\" column was added\nfilter_info |>\n  mutate(\n    terms = factor(terms),\n    terms = reorder(terms, score)\n  ) |>\n  ggplot(\n    aes(x = score, \n        y = terms, \n        fill = truth)\n  ) +\n  geom_bar(stat = \"identity\") + \n  labs(x = \"RF Importance\", y = NULL) + \n  scale_fill_brewer(palette = \"Set2\")\n```\n\n::: {.cell-output-display}\n![](advanced-07-feature-selection_files/figure-revealjs/rf-imp-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## The simulation\n\nThe simulation system is documented [here](https://modeldata.tidymodels.org/reference/sim_classification.html#method-caret-) with `method = \"caret\"`. The two most important predictors being retained correspond to: \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\n# In logit units: \n- 4 * two_factor_1 + 4 * two_factor_2 + 2 * two_factor_1 * two_factor_2 \n```\n:::\n\n\n\n<br>\n\nMost of the others are small linear effects and tree-based models are not great at modeling those. \n\n<br>\n\nAlso, the noise predictors were simulated to have fairly high correlations with one another. That can often [compromise random forest importance scores](https://bookdown.org/max/FES/recursive-feature-elimination.html#fig:greedy-rf-imp). \n\n\n## Other steps\n\nThe important package has two other feature selection steps that can be used with multiple scores: \n\n. . .\n\n - [`step_predictor_retain()`](https://important.tidymodels.org/reference/step_predictor_retain.html): choose predictors based on a logical statement. Example: \n \n```r\nimp_rf > 2 & cor_pearson >= 0.75\n```\n\n. . .\n\n - [`step_predictor_desirability()`](https://important.tidymodels.org/reference/step_predictor_desirability.html): choose multiple scores to compute then use desirability functions to rank them: \n \n```r\ndesirability(\n  maximize(correlation),\n  maximize(imp_rf)\n)\n```    \n\n# Proceed to the test set {background-color=\"#417839FF\"}\n\n## Manual approach  ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/broom.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\nWe already have our fitted model and, if we are happy with it:  \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1|2|6-9|\"}\ntest_pred <- augment(knn_fit, sim_test)\ntest_pred |> cls_mtr(class, estimate = .pred_class, .pred_event)\n#> # A tibble: 4 × 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 sensitivity binary        0.982 \n#> 2 specificity binary        0.874 \n#> 3 brier_class binary        0.0246\n#> 4 roc_auc     binary        0.981\n```\n:::\n\n\n\n. . .\n\nResampling estimates:\n\n\n\n::: {.cell}\n\n```\n#> # A tibble: 4 × 4\n#>   .metric       mean     n std_err\n#>   <chr>        <dbl> <int>   <dbl>\n#> 1 sensitivity 0.958     10 0.0154 \n#> 2 specificity 0.867     10 0.00948\n#> 3 brier_class 0.0349    10 0.00201\n#> 4 roc_auc     0.968     10 0.00714\n```\n:::\n\n\n\n## Checking (Approximate) Calibration ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/probably.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_pred|>\n  cal_plot_windowed(\n    truth = class,\n    estimate = .pred_event,\n    window_size = 0.2,\n    step_size = 0.025,\n  )\n```\n:::\n\n\n\n<br>\n\nLooks alright. The small _effective_ sample size (57 events) makes it pretty noisy.\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](advanced-07-feature-selection_files/figure-revealjs/knn-cal-plot-manual-1.svg){width=90%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n\n## Automated approach   ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nSimilar to `fit_best()`, there is a convenience function that can be used to get the final model and the test set results.\n\n<br>\n\nWe have to start with a finalized workflow (i.e., no `tune()` values):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_best <- select_best(knn_res, metric = \"brier_class\")\nknn_last_wflow <- finalize_workflow(knn_wflow, knn_best)\n```\n:::\n\n\n\n## Automated approach\n\n`last_fit()` uses the original split object to fit, predict, and measure the model using the test set:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2-3|\"}\nknn_test_res <- \n  knn_last_wflow |> \n  last_fit(sim_split, metrics = cls_mtr)\n  \nknn_test_res\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 × 6\n#>   splits             id               .metrics .notes   .predictions .workflow \n#>   <list>             <chr>            <list>   <list>   <list>       <list>    \n#> 1 <split [1499/501]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n\n\n\n## Automated approach\n\nWe can pick out the parts that we want: \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1-3|\"}\nknn_final_fit <- knn_test_res |> extract_workflow()\nknn_test_pred <- knn_test_res |> collect_predictions()\nknn_test_mtr  <- knn_test_res |> collect_metrics()\n\nknn_test_mtr\n#> # A tibble: 4 × 4\n#>   .metric     .estimator .estimate .config        \n#>   <chr>       <chr>          <dbl> <chr>          \n#> 1 sensitivity binary        0.982  pre0_mod0_post0\n#> 2 specificity binary        0.874  pre0_mod0_post0\n#> 3 brier_class binary        0.0246 pre0_mod0_post0\n#> 4 roc_auc     binary        0.981  pre0_mod0_post0\n```\n:::\n\n\n\nEasy peasy!\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}