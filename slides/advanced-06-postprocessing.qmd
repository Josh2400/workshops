---
title: "6 - Postprocessing"
subtitle: "Getting More Out of Feature Engineering and Tuning for Machine Learning"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

```{r setup}
#| include: false
#| file: setup.R
```


```{r}
#| label: start-quiet
#| include: false
load("lgbm_times.RData")
```

## Startup!  `r hexes(c("probably", "tailor", "tidymodels"))`

```{r}
#| label: user-startup
library(tidymodels)
library(desirability2)
library(probably)
library(mirai)

# check torch:
if (torch::torch_is_installed()) {
  library(torch)
}

tidymodels_prefer()
theme_set(theme_bw())
options(pillar.advice = FALSE, pillar.min_title_chars = Inf)
daemons(parallel::detectCores())
```

## More startup!   `r hexes(c("rsample"))`

```{r}
#| label: user-startup-2
# Load our example data for this section
"https://raw.githubusercontent.com/tidymodels/" |> 
  paste0("workshops/main/slides/class_data.RData") |> 
  url() |> 
  load()

set.seed(429)
sim_split <- initial_split(class_data, prop = 0.75, strata = class)
sim_train <- training(sim_split)
sim_test  <- testing(sim_split)

set.seed(523)
sim_rs <- vfold_cv(sim_train, v = 10, strata = class)
```

## Our neural network model `r hexes(c("parsnip", "recipes", "dials", "brulee"))`

```{r}
#| label: model-startup
rec <- 
  recipe(class ~ ., data = sim_train) |> 
  step_normalize(all_numeric_predictors())

nnet_spec <- 
  mlp(hidden_units = tune(), penalty = tune(), learn_rate = tune(), 
      epochs = 100, activation = tune()) |> 
  # Remove the class_weights argument
  set_engine("brulee", stop_iter = 10) |> 
  set_mode("classification")
  
nnet_wflow <- workflow(rec, nnet_spec)

nnet_param <- 
  nnet_wflow |> 
  extract_parameter_set_dials()   
```  

# What is postprocessing?  {background-color="#28A8B8FF"}

## Adjusting model predictions

How can we modify our predictions? Some examples: 

 - Fixing calibration issues<sup>*</sup>.
 - Limit the range of predictions.
 - Alternative cutoffs for binary data.
 - Declining to predict.

. . .

<sup>*</sup> Requires further estimation (and data).

<br>

Let's first consider the easiest case: alternative cutoffs.

## Alternative thresholds

Instead of up-weighting the samples in the minority (via `class_weights`), we can try to fit the best model and then define what it means to be an "event." 

<br> 

Instead of using a 50% threshold, we might lower the level of evidence needed to call a prediction an event. 

<br> 

How do we tune the threshold? 

## Tailors `r hexes(c("tailor"))`

The tailor package is similar to recipes but specifies how to adjust predictions. 

A simple example: 

```{r}
#| label: simple-tailor
#| code-line-numbers: "|2|3|"

thrsh_tlr <-
  tailor() |>
  adjust_probability_threshold(threshold = 1 / 3)

thrsh_tlr
```

 - Like a recipe, this initial call doesn't do anything but declare intent. 

 - Unlike a recipe, it does not need the data (i.e., predictions) at this point. 
   - Relevant prediction columns are selected when `fit()` is used (next slide). 

## Manual use of a tailor  `r hexes(c("tailor"))`

There is a `fit()` method that requires data and the names of the prediction columns: 

```{r}
#| label: simple-tailor-fit
#| code-line-numbers: "|5|14|15|16-17|"
three_rows <- 
  tribble(
     ~ class, ~ .pred_class, ~.pred_event, ~.pred_nonevent,
     "event",       "event",          0.6,             0.4,
     "event",    "nonevent",          0.4,             0.6, 
  "nonevent",    "nonevent",          0.1,             0.9  
  ) |> 
  mutate(across(where(is.character), factor))

thrsh_fit <-
  thrsh_tlr |>
  fit(
    three_rows,
    outcome = class,
    estimate = .pred_class,
    .pred_event:.pred_nonevent  # No argument name and order matches factor levels
  )
```

## Manual use of a tailor  `r hexes(c("tailor"))`

`predict()` applies the adjustments: 

```{r}
#| label: simple-tailor-predict
#| code-line-numbers: "|3|8|"
thrsh_fit

predict(thrsh_fit, three_rows)
```

<br> 

## tailors within workflows  `r hexes(c("tailor", "workflows"))`

In practice, we would add the tailor to a workflow to make it easier to use: 

<br>

```{r}
#| label: simple-tailor-wflow
nnet_wflow <- workflow(rec, nnet_spec, thrsh_tlr)
```

<br>

- We don't have to set the names of the outcome or prediction columns (yet). 
- `fit()` and `predict()` happen automatically. 


## Current adjustments

- [`adjust_equivocal_zone()`](https://tailor.tidymodels.org/reference/adjust_equivocal_zone.html): decline to predict.
 - [`adjust_numeric_calibration()`](https://tailor.tidymodels.org/reference/adjust_numeric_calibration.html): try to readjust predictions to be consistent with numeric predictions.
 - [`adjust_numeric_range()`](https://tailor.tidymodels.org/reference/adjust_numeric_range.html): restrict the range of predictions.
 - [`adjust_predictions_custom()`](https://tailor.tidymodels.org/reference/adjust_predictions_custom.html): similar to `dplyr::mutate()`.
 - [`adjust_probability_calibration()`](https://tailor.tidymodels.org/reference/adjust_probability_calibration.html): try to readjust predictions to be consistent with probability predictions.
 - [`adjust_probability_threshold()`](https://tailor.tidymodels.org/reference/adjust_probability_threshold.html): custom rule for hard-class predictions from probabilities. 

## Some notes

- Adjustment order matters; tailor will error early if the ordering rules are violated. 

- Adjustments that change class probabilities also affect hard class predictions. 

- Adjustments happen before performance estimation. 
   - Undoing something like a log transformation is a bad idea here. 
   
- We have more calibration methods in mind. 

## Your turn {transition="slide-in"}

- Discuss with those around you what the "ordering rules" could be.

<br>

```{r ordering-rules}
#| echo: false
countdown::countdown(minutes = 3, id = "ordering-rules")
```


## More Notes

- When estimation is required, the data considerations become more complex. 

- Most arguments can be tuned. 

- For grid search, we use a conditional execution algorithm that avoids redundant retraining of the preprocessor or model. 

# Back to our neural network  {background-color="#90D0C8FF"}

## Tuning the probability threshold   `r hexes(c("tailor", "tune", "dials", "workflows"))`

```{r}
#| label: tune-threshold
#| code-line-numbers: "|3|5|10|"
thrsh_tlr <-
  tailor() |>
  adjust_probability_threshold(threshold = tune())

nnet_thrsh_wflow <- workflow(rec, nnet_spec, thrsh_tlr)
  
nnet_thrsh_param <- 
  nnet_thrsh_wflow |> 
  extract_parameter_set_dials() |> 
  update(threshold = threshold(c(0.001, 0.5)))
```

## Tuning the probability threshold    `r hexes(c("tune", "yardstick"))`

Nearly the same code as before: 

<br>

```{r}
#| label: tune-grid-threshold
#| dependson: "tune-threshold"
#| cache: true
ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)
cls_mtr <- metric_set(brier_class, roc_auc, sensitivity, specificity)

set.seed(12)
nnet_thrsh_res <-
  nnet_thrsh_wflow |>
  tune_grid(
    resamples = sim_rs,
    grid = 25,
    param_info = nnet_thrsh_param, 
    control = ctrl,
    metrics = cls_mtr
  )
```


## Grid results `r hexes(c("tune"))`

```{r}
#| label: sim-nnet-threshold
#| fig-width: 12
#| fig-height: 5
#| fig-align: center
#| out-width: "100%"
#| dev-args:
#|   bg: "transparent"
autoplot(nnet_thrsh_res)
```


## Grid results

 - `tanh` activation is doing much better. 
 - `threshold` should not (and does not) affect the Brier or ROC metrics. 
 - We can achieve low Brier scores.
 - We could run another grid with values <2% for a better threshold estimate.

## Multimetric optimization `r hexes(c("tune"))`


```{r}
#| label: nnet-metrics-desire-sens
#| code-line-numbers: "|1|3|4|5|11|"
nnet_thrsh_res |>
  show_best_desirability(
    maximize(sensitivity),
    minimize(brier_class),
    constrain(specificity, low = 0.8, high = 1.0)
  ) |>
  relocate(threshold, sensitivity, specificity, brier_class, .d_overall)
```

## Calibration`r hexes(c("tune", "probably"))`

```{r}
#| label: nnet-cal-plot-sens
#| output-location: column
#| out-width: 90%
#| fig-width: 5
#| fig-height: 5
more_sens <-
  nnet_thrsh_res |>
  select_best_desirability(
    maximize(sensitivity),
    minimize(brier_class),
    constrain(specificity, low = 0.8, high = 1.0)
  )

nnet_thrsh_res |>
  collect_predictions(
    parameters = more_sens
  ) |>
  cal_plot_windowed(
    truth = class,
    estimate = .pred_event,
    window_size = 0.2,
    step_size = 0.025,
  )
```

## Thoughts about these results

The calibration issue in the previous plot shows that some very likely non-events will have underestimated probabilities.

- That may not matter if we are very focused on events. 
- Thresholding does not affect calibration. 
- We might be able to:
    - Further tune the neural network to solve the issue and/or 
    - Add a calibration postprocessor 

## Thoughts about the approach

Let's say that we pick a threshold of 2%. Our explanation to the user/stakeholder would be 
   
   > "As long as the model is at least 2% sure it is an event, we will call it an event". 
   
It may be challenging to convince someone that this is the best option. 

<br> 

That said, this is probably a better approach than cost-sensitive learning. 

# Fitting a postprocessor {background-color="#B0D0B8FF"}

## Data to train the adjustments

If an adjustment requires data, where do we get it from?

::: incremental
- Fitting a calibration model to the training set re-predictions would be bad. 

- Also, we don't want to touch the validation or test sets. 
:::

. . .

<br>

We need another data set.

## Data sources

Two possibilities: 

1. Shave some data off the training set to create a _calibration set_. 
	- During resampling, we can do the same to the analysis set.
	- The "shaving" process emulates the original sampling method. 
	- There are fewer data points for training the preprocessor and primary model. 
	
2. Use a static calibration set outside our training/validation/testing splits. 

Currently, we have implemented the first method. 

## Example: 3-fold CV

![](images/3CV-split.svg){fig-align='center'}

## Example: 3-fold CV _inner split_


![](images/3CV-inner-split.svg){fig-align='center'}


## Breakdown for the class imbalance data

<br>

```{r}
#| label: data-shaves
#| echo: false
#| results: markup
xtab_all <- class_data |>
  count(class) |>
  mutate(Data = "Original", Strategy = "All")

xtab_train <- sim_split |>
  training() |> 
  count(class) |>
  mutate(Data = "Training", Strategy = "No Calibration")

inner_split_initial <-
  sim_split|>
  rsample:::internal_calibration_split(rsample::.get_split_args(sim_split))

xtab_train_cal <- 
  inner_split_initial |>
  training() |> 
  count(class) |>
  mutate(Data = "Training", Strategy = "Calibration")

xtab_model <- sim_rs$splits[[1]] |>
  analysis() |>
  count(class) |>
  mutate(Data = "Analysis", Strategy = "No Calibration")

inner_split_rs <-
  sim_rs$splits[[1]] |>
  rsample:::internal_calibration_split(rsample::.get_split_args(sim_rs))

xtab_model_cal <-
  inner_split_rs |>
  analysis() |>
  count(class) |>
  mutate(Data = "Analysis", Strategy = "Calibration")

xtab_cal_cal <-
  inner_split_rs |>
  calibration() |>
  count(class) |>
  mutate(Data = "Calibration", Strategy = "Calibration")

xtab_counts <- 
  bind_rows(xtab_all, xtab_train, xtab_model, xtab_train_cal, xtab_model_cal, xtab_cal_cal) |> 
  pivot_wider(id_cols = c(Data, Strategy), names_from = class, values_from = n)

knitr::kable(xtab_counts)  
```  

## Calibration

Calibration models, in essence, try to predict the true class using the model predictions. _Symbolically_: 

- For regression models: `outcome ~ .pred`
- For binary classifiers: `class ~ .pred_class_1`
- For multiclass: `class ~ .pred_class_1 + .pred_class_2 + ...`

Each calibration method works slightly differently. 

For example, in regression, a (generalized) linear model is fit, and the residuals are added to new predictions. 

## Calibration expectations

Keep expectations low. For these methods to work: 

 - The systematic issue will need to be large, or at least not subtle.
 - A large calibration set is needed to work effectively. 
 
The [example in ALM4TD](https://aml4td.org/chapters/cls-metrics.html#sec-cls-calibration) is an illustrative example and details. 

In many cases, trying a different model or tuning parameters would be better. 

<br>

You can tune the calibration method, one of which is _no calibration_.
 

## Your turn {transition="slide-in"}

- Based on previous results, choose and fix a specific activation type (i.e., no `tune()`).
- Add a calibrator to your tailor with a `method = tune()` value. 
- Run another grid search

Does it help with this data set?

<br>

```{r ex-calibration}
#| echo: false
countdown::countdown(minutes = 10, id = "class-data")
```

