[
  {
    "objectID": "archive/2025-08-nyr/index.html",
    "href": "archive/2025-08-nyr/index.html",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels offered at the 2025 New York Data Science & AI Conference. The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. This website hosts the materials for both the Introduction to Machine Learning in R with tidymodels and Getting More Out of Feature Engineering and Tuning for Machine Learning courses.\nIntroduction to Machine Learning in R with tidymodels will teach you core tidymodels packages and their uses: data splitting/resampling with rsample, model fitting with parsnip, measuring model performance with yardstick, and model optimization using the tune package. Time permitting, youâ€™ll be introduced to basic pre-processing with recipes. Youâ€™ll learn tidymodels syntax as well as the process of predictive modeling for tabular data.\nGetting More Out of Feature Engineering and Tuning for Machine Learning will teach you about model optimization using the tune and finetune packages, including racing and iterative methods. Youâ€™ll be able to do more sophisticated feature engineering with recipes. Time permitting, model ensembles via stacking will be introduced. This course is focused on the analysis of tabular data and does not include deep learning methods."
  },
  {
    "objectID": "archive/2025-08-nyr/index.html#welcome",
    "href": "archive/2025-08-nyr/index.html#welcome",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels offered at the 2025 New York Data Science & AI Conference. The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. This website hosts the materials for both the Introduction to Machine Learning in R with tidymodels and Getting More Out of Feature Engineering and Tuning for Machine Learning courses.\nIntroduction to Machine Learning in R with tidymodels will teach you core tidymodels packages and their uses: data splitting/resampling with rsample, model fitting with parsnip, measuring model performance with yardstick, and model optimization using the tune package. Time permitting, youâ€™ll be introduced to basic pre-processing with recipes. Youâ€™ll learn tidymodels syntax as well as the process of predictive modeling for tabular data.\nGetting More Out of Feature Engineering and Tuning for Machine Learning will teach you about model optimization using the tune and finetune packages, including racing and iterative methods. Youâ€™ll be able to do more sophisticated feature engineering with recipes. Time permitting, model ensembles via stacking will be introduced. This course is focused on the analysis of tabular data and does not include deep learning methods."
  },
  {
    "objectID": "archive/2025-08-nyr/index.html#is-this-workshop-for-me",
    "href": "archive/2025-08-nyr/index.html#is-this-workshop-for-me",
    "title": "Machine learning with tidymodels",
    "section": "Is this workshop for me? ",
    "text": "Is this workshop for me? \nDepending on your background, one of Introduction to Machine Learning in R with tidymodels or Getting More Out of Feature Engineering and Tuning for Machine Learning might serve you better than the other.\n\nIntroduction to Machine Learning in R with tidymodels\nThis workshop is for you if you:\n\nare comfortable using tidyverse packages to read data into R, transform and reshape data, and make a variety of graphs, and\nhave had some exposure to basic statistical concepts such as linear models, residuals, etc.\n\nIntermediate or expert familiarity with modeling or machine learning is not required. Interested students who have intermediate or expert familiarity with modeling or machine learning may be interested in the Getting More Out of Feature Engineering and Tuning for Machine Learning workshop.\n\n\nGetting More Out of Feature Engineering and Tuning for Machine Learning\nThis workshop is for you if you:\n\nhave the prerequisite skills listed for the Introduction to Machine Learning in R with tidymodels workshops,\nhave used tidymodels packages like recipes, rsample, and parsnip, and\nhave some experience with evaluating statistical models using resampling techniques like v-fold cross-validation or the bootstrap.\n\nParticipants who are new to tidymodels or machine learning will benefit from taking the Introduction to Machine Learning in R with tidymodels workshop before joining this one. Participants who have completed the â€œIntroduction to Machine Learning in R with tidymodelsâ€ workshop previously will be well-prepared for this course."
  },
  {
    "objectID": "archive/2025-08-nyr/index.html#preparation",
    "href": "archive/2025-08-nyr/index.html#preparation",
    "title": "Machine learning with tidymodels",
    "section": "Preparation",
    "text": "Preparation\nThe process to set up your computer for either workshop will look the same. Please join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2024.04.0), available at https://posit.co/download/rstudio-desktop/\nFor all of the slides, the following R packages can be installed from the R console:\n\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"bonsai\", \"Cubist\", \"doParallel\", \"earth\", \"embed\", \"finetune\",\n    \"forested\", \"lightgbm\", \"lme4\", \"pak\", \"parallelly\", \"plumber\", \n    \"probably\", \"ranger\", \"rpart\", \"rpart.plot\", \"rules\", \"splines2\", \n    \"stacks\", \"text2vec\", \"textrecipes\", \"tidymodels\", \"vetiver\")\n\ninstall.packages(pkgs)\n\nIf youâ€™re a Windows user and encounter an error message during installation noting a missing Rtools installation, install Rtools using the installer linked here."
  },
  {
    "objectID": "archive/2025-08-nyr/index.html#slides",
    "href": "archive/2025-08-nyr/index.html#slides",
    "title": "Machine learning with tidymodels",
    "section": "Slides",
    "text": "Slides\nThese slides are designed to use with live teaching and are published for workshop participantsâ€™ convenience. They are not meant as standalone learning materials. For that, we recommend tidymodels.org and Tidy Modeling with R.\n\nIntroduction to Machine Learning in R with tidymodels\n\n\nGetting More Out of Feature Engineering and Tuning for Machine Learning\n\n\nExtra content (time permitting)"
  },
  {
    "objectID": "archive/2025-08-nyr/index.html#code",
    "href": "archive/2025-08-nyr/index.html#code",
    "title": "Machine learning with tidymodels",
    "section": "Code",
    "text": "Code\nQuarto files for working along are available on GitHub. (Donâ€™t worry if you havenâ€™t used Quarto before; it will feel familiar to R Markdown users.)"
  },
  {
    "objectID": "archive/2025-08-nyr/index.html#past-workshops",
    "href": "archive/2025-08-nyr/index.html#past-workshops",
    "title": "Machine learning with tidymodels",
    "section": "Past workshops",
    "text": "Past workshops\n\nEnglish\n\nAugust 2024 at posit::conf()\nSeptember 2023 at posit::conf()\nJuly 2023 at the New York R Conference\nAugust 2022 in Reykjavik\nJuly 2022 at rstudio::conf()\n\n\n\nSpanish\n\nMarch 2024 at conectaR"
  },
  {
    "objectID": "archive/2025-08-nyr/index.html#acknowledgments",
    "href": "archive/2025-08-nyr/index.html#acknowledgments",
    "title": "Machine learning with tidymodels",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including the slides, is made with Quarto. Please submit an issue on the GitHub repo for this workshop if you find something that could be fixed or improved."
  },
  {
    "objectID": "archive/2025-08-nyr/index.html#reuse-and-licensing",
    "href": "archive/2025-08-nyr/index.html#reuse-and-licensing",
    "title": "Machine learning with tidymodels",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e.Â not an original creation and reused from another source), these educational materials are licensed under Creative Commons Attribution CC BY-SA 4.0."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#looking-at-the-predictors",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#looking-at-the-predictors",
    "title": "Extras - Recipes",
    "section": "Looking at the predictors",
    "text": "Looking at the predictors\n\nforested_train\n#&gt; # A tibble: 5,685 Ã— 19\n#&gt;    forested  year elevation eastness northness roughness tree_no_tree dew_temp precip_annual temp_annual_mean temp_annual_min temp_annual_max temp_january_min vapor_min vapor_max canopy_cover   lon\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 No        2016       464       -5       -99         7 No tree          1.71           282             9.76           -4.44           16.6              2.96       191      1534            4 -121.\n#&gt;  2 Yes       2016       166       92        37         7 Tree             6             1298            10.2             0.72           14.3              6.12        60       747           33 -122.\n#&gt;  3 No        2016       644      -85       -52        24 No tree          0.67           288             8.77           -6.32           14.6              2.98       219      1396            0 -120.\n#&gt;  4 Yes       2014      1285        4        99        79 Tree             1.91          1621             5.61           -2.48            9.48             1.73        88       545           74 -123.\n#&gt;  5 Yes       2013       822       87        48        68 Tree             1.95          2200             8.62           -0.68           12.9              4.35       147       861           48 -121.\n#&gt;  6 Yes       2017         3        6       -99         5 Tree             7.93          2211            10.6             3.77           14.2              7.02        34       578           79 -124.\n#&gt;  7 Yes       2014      2041      -95        28        49 Tree            -4.22          1551             0.75           -9.47            5.17            -3.66        73       481           48 -120.\n#&gt;  8 Yes       2015      1009       -8        99        72 Tree             1.72          2396             6.59           -2.98           11.3              1.88        92       781           76 -122.\n#&gt;  9 No        2017       436      -98        19        10 No tree          1.8            234             9.8            -4.23           16.3              3.32       178      1527            0 -119.\n#&gt; 10 No        2018       775       63        76       103 No tree          0.62           432             8.51           -5.5            13.7              3.32       241      1237            7 -120.\n#&gt; # â„¹ 5,675 more rows\n#&gt; # â„¹ 2 more variables: lat &lt;dbl&gt;, land_type &lt;fct&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#working-with-other-models",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#working-with-other-models",
    "title": "Extras - Recipes",
    "section": "Working with other models",
    "text": "Working with other models\nSome models canâ€™t handle non-numeric data\n\nLinear Regression\nK Nearest Neighbors\n\n\n\nSome models struggle if numeric predictors arenâ€™t scaled\n\nK Nearest Neighbors\nAnything using gradient descent"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#types-of-needed-preprocessing",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#types-of-needed-preprocessing",
    "title": "Extras - Recipes",
    "section": "Types of needed preprocessing",
    "text": "Types of needed preprocessing\n\nDo qualitative predictors require a numeric encoding?\nShould columns with a single unique value be removed?\nDoes the model struggle with missing data?\nDoes the model struggle with correlated predictors?\nShould predictors be centered and scaled?\nIs it helpful to transform predictors to be more symmetric?\n\n\nhttps://www.tmwr.org/pre-proc-table.html"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#two-types-of-preprocessing",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#two-types-of-preprocessing",
    "title": "Extras - Recipes",
    "section": "Two types of preprocessing",
    "text": "Two types of preprocessing"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#two-types-of-preprocessing-1",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#two-types-of-preprocessing-1",
    "title": "Extras - Recipes",
    "section": "Two types of preprocessing",
    "text": "Two types of preprocessing"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#general-definitions",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#general-definitions",
    "title": "Extras - Recipes",
    "section": "General definitions",
    "text": "General definitions\n\nData preprocessing is what you do to make your model successful.\nFeature engineering is what you do to the original predictors to make the model do the least work to perform great."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#working-with-dates",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#working-with-dates",
    "title": "Extras - Recipes",
    "section": "Working with dates",
    "text": "Working with dates\nDatetime variables are automatically converted to an integer if given as a raw predictor. To avoid this, it can be re-encoded as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nLeap year\nIndicators for holidays"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#two-types-of-transformations",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#two-types-of-transformations",
    "title": "Extras - Recipes",
    "section": "Two types of transformations",
    "text": "Two types of transformations\n\n\n\nStatic\n\nSquare root, log, inverse\nDummies for known levels\nDate time extractions\n\n\nTrained\n\nCentering & scaling\nImputation\nPCA\nAnything for unknown factor levels\n\n\n\nTrained methods need to calculate sufficient information to be applied again."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#the-recipes-package",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#the-recipes-package",
    "title": "Extras - Recipes",
    "section": "The recipes package",
    "text": "The recipes package\n\n\nModular + extensible\nWorks well with pipes ,|&gt; and %&gt;%\nDeferred evaluation\nIsolates test data from training data\nCan do things formulas canâ€™t"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) %&gt;%\nÂ Â step_dummy(all_nominal_predictors()) %&gt;%\nÂ Â step_zv(all_predictors()) %&gt;%\nÂ Â step_log(canopy_cover, offset = 0.5) %&gt;%\nÂ Â step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe-1",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe-1",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) %&gt;%\nÂ Â step_dummy(all_nominal_predictors()) %&gt;%\nÂ Â step_zv(all_predictors()) %&gt;%\nÂ Â step_log(canopy_cover, offset = 0.5) %&gt;%\nÂ Â step_normalize(all_numeric_predictors())\n\n\nStart by calling recipe() to denote the data source and variables used."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe-2",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe-2",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) %&gt;%\nÂ Â step_dummy(all_nominal_predictors()) %&gt;%\nÂ Â step_zv(all_predictors()) %&gt;%\nÂ Â step_log(canopy_cover, offset = 0.5) %&gt;%\nÂ Â step_normalize(all_numeric_predictors())\n\n\nSpecify what actions to take by adding step_*()s."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe-3",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe-3",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) %&gt;%\nÂ Â step_dummy(all_nominal_predictors()) %&gt;%\nÂ Â step_zv(all_predictors()) %&gt;%\nÂ Â step_log(canopy_cover, offset = 0.5) %&gt;% Â Â step_normalize(all_numeric_predictors())\n\n\nUse {tidyselect} and recipes-specific selectors to denote affected variables."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#using-a-recipe",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#using-a-recipe",
    "title": "Extras - Recipes",
    "section": "Using a recipe",
    "text": "Using a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) %&gt;%\nÂ Â step_dummy(all_nominal_predictors()) %&gt;%\nÂ Â step_zv(all_predictors()) %&gt;%\nÂ Â step_log(canopy_cover, offset = 0.5) %&gt;% Â Â step_normalize(all_numeric_predictors())\n\n\nSave the recipe we like so that we can use it in various places, e.g., with different models."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#using-a-recipe-with-workflows",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#using-a-recipe-with-workflows",
    "title": "Extras - Recipes",
    "section": "Using a recipe with workflows",
    "text": "Using a recipe with workflows\nRecipes are typically combined with a model in a workflow() object:\n\n\nforested_wflow &lt;- workflow() %&gt;%\nÂ Â add_recipe(forested_rec) %&gt;%\nÂ Â add_model(linear_reg())"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#recipes-are-estimated",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#recipes-are-estimated",
    "title": "Extras - Recipes",
    "section": "Recipes are estimated",
    "text": "Recipes are estimated\nEvery preprocessing step in a recipe that involved calculations uses the training set. For example:\n\nLevels of a factor\nDetermination of zero-variance\nNormalization\nFeature extraction\n\nOnce a recipe is added to a workflow, this occurs when fit() is called."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#debugging-a-recipe",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#debugging-a-recipe",
    "title": "Extras - Recipes",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g.Â forested_rec) can be estimated manually with a function called prep(). It is analogous to fit(). See TMwR section 16.4.\n\n\n\n\nAnother function, bake(), is analogous to predict(), and gives you the processed data back."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#your-turn",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#your-turn",
    "title": "Extras - Recipes",
    "section": "Your turn",
    "text": "Your turn\n\n\nTake the recipe and prep() then bake() it to see what the resulting data set looks like.\nTry removing steps to see how the result changes.\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#printing-a-recipe",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#printing-a-recipe",
    "title": "Extras - Recipes",
    "section": "Printing a recipe",
    "text": "Printing a recipe\n\nforested_rec\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 18\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Dummy variables from: all_nominal_predictors()\n#&gt; â€¢ Zero variance filter on: all_predictors()\n#&gt; â€¢ Log transformation on: canopy_cover\n#&gt; â€¢ Centering and scaling for: all_numeric_predictors()"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#prepping-a-recipe",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#prepping-a-recipe",
    "title": "Extras - Recipes",
    "section": "Prepping a recipe",
    "text": "Prepping a recipe\n\nprep(forested_rec)\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 18\n#&gt; \n#&gt; â”€â”€ Training information\n#&gt; Training data contained 5685 data points and no incomplete rows.\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Dummy variables from: tree_no_tree and land_type | Trained\n#&gt; â€¢ Zero variance filter removed: &lt;none&gt; | Trained\n#&gt; â€¢ Log transformation on: canopy_cover | Trained\n#&gt; â€¢ Centering and scaling for: year and elevation, ... | Trained"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#baking-a-recipe",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#baking-a-recipe",
    "title": "Extras - Recipes",
    "section": "Baking a recipe",
    "text": "Baking a recipe\n\nprep(forested_rec) %&gt;%\n  bake(new_data = forested_train)\n#&gt; # A tibble: 5,685 Ã— 20\n#&gt;      year elevation eastness northness roughness dew_temp precip_annual temp_annual_mean temp_annual_min temp_annual_max temp_january_min vapor_min vapor_max canopy_cover     lon    lat forested\n#&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   \n#&gt;  1  0.206   -0.450   -0.0203    -1.38    -0.874   -0.169         -0.864          0.532           -0.403           0.959           -0.0702     0.755     1.21        -0.566 -0.175  -0.988 No      \n#&gt;  2  0.206   -1.07     1.38       0.563   -0.874    1.33           0.132          0.726            1.24            0.159            1.34      -1.00     -1.00         0.494 -0.856   1.10  Yes     \n#&gt;  3  0.206   -0.0762  -1.17      -0.711   -0.506   -0.533         -0.858          0.116           -1.00            0.248           -0.0613     1.13      0.820       -1.73   0.270   0.175 No      \n#&gt;  4 -0.413    1.26     0.109      1.45     0.683   -0.0987         0.448         -1.21             0.221          -1.56            -0.621     -0.625    -1.57         0.917 -1.52    0.654 Yes     \n#&gt;  5 -0.723    0.294    1.31       0.721    0.445   -0.0847         1.02           0.0529           0.795          -0.346            0.552      0.166    -0.683        0.690 -0.419   1.32  Yes     \n#&gt;  6  0.516   -1.41     0.138     -1.38    -0.917    2.01           1.03           0.894            2.21            0.127            1.75      -1.35     -1.48         0.951 -1.84   -0.338 Yes     \n#&gt;  7 -0.413    2.83    -1.32       0.435    0.0343  -2.25           0.380         -3.26            -2.01           -3.09            -3.03      -0.826    -1.75         0.690 -0.0159  1.65  Yes     \n#&gt;  8 -0.103    0.682   -0.0636     1.45     0.532   -0.165          1.21          -0.801            0.0620         -0.915           -0.554     -0.571    -0.908        0.931 -0.636  -1.33  Yes     \n#&gt;  9  0.516   -0.508   -1.36       0.306   -0.809   -0.137         -0.911          0.549           -0.336           0.856            0.0909     0.581     1.19        -1.73   0.760  -0.209 No      \n#&gt; 10  0.826    0.196    0.960      1.12     1.20    -0.550         -0.717          0.00667         -0.741          -0.0616           0.0909     1.43      0.373       -0.296  0.155  -0.204 No      \n#&gt; # â„¹ 5,675 more rows\n#&gt; # â„¹ 3 more variables: tree_no_tree_No.tree &lt;dbl&gt;, land_type_Non.tree.vegetation &lt;dbl&gt;, land_type_Tree &lt;dbl&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#tidying-a-recipe",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#tidying-a-recipe",
    "title": "Extras - Recipes",
    "section": "Tidying a recipe",
    "text": "Tidying a recipe\nOnce a recipe as been estimated, there are various bits of information saved in it.\n\nThe tidy() function can be used to get specific results from the recipe."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#your-turn-1",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#your-turn-1",
    "title": "Extras - Recipes",
    "section": "Your turn",
    "text": "Your turn\n\nTake a prepped recipe and use the tidy() function on it.\nUse the number argument to inspect different steps.\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#tidying-a-recipe-1",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#tidying-a-recipe-1",
    "title": "Extras - Recipes",
    "section": "Tidying a recipe",
    "text": "Tidying a recipe\n\nprep(forested_rec) %&gt;%\n  tidy()\n#&gt; # A tibble: 4 Ã— 6\n#&gt;   number operation type      trained skip  id             \n#&gt;    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n#&gt; 1      1 step      dummy     TRUE    FALSE dummy_jlmcG    \n#&gt; 2      2 step      zv        TRUE    FALSE zv_mYCvS       \n#&gt; 3      3 step      log       TRUE    FALSE log_eme6b      \n#&gt; 4      4 step      normalize TRUE    FALSE normalize_ScVef"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#tidying-a-recipe-2",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#tidying-a-recipe-2",
    "title": "Extras - Recipes",
    "section": "Tidying a recipe",
    "text": "Tidying a recipe\n\nprep(forested_rec) %&gt;%\n  tidy(number = 1)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   terms        columns             id         \n#&gt;   &lt;chr&gt;        &lt;chr&gt;               &lt;chr&gt;      \n#&gt; 1 tree_no_tree No tree             dummy_jlmcG\n#&gt; 2 land_type    Non-tree vegetation dummy_jlmcG\n#&gt; 3 land_type    Tree                dummy_jlmcG"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#using-a-recipe-in-tidymodels",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#using-a-recipe-in-tidymodels",
    "title": "Extras - Recipes",
    "section": "Using a recipe in tidymodels",
    "text": "Using a recipe in tidymodels\nThe recommended way to use a recipe in tidymodels is to use it as part of a workflow().\n\nforested_wflow &lt;- workflow() %&gt;%  \n  add_recipe(forested_rec) %&gt;%  \n  add_model(linear_reg())\n\nWhen used in this way, you donâ€™t need to worry about prep() and bake() as it is handled for you."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-recipes.html#more-information",
    "href": "archive/2024-08-posit-conf/intro-extra-recipes.html#more-information",
    "title": "Extras - Recipes",
    "section": "More information",
    "text": "More information\n\nhttps://recipes.tidymodels.org/\nhttps://www.tmwr.org/recipes.html"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-05-tuning-models.html#tuning-parameters",
    "href": "archive/2024-08-posit-conf/intro-05-tuning-models.html#tuning-parameters",
    "title": "5 - Tuning models",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-05-tuning-models.html#optimize-tuning-parameters",
    "href": "archive/2024-08-posit-conf/intro-05-tuning-models.html#optimize-tuning-parameters",
    "title": "5 - Tuning models",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-05-tuning-models.html#optimize-tuning-parameters-1",
    "href": "archive/2024-08-posit-conf/intro-05-tuning-models.html#optimize-tuning-parameters-1",
    "title": "5 - Tuning models",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search ğŸ’  which tests a pre-defined set of candidate values\nIterative search ğŸŒ€ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-05-tuning-models.html#specifying-tuning-parameters",
    "href": "archive/2024-08-posit-conf/intro-05-tuning-models.html#specifying-tuning-parameters",
    "title": "5 - Tuning models",
    "section": "Specifying tuning parameters",
    "text": "Specifying tuning parameters\nLetâ€™s take our previous random forest workflow and tag for tuning the minimum number of data points in each node:\n\nrf_spec &lt;- rand_forest(min_n = tune()) %&gt;% \n  set_mode(\"classification\")\n\nrf_wflow &lt;- workflow(forested ~ ., rf_spec)\nrf_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   min_n = tune()\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-05-tuning-models.html#try-out-multiple-values",
    "href": "archive/2024-08-posit-conf/intro-05-tuning-models.html#try-out-multiple-values",
    "title": "5 - Tuning models",
    "section": "Try out multiple values",
    "text": "Try out multiple values\ntune_grid() works similar to fit_resamples() but covers multiple parameter values:\n\nset.seed(22)\nrf_res &lt;- tune_grid(\n  rf_wflow,\n  forested_folds,\n  grid = 5\n)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-05-tuning-models.html#compare-results",
    "href": "archive/2024-08-posit-conf/intro-05-tuning-models.html#compare-results",
    "title": "5 - Tuning models",
    "section": "Compare results",
    "text": "Compare results\nInspecting results and selecting the best-performing hyperparameter(s):\n\nshow_best(rf_res)\n#&gt; # A tibble: 5 Ã— 7\n#&gt;   min_n .metric .estimator  mean     n std_err .config             \n#&gt;   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1    21 roc_auc binary     0.972    10 0.00295 Preprocessor1_Model4\n#&gt; 2     6 roc_auc binary     0.972    10 0.00303 Preprocessor1_Model2\n#&gt; 3    31 roc_auc binary     0.972    10 0.00317 Preprocessor1_Model3\n#&gt; 4    13 roc_auc binary     0.972    10 0.00311 Preprocessor1_Model5\n#&gt; 5    33 roc_auc binary     0.972    10 0.00322 Preprocessor1_Model1\n\nbest_parameter &lt;- select_best(rf_res)\nbest_parameter\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   min_n .config             \n#&gt;   &lt;int&gt; &lt;chr&gt;               \n#&gt; 1    21 Preprocessor1_Model4\n\ncollect_metrics() and autoplot() are also available."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-05-tuning-models.html#the-final-fit",
    "href": "archive/2024-08-posit-conf/intro-05-tuning-models.html#the-final-fit",
    "title": "5 - Tuning models",
    "section": "The final fit",
    "text": "The final fit\n\nrf_wflow &lt;- finalize_workflow(rf_wflow, best_parameter)\n\nfinal_fit &lt;- last_fit(rf_wflow, forested_split) \n\ncollect_metrics(final_fit)\n#&gt; # A tibble: 3 Ã— 4\n#&gt;   .metric     .estimator .estimate .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary        0.906  Preprocessor1_Model1\n#&gt; 2 roc_auc     binary        0.970  Preprocessor1_Model1\n#&gt; 3 brier_class binary        0.0656 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-05-tuning-models.html#your-turn",
    "href": "archive/2024-08-posit-conf/intro-05-tuning-models.html#your-turn",
    "title": "5 - Tuning models",
    "section": "Your turn",
    "text": "Your turn\n\nModify your model workflow to tune one or more parameters.\nUse grid search to find the best parameter(s).\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#your-turn",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#your-turn",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nHow do you fit a linear model in R?\nHow many different ways can you think of?\n\n\n\nâˆ’+\n03:00\n\n\n\n\n\nlm for linear model\nglmnet for regularized regression\nkeras for regression using TensorFlow\nstan for Bayesian regression\nspark for large data sets\nbrulee for regression using torch"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-1",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-1",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg()\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\n\nModels have default engines"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-2",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-2",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-3",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-3",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg() %&gt;%\n  set_engine(\"glmnet\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glmnet"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-4",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-4",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg() %&gt;%\n  set_engine(\"stan\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: stan"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-5",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-5",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-6",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-6",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree()\n#&gt; Decision Tree Model Specification (unknown mode)\n#&gt; \n#&gt; Computational engine: rpart\n\n\nSome models have a default mode"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-7",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-7",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree() %&gt;% \n  set_mode(\"classification\")\n#&gt; Decision Tree Model Specification (classification)\n#&gt; \n#&gt; Computational engine: rpart\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-8",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-8",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#your-turn-1",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#your-turn-1",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_spec chunk in your .qmd.\nEdit this code to use a logistic regression model.\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/\n\n\nExtension/Challenge: Edit this code to use a different model. For example, try using a conditional inference tree as implemented in the partykit package by changing the engine - or try an entirely different model type!\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#models-well-be-using-today",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#models-well-be-using-today",
    "title": "3 - What makes a model?",
    "section": "Models weâ€™ll be using today",
    "text": "Models weâ€™ll be using today\n\nLogistic regression\nDecision trees"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#logistic-regression",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#logistic-regression",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#logistic-regression-1",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#logistic-regression-1",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#logistic-regression-2",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#logistic-regression-2",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogit of outcome probability modeled as linear combination of predictors:\n\n\\(log(\\frac{p}{1 - p}) = \\beta_0 + \\beta_1\\cdot \\text{A}\\)\n\nFind a sigmoid line that separates the two classes"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#decision-trees",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#decision-trees",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#decision-trees-1",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#decision-trees-1",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#decision-trees-2",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#decision-trees-2",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "title": "3 - What makes a model?",
    "section": "All models are wrong, but some are useful!",
    "text": "All models are wrong, but some are useful!\n\n\nLogistic regression\n\n\n\n\n\n\n\n\n\n\nDecision trees"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "title": "3 - What makes a model?",
    "section": "Workflows bind preprocessors and models",
    "text": "Workflows bind preprocessors and models\n\n\nExplain that PCA that is a preprocessor / dimensionality reduction, used to decorrelate data"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#what-is-wrong-with-this",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#what-is-wrong-with-this",
    "title": "3 - What makes a model?",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#why-a-workflow",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#why-a-workflow",
    "title": "3 - What makes a model?",
    "section": "Why a workflow()? ",
    "text": "Why a workflow()? \n\n\nWorkflows handle new data better than base R tools in terms of new factor levels\n\n\n\n\nYou can use other preprocessors besides formulas (more on feature engineering in Advanced tidymodels!)\n\n\n\n\nThey can help organize your work when working with multiple models\n\n\n\n\nMost importantly, a workflow captures the entire modeling process: fit() and predict() apply to the preprocessing steps in addition to the actual model fit\n\n\nTwo ways workflows handle levels better than base R:\n\nEnforces that new levels are not allowed at prediction time (this is an optional check that can be turned off)\nRestores missing levels that were present at fit time, but happen to be missing at prediction time (like, if your â€œnewâ€ data just doesnâ€™t have an instance of that level)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#a-model-workflow-1",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#a-model-workflow-1",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"classification\")\n\ntree_spec %&gt;% \n  fit(forested ~ ., data = forested_train) \n#&gt; parsnip model object\n#&gt; \n#&gt; n= 5685 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 5685 2550 Yes (0.55145119 0.44854881)  \n#&gt;    2) land_type=Tree 3064  300 Yes (0.90208877 0.09791123) *\n#&gt;    3) land_type=Barren,Non-tree vegetation 2621  371 No (0.14154903 0.85845097)  \n#&gt;      6) temp_annual_max&lt; 13.395 347  153 Yes (0.55907781 0.44092219)  \n#&gt;       12) tree_no_tree=Tree 92    6 Yes (0.93478261 0.06521739) *\n#&gt;       13) tree_no_tree=No tree 255  108 No (0.42352941 0.57647059) *\n#&gt;      7) temp_annual_max&gt;=13.395 2274  177 No (0.07783641 0.92216359) *"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#a-model-workflow-2",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#a-model-workflow-2",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"classification\")\n\nworkflow() %&gt;%\n  add_formula(forested ~ .) %&gt;%\n  add_model(tree_spec) %&gt;%\n  fit(data = forested_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 5685 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 5685 2550 Yes (0.55145119 0.44854881)  \n#&gt;    2) land_type=Tree 3064  300 Yes (0.90208877 0.09791123) *\n#&gt;    3) land_type=Barren,Non-tree vegetation 2621  371 No (0.14154903 0.85845097)  \n#&gt;      6) temp_annual_max&lt; 13.395 347  153 Yes (0.55907781 0.44092219)  \n#&gt;       12) tree_no_tree=Tree 92    6 Yes (0.93478261 0.06521739) *\n#&gt;       13) tree_no_tree=No tree 255  108 No (0.42352941 0.57647059) *\n#&gt;      7) temp_annual_max&gt;=13.395 2274  177 No (0.07783641 0.92216359) *"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#a-model-workflow-3",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#a-model-workflow-3",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"classification\")\n\nworkflow(forested ~ ., tree_spec) %&gt;% \n  fit(data = forested_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 5685 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 5685 2550 Yes (0.55145119 0.44854881)  \n#&gt;    2) land_type=Tree 3064  300 Yes (0.90208877 0.09791123) *\n#&gt;    3) land_type=Barren,Non-tree vegetation 2621  371 No (0.14154903 0.85845097)  \n#&gt;      6) temp_annual_max&lt; 13.395 347  153 Yes (0.55907781 0.44092219)  \n#&gt;       12) tree_no_tree=Tree 92    6 Yes (0.93478261 0.06521739) *\n#&gt;       13) tree_no_tree=No tree 255  108 No (0.42352941 0.57647059) *\n#&gt;      7) temp_annual_max&gt;=13.395 2274  177 No (0.07783641 0.92216359) *"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#your-turn-2",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#your-turn-2",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_wflow chunk in your .qmd.\nEdit this code to make a workflow with your own model of choice.\n\nExtension/Challenge: Other than formulas, what kinds of preprocessors are supported?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#predict-with-your-model",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#predict-with-your-model",
    "title": "3 - What makes a model?",
    "section": "Predict with your model  ",
    "text": "Predict with your model  \nHow do you use your new tree_fit model?\n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"classification\")\n\ntree_fit &lt;-\n  workflow(forested ~ ., tree_spec) %&gt;% \n  fit(data = forested_train)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#your-turn-3",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#your-turn-3",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\npredict(tree_fit, new_data = forested_test)\nWhat do you notice about the structure of the result?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#your-turn-4",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#your-turn-4",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\naugment(tree_fit, new_data = forested_test)\nHow does the output compare to the output from predict()?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#understand-your-model",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#understand-your-model",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#understand-your-model-1",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#understand-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nlibrary(rpart.plot)\ntree_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot(roundint = FALSE)\n\nYou can extract_*() several components of your fitted workflow.\n\nâš ï¸ Never predict() with any extracted components!\n\nroundint = FALSE is only to quiet a warning"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#understand-your-model-2",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#understand-your-model-2",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nYou can use your fitted workflow for model and/or prediction explanations:\n\n\n\noverall variable importance, such as with the vip package\n\n\n\n\nflexible model explainers, such as with the DALEXtra package\n\n\n\nLearn more at https://www.tmwr.org/explain.html"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#your-turn-5",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#your-turn-5",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\n\nExtract the model engine object from your fitted workflow and check it out.\n\n\n\nâˆ’+\n05:00\n\n\n\n\nAfterward, ask what kind of object people got from the extraction, and what they did with it (e.g.Â give it to summary(), plot(), broom::tidy() ). Live code along"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#the-whole-game---status-update",
    "href": "archive/2024-08-posit-conf/intro-03-what-makes-a-model.html#the-whole-game---status-update",
    "title": "3 - What makes a model?",
    "section": "The whole game - status update",
    "text": "The whole game - status update\n\n\nStress that fitting a model on the entire training set was only for illustrating how to fit a model"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#venue-information",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#venue-information",
    "title": "1 - Introduction",
    "section": "Venue information",
    "text": "Venue information\n\nThere are gender neutral bathrooms located on levels 3, 4, 5, 6 & 7\nA meditation/prayer room is located in 503\n(Mon & Tue 7am - 7pm, and Wed 7am - 5pm)\nA lactation room is located in 509\n(Mon & Tue 7am - 7pm, and Wed 7am - 5pm)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#workshop-policies",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#workshop-policies",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease review the posit::conf code of conduct, which applies to all workshops: https://posit.co/code-of-conduct\nCoC site has info on how to report a problem (in person, email, phone)\nPlease do not photograph people wearing red lanyards"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#who-are-you",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %&gt;% or base R |&gt; pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have some exposure to basic statistical concepts like linear models and residuals\nYou do not need intermediate or expert familiarity with modeling or ML"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#who-are-tidymodels",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#who-are-tidymodels",
    "title": "1 - Introduction",
    "section": "Who are tidymodels?",
    "text": "Who are tidymodels?\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\n+ our TA today, Sara Altman!\n\n\nMany thanks to Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill, and DesirÃ©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#asking-for-help",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#asking-for-help",
    "title": "1 - Introduction",
    "section": "Asking for help",
    "text": "Asking for help\n\nğŸŸª â€œIâ€™m stuck and need help!â€\n\n\nğŸŸ© â€œI finished the exerciseâ€"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#discord",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#discord",
    "title": "1 - Introduction",
    "section": "Discord ",
    "text": "Discord \n\npos.it/conf-event-portal (login)\nClick on â€œJoin Discord, the virtual networking platform!â€\nBrowse Channels -&gt; #workshop-tidymodels-intro"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#section-2",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#section-2",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#section-3",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#section-3",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#plan-for-this-workshop",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Plan for this workshop",
    "text": "Plan for this workshop\n\nYour data budget\nWhat makes a model\nEvaluating models\nTuning models\n\n\nThis workshop will well-prepare folks going on to the Advanced tidymodels workshop, which will cover feature engineering and much more on hyperparameter tuning."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#section-4",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#section-4",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors ğŸ‘‹\n\n Log in to Posit Cloud (free): TODO-ADD-LATER"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#what-is-machine-learning",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#what-is-machine-learning",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nhttps://xkcd.com/1838/"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#what-is-machine-learning-1",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#what-is-machine-learning-1",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#what-is-machine-learning-2024-edition",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#what-is-machine-learning-2024-edition",
    "title": "1 - Introduction",
    "section": "What is machine learning? (2024 edition)",
    "text": "What is machine learning? (2024 edition)\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/\n\n\nIn the early 2010s, â€œArtificial intelligenceâ€ (AI) was largely synonymous with what weâ€™ll refer to as â€œmachine learningâ€ in this workshop. In the late 2010s and early 2020s, AI usually referred to deep learning methods. Since the release of ChatGPT in late 2022, â€œAIâ€ has come to also encompass large language models / generative models."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#what-is-machine-learning-2",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#what-is-machine-learning-2",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#your-turn",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\n\n\nHow are statistics and machine learning related?\nHow are they similar? Different?\n\n\n\nâˆ’+\n03:00\n\n\n\n\nthe â€œtwo culturesâ€\nmodel first vs.Â data first\ninference vs.Â prediction"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#what-is-tidymodels",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#what-is-tidymodels",
    "title": "1 - Introduction",
    "section": "What is tidymodels? ",
    "text": "What is tidymodels? \n\nlibrary(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.2.0 â”€â”€\n#&gt; âœ” broom        1.0.6     âœ” rsample      1.2.1\n#&gt; âœ” dials        1.3.0     âœ” tibble       3.2.1\n#&gt; âœ” dplyr        1.1.4     âœ” tidyr        1.3.1\n#&gt; âœ” infer        1.0.7     âœ” tune         1.2.1\n#&gt; âœ” modeldata    1.4.0     âœ” workflows    1.1.4\n#&gt; âœ” parsnip      1.2.1     âœ” workflowsets 1.1.0\n#&gt; âœ” purrr        1.0.2     âœ” yardstick    1.3.1\n#&gt; âœ” recipes      1.1.0\n#&gt; â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\n#&gt; âœ– purrr::discard() masks scales::discard()\n#&gt; âœ– dplyr::filter()  masks stats::filter()\n#&gt; âœ– dplyr::lag()     masks stats::lag()\n#&gt; âœ– recipes::step()  masks stats::step()\n#&gt; â€¢ Use tidymodels_prefer() to resolve common conflicts."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\nRoadmap for today\nMinimal version of predictive modeling process\nFeature engineering and tuning as iterative extensions"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-1",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-1",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-2",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-2",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\n\nStress that we are not fitting a model on the entire training set other than for illustrative purposes in deck 2."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-3",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-3",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-4",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-4",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-5",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-5",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-6",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-6",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-7",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#the-whole-game-7",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#lets-install-some-packages",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Letâ€™s install some packages",
    "text": "Letâ€™s install some packages\nIf you are using your own laptop instead of Posit Cloud:\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"bonsai\", \"Cubist\", \"doParallel\", \"earth\", \"embed\", \"finetune\", \n    \"forested\", \"lightgbm\", \"lme4\", \"parallelly\", \"plumber\", \"probably\", \n    \"ranger\", \"rpart\", \"rpart.plot\", \"rules\", \"splines2\", \"stacks\", \n    \"text2vec\", \"textrecipes\", \"tidymodels\", \"vetiver\")\n\ninstall.packages(pkgs)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-01-introduction.html#our-versions",
    "href": "archive/2024-08-posit-conf/intro-01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\nR version 4.4.1 (2024-06-14), Quarto (1.6.1)\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nbonsai\n0.3.1\n\n\nbroom\n1.0.6\n\n\nCubist\n0.4.4\n\n\ndials\n1.3.0\n\n\ndoParallel\n1.0.17\n\n\ndplyr\n1.1.4\n\n\nearth\n5.3.3\n\n\nembed\n1.1.4\n\n\nfinetune\n1.2.0\n\n\nforested\n0.1.0\n\n\nFormula\n1.2-5\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nggplot2\n3.5.1\n\n\nlattice\n0.22-6\n\n\nlightgbm\n4.5.0\n\n\nlme4\n1.1-35.5\n\n\nmodeldata\n1.4.0\n\n\nparallelly\n1.38.0\n\n\nparsnip\n1.2.1\n\n\nplotmo\n3.6.3\n\n\nplotrix\n3.8-4\n\n\nplumber\n1.2.2\n\n\nprobably\n1.0.3\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\npurrr\n1.0.2\n\n\nranger\n0.16.0\n\n\nrecipes\n1.1.0\n\n\nrpart\n4.1.23\n\n\nrpart.plot\n3.1.2\n\n\nrsample\n1.2.1\n\n\nrules\n1.0.2\n\n\nscales\n1.3.0\n\n\nsplines2\n0.5.3\n\n\nstacks\n1.0.5\n\n\ntext2vec\n0.6.4\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\ntextrecipes\n1.0.6\n\n\ntibble\n3.2.1\n\n\ntidymodels\n1.2.0\n\n\ntidyr\n1.3.1\n\n\ntune\n1.2.1\n\n\nvetiver\n0.2.5\n\n\nworkflows\n1.1.4\n\n\nworkflowsets\n1.1.0\n\n\nyardstick\n1.3.1"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-vetiver.html#deploying-a-model",
    "href": "archive/2024-08-posit-conf/extras-vetiver.html#deploying-a-model",
    "title": "Extras - Model deployment",
    "section": "Deploying a model ",
    "text": "Deploying a model \nWe have a decision tree, tree_fit, to model whether or not a plot of land in Washington is forested or not.\nHow do we use our model in production?\n\nlibrary(vetiver)\nv &lt;- vetiver_model(tree_fit, \"forested\")\nv\n#&gt; \n#&gt; â”€â”€ forested â”€ &lt;bundled_workflow&gt; model for deployment \n#&gt; A rpart classification modeling workflow using 18 features\n\nLearn more at https://vetiver.rstudio.com"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-vetiver.html#deploy-your-model-1",
    "href": "archive/2024-08-posit-conf/extras-vetiver.html#deploy-your-model-1",
    "title": "Extras - Model deployment",
    "section": "Deploy your model ",
    "text": "Deploy your model \nHow do we use our model in production?\n\nlibrary(plumber)\npr() %&gt;%\n  vetiver_api(v)\n#&gt; # Plumber router with 4 endpoints, 4 filters, and 1 sub-router.\n#&gt; # Use `pr_run()` on this object to start the API.\n#&gt; â”œâ”€â”€[queryString]\n#&gt; â”œâ”€â”€[body]\n#&gt; â”œâ”€â”€[cookieParser]\n#&gt; â”œâ”€â”€[sharedSecret]\n#&gt; â”œâ”€â”€/logo\n#&gt; â”‚  â”‚ # Plumber static router serving from directory: /Users/simoncouch/Library/R/arm64/4.4/library/vetiver\n#&gt; â”œâ”€â”€/metadata (GET)\n#&gt; â”œâ”€â”€/ping (GET)\n#&gt; â”œâ”€â”€/predict (POST)\n#&gt; â””â”€â”€/prototype (GET)\n\nLearn more at https://vetiver.rstudio.com\n\nLive-code making a prediction"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-vetiver.html#your-turn",
    "href": "archive/2024-08-posit-conf/extras-vetiver.html#your-turn",
    "title": "Extras - Model deployment",
    "section": "Your turn",
    "text": "Your turn\n\nRun the vetiver chunk in your .qmd.\nCheck out the automated visual documentation.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-effect-encodings.html#previously---setup",
    "href": "archive/2024-08-posit-conf/extras-effect-encodings.html#previously---setup",
    "title": "Extras - Effect Encodings",
    "section": "Previously - Setup",
    "text": "Previously - Setup\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-effect-encodings.html#previously---data-usage",
    "href": "archive/2024-08-posit-conf/extras-effect-encodings.html#previously---data-usage",
    "title": "Extras - Effect Encodings",
    "section": "Previously - Data Usage",
    "text": "Previously - Data Usage\n\nset.seed(4028)\nhotel_split &lt;-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-effect-encodings.html#what-do-we-do-with-the-agent-and-company-data",
    "href": "archive/2024-08-posit-conf/extras-effect-encodings.html#what-do-we-do-with-the-agent-and-company-data",
    "title": "Extras - Effect Encodings",
    "section": "What do we do with the agent and company data?",
    "text": "What do we do with the agent and company data?\nThere are 98 unique agent values and 100 companies in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables ğŸ˜³\nlump agents and companies that rarely occur into an â€œotherâ€ group\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the agent and company columns with the estimated effect of that predictor"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-effect-encodings.html#per-agent-statistics",
    "href": "archive/2024-08-posit-conf/extras-effect-encodings.html#per-agent-statistics",
    "title": "Extras - Effect Encodings",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-effect-encodings.html#what-is-an-effect-encoding",
    "href": "archive/2024-08-posit-conf/extras-effect-encodings.html#what-is-an-effect-encoding",
    "title": "Extras - Effect Encodings",
    "section": "What is an effect encoding?",
    "text": "What is an effect encoding?\nWe replace the qualitativeâ€™s predictor data with their effect on the outcome.\n\n\nData before:\n\nbefore\n#&gt; # A tibble: 7 Ã— 3\n#&gt;   avg_price_per_room agent            .row\n#&gt;                &lt;dbl&gt; &lt;fct&gt;           &lt;int&gt;\n#&gt; 1               52.7 cynthia_worsley     1\n#&gt; 2               51.8 carlos_bryant       2\n#&gt; 3               53.8 lance_hitchcock     3\n#&gt; 4               51.8 lance_hitchcock     4\n#&gt; 5               46.8 cynthia_worsley     5\n#&gt; 6               54.7 charles_najera      6\n#&gt; 7               46.8 cynthia_worsley     7\n\n\nData after:\n\nafter\n#&gt; # A tibble: 7 Ã— 3\n#&gt;   avg_price_per_room agent  .row\n#&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1               52.7  88.5     1\n#&gt; 2               51.8  89.5     2\n#&gt; 3               53.8  79.8     3\n#&gt; 4               51.8  79.8     4\n#&gt; 5               46.8  88.5     5\n#&gt; 6               54.7 109.      6\n#&gt; 7               46.8  88.5     7\n\n\nThe agent column is replaced with an estimate of the ADR."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-effect-encodings.html#per-agent-statistics-again",
    "href": "archive/2024-08-posit-conf/extras-effect-encodings.html#per-agent-statistics-again",
    "title": "Extras - Effect Encodings",
    "section": "Per-agent statistics again",
    "text": "Per-agent statistics again\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood statistical methods for estimating these means use partial pooling.\nPooling borrows strength across agents and shrinks extreme values towards the mean for agents with very few transations\nThe embed package has recipe steps for effect encodings.\n\n\n\nPartial pooling gives better estimates for agents with fewer reservations by shrinking the estimate to the overall ADR mean"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-effect-encodings.html#partial-pooling",
    "href": "archive/2024-08-posit-conf/extras-effect-encodings.html#partial-pooling",
    "title": "Extras - Effect Encodings",
    "section": "Partial pooling",
    "text": "Partial pooling"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-effect-encodings.html#agent-effects",
    "href": "archive/2024-08-posit-conf/extras-effect-encodings.html#agent-effects",
    "title": "Extras - Effect Encodings",
    "section": "Agent effects  ",
    "text": "Agent effects  \n\nlibrary(embed)\n\nhotel_effect_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_YeoJohnson(lead_time) %&gt;%\n  step_lencode_mixed(agent, company, outcome = vars(avg_price_per_room)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\n\nIt is very important to appropriately validate the effect encoding step to make sure that we are not overfitting."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-effect-encodings.html#effect-encoding-results",
    "href": "archive/2024-08-posit-conf/extras-effect-encodings.html#effect-encoding-results",
    "title": "Extras - Effect Encodings",
    "section": "Effect encoding results    ",
    "text": "Effect encoding results    \n\nhotel_effect_wflow &lt;-\n  workflow() %&gt;%\n  add_model(linear_reg()) %&gt;% \n  update_recipe(hotel_effect_rec)\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\nhotel_effect_res &lt;-\n  hotel_effect_wflow %&gt;%\n  fit_resamples(hotel_rs, metrics = reg_metrics)\n\ncollect_metrics(hotel_effect_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   17.8      10 0.189   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.870    10 0.00357 Preprocessor1_Model1\n\nSlightly worse but it can handle new agents (if they occur)."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-06-wrapping-up.html#your-turn",
    "href": "archive/2024-08-posit-conf/advanced-06-wrapping-up.html#your-turn",
    "title": "6 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-06-wrapping-up.html#resources-to-keep-learning",
    "href": "archive/2024-08-posit-conf/advanced-06-wrapping-up.html#resources-to-keep-learning",
    "title": "6 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://smltar.com/\n\n\n\nFollow us on Mastodon and at the tidyverse blog for updates!"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#previously---setup",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#previously---setup",
    "title": "4 - Grid Search via Racing",
    "section": "Previously - Setup ",
    "text": "Previously - Setup \n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#previously---data-usage",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#previously---data-usage",
    "title": "4 - Grid Search via Racing",
    "section": "Previously - Data Usage ",
    "text": "Previously - Data Usage \n\nset.seed(4028)\nhotel_split &lt;-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#previously---boosting-model",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#previously---boosting-model",
    "title": "4 - Grid Search via Racing",
    "section": "Previously - Boosting Model    ",
    "text": "Previously - Boosting Model    \n\nhotel_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) %&gt;%\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) %&gt;%\n  step_zv(all_predictors())\n\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune(), min_n = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow &lt;- workflow(hotel_rec, lgbm_spec)\n\nlgbm_param &lt;-\n  lgbm_wflow %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#first-a-shameless-promotion",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#first-a-shameless-promotion",
    "title": "4 - Grid Search via Racing",
    "section": "First, a shameless promotion",
    "text": "First, a shameless promotion"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#making-grid-search-more-efficient",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#making-grid-search-more-efficient",
    "title": "4 - Grid Search via Racing",
    "section": "Making Grid Search More Efficient",
    "text": "Making Grid Search More Efficient\nIn the last section, we evaluated 250 models (25 candidates times 10 resamples).\nWe can make this go faster using parallel processing.\nAlso, for some models, we can fit far fewer models than the number that are being evaluated.\n\nFor boosting, a model with X trees can often predict on candidates with less than X trees.\n\nBoth of these methods can lead to enormous speed-ups."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#model-racing",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#model-racing",
    "title": "4 - Grid Search via Racing",
    "section": "Model Racing",
    "text": "Model Racing\nRacing is an old tool that we can use to go even faster.\n\nEvaluate all of the candidate models but only for a few resamples.\nDetermine which candidates have a low probability of being selected.\nEliminate poor candidates.\nRepeat with next resample (until no more resamples remain)\n\nThis can result in fitting a small number of models."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#discarding-candidates",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#discarding-candidates",
    "title": "4 - Grid Search via Racing",
    "section": "Discarding Candidates",
    "text": "Discarding Candidates\nHow do we eliminate tuning parameter combinations?\nThere are a few methods to do so. Weâ€™ll use one based on analysis of variance (ANOVA).\nHoweverâ€¦ there is typically a large difference between resamples in the results."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#resampling-results-non-racing",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#resampling-results-non-racing",
    "title": "4 - Grid Search via Racing",
    "section": "Resampling Results (Non-Racing)",
    "text": "Resampling Results (Non-Racing)\n\n\nHere are some realistic (but simulated) examples of two candidate models.\nAn error estimate is measured for each of 10 resamples.\n\nThe lines connect resamples.\n\nThere is usually a significant resample-to-resample effect (rank corr: 0.83)."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#are-candidates-different",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#are-candidates-different",
    "title": "4 - Grid Search via Racing",
    "section": "Are Candidates Different?",
    "text": "Are Candidates Different?\nOne way to evaluate these models is to do a paired t-test\n\nor a t-test on their differences matched by resamples\n\nWith \\(n = 10\\) resamples, the confidence interval for the difference in RMSE is (0.99, 2.8), indicating that candidate number 2 has smaller error."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#evaluating-differences-in-candidates",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#evaluating-differences-in-candidates",
    "title": "4 - Grid Search via Racing",
    "section": "Evaluating Differences in Candidates",
    "text": "Evaluating Differences in Candidates\n\n\nWhat if we were to have compared the candidates while we seqeuntially evaluated each resample?\nğŸ‘‰\n\nOne candidate shows superiority when 4 resamples have been evaluated."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#interim-analysis-of-results",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#interim-analysis-of-results",
    "title": "4 - Grid Search via Racing",
    "section": "Interim Analysis of Results",
    "text": "Interim Analysis of Results\nOne version of racing uses a mixed model ANOVA to construct one-sided confidence intervals for each candidate versus the current best.\nAny candidates whose bound does not include zero are discarded. Here is an animation.\nThe resamples are analyzed in a random order (so set the seed).\n\nKuhn (2014) has examples and simulations to show that the method works.\nThe finetune package has functions tune_race_anova() and tune_race_win_loss()."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#racing",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#racing",
    "title": "4 - Grid Search via Racing",
    "section": "Racing     ",
    "text": "Racing     \n\n# Let's use a larger grid\nset.seed(8945)\nlgbm_grid &lt;- \n  lgbm_param %&gt;% \n  grid_space_filling(size = 50)\n\nlibrary(finetune)\n\nset.seed(9)\nlgbm_race_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_race_anova(\n    resamples = hotel_rs,\n    grid = lgbm_grid, \n    metrics = reg_metrics\n  )\n\nThe syntax and helper functions are extremely similar to those shown for tune_grid()."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#racing-results",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#racing-results",
    "title": "4 - Grid Search via Racing",
    "section": "Racing Results ",
    "text": "Racing Results \n\nshow_best(lgbm_race_res, metric = \"mae\")\n#&gt; # A tibble: 2 Ã— 11\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1  1347     5     0.0655           66             26 mae     standard    9.64    10   0.173 Preprocessor34_Model1\n#&gt; 2   980     8     0.0429           17            135 mae     standard    9.76    10   0.164 Preprocessor25_Model1"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#racing-results-1",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#racing-results-1",
    "title": "4 - Grid Search via Racing",
    "section": "Racing Results ",
    "text": "Racing Results \n\n\nOnly 171 models were fit (out of 500).\nselect_best() never considers candidate models that did not get to the end of the race.\nThere is a helper function to see how candidate models were removed from consideration.\n\n\nplot_race(lgbm_race_res) + \n  scale_x_continuous(breaks = pretty_breaks())"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-04-racing.html#your-turn",
    "href": "archive/2024-08-posit-conf/advanced-04-racing.html#your-turn",
    "title": "4 - Grid Search via Racing",
    "section": "Your turn",
    "text": "Your turn\n\nRun tune_race_anova() with a different seed.\nDid you get the same or similar results?\n\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#working-with-our-predictors",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#working-with-our-predictors",
    "title": "2 - Feature Engineering",
    "section": "Working with our predictors",
    "text": "Working with our predictors\nWe might want to modify our predictors columns for a few reasons:\n\nThe model requires them in a different format (e.g.Â dummy variables for linear regression).\nThe model needs certain data qualities (e.g.Â same units for K-NN).\nThe outcome is better predicted when one or more columns are transformed in some way (a.k.a â€œfeature engineeringâ€).\n\n\nThe first two reasons are fairly predictable (next page).\nThe last one depends on your modeling problem."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#what-is-feature-engineering",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#what-is-feature-engineering",
    "title": "2 - Feature Engineering",
    "section": "What is feature engineering?",
    "text": "What is feature engineering?\nThink of a feature as some representation of a predictor that will be used in a model.\n\nExample representations:\n\nInteractions\nPolynomial expansions/splines\nPrincipal component analysis (PCA) feature extraction\n\nThere are a lot of examples in Feature Engineering and Selection (FES) and Applied Machine Learning for Tabular Data (aml4td)."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#example-dates",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#example-dates",
    "title": "2 - Feature Engineering",
    "section": "Example: Dates",
    "text": "Example: Dates\nHow can we represent date columns for our model?\n\nWhen we use a date column in its native format, most models in R convert it to an integer.\n\n\nWe can re-engineer it as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nIndicators for holidays\n\n\nThe main point is that we try to maximize performance with different versions of the predictors.\nMention that, for the Chicago data, the day or the week features are usually the most important ones in the model."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#general-definitions",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#general-definitions",
    "title": "2 - Feature Engineering",
    "section": "General definitions ",
    "text": "General definitions \n\nData preprocessing steps allow your model to fit.\nFeature engineering steps help the model do the least work to predict the outcome as well as possible.\n\nThe recipes package can handle both!\n\nThese terms are often used interchangeably in the ML community but we want to distinguish them."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#previously---setup",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#previously---setup",
    "title": "2 - Feature Engineering",
    "section": "Previously - Setup ",
    "text": "Previously - Setup \n\n\n\nlibrary(tidymodels)\n\n# Add another package:\nlibrary(textrecipes)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#previously---data-usage",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#previously---data-usage",
    "title": "2 - Feature Engineering",
    "section": "Previously - Data Usage ",
    "text": "Previously - Data Usage \n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\n\nWeâ€™ll go from here and create a set of resamples to use for model assessments."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#resampling-strategy",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#resampling-strategy",
    "title": "2 - Feature Engineering",
    "section": "Resampling Strategy",
    "text": "Resampling Strategy"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#resampling-strategy-1",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#resampling-strategy-1",
    "title": "2 - Feature Engineering",
    "section": "Resampling Strategy ",
    "text": "Resampling Strategy \nWeâ€™ll use simple 10-fold cross-validation (stratified sampling):\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)\nhotel_rs\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [3372/377]&gt; Fold01\n#&gt;  2 &lt;split [3373/376]&gt; Fold02\n#&gt;  3 &lt;split [3373/376]&gt; Fold03\n#&gt;  4 &lt;split [3373/376]&gt; Fold04\n#&gt;  5 &lt;split [3373/376]&gt; Fold05\n#&gt;  6 &lt;split [3374/375]&gt; Fold06\n#&gt;  7 &lt;split [3375/374]&gt; Fold07\n#&gt;  8 &lt;split [3376/373]&gt; Fold08\n#&gt;  9 &lt;split [3376/373]&gt; Fold09\n#&gt; 10 &lt;split [3376/373]&gt; Fold10"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#prepare-your-data-for-modeling",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#prepare-your-data-for-modeling",
    "title": "2 - Feature Engineering",
    "section": "Prepare your data for modeling ",
    "text": "Prepare your data for modeling \n\nThe recipes package is an extensible framework for pipeable sequences of preprocessing and feature engineering steps.\n\n\n\nStatistical parameters for the steps can be estimated from an initial data set and then applied to other data sets.\n\n\n\n\nThe resulting processed output can be used as inputs for statistical or machine learning models."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#a-first-recipe",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#a-first-recipe",
    "title": "2 - Feature Engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train)\n\n\n\nThe recipe() function assigns columns to roles of â€œoutcomeâ€ or â€œpredictorâ€ using the formula"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#a-first-recipe-1",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#a-first-recipe-1",
    "title": "2 - Feature Engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nsummary(hotel_rec)\n#&gt; # A tibble: 27 Ã— 4\n#&gt;    variable                type      role      source  \n#&gt;    &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 lead_time               &lt;chr [2]&gt; predictor original\n#&gt;  2 stays_in_weekend_nights &lt;chr [2]&gt; predictor original\n#&gt;  3 stays_in_week_nights    &lt;chr [2]&gt; predictor original\n#&gt;  4 adults                  &lt;chr [2]&gt; predictor original\n#&gt;  5 children                &lt;chr [2]&gt; predictor original\n#&gt;  6 babies                  &lt;chr [2]&gt; predictor original\n#&gt;  7 meal                    &lt;chr [3]&gt; predictor original\n#&gt;  8 country                 &lt;chr [3]&gt; predictor original\n#&gt;  9 market_segment          &lt;chr [3]&gt; predictor original\n#&gt; 10 distribution_channel    &lt;chr [3]&gt; predictor original\n#&gt; # â„¹ 17 more rows\n\nThe type column contains information on the variables"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#your-turn",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#your-turn",
    "title": "2 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\nWhat do you think are in the type vectors for the lead_time and country columns?\n\n\n\nâˆ’+\n02:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#create-indicator-variables",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#create-indicator-variables",
    "title": "2 - Feature Engineering",
    "section": "Create indicator variables ",
    "text": "Create indicator variables \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\n\nFor any factor or character predictors, make binary indicators.\nThere are many recipe steps that can convert categorical predictors to numeric columns.\nstep_dummy() records the levels of the categorical predictors in the training set."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#filter-out-constant-columns",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#filter-out-constant-columns",
    "title": "2 - Feature Engineering",
    "section": "Filter out constant columns ",
    "text": "Filter out constant columns \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())\n\n\nIn case there is a factor level that was never observed in the training data (resulting in a column of all 0s), we can delete any zero-variance predictors that have a single unique value.\n\nNote that the selector chooses all columns with a role of â€œpredictorâ€"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#normalization",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#normalization",
    "title": "2 - Feature Engineering",
    "section": "Normalization ",
    "text": "Normalization \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\n\n\nThis centers and scales the numeric predictors.\nThe recipe will use the training set to estimate the means and standard deviations of the data.\n\n\n\n\nAll data the recipe is applied to will be normalized using those statistics (there is no re-estimation)."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#reduce-correlation",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#reduce-correlation",
    "title": "2 - Feature Engineering",
    "section": "Reduce correlation ",
    "text": "Reduce correlation \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_corr(all_numeric_predictors(), threshold = 0.9)\n\n\nTo deal with highly correlated predictors, find the minimum set of predictor columns that make the pairwise correlations less than the threshold."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#other-possible-steps",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#other-possible-steps",
    "title": "2 - Feature Engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_pca(all_numeric_predictors())\n\n\nPCA feature extractionâ€¦"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#other-possible-steps-1",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#other-possible-steps-1",
    "title": "2 - Feature Engineering",
    "section": "Other possible steps  ",
    "text": "Other possible steps  \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  embed::step_umap(all_numeric_predictors(), outcome = vars(avg_price_per_room))\n\n\nA fancy machine learning supervised dimension reduction technique called UMAPâ€¦\n\nNote that this uses the outcome, and it is from an extension package"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#other-possible-steps-2",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#other-possible-steps-2",
    "title": "2 - Feature Engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10)\n\n\nNonlinear transforms like natural splines, and so on!"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#your-turn-1",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#your-turn-1",
    "title": "2 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a recipe() for the hotel data to:\n\nuse a Yeo-Johnson (YJ) transformation on lead_time\nconvert factors to indicator variables\nremove zero-variance variables\nadd the spline technique shown above\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#minimal-recipe",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#minimal-recipe",
    "title": "2 - Feature Engineering",
    "section": "Minimal recipe ",
    "text": "Minimal recipe \n\nhotel_indicators &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_YeoJohnson(lead_time) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#measuring-performance",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#measuring-performance",
    "title": "2 - Feature Engineering",
    "section": "Measuring Performance ",
    "text": "Measuring Performance \nWeâ€™ll compute two measures: mean absolute error and the coefficient of determination (a.k.a \\(R^2\\)).\n\\[\\begin{align}\nMAE &= \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| \\notag \\\\\nR^2 &= cor(y_i, \\hat{y}_i)^2\n\\end{align}\\]\nThe focus will be on MAE for parameter optimization. Weâ€™ll use a metric set to compute these:\n\nreg_metrics &lt;- metric_set(mae, rsq)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#using-a-workflow",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#using-a-workflow",
    "title": "2 - Feature Engineering",
    "section": "Using a workflow    ",
    "text": "Using a workflow    \n\nset.seed(9)\n\nhotel_lm_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(hotel_indicators) %&gt;%\n  add_model(linear_reg())\n \nctrl &lt;- control_resamples(save_pred = TRUE)\nhotel_lm_res &lt;-\n  hotel_lm_wflow %&gt;%\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\ncollect_metrics(hotel_lm_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   16.6      10 0.214   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.884    10 0.00339 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#your-turn-2",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#your-turn-2",
    "title": "2 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() to fit your workflow with a recipe.\nCollect the predictions from the results.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#holdout-predictions",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#holdout-predictions",
    "title": "2 - Feature Engineering",
    "section": "Holdout predictions    ",
    "text": "Holdout predictions    \n\n# Since we used `save_pred = TRUE`\nlm_cv_pred &lt;- collect_predictions(hotel_lm_res)\nlm_cv_pred %&gt;% print(n = 7)\n#&gt; # A tibble: 3,749 Ã— 5\n#&gt;   .pred id      .row avg_price_per_room .config             \n#&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;              &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1  75.1 Fold01    20                 40 Preprocessor1_Model1\n#&gt; 2  49.3 Fold01    28                 54 Preprocessor1_Model1\n#&gt; 3  64.9 Fold01    45                 50 Preprocessor1_Model1\n#&gt; 4  52.8 Fold01    49                 42 Preprocessor1_Model1\n#&gt; 5  48.6 Fold01    61                 49 Preprocessor1_Model1\n#&gt; 6  29.8 Fold01    66                 40 Preprocessor1_Model1\n#&gt; 7  36.9 Fold01    88                 49 Preprocessor1_Model1\n#&gt; # â„¹ 3,742 more rows"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#calibration-plot",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#calibration-plot",
    "title": "2 - Feature Engineering",
    "section": "Calibration Plot ",
    "text": "Calibration Plot \n\nlibrary(probably)\n\ncal_plot_regression(hotel_lm_res)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#what-do-we-do-with-the-agent-and-company-data",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#what-do-we-do-with-the-agent-and-company-data",
    "title": "2 - Feature Engineering",
    "section": "What do we do with the agent and company data?",
    "text": "What do we do with the agent and company data?\nThere are 98 unique agent values and 100 unique companies in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables ğŸ˜³\nlump agents and companies that rarely occur into an â€œotherâ€ group\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the agent and company columns with the estimated effect of that predictor (in the extra materials)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#per-agent-statistics",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#per-agent-statistics",
    "title": "2 - Feature Engineering",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#collapsing-factor-levels",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#collapsing-factor-levels",
    "title": "2 - Feature Engineering",
    "section": "Collapsing factor levels ",
    "text": "Collapsing factor levels \nThere is a recipe step that will redefine factor levels based on their frequency in the training set:\n\nhotel_other_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_YeoJohnson(lead_time) %&gt;%\n  step_other(agent, threshold = 0.001) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10)\n\nUsing this code, 34 agents (out of 98) were collapsed into â€œotherâ€ based on the training set.\nWe could try to optimize the threshold for collapsing (see the next set of slides on model tuning)."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#does-othering-help",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#does-othering-help",
    "title": "2 - Feature Engineering",
    "section": "Does othering help?  ",
    "text": "Does othering help?  \n\nhotel_other_wflow &lt;-\n  hotel_lm_wflow %&gt;%\n  update_recipe(hotel_other_rec)\n\nhotel_other_res &lt;-\n  hotel_other_wflow %&gt;%\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\ncollect_metrics(hotel_other_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   16.7      10 0.213   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.884    10 0.00341 Preprocessor1_Model1\n\nAbout the same MAE and much faster to complete.\nNow letâ€™s look at a more sophisticated tool called effect feature hashing."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#feature-hashing",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#feature-hashing",
    "title": "2 - Feature Engineering",
    "section": "Feature Hashing",
    "text": "Feature Hashing\nBetween agent and company, simple dummy variables would create 198 new columns (that are mostly zeros).\nAnother option is to have a binary indicator that combines some levels of these variables.\nFeature hashing (for more see FES, SMLTAR, TMwR, and aml4td):\n\nuses the character values of the levels\nconverts them to integer hash values\nuses the integers to assign them to a specific indicator column."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#feature-hashing-1",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#feature-hashing-1",
    "title": "2 - Feature Engineering",
    "section": "Feature Hashing",
    "text": "Feature Hashing\nSuppose we want to use 32 indicator variables for agent.\nFor a agent with value â€œMax_Kuhnâ€, a hashing function converts it to an integer (say 210397726).\nTo assign it to one of the 32 columns, we would use modular arithmetic to assign it to a column:\n\n# For \"Max_Kuhn\" put a '1' in column: \n210397726 %% 32\n#&gt; [1] 30\n\nHash functions are meant to emulate randomness."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#feature-hashing-pros",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#feature-hashing-pros",
    "title": "2 - Feature Engineering",
    "section": "Feature Hashing Pros",
    "text": "Feature Hashing Pros\n\nThe procedure will automatically work on new values of the predictors.\nIt is fast.\nâ€œSignedâ€ hashes add a sign to help avoid aliasing."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#feature-hashing-cons",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#feature-hashing-cons",
    "title": "2 - Feature Engineering",
    "section": "Feature Hashing Cons",
    "text": "Feature Hashing Cons\n\nThere is no real logic behind which factor levels are combined.\nWe donâ€™t know how many columns to add (more in the next section).\nSome columns may have all zeros.\nIf a indicator column is important to the model, we canâ€™t easily determine why.\n\n\nThe signed hash make it slightly more possible to differentiate between confounded levels"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#feature-hashing-in-recipes",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#feature-hashing-in-recipes",
    "title": "2 - Feature Engineering",
    "section": "Feature Hashing in recipes   ",
    "text": "Feature Hashing in recipes   \nThe textrecipes package has a step that can be added to the recipe:\n\nlibrary(textrecipes)\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  # Defaults to 32 signed indicator columns\n  step_dummy_hash(agent) %&gt;%\n  step_dummy_hash(company) %&gt;%\n  # Regular indicators for the others\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10)\n\nhotel_hash_wflow &lt;-\n  hotel_lm_wflow %&gt;%\n  update_recipe(hash_rec)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#feature-hashing-in-recipes-1",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#feature-hashing-in-recipes-1",
    "title": "2 - Feature Engineering",
    "section": "Feature Hashing in recipes   ",
    "text": "Feature Hashing in recipes   \n\nhotel_hash_res &lt;-\n  hotel_hash_wflow %&gt;%\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\ncollect_metrics(hotel_hash_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   16.7      10 0.239   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.884    10 0.00324 Preprocessor1_Model1\n\nAbout the same performance but now we can handle new values."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#debugging-a-recipe",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#debugging-a-recipe",
    "title": "2 - Feature Engineering",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g.Â hash_rec) can be estimated manually with a function called prep(). It is analogous to fit(). See TMwR section 16.4\n\n\n\n\nAnother function (bake()) is analogous to predict(), and gives you the processed data back.\n\n\n\n\nThe tidy() function can be used to get specific results from the recipe."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#example",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#example",
    "title": "2 - Feature Engineering",
    "section": "Example  ",
    "text": "Example  \n\nhash_rec_fit &lt;- prep(hash_rec)\n\n# Get the transformation coefficient\ntidy(hash_rec_fit, number = 1)\n\n# Get the processed data\nbake(hash_rec_fit, hotel_train %&gt;% slice(1:3), contains(\"_agent_\"))"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#more-on-recipes",
    "href": "archive/2024-08-posit-conf/advanced-02-feature-engineering.html#more-on-recipes",
    "title": "2 - Feature Engineering",
    "section": "More on recipes",
    "text": "More on recipes\n\nOnce fit() is called on a workflow, changing the model does not re-fit the recipe.\n\n\n\nA list of all known steps is at https://www.tidymodels.org/find/recipes/.\n\n\n\n\nSome steps can be skipped when using predict().\n\n\n\n\nThe order of the steps matters."
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-06-para-terminar.html#tu-turno",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-06-para-terminar.html#tu-turno",
    "title": "6 - Para terminar",
    "section": "Tu turno",
    "text": "Tu turno\n\nÂ¿Que fue algo que te sorprendio aprender?\nÂ¿Cual serÃ¡ lo que aprendiste que planeas usar?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-06-para-terminar.html#recursos-para-seguir-aprendiendo",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-06-para-terminar.html#recursos-para-seguir-aprendiendo",
    "title": "6 - Para terminar",
    "section": "Recursos para seguir aprendiendo",
    "text": "Recursos para seguir aprendiendo\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://smltar.com/"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#viendo-las-predicciones",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#viendo-las-predicciones",
    "title": "4 - Evaluar modelos",
    "section": "Viendo las predicciones",
    "text": "Viendo las predicciones\n\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;%\n  relocate(propina, .pred_class, .pred_si, .pred_no)\n#&gt; # A tibble: 8,000 Ã— 10\n#&gt;    propina .pred_class .pred_si .pred_no distancia compania    local dia   mes  \n#&gt;    &lt;fct&gt;   &lt;fct&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;\n#&gt;  1 si      si             0.967   0.0333     17.2  Chicago Inâ€¦ no    Jue   Feb  \n#&gt;  2 si      si             0.935   0.0646      0.88 City Serviâ€¦ si    Jue   Mar  \n#&gt;  3 si      si             0.967   0.0333     18.1  otra        no    Lun   Feb  \n#&gt;  4 si      si             0.949   0.0507     12.2  Chicago Inâ€¦ no    Dom   Mar  \n#&gt;  5 si      si             0.821   0.179       0.94 Sun Taxi    si    Sab   Abr  \n#&gt;  6 si      si             0.967   0.0333     17.5  Flash Cab   no    Vie   Mar  \n#&gt;  7 si      si             0.967   0.0333     17.7  otra        no    Dom   Ene  \n#&gt;  8 si      si             0.938   0.0616      1.85 Taxicab Inâ€¦ no    Vie   Abr  \n#&gt;  9 si      si             0.938   0.0616      0.53 Sun Taxi    no    Mar   Mar  \n#&gt; 10 si      si             0.931   0.0694      6.65 Taxicab Inâ€¦ no    Dom   Abr  \n#&gt; # â„¹ 7,990 more rows\n#&gt; # â„¹ 1 more variable: hora &lt;int&gt;"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#matriz-de-confusiÃ³n",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#matriz-de-confusiÃ³n",
    "title": "4 - Evaluar modelos",
    "section": "Matriz de confusiÃ³n ",
    "text": "Matriz de confusiÃ³n"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#matriz-de-confusiÃ³n-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#matriz-de-confusiÃ³n-1",
    "title": "4 - Evaluar modelos",
    "section": "Matriz de confusiÃ³n ",
    "text": "Matriz de confusiÃ³n \n\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;%\n  conf_mat(truth = propina, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction   si   no\n#&gt;         si 7341  536\n#&gt;         no   43   80"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#matriz-de-confusiÃ³n-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#matriz-de-confusiÃ³n-2",
    "title": "4 - Evaluar modelos",
    "section": "Matriz de confusiÃ³n ",
    "text": "Matriz de confusiÃ³n \n\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;%\n  conf_mat(truth = propina, estimate = .pred_class) %&gt;%\n  autoplot(type = \"heatmap\")"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#mediciones-de-la-calidad-del-modelo",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#mediciones-de-la-calidad-del-modelo",
    "title": "4 - Evaluar modelos",
    "section": "Mediciones de la calidad del modelo ",
    "text": "Mediciones de la calidad del modelo \n\n\n\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;%\n  accuracy(truth = propina, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.928"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#el-riesgo-de-concentrarse-en-la-exactitud",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#el-riesgo-de-concentrarse-en-la-exactitud",
    "title": "4 - Evaluar modelos",
    "section": "El riesgo de concentrarse en la exactitud ",
    "text": "El riesgo de concentrarse en la exactitud \nHay que tener cuidado utilizando exactitud (accuracy()) ya que nos puede dar â€œbuenosâ€ resultado se predecimos con datos que no estÃ¡n balanceados\n\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;%\n  mutate(.pred_class = factor(\"si\", levels = c(\"si\", \"no\"))) %&gt;%\n  accuracy(truth = propina, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.923"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#mediciones-de-la-calidad-del-modelo-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#mediciones-de-la-calidad-del-modelo-1",
    "title": "4 - Evaluar modelos",
    "section": "Mediciones de la calidad del modelo ",
    "text": "Mediciones de la calidad del modelo \n\n\n\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;%\n  sensitivity(truth = propina, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.994"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#mediciones-de-la-calidad-del-modelo-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#mediciones-de-la-calidad-del-modelo-2",
    "title": "4 - Evaluar modelos",
    "section": "Mediciones de la calidad del modelo ",
    "text": "Mediciones de la calidad del modelo \n\n\n\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;%\n  sensitivity(truth = propina, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.994\n\n\n\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;%\n  specificity(truth = propina, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 specificity binary         0.130"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#mediciones-de-la-calidad-del-modelo-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#mediciones-de-la-calidad-del-modelo-3",
    "title": "4 - Evaluar modelos",
    "section": "Mediciones de la calidad del modelo ",
    "text": "Mediciones de la calidad del modelo \nPara combinar multiples cÃ¡lculos en una tabla, usa metric_set()\n\ntaxi_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;%\n  taxi_metrics(truth = propina, estimate = .pred_class)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy    binary         0.928\n#&gt; 2 specificity binary         0.130\n#&gt; 3 sensitivity binary         0.994"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#mediciones-de-la-calidad-del-modelo-4",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#mediciones-de-la-calidad-del-modelo-4",
    "title": "4 - Evaluar modelos",
    "section": "Mediciones de la calidad del modelo ",
    "text": "Mediciones de la calidad del modelo \n\ntaxi_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;%\n  group_by(local) %&gt;%\n  taxi_metrics(truth = propina, estimate = .pred_class)\n#&gt; # A tibble: 6 Ã— 4\n#&gt;   local .metric     .estimator .estimate\n#&gt;   &lt;fct&gt; &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 si    accuracy    binary         0.898\n#&gt; 2 no    accuracy    binary         0.935\n#&gt; 3 si    specificity binary         0.169\n#&gt; 4 no    specificity binary         0.116\n#&gt; 5 si    sensitivity binary         0.987\n#&gt; 6 no    sensitivity binary         0.996"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#resultados-de-dos-clases",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#resultados-de-dos-clases",
    "title": "4 - Evaluar modelos",
    "section": "Resultados de dos clases",
    "text": "Resultados de dos clases\nEstas mÃ©tricas asumen que sabemos cual es lÃ­mite para convertir probabilidades de predicciÃ³n â€œsuavesâ€ a prediciones de clase â€œdurasâ€ . . .\nÂ¿Es bueno un lÃ­mite de 50%?\nÂ¿Que pasarÃ­a si lo cambiamos a 80%?\n\nsensibilidad â¬‡ï¸, especificidad â¬†ï¸\n\n\nÂ¿Y si lo cambiamos a 20%?\n\nsensibilidad â¬†ï¸, especificidad â¬‡ï¸"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#varying-the-threshold",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#varying-the-threshold",
    "title": "4 - Evaluar modelos",
    "section": "Varying the threshold",
    "text": "Varying the threshold"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#curvas-roc",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#curvas-roc",
    "title": "4 - Evaluar modelos",
    "section": "Curvas ROC",
    "text": "Curvas ROC\nPara crear una â€œcurva ROCâ€, osea una curva de caracterÃ­stica operativa del receptor hacemos lo siguiente:\n\nCalcular la sensibilidad y especificidad de todos los lÃ­mites posibles\nGrÃ¡ficar los falsos positivos en el axis X, contra los positivos verdaderos en el axis Y.\n\nYa que la sensibilidad es la proporciÃ³n de positivos verdaderos, y la especificidad es la de los negativos verdaderos, entonces 1 - especificidad es la proporciÃ³n de los falsos positivos.\n\nPodemos usar el area debajo de la curva (AUC = area under de curve) ROC como una mÃ©trica de clasificaciÃ³n:\n\nROC AUC = 1 ğŸ’¯\nROC AUC = 1/2 ğŸ˜¢\n\n\nROC curves are insensitive to class imbalance."
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#curvas-roc-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#curvas-roc-1",
    "title": "4 - Evaluar modelos",
    "section": "Curvas ROC ",
    "text": "Curvas ROC \n\n# Assumes _first_ factor level is event; there are options to change that\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;% \n  roc_curve(truth = propina, .pred_si) %&gt;%\n  slice(1, 20, 50)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .threshold specificity sensitivity\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1   -Inf           0         1      \n#&gt; 2      0.783       0.209     0.981  \n#&gt; 3      1           1         0.00135\n\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;% \n  roc_auc(truth = propina, .pred_si)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.691"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#grÃ¡fica-de-curvas-roc",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#grÃ¡fica-de-curvas-roc",
    "title": "4 - Evaluar modelos",
    "section": "GrÃ¡fica de curvas ROC ",
    "text": "GrÃ¡fica de curvas ROC \n\n\naugment(taxi_ajustado, new_data = taxi_entrenar) %&gt;% \n  roc_curve(truth = propina, .pred_si) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#tu-turno",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#tu-turno",
    "title": "4 - Evaluar modelos",
    "section": "Tu turno",
    "text": "Tu turno\n\nCalcule y grÃ¡fique una curva ROC con su modelo\nÂ¿Cuales son los datos que se utilizaron para esta curva ROC?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-1",
    "title": "4 - Evaluar modelos",
    "section": "Los peligros del sobreajustar âš ï¸",
    "text": "Los peligros del sobreajustar âš ï¸"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-2",
    "title": "4 - Evaluar modelos",
    "section": "Los peligros del sobreajustar âš ï¸",
    "text": "Los peligros del sobreajustar âš ï¸"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-3",
    "title": "4 - Evaluar modelos",
    "section": "Los peligros del sobreajustar âš ï¸ ",
    "text": "Los peligros del sobreajustar âš ï¸ \n\ntaxi_ajustado %&gt;%\n  augment(taxi_entrenar)\n#&gt; # A tibble: 8,000 Ã— 10\n#&gt;    .pred_class .pred_si .pred_no propina distancia compania    local dia   mes  \n#&gt;    &lt;fct&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;\n#&gt;  1 si             0.967   0.0333 si          17.2  Chicago Inâ€¦ no    Jue   Feb  \n#&gt;  2 si             0.935   0.0646 si           0.88 City Serviâ€¦ si    Jue   Mar  \n#&gt;  3 si             0.967   0.0333 si          18.1  otra        no    Lun   Feb  \n#&gt;  4 si             0.949   0.0507 si          12.2  Chicago Inâ€¦ no    Dom   Mar  \n#&gt;  5 si             0.821   0.179  si           0.94 Sun Taxi    si    Sab   Abr  \n#&gt;  6 si             0.967   0.0333 si          17.5  Flash Cab   no    Vie   Mar  \n#&gt;  7 si             0.967   0.0333 si          17.7  otra        no    Dom   Ene  \n#&gt;  8 si             0.938   0.0616 si           1.85 Taxicab Inâ€¦ no    Vie   Abr  \n#&gt;  9 si             0.938   0.0616 si           0.53 Sun Taxi    no    Mar   Mar  \n#&gt; 10 si             0.931   0.0694 si           6.65 Taxicab Inâ€¦ no    Dom   Abr  \n#&gt; # â„¹ 7,990 more rows\n#&gt; # â„¹ 1 more variable: hora &lt;int&gt;\n\nA esto le llamamos â€œresubstituciÃ³nâ€ Ã³ â€œrepredecir en los datos de entrenamientoâ€"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-4",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-4",
    "title": "4 - Evaluar modelos",
    "section": "Los peligros del sobreajustar âš ï¸ ",
    "text": "Los peligros del sobreajustar âš ï¸ \n\ntaxi_ajustado %&gt;%\n  augment(taxi_entrenar) %&gt;%\n  accuracy(propina, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.928\n\nA esto le llamamos â€œresubstituciÃ³n de la estimaciÃ³nâ€"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-5",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-5",
    "title": "4 - Evaluar modelos",
    "section": "Los peligros del sobreajustar âš ï¸ ",
    "text": "Los peligros del sobreajustar âš ï¸ \n\n\n\ntaxi_ajustado %&gt;%\n  augment(taxi_entrenar) %&gt;%\n  accuracy(propina, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.928"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-6",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-6",
    "title": "4 - Evaluar modelos",
    "section": "Los peligros del sobreajustar âš ï¸ ",
    "text": "Los peligros del sobreajustar âš ï¸ \n\n\n\ntaxi_ajustado %&gt;%\n  augment(taxi_entrenar) %&gt;%\n  accuracy(propina, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.928\n\n\n\ntaxi_ajustado %&gt;%\n  augment(taxi_prueba) %&gt;%\n  accuracy(propina, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.908\n\n\n\nâš ï¸ Acuerdate que estamos demonstrando el sobreajuste\n\n\nâš ï¸ No utilizes el set the prueba sino hasta el fin de tu analisis"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#ja-ja-ja-estoy-en-peligro",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#ja-ja-ja-estoy-en-peligro",
    "title": "4 - Evaluar modelos",
    "section": "â€œJa-ja-ja, estoy en peligroâ€",
    "text": "â€œJa-ja-ja, estoy en peligroâ€"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#tu-turno-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#tu-turno-1",
    "title": "4 - Evaluar modelos",
    "section": "Tu turno",
    "text": "Tu turno\n\nUsa augment() y una funciÃ³n de mÃ©trica para calcular una mÃ©trica de classificaciÃ³n, por ejemplo brier_class()\nCalcula las mÃ©tricas para los datos de entrenamiento y de prueba para demonstrar el sobreajuste\nNota la evidencia de sobreajuste âš ï¸\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-7",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#los-peligros-del-sobreajustar-7",
    "title": "4 - Evaluar modelos",
    "section": "Los peligros del sobreajustar âš ï¸ ",
    "text": "Los peligros del sobreajustar âš ï¸ \n\n\n\ntaxi_ajustado %&gt;%\n  augment(taxi_entrenar) %&gt;%\n  brier_class(propina, .pred_si)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary        0.0632\n\n\n\ntaxi_ajustado %&gt;%\n  augment(taxi_prueba) %&gt;%\n  brier_class(propina, .pred_si)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary        0.0782\n\n\n\nÂ¿Que tal si queremos comparar mÃ¡s modelos?\n\n\nâ€¦y comparar configuraciones para los modelos?\n\n\nY tambien queremos saber si las diferencias son importantes"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#remuestreo",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#remuestreo",
    "title": "4 - Evaluar modelos",
    "section": "Remuestreo",
    "text": "Remuestreo\n\n\n\n\n\nflowchart TD\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts\n  rm1[Remuestreo 1]\n  style rm1 fill:#fff,stroke:#666,color:#000\n  tr --&gt; rm1\n  rm2[Remuestreo 2]\n  style rm2 fill:#fff,stroke:#666,color:#000\n  tr --&gt; rm2\n  rm3[Remuestreo B]\n  style rm3 fill:#fff,stroke:#666,color:#000\n  tr --&gt; rm3\n  an1[Analysis]\n  style an1 fill:#FBE9BF,stroke:#666,color:#000\n  rm1 --&gt; an1\n  vl1[ValidaciÃ³n]\n  style vl1 fill:#E5E7FD,stroke:#666,color:#000\n  rm1 --&gt; vl1\n  an2[Analysis]\n  style an2 fill:#FBE9BF,stroke:#666,color:#000\n  rm2 --&gt; an2\n  vl2[ValidaciÃ³n]\n  style vl2 fill:#E5E7FD,stroke:#666,color:#000\n  rm2 --&gt; vl2  \n  an3[Analysis]\n  style an3 fill:#FBE9BF,stroke:#666,color:#000\n  rm3 --&gt; an3\n  vl3[ValidaciÃ³n]\n  style vl3 fill:#E5E7FD,stroke:#666,color:#000\n  rm3 --&gt; vl3"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada",
    "title": "4 - Evaluar modelos",
    "section": "ValidaciÃ³n cruzada",
    "text": "ValidaciÃ³n cruzada"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-1",
    "title": "4 - Evaluar modelos",
    "section": "ValidaciÃ³n cruzada",
    "text": "ValidaciÃ³n cruzada"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#tu-turno-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#tu-turno-2",
    "title": "4 - Evaluar modelos",
    "section": "Tu turno",
    "text": "Tu turno\n\nSi usamos 10 plieges (folds), cual es el porcentaje de datos de entrenamiento\n\ncuantos terminan en anÃ¡lisis\ncuantos terminan en evaluaciÃ³n (assesment)\n\nâ€¦para cada pliege?\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-2",
    "title": "4 - Evaluar modelos",
    "section": "ValidaciÃ³n cruzada ",
    "text": "ValidaciÃ³n cruzada \n\nvfold_cv(taxi_entrenar) \n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [7200/800]&gt; Fold01\n#&gt;  2 &lt;split [7200/800]&gt; Fold02\n#&gt;  3 &lt;split [7200/800]&gt; Fold03\n#&gt;  4 &lt;split [7200/800]&gt; Fold04\n#&gt;  5 &lt;split [7200/800]&gt; Fold05\n#&gt;  6 &lt;split [7200/800]&gt; Fold06\n#&gt;  7 &lt;split [7200/800]&gt; Fold07\n#&gt;  8 &lt;split [7200/800]&gt; Fold08\n#&gt;  9 &lt;split [7200/800]&gt; Fold09\n#&gt; 10 &lt;split [7200/800]&gt; Fold10"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-3",
    "title": "4 - Evaluar modelos",
    "section": "ValidaciÃ³n cruzada ",
    "text": "ValidaciÃ³n cruzada \nÂ¿Que hay en este?\n\ntaxi_plieges &lt;- vfold_cv(taxi_entrenar)\ntaxi_plieges$splits[1:3]\n#&gt; [[1]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;7200/800/8000&gt;\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;7200/800/8000&gt;\n#&gt; \n#&gt; [[3]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;7200/800/8000&gt;\n\n\nTalk about a list column, storing non-atomic types in dataframe"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-4",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-4",
    "title": "4 - Evaluar modelos",
    "section": "ValidaciÃ³n cruzada ",
    "text": "ValidaciÃ³n cruzada \n\nvfold_cv(taxi_entrenar, v = 5)\n#&gt; #  5-fold cross-validation \n#&gt; # A tibble: 5 Ã— 2\n#&gt;   splits              id   \n#&gt;   &lt;list&gt;              &lt;chr&gt;\n#&gt; 1 &lt;split [6400/1600]&gt; Fold1\n#&gt; 2 &lt;split [6400/1600]&gt; Fold2\n#&gt; 3 &lt;split [6400/1600]&gt; Fold3\n#&gt; 4 &lt;split [6400/1600]&gt; Fold4\n#&gt; 5 &lt;split [6400/1600]&gt; Fold5"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-5",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-5",
    "title": "4 - Evaluar modelos",
    "section": "ValidaciÃ³n cruzada ",
    "text": "ValidaciÃ³n cruzada \n\nvfold_cv(taxi_entrenar, strata = propina)\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [7200/800]&gt; Fold01\n#&gt;  2 &lt;split [7200/800]&gt; Fold02\n#&gt;  3 &lt;split [7200/800]&gt; Fold03\n#&gt;  4 &lt;split [7200/800]&gt; Fold04\n#&gt;  5 &lt;split [7200/800]&gt; Fold05\n#&gt;  6 &lt;split [7200/800]&gt; Fold06\n#&gt;  7 &lt;split [7200/800]&gt; Fold07\n#&gt;  8 &lt;split [7200/800]&gt; Fold08\n#&gt;  9 &lt;split [7200/800]&gt; Fold09\n#&gt; 10 &lt;split [7200/800]&gt; Fold10\n\n\nEstratificar usualmente ayuda, y con pocos malas consecuencias"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-6",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-6",
    "title": "4 - Evaluar modelos",
    "section": "ValidaciÃ³n cruzada ",
    "text": "ValidaciÃ³n cruzada \nUsaremos esto:\n\nset.seed(123)\ntaxi_plieges &lt;- vfold_cv(taxi_entrenar, v = 10, strata = propina)\ntaxi_plieges\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [7200/800]&gt; Fold01\n#&gt;  2 &lt;split [7200/800]&gt; Fold02\n#&gt;  3 &lt;split [7200/800]&gt; Fold03\n#&gt;  4 &lt;split [7200/800]&gt; Fold04\n#&gt;  5 &lt;split [7200/800]&gt; Fold05\n#&gt;  6 &lt;split [7200/800]&gt; Fold06\n#&gt;  7 &lt;split [7200/800]&gt; Fold07\n#&gt;  8 &lt;split [7200/800]&gt; Fold08\n#&gt;  9 &lt;split [7200/800]&gt; Fold09\n#&gt; 10 &lt;split [7200/800]&gt; Fold10\n\n\nEspecifica la semilla (seed) cuando estas creando remuestreos"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#ajustemos-nuestro-modelo-usando-los-remuestreos",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#ajustemos-nuestro-modelo-usando-los-remuestreos",
    "title": "4 - Evaluar modelos",
    "section": "Ajustemos nuestro modelo usando los remuestreos",
    "text": "Ajustemos nuestro modelo usando los remuestreos\n\ntaxi_res &lt;- fit_resamples(arbol_flujo, taxi_plieges)\ntaxi_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 4\n#&gt;    splits             id     .metrics         .notes          \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n#&gt;  1 &lt;split [7200/800]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  2 &lt;split [7200/800]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  3 &lt;split [7200/800]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  4 &lt;split [7200/800]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  5 &lt;split [7200/800]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  6 &lt;split [7200/800]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  7 &lt;split [7200/800]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  8 &lt;split [7200/800]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  9 &lt;split [7200/800]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt; 10 &lt;split [7200/800]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#evaluando-la-calidad-del-modelo",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#evaluando-la-calidad-del-modelo",
    "title": "4 - Evaluar modelos",
    "section": "Evaluando la calidad del modelo ",
    "text": "Evaluando la calidad del modelo \n\ntaxi_res %&gt;%\n  collect_metrics()\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.915    10 0.00309 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.624    10 0.0105  Preprocessor1_Model1\n\n\ncollect_metrics() is one of a suite of collect_*() functions that can be used to work with columns of tuning results. Most columns in a tuning result prefixed with . have a corresponding collect_*() function with options for common summaries.\n\n\nPodemos medir correctamente la la calidad del modelo usando solo los datos de entrenamiento ğŸ‰"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#comparando-las-mÃ©tricas",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#comparando-las-mÃ©tricas",
    "title": "4 - Evaluar modelos",
    "section": "Comparando las mÃ©tricas ",
    "text": "Comparando las mÃ©tricas \nÂ¿Que diferencia hay entre las mÃ©tricas usando los datos de remuestreo, y usando los datos de entrenamiento y prueba?\n\n\n\ntaxi_res %&gt;%\n  collect_metrics() %&gt;% \n  select(.metric, mean, n)\n#&gt; # A tibble: 2 Ã— 3\n#&gt;   .metric   mean     n\n#&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 accuracy 0.915    10\n#&gt; 2 roc_auc  0.624    10\n\n\nEl ROC AUC antes era:\n\n0.69 para el set the entrenamiento\n0.64 para el set the prueba\n\n\n\nAcuerdate que:\nâš ï¸ Los datos de entrenamiento da mÃ©tricas demasiado optimÃ­sticas\nâš ï¸ Los datos de prueba son valiosos"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#evaluando-la-calidad-del-modelo-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#evaluando-la-calidad-del-modelo-1",
    "title": "4 - Evaluar modelos",
    "section": "Evaluando la calidad del modelo ",
    "text": "Evaluando la calidad del modelo \n\n\nctrl_taxi &lt;- control_resamples(save_pred = TRUE)\ntaxi_res &lt;- fit_resamples(arbol_flujo, taxi_plieges, control = ctrl_taxi)\n\ntaxi_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [7200/800]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [7200/800]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [7200/800]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [7200/800]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [7200/800]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [7200/800]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [7200/800]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [7200/800]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [7200/800]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [7200/800]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#evaluando-la-calidad-del-modelo-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#evaluando-la-calidad-del-modelo-2",
    "title": "4 - Evaluar modelos",
    "section": "Evaluando la calidad del modelo ",
    "text": "Evaluando la calidad del modelo \n\n# Guarde los resultados de las evaluaciones\ntaxi_preds &lt;- collect_predictions(taxi_res)\ntaxi_preds\n#&gt; # A tibble: 8,000 Ã— 7\n#&gt;    id     .pred_si .pred_no  .row .pred_class propina .config             \n#&gt;    &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;   &lt;chr&gt;               \n#&gt;  1 Fold01    0.938   0.0615    14 si          si      Preprocessor1_Model1\n#&gt;  2 Fold01    0.946   0.0544    19 si          si      Preprocessor1_Model1\n#&gt;  3 Fold01    0.973   0.0269    33 si          si      Preprocessor1_Model1\n#&gt;  4 Fold01    0.903   0.0971    43 si          si      Preprocessor1_Model1\n#&gt;  5 Fold01    0.973   0.0269    74 si          si      Preprocessor1_Model1\n#&gt;  6 Fold01    0.903   0.0971   103 si          si      Preprocessor1_Model1\n#&gt;  7 Fold01    0.915   0.0851   104 si          no      Preprocessor1_Model1\n#&gt;  8 Fold01    0.903   0.0971   124 si          si      Preprocessor1_Model1\n#&gt;  9 Fold01    0.667   0.333    126 si          si      Preprocessor1_Model1\n#&gt; 10 Fold01    0.949   0.0510   128 si          si      Preprocessor1_Model1\n#&gt; # â„¹ 7,990 more rows"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#evaluando-la-calidad-del-modelo-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#evaluando-la-calidad-del-modelo-3",
    "title": "4 - Evaluar modelos",
    "section": "Evaluando la calidad del modelo ",
    "text": "Evaluando la calidad del modelo \n\ntaxi_preds %&gt;% \n  group_by(id) %&gt;%\n  taxi_metrics(truth = propina, estimate = .pred_class)\n#&gt; # A tibble: 30 Ã— 4\n#&gt;    id     .metric  .estimator .estimate\n#&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt;  1 Fold01 accuracy binary         0.905\n#&gt;  2 Fold02 accuracy binary         0.925\n#&gt;  3 Fold03 accuracy binary         0.926\n#&gt;  4 Fold04 accuracy binary         0.915\n#&gt;  5 Fold05 accuracy binary         0.902\n#&gt;  6 Fold06 accuracy binary         0.912\n#&gt;  7 Fold07 accuracy binary         0.906\n#&gt;  8 Fold08 accuracy binary         0.91 \n#&gt;  9 Fold09 accuracy binary         0.918\n#&gt; 10 Fold10 accuracy binary         0.931\n#&gt; # â„¹ 20 more rows"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#donde-estan-los-modelos-que-ajustamos",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#donde-estan-los-modelos-que-ajustamos",
    "title": "4 - Evaluar modelos",
    "section": "Â¿Donde estan los modelos que ajustamos? ",
    "text": "Â¿Donde estan los modelos que ajustamos? \n\ntaxi_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [7200/800]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [7200/800]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [7200/800]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [7200/800]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [7200/800]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [7200/800]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [7200/800]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [7200/800]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [7200/800]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [7200/800]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;\n\n\nğŸ—‘ï¸"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#bootstrapping",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#bootstrapping",
    "title": "4 - Evaluar modelos",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#bootstrapping-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#bootstrapping-1",
    "title": "4 - Evaluar modelos",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(3214)\nbootstraps(taxi_entrenar)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 25 Ã— 2\n#&gt;    splits              id         \n#&gt;    &lt;list&gt;              &lt;chr&gt;      \n#&gt;  1 &lt;split [8000/2902]&gt; Bootstrap01\n#&gt;  2 &lt;split [8000/2916]&gt; Bootstrap02\n#&gt;  3 &lt;split [8000/3004]&gt; Bootstrap03\n#&gt;  4 &lt;split [8000/2979]&gt; Bootstrap04\n#&gt;  5 &lt;split [8000/2961]&gt; Bootstrap05\n#&gt;  6 &lt;split [8000/2962]&gt; Bootstrap06\n#&gt;  7 &lt;split [8000/3026]&gt; Bootstrap07\n#&gt;  8 &lt;split [8000/2926]&gt; Bootstrap08\n#&gt;  9 &lt;split [8000/2972]&gt; Bootstrap09\n#&gt; 10 &lt;split [8000/2972]&gt; Bootstrap10\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#expectativas-del-taller---donde-estamos",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#expectativas-del-taller---donde-estamos",
    "title": "4 - Evaluar modelos",
    "section": "Expectativas del taller - Donde estamos",
    "text": "Expectativas del taller - Donde estamos\n\n\n\n\n\nflowchart LR\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts\n  rs[Remuestras]\n  style rs fill:#FDF4E3,stroke:#666,color:#000\n  tr --&gt; rs\n  lg[RegresiÃ³n\\nlogÃ­stica]\n  style lg fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; lg\n  dt[Arbol de\\nDecisiÃ³n]\n  style dt fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; dt\n  rf[Bosque\\nAleatorio]\n  style rf fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; rf\n  sm[Seleccionar\\nmodelo]\n  style sm fill:#fff,stroke:#eee,color:#ddd\n  lg --&gt; sm\n  dt --&gt; sm\n  rf --&gt; sm\n  fm[Entrenar modelo\\nselecionado]\n  style fm fill:#fff,stroke:#eee,color:#ddd\n  sm --&gt; fm\n  tr --&gt; fm\n  vm[Verificar la\\ncalidad]\n  style vm fill:#fff,stroke:#eee,color:#ddd\n  fm --&gt; vm\n  ts --&gt; vm"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#tu-turno-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#tu-turno-3",
    "title": "4 - Evaluar modelos",
    "section": "Tu turno",
    "text": "Tu turno\n\nCrea un:\n\nSet de validaciÃ³n cruzada tipo Monte Carlo\nSet de validaciÃ³n\n\nNo te olvides de usar set.seed()\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-tipo-monte-carlo",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#validaciÃ³n-cruzada-tipo-monte-carlo",
    "title": "4 - Evaluar modelos",
    "section": "ValidaciÃ³n cruzada tipo Monte Carlo ",
    "text": "ValidaciÃ³n cruzada tipo Monte Carlo \n\nset.seed(322)\nmc_cv(taxi_entrenar, times = 10)\n#&gt; # Monte Carlo cross-validation (0.75/0.25) with 10 resamples  \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits              id        \n#&gt;    &lt;list&gt;              &lt;chr&gt;     \n#&gt;  1 &lt;split [6000/2000]&gt; Resample01\n#&gt;  2 &lt;split [6000/2000]&gt; Resample02\n#&gt;  3 &lt;split [6000/2000]&gt; Resample03\n#&gt;  4 &lt;split [6000/2000]&gt; Resample04\n#&gt;  5 &lt;split [6000/2000]&gt; Resample05\n#&gt;  6 &lt;split [6000/2000]&gt; Resample06\n#&gt;  7 &lt;split [6000/2000]&gt; Resample07\n#&gt;  8 &lt;split [6000/2000]&gt; Resample08\n#&gt;  9 &lt;split [6000/2000]&gt; Resample09\n#&gt; 10 &lt;split [6000/2000]&gt; Resample10"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#set-de-validaciÃ³n",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#set-de-validaciÃ³n",
    "title": "4 - Evaluar modelos",
    "section": "Set de validaciÃ³n ",
    "text": "Set de validaciÃ³n \n\nset.seed(853)\ntaxi_val_split &lt;- initial_validation_split(taxi, strata = propina)\nvalidation_set(taxi_val_split)\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   splits              id        \n#&gt;   &lt;list&gt;              &lt;chr&gt;     \n#&gt; 1 &lt;split [6000/2000]&gt; validation\n\n\nUn set de validaciÃ³n es solamente otro tipo de remuestreo"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#bosque-aleatorio-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#bosque-aleatorio-1",
    "title": "4 - Evaluar modelos",
    "section": "Bosque aleatorio ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ",
    "text": "Bosque aleatorio ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\n\nEnsambla varios Ã¡rboles de decisiÃ³n\nÂ¡Todos los Ã¡rboles votan! ğŸ—³ï¸\nAgregaciÃ³n tipo bootstrap + muestreo aleatorio del predictor\n\n\n\nUsualmente funciona bien sin usar afinamiento, pero solo mientras hayan suficientes Ã¡rboles"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#crear-un-modelo-de-bosque-aleatorio",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#crear-un-modelo-de-bosque-aleatorio",
    "title": "4 - Evaluar modelos",
    "section": "Crear un modelo de bosque aleatorio ",
    "text": "Crear un modelo de bosque aleatorio \n\nrf_spec &lt;- rand_forest(trees = 1000, mode = \"classification\")\nrf_spec\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#crear-un-modelo-de-bosque-aleatorio-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#crear-un-modelo-de-bosque-aleatorio-1",
    "title": "4 - Evaluar modelos",
    "section": "Crear un modelo de bosque aleatorio ",
    "text": "Crear un modelo de bosque aleatorio \n\nrf_wflow &lt;- workflow(propina ~ ., rf_spec)\nrf_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; propina ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#tu-turno-4",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#tu-turno-4",
    "title": "4 - Evaluar modelos",
    "section": "Tu turno",
    "text": "Tu turno\n\nUsa fit_resamples() y rf_wflow para lo siguiente:\n\nQuedarse con las predicciones\nCalcular las mÃ©tricas\n\n\n\n\nâˆ’+\n08:00"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#evaluando-la-calidad-del-modelo-4",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#evaluando-la-calidad-del-modelo-4",
    "title": "4 - Evaluar modelos",
    "section": "Evaluando la calidad del modelo ",
    "text": "Evaluando la calidad del modelo \n\nctrl_taxi &lt;- control_resamples(save_pred = TRUE)\n\n# Bosque aleatorio usan numeros al azar, asi que asegurate de definir la semilla\n\nset.seed(2)\nrf_res &lt;- fit_resamples(rf_wflow, taxi_plieges, control = ctrl_taxi)\ncollect_metrics(rf_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.924    10 0.00323 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.611    10 0.0149  Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#expectativas-del-taller---donde-estamos-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#expectativas-del-taller---donde-estamos-1",
    "title": "4 - Evaluar modelos",
    "section": "Expectativas del taller - Donde estamos",
    "text": "Expectativas del taller - Donde estamos\n\n\n\n\n\nflowchart LR\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts\n  rs[Remuestras]\n  style rs fill:#FDF4E3,stroke:#666,color:#000\n  tr --&gt; rs\n  lg[RegresiÃ³n\\nlogÃ­stica]\n  style lg fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; lg\n  dt[Arbol de\\nDecisiÃ³n]\n  style dt fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; dt\n  rf[Bosque\\nAleatorio]\n  style rf fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; rf\n  sm[Seleccionar\\nmodelo]\n  style sm fill:#FDF4E3,stroke:#666,color:#000\n  lg --&gt; sm\n  dt --&gt; sm\n  rf --&gt; sm\n  fm[Entrenar modelo\\nselecionado]\n  style fm fill:#fff,stroke:#eee,color:#ddd\n  sm --&gt; fm\n  tr --&gt; fm\n  vm[Verificar la\\ncalidad]\n  style vm fill:#fff,stroke:#eee,color:#ddd\n  fm --&gt; vm\n  ts --&gt; vm"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#el-ajuste-final",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#el-ajuste-final",
    "title": "4 - Evaluar modelos",
    "section": "El ajuste final ",
    "text": "El ajuste final \nDigamos que estamos satisfechos con usar nuestro modelo de bosque aleatorio\nAjustemos el modelo usando todos los datos en el set the entrenamiento, y despues midamos la calidad del modelo con el set the prueba\n\nHemos usado fit() y predict() (+ augment()), pero hay un atajo:\n\n# taxi_separar has train + test info\najuste_final &lt;- last_fit(rf_wflow, taxi_separar) \n\najuste_final\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits              id               .metrics .notes   .predictions .workflow \n#&gt;   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n#&gt; 1 &lt;split [8000/2000]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#que-contiene-ajuste_final",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#que-contiene-ajuste_final",
    "title": "4 - Evaluar modelos",
    "section": "Â¿Que contiene ajuste_final? ",
    "text": "Â¿Que contiene ajuste_final? \n\ncollect_metrics(ajuste_final)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric  .estimator .estimate .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary         0.914 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary         0.630 Preprocessor1_Model1\n\n\nLas mÃ©tricas fueron calculadas con el set de prueba"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#que-contiene-ajuste_final-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#que-contiene-ajuste_final-1",
    "title": "4 - Evaluar modelos",
    "section": "Â¿Que contiene ajuste_final? ",
    "text": "Â¿Que contiene ajuste_final? \n\ncollect_predictions(ajuste_final)\n#&gt; # A tibble: 2,000 Ã— 7\n#&gt;    id               .pred_si .pred_no  .row .pred_class propina .config         \n#&gt;    &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;   &lt;chr&gt;           \n#&gt;  1 train/test split    0.949   0.0513     4 si          si      Preprocessor1_Mâ€¦\n#&gt;  2 train/test split    0.922   0.0777    10 si          si      Preprocessor1_Mâ€¦\n#&gt;  3 train/test split    0.961   0.0395    19 si          si      Preprocessor1_Mâ€¦\n#&gt;  4 train/test split    0.889   0.111     23 si          si      Preprocessor1_Mâ€¦\n#&gt;  5 train/test split    0.936   0.0639    28 si          si      Preprocessor1_Mâ€¦\n#&gt;  6 train/test split    0.969   0.0306    34 si          si      Preprocessor1_Mâ€¦\n#&gt;  7 train/test split    0.962   0.0377    35 si          si      Preprocessor1_Mâ€¦\n#&gt;  8 train/test split    0.939   0.0606    38 si          si      Preprocessor1_Mâ€¦\n#&gt;  9 train/test split    0.986   0.0138    40 si          si      Preprocessor1_Mâ€¦\n#&gt; 10 train/test split    0.957   0.0431    42 si          no      Preprocessor1_Mâ€¦\n#&gt; # â„¹ 1,990 more rows"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#que-contiene-ajuste_final-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#que-contiene-ajuste_final-2",
    "title": "4 - Evaluar modelos",
    "section": "Â¿Que contiene ajuste_final? ",
    "text": "Â¿Que contiene ajuste_final? \n\nextract_workflow(ajuste_final)\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; propina ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  1000 \n#&gt; Sample size:                      8000 \n#&gt; Number of independent variables:  6 \n#&gt; Mtry:                             2 \n#&gt; Target node size:                 10 \n#&gt; Variable importance mode:         none \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.07070342\n\n\nUse this for prediction on new data, like for deploying"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#expectativas-del-taller---ya-llegamos",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-04-evaluar-modelos.html#expectativas-del-taller---ya-llegamos",
    "title": "4 - Evaluar modelos",
    "section": "Expectativas del taller - Â¡Ya llegamos!",
    "text": "Expectativas del taller - Â¡Ya llegamos!\n\n\n\n\n\nflowchart LR\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts\n  rs[Remuestreo]\n  style rs fill:#FDF4E3,stroke:#666,color:#000\n  tr --&gt; rs\n  lg[RegresiÃ³n\\nlogÃ­stica]\n  style lg fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; lg\n  dt[Arbol de\\nDecisiÃ³n]\n  style dt fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; dt\n  rf[Bosque\\nAleatorio]\n  style rf fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; rf\n  sm[Seleccionar\\nmodelo]\n  style sm fill:#FDF4E3,stroke:#666,color:#000\n  lg --&gt; sm\n  dt --&gt; sm\n  rf --&gt; sm\n  fm[Entrenar modelo\\nselecionado]\n  style fm fill:#FBE9BF,stroke:#666,color:#000\n  sm --&gt; fm\n  tr --&gt; fm\n  vm[Verificar la\\ncalidad]\n  style vm fill:#E5E7FD,stroke:#666,color:#000\n  fm --&gt; vm\n  ts --&gt; vm"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section",
    "title": "2 - Tu presupuesto de datos",
    "section": "",
    "text": "â€œÂ¡Â¡Que emociÃ³n!!â€"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#datos-de-viajes-en-taxi-de-chicago",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#datos-de-viajes-en-taxi-de-chicago",
    "title": "2 - Tu presupuesto de datos",
    "section": "Datos de viajes en taxi de Chicago",
    "text": "Datos de viajes en taxi de Chicago\n\n\n\nLa ciudad de Chicago publica datos a nivel de viaje\nSacamos una muestra de 10,000 viajes que ocurrieron al principio del 2022\n\n\n\n\n\nhttps://www.svgrepo.com/svg/8322/taxi"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#datos-de-viajes-en-taxi-de-chicago-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#datos-de-viajes-en-taxi-de-chicago-1",
    "title": "2 - Tu presupuesto de datos",
    "section": "Datos de viajes en taxi de Chicago",
    "text": "Datos de viajes en taxi de Chicago\n\n\n\nN = 10,000\nUn resultado nominale, propina, con los niveles \"si\" y \"no\"\nVarias variables nominales, por ejemplo la identificaciÃ³n del Taxi, y el tipo de pago\nVarias variables numÃ©ricas, por ejemplo la distancia del viaje, y los subtotales del cargo por el viaje\n\n\n\n\n\nCredit: https://unsplash.com/photos/7_r85l4eht8\n\n\nâ€œFare subtotalsâ€ refers to the fare itself, tax, tolls, tip amount.\nActual variables in our data:\ntip: Whether the rider left a tip. A factor with levels â€œyesâ€ and â€œnoâ€.\ndistance: The trip distance, in odometer miles.\ncompany: The taxi company, as a factor. Companies that occurred few times were binned as â€œotherâ€.\nlocal: Whether the trip started in the same community area as it began. See the source data for community area values.\ndow: The day of the week in which the trip began, as a factor.\nmonth: The month in which the trip began, as a factor.\nhour: The hour of the day in which the trip began, as a numeric."
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#se-pueden-usar-estas-variables",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#se-pueden-usar-estas-variables",
    "title": "2 - Tu presupuesto de datos",
    "section": "Â¿Se pueden usar estas variables?",
    "text": "Â¿Se pueden usar estas variables?\n\nÂ¿SerÃ¡ Ã©tico, o hasta legal, utilizar esta variable?\nÂ¿La variable estarÃ¡ disponible al momento de predecir?\nÂ¿La variable contribuye a la explicaciÃ³n de los resultados?"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#datos-de-viajes-en-taxi-de-chicago-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#datos-de-viajes-en-taxi-de-chicago-2",
    "title": "2 - Tu presupuesto de datos",
    "section": "Datos de viajes en taxi de Chicago",
    "text": "Datos de viajes en taxi de Chicago\nBaja los datos, y cargarlos en tu sesiÃ³n de R\n\ndownload.file(\"https://github.com/edgararuiz/tidymodels-workshops/blob/main/archive/2024-03-conectaR-spanish/taxi.rds\",\n              \"taxi.rds\")\n\ntaxi &lt;- readRDS(taxi.rds)\n\n\ntaxi\n#&gt; # A tibble: 10,000 Ã— 7\n#&gt;    propina distancia compania                     local dia   mes    hora\n#&gt;    &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;                        &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n#&gt;  1 si          17.2  Chicago Independents         no    Jue   Feb      16\n#&gt;  2 si           0.88 City Service                 si    Jue   Mar       8\n#&gt;  3 si          18.1  otra                         no    Lun   Feb      18\n#&gt;  4 si          20.7  Chicago Independents         no    Lun   Abr       8\n#&gt;  5 si          12.2  Chicago Independents         no    Dom   Mar      21\n#&gt;  6 si           0.94 Sun Taxi                     si    Sab   Abr      23\n#&gt;  7 si          17.5  Flash Cab                    no    Vie   Mar      12\n#&gt;  8 si          17.7  otra                         no    Dom   Ene       6\n#&gt;  9 si           1.85 Taxicab Insurance Agency Llc no    Vie   Abr      12\n#&gt; 10 si           1.47 City Service                 no    Mar   Mar      14\n#&gt; # â„¹ 9,990 more rows"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#separando-y-gastando-datos",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#separando-y-gastando-datos",
    "title": "2 - Tu presupuesto de datos",
    "section": "Separando y â€œgastandoâ€ datos",
    "text": "Separando y â€œgastandoâ€ datos\nPara el aprendizaje automatico, separamos los datos, unos para entranamiento, y los otros para la â€œprueba finalâ€:\n\n\nLos datos de entrenamiento se utilizan para estimar los parametros del modelo\nLos datos de prueba  se ponen aparte para medir la calidad del modelo\n\n\n\nğŸš« No utilize los datos de prueba durante el entrenamiento ğŸš«"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#separando-y-gastando-datos-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#separando-y-gastando-datos-1",
    "title": "2 - Tu presupuesto de datos",
    "section": "Separando y â€œgastandoâ€ datos",
    "text": "Separando y â€œgastandoâ€ datos"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#separando-y-gastando-datos-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#separando-y-gastando-datos-2",
    "title": "2 - Tu presupuesto de datos",
    "section": "Separando y â€œgastandoâ€ datos",
    "text": "Separando y â€œgastandoâ€ datos\n\nGastando mucho datos en el entrenamiento previene que tengamos una buena manera de medir la calidad del modelo\n\n\n\nGastando mucho datos en prueba previente que tegamos una buena estimaciÃ³n de los parametros"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#tu-turno",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#tu-turno",
    "title": "2 - Tu presupuesto de datos",
    "section": "Tu turno",
    "text": "Tu turno\n\nÂ¿Cuando deberiamos separar nuestros datos?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#la-separaciÃ³n-inicial",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#la-separaciÃ³n-inicial",
    "title": "2 - Tu presupuesto de datos",
    "section": "La separaciÃ³n inicial ",
    "text": "La separaciÃ³n inicial \n\nset.seed(123)\ntaxi_separar &lt;- initial_split(taxi)\ntaxi_separar\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;7500/2500/10000&gt;\n\n\nHow much data in training vs testing? This function uses a good default, but this depends on your specific goal/data We will talk about more powerful ways of splitting, like stratification, later"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#para-que-es-set.seed",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#para-que-es-set.seed",
    "title": "2 - Tu presupuesto de datos",
    "section": "Â¿Para que es set.seed()?",
    "text": "Â¿Para que es set.seed()?\n\nPara el propÃ³sito de separar datos, R puede generar nÃºmeros â€œseudo-aleatoriosâ€, que aunque parezcan comportarse como nÃºmeros aleatorios, en realidad se generan de una manera especÃ­fica basada en una â€œsemillaâ€ (seed)\n\n\nSi utilizamos la misma semilla nos permite reproducir los mismos resultados.\n\n\nEl nÃºmero en sÃ­ que elijes realmente no importa. Lo que si importa, es que no experimentes con diferentes nÃºmeros de la semilla con el proposito de mejorar la calidad del modelo."
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#accediendo-los-datos",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#accediendo-los-datos",
    "title": "2 - Tu presupuesto de datos",
    "section": "Accediendo los datos ",
    "text": "Accediendo los datos \n\ntaxi_entrenar &lt;- training(taxi_separar)\ntaxi_test &lt;- testing(taxi_separar)"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#el-set-the-entrenamiento",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#el-set-the-entrenamiento",
    "title": "2 - Tu presupuesto de datos",
    "section": "El set the entrenamiento ",
    "text": "El set the entrenamiento \n\ntaxi_entrenar\n#&gt; # A tibble: 7,500 Ã— 7\n#&gt;    propina distancia compania                  local dia   mes    hora\n#&gt;    &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;                     &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n#&gt;  1 si           0.7  Taxi Affiliation Services si    Mar   Mar      18\n#&gt;  2 si           0.99 Sun Taxi                  si    Mar   Ene       8\n#&gt;  3 si           1.78 otra                      no    Sab   Mar      22\n#&gt;  4 si           0    Taxi Affiliation Services si    Mie   Abr      15\n#&gt;  5 si           0    Taxi Affiliation Services no    Dom   Ene      21\n#&gt;  6 si           2.3  otra                      no    Sab   Abr      21\n#&gt;  7 si           6.35 Sun Taxi                  no    Mie   Mar      16\n#&gt;  8 si           2.79 otra                      no    Dom   Feb      14\n#&gt;  9 si          16.6  otra                      no    Dom   Abr      18\n#&gt; 10 si           0.02 Chicago Independents      si    Dom   Abr      15\n#&gt; # â„¹ 7,490 more rows"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#el-set-de-prueba",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#el-set-de-prueba",
    "title": "2 - Tu presupuesto de datos",
    "section": "El set de prueba ",
    "text": "El set de prueba \nğŸ™ˆ\n\nEn el set de prueba, hay 2500 entradas, con 7 columnas"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#tu-turno-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#tu-turno-1",
    "title": "2 - Tu presupuesto de datos",
    "section": "Tu turno",
    "text": "Tu turno\n\nSepara tus datos, el 20% tiene que ser para prueba\nTrata diferentes valores para set.seed() para ver como cambian los resultados\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#separando-y-gastando-datos-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#separando-y-gastando-datos-3",
    "title": "2 - Tu presupuesto de datos",
    "section": "Separando y â€œgastandoâ€ datos ",
    "text": "Separando y â€œgastandoâ€ datos \n\nset.seed(123)\ntaxi_separar &lt;- initial_split(taxi, prop = 0.8)\ntaxi_entrenar &lt;- training(taxi_separar)\ntaxi_test &lt;- testing(taxi_separar)\n\nnrow(taxi_entrenar)\n#&gt; [1] 8000\nnrow(taxi_test)\n#&gt; [1] 2000"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-1",
    "title": "2 - Tu presupuesto de datos",
    "section": "",
    "text": "flowchart TD\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000  \n  vl[ValidaciÃ³n]\n  style vl fill:#E5E7FD,stroke:#666,color:#000    \n  ad --&gt; tr\n  ad --&gt; ts\n  ad --&gt; vl"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#datos-de-validaciÃ³n",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#datos-de-validaciÃ³n",
    "title": "2 - Tu presupuesto de datos",
    "section": "Datos de validaciÃ³n",
    "text": "Datos de validaciÃ³n\n\nset.seed(123)\ninitial_validation_split(taxi, prop = c(0.6, 0.2))\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;6000/2000/2000/10000&gt;"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#tu-turno-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#tu-turno-2",
    "title": "2 - Tu presupuesto de datos",
    "section": "Tu turno",
    "text": "Tu turno\n\nExplora los datos en taxi_entrenar por tÃ­ mismo\n\nÂ¿Cual es la distribuciÃ³n de los resultados de propina?\nÂ¿Cual es la distribuciÃ³n de las variables numÃ©ricas, por ejemplo distancia?\nÂ¿Como cambian los resultados de propina a travÃ©s de las variables categÃ³ricas?\n\n\n\n\nâˆ’+\n08:00\n\n\n\n\nMake a plot or summary and then share with neighbor"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-2",
    "title": "2 - Tu presupuesto de datos",
    "section": "",
    "text": "taxi_entrenar %&gt;% \n  ggplot(aes(x = propina)) +\n  geom_bar()"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-3",
    "title": "2 - Tu presupuesto de datos",
    "section": "",
    "text": "taxi_entrenar %&gt;% \n  ggplot(aes(x = propina, fill = local)) +\n  geom_bar() +\n  scale_fill_viridis_d(end = .5)"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-4",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-4",
    "title": "2 - Tu presupuesto de datos",
    "section": "",
    "text": "taxi_entrenar %&gt;% \n  ggplot(aes(x = hora, fill = propina)) +\n  geom_bar()"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-5",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-5",
    "title": "2 - Tu presupuesto de datos",
    "section": "",
    "text": "taxi_entrenar %&gt;% \n  ggplot(aes(x = hora, fill = propina)) +\n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-6",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-6",
    "title": "2 - Tu presupuesto de datos",
    "section": "",
    "text": "taxi_entrenar %&gt;% \n  ggplot(aes(x = distancia)) +\n  geom_histogram(bins = 100) +\n  facet_grid(vars(propina))"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-7",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#section-7",
    "title": "2 - Tu presupuesto de datos",
    "section": "",
    "text": "Estratificar la muestra separarÃ¡ los datos dentro los valores de respuesta\n\nBased on our EDA, we know that the source data contains fewer \"no\" tip values than \"yes\". We want to make sure we allot equal proportions of those responses so that both the training and testing data have enough of each to give accurate estimates."
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#estratificar",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#estratificar",
    "title": "2 - Tu presupuesto de datos",
    "section": "Estratificar",
    "text": "Estratificar\nUse strata = propina\n\nset.seed(123)\ntaxi_separar &lt;- initial_split(taxi, prop = 0.8, strata = propina)\ntaxi_separar\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;8000/2000/10000&gt;"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#estratificar-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#estratificar-1",
    "title": "2 - Tu presupuesto de datos",
    "section": "Estratificar",
    "text": "Estratificar\nEstratificar la muestra usualmente ayuda, y con pocas consecuencias"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#expectativas-del-taller---donde-estamos",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-02-presupuesto-de-datos.html#expectativas-del-taller---donde-estamos",
    "title": "2 - Tu presupuesto de datos",
    "section": "Expectativas del taller - Donde estamos",
    "text": "Expectativas del taller - Donde estamos\n\n\n\n\n\nflowchart LR\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts\n  rs[Remuestreo]\n  style rs fill:#fff,stroke:#eee,color:#ddd\n  tr --&gt; rs\n  lg[RegresiÃ³n\\nlogÃ­stica]\n  style lg fill:#fff,stroke:#eee,color:#ddd\n  rs --&gt; lg\n  dt[Arbol de\\nDecisiÃ³n]\n  style dt fill:#fff,stroke:#eee,color:#ddd\n  rs --&gt; dt\n  rf[Bosque\\nAleatorio]\n  style rf fill:#fff,stroke:#eee,color:#ddd\n  rs --&gt; rf\n  sm[Seleccionar\\nmodelo]\n  style sm fill:#fff,stroke:#eee,color:#ddd\n  lg --&gt; sm\n  dt --&gt; sm\n  rf --&gt; sm\n  fm[Entrenar modelo\\nselecionado]\n  style fm fill:#fff,stroke:#eee,color:#ddd\n  sm --&gt; fm\n  tr --&gt; fm\n  vm[Verificar la\\ncalidad]\n  style vm fill:#fff,stroke:#eee,color:#ddd\n  fm --&gt; vm\n  ts --&gt; vm"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-workflowsets.html#how-can-we-compare-multiple-model-workflows-at-once",
    "href": "archive/2023-09-posit-conf/intro-extra-workflowsets.html#how-can-we-compare-multiple-model-workflows-at-once",
    "title": "Extras - workflowsets",
    "section": "How can we compare multiple model workflows at once?",
    "text": "How can we compare multiple model workflows at once?"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-workflowsets.html#evaluate-a-workflow-set",
    "href": "archive/2023-09-posit-conf/intro-extra-workflowsets.html#evaluate-a-workflow-set",
    "title": "Extras - workflowsets",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(tip ~ .), list(tree_spec, rf_spec))\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result    \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-workflowsets.html#evaluate-a-workflow-set-1",
    "href": "archive/2023-09-posit-conf/intro-extra-workflowsets.html#evaluate-a-workflow-set-1",
    "title": "Extras - workflowsets",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(tip ~ .), list(tree_spec, rf_spec)) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = taxi_folds)\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result   \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-workflowsets.html#evaluate-a-workflow-set-2",
    "href": "archive/2023-09-posit-conf/intro-extra-workflowsets.html#evaluate-a-workflow-set-2",
    "title": "Extras - workflowsets",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(tip ~ .), list(tree_spec, rf_spec)) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = taxi_folds) %&gt;%\n  rank_results()\n#&gt; # A tibble: 4 Ã— 9\n#&gt;   wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n#&gt;   &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n#&gt; 1 formula_decisionâ€¦ Preproâ€¦ accuraâ€¦ 0.915 0.00309    10 formula      deciâ€¦     1\n#&gt; 2 formula_decisionâ€¦ Preproâ€¦ roc_auc 0.624 0.0105     10 formula      deciâ€¦     1\n#&gt; 3 formula_rand_forâ€¦ Preproâ€¦ accuraâ€¦ 0.924 0.00326    10 formula      randâ€¦     2\n#&gt; 4 formula_rand_forâ€¦ Preproâ€¦ roc_auc 0.615 0.0151     10 formula      randâ€¦     2\n\nThe first metric of the metric set is used for ranking. Use rank_metric to change that.\n\nLots more available with workflow sets, like collect_metrics(), autoplot() methods, and more!"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-workflowsets.html#your-turn",
    "href": "archive/2023-09-posit-conf/intro-extra-workflowsets.html#your-turn",
    "title": "Extras - workflowsets",
    "section": "Your turn",
    "text": "Your turn\n\nWhen do you think a workflow set would be useful?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#looking-at-the-predictors",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#looking-at-the-predictors",
    "title": "Extras - Recipes",
    "section": "Looking at the predictors",
    "text": "Looking at the predictors\n\ntaxi_train\n#&gt; # A tibble: 8,000 Ã— 7\n#&gt;    tip   distance company                      local dow   month  hour\n#&gt;    &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;                        &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n#&gt;  1 yes      17.2  Chicago Independents         no    Thu   Feb      16\n#&gt;  2 yes       0.88 City Service                 yes   Thu   Mar       8\n#&gt;  3 yes      18.1  other                        no    Mon   Feb      18\n#&gt;  4 yes      12.2  Chicago Independents         no    Sun   Mar      21\n#&gt;  5 yes       0.94 Sun Taxi                     yes   Sat   Apr      23\n#&gt;  6 yes      17.5  Flash Cab                    no    Fri   Mar      12\n#&gt;  7 yes      17.7  other                        no    Sun   Jan       6\n#&gt;  8 yes       1.85 Taxicab Insurance Agency Llc no    Fri   Apr      12\n#&gt;  9 yes       0.53 Sun Taxi                     no    Tue   Mar      18\n#&gt; 10 yes       6.65 Taxicab Insurance Agency Llc no    Sun   Apr      11\n#&gt; # â„¹ 7,990 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#working-with-other-models",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#working-with-other-models",
    "title": "Extras - Recipes",
    "section": "Working with other models",
    "text": "Working with other models\nSome models canâ€™t handle non-numeric data\n\nLinear Regression\nK Nearest Neighbors\n\n\n\nSome models struggle if numeric predictors arenâ€™t scaled\n\nK Nearest Neighbors\nAnything using gradient descent"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#types-of-needed-preprocessing",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#types-of-needed-preprocessing",
    "title": "Extras - Recipes",
    "section": "Types of needed preprocessing",
    "text": "Types of needed preprocessing\n\nDo qualitative predictors require a numeric encoding?\nShould columns with a single unique value be removed?\nDoes the model struggle with missing data?\nDoes the model struggle with correlated predictors?\nShould predictors be centered and scaled?\nIs it helpful to transform predictors to be more symmetric?\n\n\nhttps://www.tmwr.org/pre-proc-table.html"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#two-types-of-preprocessing",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#two-types-of-preprocessing",
    "title": "Extras - Recipes",
    "section": "Two types of preprocessing",
    "text": "Two types of preprocessing"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#two-types-of-preprocessing-1",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#two-types-of-preprocessing-1",
    "title": "Extras - Recipes",
    "section": "Two types of preprocessing",
    "text": "Two types of preprocessing"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#general-definitions",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#general-definitions",
    "title": "Extras - Recipes",
    "section": "General definitions",
    "text": "General definitions\n\nData preprocessing are the steps that you take to make your model successful.\nFeature engineering are what you do to the original predictors to make the model do the least work to perform great."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#working-with-dates",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#working-with-dates",
    "title": "Extras - Recipes",
    "section": "Working with dates",
    "text": "Working with dates\nDatetime variables are automatically converted to an integer if given as a raw predictor. To avoid this, it can be re-encoded as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nLeap year\nIndicators for holidays"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#your-turn",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#your-turn",
    "title": "Extras - Recipes",
    "section": "Your turn",
    "text": "Your turn\n\n\nWhat other transformations could we do with the raw time variable?\nRemember that the transformations are tied to the specific modeling problem.\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#two-types-of-transformations",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#two-types-of-transformations",
    "title": "Extras - Recipes",
    "section": "Two types of transformations",
    "text": "Two types of transformations\n\n\n\nStatic\n\nSquare root, log, inverse\nDummies for known levels\nDate time extractions\n\n\nTrained\n\nCentering & scaling\nImputation\nPCA\nAnything for unknown factor levels\n\n\n\nTrained methods need to calculate sufficient information to be applied again."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#the-recipes-package",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#the-recipes-package",
    "title": "Extras - Recipes",
    "section": "The recipes package",
    "text": "The recipes package\n\n\nModular + extensible\nWorks well with pipes ,|&gt; and %&gt;%\nDeferred evaluation\nIsolates test data from training data\nCan do things formulas canâ€™t"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\ntaxi_rec &lt;- recipe(tip ~ ., data = taxi_train) %&gt;%\nÂ Â step_unknown(all_nominal_predictors()) %&gt;%\nÂ Â step_dummy(all_nominal_predictors()) %&gt;%\nÂ Â step_zv(all_predictors()) %&gt;%\nÂ Â step_log(distance, offset = 0.5) %&gt;%\nÂ Â step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe-1",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe-1",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\ntaxi_rec &lt;- recipe(tip ~ ., data = taxi_train) %&gt;%\nÂ Â step_unknown(all_nominal_predictors()) %&gt;%\nÂ Â step_dummy(all_nominal_predictors()) %&gt;%\nÂ Â step_zv(all_predictors()) %&gt;%\nÂ Â step_log(distance, offset = 0.5) %&gt;%\nÂ Â step_normalize(all_numeric_predictors())\n\n\nStart by calling recipe() to denote the data source and variables used."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe-2",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe-2",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\ntaxi_rec &lt;- recipe(tip ~ ., data = taxi_train) %&gt;%\nÂ Â step_unknown(all_nominal_predictors()) %&gt;%\nÂ Â step_dummy(all_nominal_predictors()) %&gt;%\nÂ Â step_zv(all_predictors()) %&gt;%\nÂ Â step_log(distance, offset = 0.5) %&gt;%\nÂ Â step_normalize(all_numeric_predictors())\n\n\nSpecify what actions to take by adding step_*()s."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe-3",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#how-to-write-a-recipe-3",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\ntaxi_rec &lt;- recipe(tip ~ ., data = taxi_train) %&gt;%\nÂ Â step_unknown(all_nominal_predictors()) %&gt;%\nÂ Â step_dummy(all_nominal_predictors()) %&gt;%\nÂ Â step_zv(all_predictors()) %&gt;%\nÂ Â step_log(distance, offset = 0.5) %&gt;% Â Â step_normalize(all_numeric_predictors())\n\n\nUse {tidyselect} and recipes-specific selectors to denote affected variables."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#using-a-recipe",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#using-a-recipe",
    "title": "Extras - Recipes",
    "section": "Using a recipe",
    "text": "Using a recipe\n\ntaxi_rec &lt;- recipe(tip ~ ., data = taxi_train) %&gt;%\nÂ Â step_unknown(all_nominal_predictors()) %&gt;%\nÂ Â step_dummy(all_nominal_predictors()) %&gt;%\nÂ Â step_zv(all_predictors()) %&gt;%\nÂ Â step_log(distance, offset = 0.5) %&gt;% Â Â step_normalize(all_numeric_predictors())\n\n\nSave the recipe we like so that we can use it in various places, e.g., with different models."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#using-a-recipe-with-workflows",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#using-a-recipe-with-workflows",
    "title": "Extras - Recipes",
    "section": "Using a recipe with workflows",
    "text": "Using a recipe with workflows\nRecipes are typically combined with a model in a workflow() object:\n\n\ntaxi_wflow &lt;- workflow() %&gt;%\nÂ Â add_recipe(taxi_rec) %&gt;%\nÂ Â add_model(linear_reg())"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#recipes-are-estimated",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#recipes-are-estimated",
    "title": "Extras - Recipes",
    "section": "Recipes are estimated",
    "text": "Recipes are estimated\nEvery preprocessing step in a recipe that involved calculations uses the training set. For example:\n\nLevels of a factor\nDetermination of zero-variance\nNormalization\nFeature extraction\n\nOnce a recipe is added to a workflow, this occurs when fit() is called."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#debugging-a-recipe",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#debugging-a-recipe",
    "title": "Extras - Recipes",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g.Â taxi_rec) can be estimated manually with a function called prep(). It is analogous to fit(). See TMwR section 16.4.\n\n\n\n\nAnother function, bake(), is analogous to predict(), and gives you the processed data back."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#your-turn-1",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#your-turn-1",
    "title": "Extras - Recipes",
    "section": "Your turn",
    "text": "Your turn\n\n\nTake the recipe and prep() then bake() it to see what the resulting data set looks like.\nTry removing steps to see how the result changes.\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#printing-a-recipe",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#printing-a-recipe",
    "title": "Extras - Recipes",
    "section": "Printing a recipe",
    "text": "Printing a recipe\n\ntaxi_rec\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 6\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Unknown factor level assignment for: all_nominal_predictors()\n#&gt; â€¢ Dummy variables from: all_nominal_predictors()\n#&gt; â€¢ Zero variance filter on: all_predictors()\n#&gt; â€¢ Log transformation on: distance\n#&gt; â€¢ Centering and scaling for: all_numeric_predictors()"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#prepping-a-recipe",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#prepping-a-recipe",
    "title": "Extras - Recipes",
    "section": "Prepping a recipe",
    "text": "Prepping a recipe\n\nprep(taxi_rec)\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 6\n#&gt; \n#&gt; â”€â”€ Training information\n#&gt; Training data contained 8000 data points and no incomplete rows.\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Unknown factor level assignment for: company and local, ... |\n#&gt;   Trained\n#&gt; â€¢ Dummy variables from: company, local, dow, month | Trained\n#&gt; â€¢ Zero variance filter removed: company_unknown, ... | Trained\n#&gt; â€¢ Log transformation on: distance | Trained\n#&gt; â€¢ Centering and scaling for: distance and hour, ... | Trained"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#baking-a-recipe",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#baking-a-recipe",
    "title": "Extras - Recipes",
    "section": "Baking a recipe",
    "text": "Baking a recipe\n\nprep(taxi_rec) %&gt;%\n  bake(new_data = taxi_train)\n#&gt; # A tibble: 8,000 Ã— 19\n#&gt;    distance   hour tip   company_City.Service company_Flash.Cab company_Sun.Taxi company_Taxi.Affiliatioâ€¦Â¹ company_Taxicab.Insuâ€¦Â² company_other local_no dow_Mon dow_Tue dow_Wed dow_Thu dow_Fri dow_Sat\n#&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;                &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;                     &lt;dbl&gt;                  &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1    1.38   0.418 yes                 -0.366            -0.333           -0.403                    -0.450                 -0.379        -0.609    0.484  -0.396  -0.441  -0.461   2.01   -0.428  -0.317\n#&gt;  2   -0.729 -1.42  yes                  2.73             -0.333           -0.403                    -0.450                 -0.379        -0.609   -2.07   -0.396  -0.441  -0.461   2.01   -0.428  -0.317\n#&gt;  3    1.42   0.877 yes                 -0.366            -0.333           -0.403                    -0.450                 -0.379         1.64     0.484   2.53   -0.441  -0.461  -0.497  -0.428  -0.317\n#&gt;  4    1.11   1.57  yes                 -0.366            -0.333           -0.403                    -0.450                 -0.379        -0.609    0.484  -0.396  -0.441  -0.461  -0.497  -0.428  -0.317\n#&gt;  5   -0.694  2.03  yes                 -0.366            -0.333            2.48                     -0.450                 -0.379        -0.609   -2.07   -0.396  -0.441  -0.461  -0.497  -0.428   3.15 \n#&gt;  6    1.39  -0.502 yes                 -0.366             3.01            -0.403                    -0.450                 -0.379        -0.609    0.484  -0.396  -0.441  -0.461  -0.497   2.34   -0.317\n#&gt;  7    1.40  -1.88  yes                 -0.366            -0.333           -0.403                    -0.450                 -0.379         1.64     0.484  -0.396  -0.441  -0.461  -0.497  -0.428  -0.317\n#&gt;  8   -0.289 -0.502 yes                 -0.366            -0.333           -0.403                    -0.450                  2.64         -0.609    0.484  -0.396  -0.441  -0.461  -0.497   2.34   -0.317\n#&gt;  9   -0.971  0.877 yes                 -0.366            -0.333            2.48                     -0.450                 -0.379        -0.609    0.484  -0.396   2.27   -0.461  -0.497  -0.428  -0.317\n#&gt; 10    0.631 -0.732 yes                 -0.366            -0.333           -0.403                    -0.450                  2.64         -0.609    0.484  -0.396  -0.441  -0.461  -0.497  -0.428  -0.317\n#&gt; # â„¹ 7,990 more rows\n#&gt; # â„¹ abbreviated names: Â¹â€‹company_Taxi.Affiliation.Services, Â²â€‹company_Taxicab.Insurance.Agency.Llc\n#&gt; # â„¹ 3 more variables: month_Feb &lt;dbl&gt;, month_Mar &lt;dbl&gt;, month_Apr &lt;dbl&gt;"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#tidying-a-recipe",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#tidying-a-recipe",
    "title": "Extras - Recipes",
    "section": "Tidying a recipe",
    "text": "Tidying a recipe\nOnce a recipe as been estimated, there are various bits of information saved in it.\n\nThe tidy() function can be used to get specific results from the recipe."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#your-turn-2",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#your-turn-2",
    "title": "Extras - Recipes",
    "section": "Your turn",
    "text": "Your turn\n\nTake a prepped recipe and use the tidy() function on it.\nUse the number argument to inspect different steps.\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#tidying-a-recipe-1",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#tidying-a-recipe-1",
    "title": "Extras - Recipes",
    "section": "Tidying a recipe",
    "text": "Tidying a recipe\n\nprep(taxi_rec) %&gt;%\n  tidy()\n#&gt; # A tibble: 5 Ã— 6\n#&gt;   number operation type      trained skip  id             \n#&gt;    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n#&gt; 1      1 step      unknown   TRUE    FALSE unknown_NTmu5  \n#&gt; 2      2 step      dummy     TRUE    FALSE dummy_cT3Uy    \n#&gt; 3      3 step      zv        TRUE    FALSE zv_z22dk       \n#&gt; 4      4 step      log       TRUE    FALSE log_QQ1iw      \n#&gt; 5      5 step      normalize TRUE    FALSE normalize_3cTJb"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#tidying-a-recipe-2",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#tidying-a-recipe-2",
    "title": "Extras - Recipes",
    "section": "Tidying a recipe",
    "text": "Tidying a recipe\n\nprep(taxi_rec) %&gt;%\n  tidy(number = 2)\n#&gt; # A tibble: 20 Ã— 3\n#&gt;    terms   columns                      id         \n#&gt;    &lt;chr&gt;   &lt;chr&gt;                        &lt;chr&gt;      \n#&gt;  1 company City Service                 dummy_cT3Uy\n#&gt;  2 company Flash Cab                    dummy_cT3Uy\n#&gt;  3 company Sun Taxi                     dummy_cT3Uy\n#&gt;  4 company Taxi Affiliation Services    dummy_cT3Uy\n#&gt;  5 company Taxicab Insurance Agency Llc dummy_cT3Uy\n#&gt;  6 company other                        dummy_cT3Uy\n#&gt;  7 company unknown                      dummy_cT3Uy\n#&gt;  8 local   no                           dummy_cT3Uy\n#&gt;  9 local   unknown                      dummy_cT3Uy\n#&gt; 10 dow     Mon                          dummy_cT3Uy\n#&gt; 11 dow     Tue                          dummy_cT3Uy\n#&gt; 12 dow     Wed                          dummy_cT3Uy\n#&gt; 13 dow     Thu                          dummy_cT3Uy\n#&gt; 14 dow     Fri                          dummy_cT3Uy\n#&gt; 15 dow     Sat                          dummy_cT3Uy\n#&gt; 16 dow     unknown                      dummy_cT3Uy\n#&gt; 17 month   Feb                          dummy_cT3Uy\n#&gt; 18 month   Mar                          dummy_cT3Uy\n#&gt; 19 month   Apr                          dummy_cT3Uy\n#&gt; 20 month   unknown                      dummy_cT3Uy"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#using-a-recipe-in-tidymodels",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#using-a-recipe-in-tidymodels",
    "title": "Extras - Recipes",
    "section": "Using a recipe in tidymodels",
    "text": "Using a recipe in tidymodels\nThe recommended way to use a recipe in tidymodels is to use it as part of a workflow().\n\ntaxi_wflow &lt;- workflow() %&gt;%  \n  add_recipe(taxi_rec) %&gt;%  \n  add_model(linear_reg())\n\nWhen used in this way, you donâ€™t need to worry about prep() and bake() as it is handled for you."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-recipes.html#more-information",
    "href": "archive/2023-09-posit-conf/intro-extra-recipes.html#more-information",
    "title": "Extras - Recipes",
    "section": "More information",
    "text": "More information\n\nhttps://recipes.tidymodels.org/\nhttps://www.tmwr.org/recipes.html"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-05-tuning-models.html#tuning-parameters",
    "href": "archive/2023-09-posit-conf/intro-05-tuning-models.html#tuning-parameters",
    "title": "5 - Tuning models",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-05-tuning-models.html#optimize-tuning-parameters",
    "href": "archive/2023-09-posit-conf/intro-05-tuning-models.html#optimize-tuning-parameters",
    "title": "5 - Tuning models",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-05-tuning-models.html#optimize-tuning-parameters-1",
    "href": "archive/2023-09-posit-conf/intro-05-tuning-models.html#optimize-tuning-parameters-1",
    "title": "5 - Tuning models",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search ğŸ’  which tests a pre-defined set of candidate values\nIterative search ğŸŒ€ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-05-tuning-models.html#specifying-tuning-parameters",
    "href": "archive/2023-09-posit-conf/intro-05-tuning-models.html#specifying-tuning-parameters",
    "title": "5 - Tuning models",
    "section": "Specifying tuning parameters",
    "text": "Specifying tuning parameters\nLetâ€™s take our previous random forest workflow and tag for tuning the minimum number of data points in each node:\n\nrf_spec &lt;- rand_forest(min_n = tune()) %&gt;% \n  set_mode(\"classification\")\n\nrf_wflow &lt;- workflow(tip ~ ., rf_spec)\nrf_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; tip ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   min_n = tune()\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-05-tuning-models.html#try-out-multiple-values",
    "href": "archive/2023-09-posit-conf/intro-05-tuning-models.html#try-out-multiple-values",
    "title": "5 - Tuning models",
    "section": "Try out multiple values",
    "text": "Try out multiple values\ntune_grid() works similar to fit_resamples() but covers multiple parameter values:\n\nset.seed(22)\nrf_res &lt;- tune_grid(\n  rf_wflow,\n  taxi_folds,\n  grid = 5\n)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-05-tuning-models.html#compare-results",
    "href": "archive/2023-09-posit-conf/intro-05-tuning-models.html#compare-results",
    "title": "5 - Tuning models",
    "section": "Compare results",
    "text": "Compare results\nInspecting results and selecting the best-performing hyperparameter(s):\n\nshow_best(rf_res)\n#&gt; # A tibble: 5 Ã— 7\n#&gt;   min_n .metric .estimator  mean     n std_err .config             \n#&gt;   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1    33 roc_auc binary     0.623    10  0.0149 Preprocessor1_Model1\n#&gt; 2    31 roc_auc binary     0.622    10  0.0154 Preprocessor1_Model3\n#&gt; 3    21 roc_auc binary     0.620    10  0.0149 Preprocessor1_Model4\n#&gt; 4    13 roc_auc binary     0.617    10  0.0137 Preprocessor1_Model5\n#&gt; 5     6 roc_auc binary     0.611    10  0.0156 Preprocessor1_Model2\n\nbest_parameter &lt;- select_best(rf_res)\nbest_parameter\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   min_n .config             \n#&gt;   &lt;int&gt; &lt;chr&gt;               \n#&gt; 1    33 Preprocessor1_Model1\n\ncollect_metrics() and autoplot() are also available."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-05-tuning-models.html#the-final-fit",
    "href": "archive/2023-09-posit-conf/intro-05-tuning-models.html#the-final-fit",
    "title": "5 - Tuning models",
    "section": "The final fit",
    "text": "The final fit\n\nrf_wflow &lt;- finalize_workflow(rf_wflow, best_parameter)\n\nfinal_fit &lt;- last_fit(rf_wflow, taxi_split) \n\ncollect_metrics(final_fit)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric  .estimator .estimate .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary         0.913 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary         0.648 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-05-tuning-models.html#your-turn",
    "href": "archive/2023-09-posit-conf/intro-05-tuning-models.html#your-turn",
    "title": "5 - Tuning models",
    "section": "Your turn",
    "text": "Your turn\n\nModify your model workflow to tune one or more parameters.\nUse grid search to find the best parameter(s).\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#your-turn",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#your-turn",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nHow do you fit a linear model in R?\nHow many different ways can you think of?\n\n\n\nâˆ’+\n03:00\n\n\n\n\n\nlm for linear model\nglm for generalized linear model (e.g.Â logistic regression)\nglmnet for regularized regression\nkeras for regression using TensorFlow\nstan for Bayesian regression\nspark for large data sets"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-1",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-1",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg()\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\n\nModels have default engines"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-2",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-2",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-3",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-3",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg() %&gt;%\n  set_engine(\"glmnet\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glmnet"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-4",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-4",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg() %&gt;%\n  set_engine(\"stan\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: stan"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-5",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-5",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-6",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-6",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree()\n#&gt; Decision Tree Model Specification (unknown mode)\n#&gt; \n#&gt; Computational engine: rpart\n\n\nSome models have a default mode"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-7",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-7",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree() %&gt;% \n  set_mode(\"classification\")\n#&gt; Decision Tree Model Specification (classification)\n#&gt; \n#&gt; Computational engine: rpart\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-8",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#to-specify-a-model-8",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#your-turn-1",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#your-turn-1",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_spec chunk in your .qmd.\nEdit this code to use a logistic regression model.\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/\n\n\nExtension/Challenge: Edit this code to use a different model. For example, try using a conditional inference tree as implemented in the partykit package by changing the engine - or try an entirely different model type!\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#models-well-be-using-today",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#models-well-be-using-today",
    "title": "3 - What makes a model?",
    "section": "Models weâ€™ll be using today",
    "text": "Models weâ€™ll be using today\n\nLogistic regression\nDecision trees"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#logistic-regression",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#logistic-regression",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#logistic-regression-1",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#logistic-regression-1",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#logistic-regression-2",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#logistic-regression-2",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogit of outcome probability modeled as linear combination of predictors:\n\n\\(log(\\frac{p}{1 - p}) = \\beta_0 + \\beta_1\\cdot \\text{A}\\)\n\nFind a sigmoid line that separates the two classes"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#decision-trees",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#decision-trees",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#decision-trees-1",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#decision-trees-1",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#decision-trees-2",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#decision-trees-2",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "title": "3 - What makes a model?",
    "section": "All models are wrong, but some are useful!",
    "text": "All models are wrong, but some are useful!\n\n\nLogistic regression\n\n\n\n\n\n\n\n\n\n\nDecision trees"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "title": "3 - What makes a model?",
    "section": "Workflows bind preprocessors and models",
    "text": "Workflows bind preprocessors and models\n\n\nExplain that PCA that is a preprocessor / dimensionality reduction, used to decorrelate data"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#what-is-wrong-with-this",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#what-is-wrong-with-this",
    "title": "3 - What makes a model?",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#why-a-workflow",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#why-a-workflow",
    "title": "3 - What makes a model?",
    "section": "Why a workflow()? ",
    "text": "Why a workflow()? \n\n\nWorkflows handle new data better than base R tools in terms of new factor levels\n\n\n\n\nYou can use other preprocessors besides formulas (more on feature engineering in Advanced tidymodels!)\n\n\n\n\nThey can help organize your work when working with multiple models\n\n\n\n\nMost importantly, a workflow captures the entire modeling process: fit() and predict() apply to the preprocessing steps in addition to the actual model fit\n\n\nTwo ways workflows handle levels better than base R:\n\nEnforces that new levels are not allowed at prediction time (this is an optional check that can be turned off)\nRestores missing levels that were present at fit time, but happen to be missing at prediction time (like, if your â€œnewâ€ data just doesnâ€™t have an instance of that level)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#a-model-workflow-1",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#a-model-workflow-1",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree(cost_complexity = 0.002) %&gt;% \n  set_mode(\"classification\")\n\ntree_spec %&gt;% \n  fit(tip ~ ., data = taxi_train) \n#&gt; parsnip model object\n#&gt; \n#&gt; n= 8000 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 8000 616 yes (0.92300000 0.07700000)  \n#&gt;    2) distance&gt;=14.12 2041  68 yes (0.96668300 0.03331700) *\n#&gt;    3) distance&lt; 14.12 5959 548 yes (0.90803826 0.09196174)  \n#&gt;      6) distance&lt; 5.275 5419 450 yes (0.91695885 0.08304115) *\n#&gt;      7) distance&gt;=5.275 540  98 yes (0.81851852 0.18148148)  \n#&gt;       14) company=Chicago Independents,City Service,Sun Taxi,Taxi Affiliation Services,Taxicab Insurance Agency Llc,other 478  68 yes (0.85774059 0.14225941) *\n#&gt;       15) company=Flash Cab 62  30 yes (0.51612903 0.48387097)  \n#&gt;         30) dow=Thu 12   2 yes (0.83333333 0.16666667) *\n#&gt;         31) dow=Sun,Mon,Tue,Wed,Fri,Sat 50  22 no (0.44000000 0.56000000)  \n#&gt;           62) distance&gt;=11.77 14   4 yes (0.71428571 0.28571429) *\n#&gt;           63) distance&lt; 11.77 36  12 no (0.33333333 0.66666667) *"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#a-model-workflow-2",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#a-model-workflow-2",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree(cost_complexity = 0.002) %&gt;% \n  set_mode(\"classification\")\n\nworkflow() %&gt;%\n  add_formula(tip ~ .) %&gt;%\n  add_model(tree_spec) %&gt;%\n  fit(data = taxi_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; tip ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 8000 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 8000 616 yes (0.92300000 0.07700000)  \n#&gt;    2) distance&gt;=14.12 2041  68 yes (0.96668300 0.03331700) *\n#&gt;    3) distance&lt; 14.12 5959 548 yes (0.90803826 0.09196174)  \n#&gt;      6) distance&lt; 5.275 5419 450 yes (0.91695885 0.08304115) *\n#&gt;      7) distance&gt;=5.275 540  98 yes (0.81851852 0.18148148)  \n#&gt;       14) company=Chicago Independents,City Service,Sun Taxi,Taxi Affiliation Services,Taxicab Insurance Agency Llc,other 478  68 yes (0.85774059 0.14225941) *\n#&gt;       15) company=Flash Cab 62  30 yes (0.51612903 0.48387097)  \n#&gt;         30) dow=Thu 12   2 yes (0.83333333 0.16666667) *\n#&gt;         31) dow=Sun,Mon,Tue,Wed,Fri,Sat 50  22 no (0.44000000 0.56000000)  \n#&gt;           62) distance&gt;=11.77 14   4 yes (0.71428571 0.28571429) *\n#&gt;           63) distance&lt; 11.77 36  12 no (0.33333333 0.66666667) *"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#a-model-workflow-3",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#a-model-workflow-3",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree(cost_complexity = 0.002) %&gt;% \n  set_mode(\"classification\")\n\nworkflow(tip ~ ., tree_spec) %&gt;% \n  fit(data = taxi_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; tip ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 8000 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 8000 616 yes (0.92300000 0.07700000)  \n#&gt;    2) distance&gt;=14.12 2041  68 yes (0.96668300 0.03331700) *\n#&gt;    3) distance&lt; 14.12 5959 548 yes (0.90803826 0.09196174)  \n#&gt;      6) distance&lt; 5.275 5419 450 yes (0.91695885 0.08304115) *\n#&gt;      7) distance&gt;=5.275 540  98 yes (0.81851852 0.18148148)  \n#&gt;       14) company=Chicago Independents,City Service,Sun Taxi,Taxi Affiliation Services,Taxicab Insurance Agency Llc,other 478  68 yes (0.85774059 0.14225941) *\n#&gt;       15) company=Flash Cab 62  30 yes (0.51612903 0.48387097)  \n#&gt;         30) dow=Thu 12   2 yes (0.83333333 0.16666667) *\n#&gt;         31) dow=Sun,Mon,Tue,Wed,Fri,Sat 50  22 no (0.44000000 0.56000000)  \n#&gt;           62) distance&gt;=11.77 14   4 yes (0.71428571 0.28571429) *\n#&gt;           63) distance&lt; 11.77 36  12 no (0.33333333 0.66666667) *"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#your-turn-2",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#your-turn-2",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_wflow chunk in your .qmd.\nEdit this code to make a workflow with your own model of choice.\n\nExtension/Challenge: Other than formulas, what kinds of preprocessors are supported?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#predict-with-your-model",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#predict-with-your-model",
    "title": "3 - What makes a model?",
    "section": "Predict with your model  ",
    "text": "Predict with your model  \nHow do you use your new tree_fit model?\n\ntree_spec &lt;-\n  decision_tree(cost_complexity = 0.002) %&gt;% \n  set_mode(\"classification\")\n\ntree_fit &lt;-\n  workflow(tip ~ ., tree_spec) %&gt;% \n  fit(data = taxi_train)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#your-turn-3",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#your-turn-3",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\npredict(tree_fit, new_data = taxi_test)\nWhat do you get?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#your-turn-4",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#your-turn-4",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\naugment(tree_fit, new_data = taxi_test)\nWhat do you get?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#understand-your-model",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#understand-your-model",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#understand-your-model-1",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#understand-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nlibrary(rpart.plot)\ntree_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot(roundint = FALSE)\n\nYou can extract_*() several components of your fitted workflow.\n\nâš ï¸ Never predict() with any extracted components!\n\nroundint = FALSE is only to quiet a warning"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#understand-your-model-2",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#understand-your-model-2",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nYou can use your fitted workflow for model and/or prediction explanations:\n\n\n\noverall variable importance, such as with the vip package\n\n\n\n\nflexible model explainers, such as with the DALEXtra package\n\n\n\nLearn more at https://www.tmwr.org/explain.html"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#your-turn-5",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#your-turn-5",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\n\nExtract the model engine object from your fitted workflow and check it out.\n\n\n\nâˆ’+\n05:00\n\n\n\n\nAfterward, ask what kind of object people got from the extraction, and what they did with it (e.g.Â give it to summary(), plot(), broom::tidy() ). Live code along"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#the-whole-game---status-update",
    "href": "archive/2023-09-posit-conf/intro-03-what-makes-a-model.html#the-whole-game---status-update",
    "title": "3 - What makes a model?",
    "section": "The whole game - status update",
    "text": "The whole game - status update\n\n\nStress that fitting a model on the entire training set was only for illustrating how to fit a model"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#workshop-policies",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#workshop-policies",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease do not photograph people wearing red lanyards\nThere are gender-neutral bathrooms located are among the Grand Suite Bathrooms\nThere are two meditation/prayer rooms: Grand Suite 2A and 2B\nA lactation room is located in Grand Suite 1\nThe meditation/prayer and lactation rooms are open\nSun - Tue 7:30am - 7:00pm, Wed 8:00am - 6:00pm"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#workshop-policies-1",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#workshop-policies-1",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease review the code of conduct and COVID policies, which apply to all workshops: https://posit.co/code-of-conduct/.\nCoC site has info on how to report a problem (in person, email, phone)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#who-are-you",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %&gt;% or base R |&gt; pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have exposure to basic statistical concepts\nYou do not need intermediate or expert familiarity with modeling or ML"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#who-are-tidymodels",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#who-are-tidymodels",
    "title": "1 - Introduction",
    "section": "Who are tidymodels?",
    "text": "Who are tidymodels?\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\nSilvia CanelÃ³n is TAing today!\n\n\nMany thanks to Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill, and DesirÃ©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#asking-for-help",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#asking-for-help",
    "title": "1 - Introduction",
    "section": "Asking for help",
    "text": "Asking for help\n\nğŸŸª â€œIâ€™m stuck and need help!â€\n\n\nğŸŸ© â€œI finished the exerciseâ€"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#section-2",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#section-2",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#plan-for-this-workshop",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Plan for this workshop",
    "text": "Plan for this workshop\n\nYour data budget\nWhat makes a model\nEvaluating models\nTuning models\n\n\nThis workshop will well-prepare folks going on to the Advanced tidymodels workshop, which will cover feature engineering and much more on hyperparameter tuning."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#section-3",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#section-3",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors ğŸ‘‹\n\n Log in to Posit Cloud (free):\nCheck the workshop channel on Discord for the link!"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#what-is-machine-learning",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#what-is-machine-learning",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nhttps://xkcd.com/1838/"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#what-is-machine-learning-1",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#what-is-machine-learning-1",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#what-is-machine-learning-2",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#what-is-machine-learning-2",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#your-turn",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\n\n\nHow are statistics and machine learning related?\nHow are they similar? Different?\n\n\n\nâˆ’+\n03:00\n\n\n\n\nthe â€œtwo culturesâ€\nmodel first vs.Â data first\ninference vs.Â prediction"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#what-is-tidymodels",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#what-is-tidymodels",
    "title": "1 - Introduction",
    "section": "What is tidymodels? ",
    "text": "What is tidymodels? \n\nlibrary(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.1.1 â”€â”€\n#&gt; âœ” broom        1.0.5     âœ” rsample      1.2.0\n#&gt; âœ” dials        1.2.0     âœ” tibble       3.2.1\n#&gt; âœ” dplyr        1.1.3     âœ” tidyr        1.3.0\n#&gt; âœ” infer        1.0.5     âœ” tune         1.1.2\n#&gt; âœ” modeldata    1.2.0     âœ” workflows    1.1.3\n#&gt; âœ” parsnip      1.1.1     âœ” workflowsets 1.0.1\n#&gt; âœ” purrr        1.0.2     âœ” yardstick    1.2.0\n#&gt; âœ” recipes      1.0.8\n#&gt; â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\n#&gt; âœ– purrr::discard() masks scales::discard()\n#&gt; âœ– dplyr::filter()  masks stats::filter()\n#&gt; âœ– dplyr::lag()     masks stats::lag()\n#&gt; âœ– recipes::step()  masks stats::step()\n#&gt; â€¢ Learn how to get started at https://www.tidymodels.org/start/"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\nRoadmap for today\nMinimal version of predictive modeling process\nFeature engineering and tuning as iterative extensions"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-1",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-1",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-2",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-2",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\n\nStress that we are not fitting a model on the entire training set other than for illustrative purposes in deck 2."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-3",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-3",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-4",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-4",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-5",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-5",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-6",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-6",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-7",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#the-whole-game-7",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#lets-install-some-packages",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Letâ€™s install some packages",
    "text": "Letâ€™s install some packages\nIf you are using your own laptop instead of Posit Cloud:\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"bonsai\", \"doParallel\", \"embed\", \"finetune\", \"lightgbm\", \"lme4\",\n    \"plumber\", \"probably\", \"ranger\", \"rpart\", \"rpart.plot\", \"rules\",\n    \"splines2\", \"stacks\", \"text2vec\", \"textrecipes\", \"tidymodels\", \n    \"vetiver\", \"remotes\")\n\ninstall.packages(pkgs)\n\n\n\n Or log in to Posit Cloud\nLink in our Discord channel!"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-01-introduction.html#our-versions",
    "href": "archive/2023-09-posit-conf/intro-01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\nR version 4.2.2 (2022-10-31), Quarto (1.4.104)\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nbonsai\n0.2.1\n\n\nbroom\n1.0.5\n\n\ndials\n1.2.0\n\n\ndoParallel\n1.0.17\n\n\ndplyr\n1.1.3\n\n\nembed\n1.1.2\n\n\nfinetune\n1.1.0\n\n\nggplot2\n3.4.3\n\n\nlightgbm\n3.3.5\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nlme4\n1.1-34\n\n\nmodeldata\n1.2.0\n\n\nparsnip\n1.1.1\n\n\nplumber\n1.2.1\n\n\nprobably\n1.0.2\n\n\npurrr\n1.0.2\n\n\nranger\n0.15.1\n\n\nrecipes\n1.0.8\n\n\nremotes\n2.4.2.1\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nrpart\n4.1.19\n\n\nrpart.plot\n3.1.1\n\n\nrsample\n1.2.0\n\n\nrules\n1.0.2\n\n\nscales\n1.2.1\n\n\nsplines2\n0.5.1\n\n\nstacks\n1.0.2\n\n\ntext2vec\n0.6.3\n\n\ntextrecipes\n1.0.4\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\ntibble\n3.2.1\n\n\ntidymodels\n1.1.1\n\n\ntidyr\n1.3.0\n\n\ntune\n1.1.2\n\n\nvetiver\n0.2.4\n\n\nworkflows\n1.1.3\n\n\nworkflowsets\n1.0.1\n\n\nyardstick\n1.2.0"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-vetiver.html#deploying-a-model",
    "href": "archive/2023-09-posit-conf/extras-vetiver.html#deploying-a-model",
    "title": "Extras - Model deployment",
    "section": "Deploying a model ",
    "text": "Deploying a model \nWe have a decision tree, tree_fit, to model whether or not a taxi trip in Chicago included a tip or not.\nHow do we use our model in production?\n\nlibrary(vetiver)\nv &lt;- vetiver_model(tree_fit, \"taxi\")\nv\n#&gt; \n#&gt; â”€â”€ taxi â”€ &lt;bundled_workflow&gt; model for deployment \n#&gt; A rpart classification modeling workflow using 6 features\n\nLearn more at https://vetiver.rstudio.com"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-vetiver.html#deploy-your-model-1",
    "href": "archive/2023-09-posit-conf/extras-vetiver.html#deploy-your-model-1",
    "title": "Extras - Model deployment",
    "section": "Deploy your model ",
    "text": "Deploy your model \nHow do we use our model in production?\n\nlibrary(plumber)\npr() %&gt;%\n  vetiver_api(v)\n#&gt; # Plumber router with 4 endpoints, 4 filters, and 1 sub-router.\n#&gt; # Use `pr_run()` on this object to start the API.\n#&gt; â”œâ”€â”€[queryString]\n#&gt; â”œâ”€â”€[body]\n#&gt; â”œâ”€â”€[cookieParser]\n#&gt; â”œâ”€â”€[sharedSecret]\n#&gt; â”œâ”€â”€/logo\n#&gt; â”‚  â”‚ # Plumber static router serving from directory: /Users/simoncouch/Library/R/arm64/4.2/library/vetiver\n#&gt; â”œâ”€â”€/metadata (GET)\n#&gt; â”œâ”€â”€/ping (GET)\n#&gt; â”œâ”€â”€/predict (POST)\n#&gt; â””â”€â”€/prototype (GET)\n\nLearn more at https://vetiver.rstudio.com\n\nLive-code making a prediction"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-vetiver.html#your-turn",
    "href": "archive/2023-09-posit-conf/extras-vetiver.html#your-turn",
    "title": "Extras - Model deployment",
    "section": "Your turn",
    "text": "Your turn\n\nRun the vetiver chunk in your .qmd.\nCheck out the automated visual documentation.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-effect-encodings.html#previously---setup",
    "href": "archive/2023-09-posit-conf/extras-effect-encodings.html#previously---setup",
    "title": "Extras - Effect Encodings",
    "section": "Previously - Setup",
    "text": "Previously - Setup\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-effect-encodings.html#previously---data-usage",
    "href": "archive/2023-09-posit-conf/extras-effect-encodings.html#previously---data-usage",
    "title": "Extras - Effect Encodings",
    "section": "Previously - Data Usage",
    "text": "Previously - Data Usage\n\nset.seed(4028)\nhotel_split &lt;-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-effect-encodings.html#what-do-we-do-with-the-agent-and-company-data",
    "href": "archive/2023-09-posit-conf/extras-effect-encodings.html#what-do-we-do-with-the-agent-and-company-data",
    "title": "Extras - Effect Encodings",
    "section": "What do we do with the agent and company data?",
    "text": "What do we do with the agent and company data?\nThere are 98 unique agent values and 100 companies in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables ğŸ˜³\nlump agents and companies that rarely occur into an â€œotherâ€ group\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the agent and company columns with the estimated effect of that predictor"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-effect-encodings.html#per-agent-statistics",
    "href": "archive/2023-09-posit-conf/extras-effect-encodings.html#per-agent-statistics",
    "title": "Extras - Effect Encodings",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-effect-encodings.html#what-is-an-effect-encoding",
    "href": "archive/2023-09-posit-conf/extras-effect-encodings.html#what-is-an-effect-encoding",
    "title": "Extras - Effect Encodings",
    "section": "What is an effect encoding?",
    "text": "What is an effect encoding?\nWe replace the qualitativeâ€™s predictor data with their effect on the outcome.\n\n\nData before:\n\nbefore\n#&gt; # A tibble: 7 Ã— 3\n#&gt;   avg_price_per_room agent            .row\n#&gt;                &lt;dbl&gt; &lt;fct&gt;           &lt;int&gt;\n#&gt; 1               52.7 cynthia_worsley     1\n#&gt; 2               51.8 carlos_bryant       2\n#&gt; 3               53.8 lance_hitchcock     3\n#&gt; 4               51.8 lance_hitchcock     4\n#&gt; 5               46.8 cynthia_worsley     5\n#&gt; 6               54.7 charles_najera      6\n#&gt; 7               46.8 cynthia_worsley     7\n\n\nData after:\n\nafter\n#&gt; # A tibble: 7 Ã— 3\n#&gt;   avg_price_per_room agent  .row\n#&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1               52.7  88.5     1\n#&gt; 2               51.8  89.5     2\n#&gt; 3               53.8  79.8     3\n#&gt; 4               51.8  79.8     4\n#&gt; 5               46.8  88.5     5\n#&gt; 6               54.7 109.      6\n#&gt; 7               46.8  88.5     7\n\n\nThe agent column is replaced with an estimate of the ADR."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-effect-encodings.html#per-agent-statistics-again",
    "href": "archive/2023-09-posit-conf/extras-effect-encodings.html#per-agent-statistics-again",
    "title": "Extras - Effect Encodings",
    "section": "Per-agent statistics again",
    "text": "Per-agent statistics again\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood statistical methods for estimating these means use partial pooling.\nPooling borrows strength across agents and shrinks extreme values towards the mean for agents with very few transations\nThe embed package has recipe steps for effect encodings.\n\n\n\nPartial pooling gives better estimates for agents with fewer reservations by shrinking the estimate to the overall ADR mean"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-effect-encodings.html#partial-pooling",
    "href": "archive/2023-09-posit-conf/extras-effect-encodings.html#partial-pooling",
    "title": "Extras - Effect Encodings",
    "section": "Partial pooling",
    "text": "Partial pooling"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-effect-encodings.html#agent-effects",
    "href": "archive/2023-09-posit-conf/extras-effect-encodings.html#agent-effects",
    "title": "Extras - Effect Encodings",
    "section": "Agent effects  ",
    "text": "Agent effects  \n\nlibrary(embed)\n\nhotel_effect_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_YeoJohnson(lead_time) %&gt;%\n  step_lencode_mixed(agent, company, outcome = vars(avg_price_per_room)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\n\nIt is very important to appropriately validate the effect encoding step to make sure that we are not overfitting."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-effect-encodings.html#effect-encoding-results",
    "href": "archive/2023-09-posit-conf/extras-effect-encodings.html#effect-encoding-results",
    "title": "Extras - Effect Encodings",
    "section": "Effect encoding results    ",
    "text": "Effect encoding results    \n\nhotel_effect_wflow &lt;-\n  workflow() %&gt;%\n  add_model(linear_reg()) %&gt;% \n  update_recipe(hotel_effect_rec)\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\nhotel_effect_res &lt;-\n  hotel_effect_wflow %&gt;%\n  fit_resamples(hotel_rs, metrics = reg_metrics)\n\ncollect_metrics(hotel_effect_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   17.8      10 0.189   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.870    10 0.00357 Preprocessor1_Model1\n\nSlightly worse but it can handle new agents (if they occur)."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-06-wrapping-up.html#your-turn",
    "href": "archive/2023-09-posit-conf/advanced-06-wrapping-up.html#your-turn",
    "title": "6 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-06-wrapping-up.html#resources-to-keep-learning",
    "href": "archive/2023-09-posit-conf/advanced-06-wrapping-up.html#resources-to-keep-learning",
    "title": "6 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://smltar.com/\n\n\n\nFollow us on Mastodon and at the tidyverse blog for updates!"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-06-wrapping-up.html#feedback",
    "href": "archive/2023-09-posit-conf/advanced-06-wrapping-up.html#feedback",
    "title": "6 - Wrapping up",
    "section": "Feedback",
    "text": "Feedback\n\nPlease go to http://pos.it/conf-workshop-survey. Your feedback is crucial!\nData from the survey informs curriculum and format decisions for future conf workshops, and we really appreciate you taking the time to provide it."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#previously---setup",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#previously---setup",
    "title": "3 - Grid Search via Racing",
    "section": "Previously - Setup ",
    "text": "Previously - Setup \n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#previously---data-usage",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#previously---data-usage",
    "title": "3 - Grid Search via Racing",
    "section": "Previously - Data Usage ",
    "text": "Previously - Data Usage \n\nset.seed(4028)\nhotel_split &lt;-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#previously---boosting-model",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#previously---boosting-model",
    "title": "3 - Grid Search via Racing",
    "section": "Previously - Boosting Model    ",
    "text": "Previously - Boosting Model    \n\nhotel_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) %&gt;%\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) %&gt;%\n  step_zv(all_predictors())\n\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune(), min_n = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow &lt;- workflow(hotel_rec, lgbm_spec)\n\nlgbm_param &lt;-\n  lgbm_wflow %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#making-grid-search-more-efficient",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#making-grid-search-more-efficient",
    "title": "3 - Grid Search via Racing",
    "section": "Making Grid Search More Efficient",
    "text": "Making Grid Search More Efficient\nIn the last section, we evaluated 250 models (25 candidates times 10 resamples).\nWe can make this go faster using parallel processing.\nAlso, for some models, we can fit far fewer models than the number that are being evaluated.\n\nFor boosting, a model with X trees can often predict on candidates with less than X trees.\n\nBoth of these methods can lead to enormous speed-ups."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#model-racing",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#model-racing",
    "title": "3 - Grid Search via Racing",
    "section": "Model Racing",
    "text": "Model Racing\nRacing is an old tool that we can use to go even faster.\n\nEvaluate all of the candidate models but only for a few resamples.\nDetermine which candidates have a low probability of being selected.\nEliminate poor candidates.\nRepeat with next resample (until no more resamples remain)\n\nThis can result in fitting a small number of models."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#discarding-candidates",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#discarding-candidates",
    "title": "3 - Grid Search via Racing",
    "section": "Discarding Candidates",
    "text": "Discarding Candidates\nHow do we eliminate tuning parameter combinations?\nThere are a few methods to do so. Weâ€™ll use one based on analysis of variance (ANOVA).\nHoweverâ€¦ there is typically a large difference between resamples in the results."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#resampling-results-non-racing",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#resampling-results-non-racing",
    "title": "3 - Grid Search via Racing",
    "section": "Resampling Results (Non-Racing)",
    "text": "Resampling Results (Non-Racing)\n\n\nHere are some realistic (but simulated) examples of two candidate models.\nAn error estimate is measured for each of 10 resamples.\n\nThe lines connect resamples.\n\nThere is usually a significant resample-to-resample effect (rank corr: 0.83)."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#are-candidates-different",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#are-candidates-different",
    "title": "3 - Grid Search via Racing",
    "section": "Are Candidates Different?",
    "text": "Are Candidates Different?\nOne way to evaluate these models is to do a paired t-test\n\nor a t-test on their differences matched by resamples\n\nWith \\(n = 10\\) resamples, the confidence interval is (0.99, 2.8), indicating that candidate number 2 has smaller error.\nWhat if we were to compare each model candidate to the current best at each resample?\nOne shows superiority when 4 resamples have been evaluated."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#evaluating-differences-in-candidates",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#evaluating-differences-in-candidates",
    "title": "3 - Grid Search via Racing",
    "section": "Evaluating Differences in Candidates",
    "text": "Evaluating Differences in Candidates"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#interim-analysis-of-results",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#interim-analysis-of-results",
    "title": "3 - Grid Search via Racing",
    "section": "Interim Analysis of Results",
    "text": "Interim Analysis of Results\nOne version of racing uses a mixed model ANOVA to construct one-sided confidence intervals for each candidate versus the current best.\nAny candidates whose bound does not include zero are discarded. Here is an animation.\nThe resamples are analyzed in a random order.\n\nKuhn (2014) has examples and simulations to show that the method works.\nThe finetune package has functions tune_race_anova() and tune_race_win_loss()."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#racing",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#racing",
    "title": "3 - Grid Search via Racing",
    "section": "Racing     ",
    "text": "Racing     \n\n# Let's use a larger grid\nset.seed(8945)\nlgbm_grid &lt;- \n  lgbm_param %&gt;% \n  grid_latin_hypercube(size = 50)\n\nlibrary(finetune)\n\nset.seed(9)\nlgbm_race_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_race_anova(\n    resamples = hotel_rs,\n    grid = lgbm_grid, \n    metrics = reg_metrics\n  )\n\nThe syntax and helper functions are extremely similar to those shown for tune_grid()."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#racing-results",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#racing-results",
    "title": "3 - Grid Search via Racing",
    "section": "Racing Results ",
    "text": "Racing Results \n\nshow_best(lgbm_race_res, metric = \"mae\")\n#&gt; # A tibble: 2 Ã— 11\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1  1516     7     0.0421          176             12 mae     standard    9.60    10   0.181 Preprocessor42_Model1\n#&gt; 2  1014     5     0.0791           35            181 mae     standard    9.61    10   0.179 Preprocessor06_Model1"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#racing-results-1",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#racing-results-1",
    "title": "3 - Grid Search via Racing",
    "section": "Racing Results ",
    "text": "Racing Results \n\n\nOnly 171 models were fit (out of 500).\nselect_best() never considers candidate models that did not get to the end of the race.\nThere is a helper function to see how candidate models were removed from consideration.\n\n\nplot_race(lgbm_race_res) + \n  scale_x_continuous(breaks = pretty_breaks())"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-04-racing.html#your-turn",
    "href": "archive/2023-09-posit-conf/advanced-04-racing.html#your-turn",
    "title": "3 - Grid Search via Racing",
    "section": "Your turn",
    "text": "Your turn\n\nRun tune_race_anova() with a different seed.\nDid you get the same or similar results?\n\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#working-with-our-predictors",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#working-with-our-predictors",
    "title": "1 - Feature Engineering",
    "section": "Working with our predictors",
    "text": "Working with our predictors\nWe might want to modify our predictors columns for a few reasons:\n\nThe model requires them in a different format (e.g.Â dummy variables for linear regression).\nThe model needs certain data qualities (e.g.Â same units for K-NN).\nThe outcome is better predicted when one or more columns are transformed in some way (a.k.a â€œfeature engineeringâ€).\n\n\nThe first two reasons are fairly predictable (next page).\nThe last one depends on your modeling problem."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#what-is-feature-engineering",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#what-is-feature-engineering",
    "title": "1 - Feature Engineering",
    "section": "What is feature engineering?",
    "text": "What is feature engineering?\nThink of a feature as some representation of a predictor that will be used in a model.\n\nExample representations:\n\nInteractions\nPolynomial expansions/splines\nPrincipal component analysis (PCA) feature extraction\n\nThere are a lot of examples in Feature Engineering and Selection (FES)."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#example-dates",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#example-dates",
    "title": "1 - Feature Engineering",
    "section": "Example: Dates",
    "text": "Example: Dates\nHow can we represent date columns for our model?\n\nWhen we use a date column in its native format, most models in R convert it to an integer.\n\n\nWe can re-engineer it as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nIndicators for holidays\n\n\nThe main point is that we try to maximize performance with different versions of the predictors.\nMention that, for the Chicago data, the day or the week features are usually the most important ones in the model."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#general-definitions",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#general-definitions",
    "title": "1 - Feature Engineering",
    "section": "General definitions ",
    "text": "General definitions \n\nData preprocessing steps allow your model to fit.\nFeature engineering steps help the model do the least work to predict the outcome as well as possible.\n\nThe recipes package can handle both!\n\nThese terms are often used interchangeably in the ML community but we want to distinguish them."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#previously---setup",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#previously---setup",
    "title": "1 - Feature Engineering",
    "section": "Previously - Setup ",
    "text": "Previously - Setup \n\n\n\nlibrary(tidymodels)\nlibrary(modeldatatoo)\n\n# Add another package:\nlibrary(textrecipes)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#previously---data-usage",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#previously---data-usage",
    "title": "1 - Feature Engineering",
    "section": "Previously - Data Usage ",
    "text": "Previously - Data Usage \n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\n\nWeâ€™ll go from here and create a set of resamples to use for model assessments."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#resampling-strategy",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#resampling-strategy",
    "title": "1 - Feature Engineering",
    "section": "Resampling Strategy",
    "text": "Resampling Strategy"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#resampling-strategy-1",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#resampling-strategy-1",
    "title": "1 - Feature Engineering",
    "section": "Resampling Strategy ",
    "text": "Resampling Strategy \nWeâ€™ll use simple 10-fold cross-validation (stratified sampling):\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)\nhotel_rs\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [3372/377]&gt; Fold01\n#&gt;  2 &lt;split [3373/376]&gt; Fold02\n#&gt;  3 &lt;split [3373/376]&gt; Fold03\n#&gt;  4 &lt;split [3373/376]&gt; Fold04\n#&gt;  5 &lt;split [3373/376]&gt; Fold05\n#&gt;  6 &lt;split [3374/375]&gt; Fold06\n#&gt;  7 &lt;split [3375/374]&gt; Fold07\n#&gt;  8 &lt;split [3376/373]&gt; Fold08\n#&gt;  9 &lt;split [3376/373]&gt; Fold09\n#&gt; 10 &lt;split [3376/373]&gt; Fold10"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#prepare-your-data-for-modeling",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#prepare-your-data-for-modeling",
    "title": "1 - Feature Engineering",
    "section": "Prepare your data for modeling ",
    "text": "Prepare your data for modeling \n\nThe recipes package is an extensible framework for pipeable sequences of preprocessing and feature engineering steps.\n\n\n\nStatistical parameters for the steps can be estimated from an initial data set and then applied to other data sets.\n\n\n\n\nThe resulting processed output can be used as inputs for statistical or machine learning models."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#a-first-recipe",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#a-first-recipe",
    "title": "1 - Feature Engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train)\n\n\n\nThe recipe() function assigns columns to roles of â€œoutcomeâ€ or â€œpredictorâ€ using the formula"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#a-first-recipe-1",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#a-first-recipe-1",
    "title": "1 - Feature Engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nsummary(hotel_rec)\n#&gt; # A tibble: 27 Ã— 4\n#&gt;    variable                type      role      source  \n#&gt;    &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 lead_time               &lt;chr [2]&gt; predictor original\n#&gt;  2 stays_in_weekend_nights &lt;chr [2]&gt; predictor original\n#&gt;  3 stays_in_week_nights    &lt;chr [2]&gt; predictor original\n#&gt;  4 adults                  &lt;chr [2]&gt; predictor original\n#&gt;  5 children                &lt;chr [2]&gt; predictor original\n#&gt;  6 babies                  &lt;chr [2]&gt; predictor original\n#&gt;  7 meal                    &lt;chr [3]&gt; predictor original\n#&gt;  8 country                 &lt;chr [3]&gt; predictor original\n#&gt;  9 market_segment          &lt;chr [3]&gt; predictor original\n#&gt; 10 distribution_channel    &lt;chr [3]&gt; predictor original\n#&gt; # â„¹ 17 more rows\n\nThe type column contains information on the variables"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#your-turn",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#your-turn",
    "title": "1 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\nWhat do you think are in the type vectors for the lead_time and country columns?\n\n\n\nâˆ’+\n02:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#create-indicator-variables",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#create-indicator-variables",
    "title": "1 - Feature Engineering",
    "section": "Create indicator variables ",
    "text": "Create indicator variables \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\n\nFor any factor or character predictors, make binary indicators.\nThere are many recipe steps that can convert categorical predictors to numeric columns.\nstep_dummy() records the levels of the categorical predictors in the training set."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#filter-out-constant-columns",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#filter-out-constant-columns",
    "title": "1 - Feature Engineering",
    "section": "Filter out constant columns ",
    "text": "Filter out constant columns \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())\n\n\nIn case there is a factor level that was never observed in the training data (resulting in a column of all 0s), we can delete any zero-variance predictors that have a single unique value.\n\nNote that the selector chooses all columns with a role of â€œpredictorâ€"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#normalization",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#normalization",
    "title": "1 - Feature Engineering",
    "section": "Normalization ",
    "text": "Normalization \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\n\n\nThis centers and scales the numeric predictors.\nThe recipe will use the training set to estimate the means and standard deviations of the data.\n\n\n\n\nAll data the recipe is applied to will be normalized using those statistics (there is no re-estimation)."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#reduce-correlation",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#reduce-correlation",
    "title": "1 - Feature Engineering",
    "section": "Reduce correlation ",
    "text": "Reduce correlation \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_corr(all_numeric_predictors(), threshold = 0.9)\n\n\nTo deal with highly correlated predictors, find the minimum set of predictor columns that make the pairwise correlations less than the threshold."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#other-possible-steps",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#other-possible-steps",
    "title": "1 - Feature Engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_pca(all_numeric_predictors())\n\n\nPCA feature extractionâ€¦"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#other-possible-steps-1",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#other-possible-steps-1",
    "title": "1 - Feature Engineering",
    "section": "Other possible steps  ",
    "text": "Other possible steps  \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  embed::step_umap(all_numeric_predictors(), outcome = vars(avg_price_per_room))\n\n\nA fancy machine learning supervised dimension reduction techniqueâ€¦\n\nNote that this uses the outcome, and it is from an extension package"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#other-possible-steps-2",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#other-possible-steps-2",
    "title": "1 - Feature Engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10)\n\n\nNonlinear transforms like natural splines, and so on!"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#your-turn-1",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#your-turn-1",
    "title": "1 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a recipe() for the hotel data to:\n\nuse a Yeo-Johnson (YJ) transformation on lead_time\nconvert factors to indicator variables\nremove zero-variance variables\nadd the spline technique shown above\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#minimal-recipe",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#minimal-recipe",
    "title": "1 - Feature Engineering",
    "section": "Minimal recipe ",
    "text": "Minimal recipe \n\nhotel_indicators &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_YeoJohnson(lead_time) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#measuring-performance",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#measuring-performance",
    "title": "1 - Feature Engineering",
    "section": "Measuring Performance ",
    "text": "Measuring Performance \nWeâ€™ll compute two measures: mean absolute error and the coefficient of determination (a.k.a \\(R^2\\)).\n\\[\\begin{align}\nMAE &= \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| \\notag \\\\\nR^2 &= cor(y_i, \\hat{y}_i)^2\n\\end{align}\\]\nThe focus will be on MAE for parameter optimization. Weâ€™ll use a metric set to compute these:\n\nreg_metrics &lt;- metric_set(mae, rsq)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#using-a-workflow",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#using-a-workflow",
    "title": "1 - Feature Engineering",
    "section": "Using a workflow    ",
    "text": "Using a workflow    \n\nset.seed(9)\n\nhotel_lm_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(hotel_indicators) %&gt;%\n  add_model(linear_reg())\n \nctrl &lt;- control_resamples(save_pred = TRUE)\nhotel_lm_res &lt;-\n  hotel_lm_wflow %&gt;%\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\ncollect_metrics(hotel_lm_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   16.6      10 0.214   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.884    10 0.00339 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#your-turn-2",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#your-turn-2",
    "title": "1 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() to fit your workflow with a recipe.\nCollect the predictions from the results.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#holdout-predictions",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#holdout-predictions",
    "title": "1 - Feature Engineering",
    "section": "Holdout predictions    ",
    "text": "Holdout predictions    \n\n# Since we used `save_pred = TRUE`\nlm_cv_pred &lt;- collect_predictions(hotel_lm_res)\nlm_cv_pred %&gt;% print(n = 7)\n#&gt; # A tibble: 3,749 Ã— 5\n#&gt;   id     .pred  .row avg_price_per_room .config             \n#&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;              &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 Fold01  75.1    20                 40 Preprocessor1_Model1\n#&gt; 2 Fold01  49.3    28                 54 Preprocessor1_Model1\n#&gt; 3 Fold01  64.9    45                 50 Preprocessor1_Model1\n#&gt; 4 Fold01  52.8    49                 42 Preprocessor1_Model1\n#&gt; 5 Fold01  48.6    61                 49 Preprocessor1_Model1\n#&gt; 6 Fold01  29.8    66                 40 Preprocessor1_Model1\n#&gt; 7 Fold01  36.9    88                 49 Preprocessor1_Model1\n#&gt; # â„¹ 3,742 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#calibration-plot",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#calibration-plot",
    "title": "1 - Feature Engineering",
    "section": "Calibration Plot ",
    "text": "Calibration Plot \n\nlibrary(probably)\n\ncal_plot_regression(hotel_lm_res, alpha = 1 / 5)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#what-do-we-do-with-the-agent-and-company-data",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#what-do-we-do-with-the-agent-and-company-data",
    "title": "1 - Feature Engineering",
    "section": "What do we do with the agent and company data?",
    "text": "What do we do with the agent and company data?\nThere are 98 unique agent values and 100 unique companies in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables ğŸ˜³\nlump agents and companies that rarely occur into an â€œotherâ€ group\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the agent and company columns with the estimated effect of that predictor (in the extra materials)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#per-agent-statistics",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#per-agent-statistics",
    "title": "1 - Feature Engineering",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#collapsing-factor-levels",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#collapsing-factor-levels",
    "title": "1 - Feature Engineering",
    "section": "Collapsing factor levels ",
    "text": "Collapsing factor levels \nThere is a recipe step that will redefine factor levels based on their frequency in the training set:\n\nhotel_other_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;% \n  step_YeoJohnson(lead_time) %&gt;%\n  step_other(agent, threshold = 0.001) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10)\n\nUsing this code, 34 agents (out of 98) were collapsed into â€œotherâ€ based on the training set.\nWe could try to optimize the threshold for collapsing (see the next set of slides on model tuning)."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#does-othering-help",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#does-othering-help",
    "title": "1 - Feature Engineering",
    "section": "Does othering help?  ",
    "text": "Does othering help?  \n\nhotel_other_wflow &lt;-\n  hotel_lm_wflow %&gt;%\n  update_recipe(hotel_other_rec)\n\nhotel_other_res &lt;-\n  hotel_other_wflow %&gt;%\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\ncollect_metrics(hotel_other_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   16.7      10 0.213   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.884    10 0.00341 Preprocessor1_Model1\n\nAbout the same MAE and much faster to complete.\nNow letâ€™s look at a more sophisticated tool called effect feature hashing."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#feature-hashing",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#feature-hashing",
    "title": "1 - Feature Engineering",
    "section": "Feature Hashing",
    "text": "Feature Hashing\nBetween agent and company, simple dummy variables would create 198 new columns (that are mostly zeros).\nAnother option is to have a binary indicator that combines some levels of these variables.\nFeature hashing (for more see FES, SMLTAR, and TMwR):\n\nuses the character values of the levels\nconverts them to integer hash values\nuses the integers to assign them to a specific indicator column."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#feature-hashing-1",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#feature-hashing-1",
    "title": "1 - Feature Engineering",
    "section": "Feature Hashing",
    "text": "Feature Hashing\nSuppose we want to use 32 indicator variables for agent.\nFor a agent with value â€œMax_Kuhnâ€, a hashing function converts it to an integer (say 210397726).\nTo assign it to one of the 32 columns, we would use modular arithmetic to assign it to a column:\n\n# For \"Max_Kuhn\" put a '1' in column: \n210397726 %% 32\n#&gt; [1] 30\n\nHash functions are meant to emulate randomness."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#feature-hashing-pros",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#feature-hashing-pros",
    "title": "1 - Feature Engineering",
    "section": "Feature Hashing Pros",
    "text": "Feature Hashing Pros\n\nThe procedure will automatically work on new values of the predictors.\nIt is fast.\nâ€œSignedâ€ hashes add a sign to help avoid aliasing."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#feature-hashing-cons",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#feature-hashing-cons",
    "title": "1 - Feature Engineering",
    "section": "Feature Hashing Cons",
    "text": "Feature Hashing Cons\n\nThere is no real logic behind which factor levels are combined.\nWe donâ€™t know how many columns to add (more in the next section).\nSome columns may have all zeros.\nIf a indicator column is important to the model, we canâ€™t easily determine why.\n\n\nThe signed hash make it slightly more possible to differentiate between confounded levels"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#feature-hashing-in-recipes",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#feature-hashing-in-recipes",
    "title": "1 - Feature Engineering",
    "section": "Feature Hashing in recipes   ",
    "text": "Feature Hashing in recipes   \nThe textrecipes package has a step that can be added to the recipe:\n\nlibrary(textrecipes)\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  # Defaults to 32 signed indicator columns\n  step_dummy_hash(agent) %&gt;%\n  step_dummy_hash(company) %&gt;%\n  # Regular indicators for the others\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_spline_natural(arrival_date_num, deg_free = 10)\n\nhotel_hash_wflow &lt;-\n  hotel_lm_wflow %&gt;%\n  update_recipe(hash_rec)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#feature-hashing-in-recipes-1",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#feature-hashing-in-recipes-1",
    "title": "1 - Feature Engineering",
    "section": "Feature Hashing in recipes   ",
    "text": "Feature Hashing in recipes   \n\nhotel_hash_res &lt;-\n  hotel_hash_wflow %&gt;%\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\ncollect_metrics(hotel_hash_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   16.7      10 0.239   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.884    10 0.00324 Preprocessor1_Model1\n\nAbout the same performance but now we can handle new values."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#debugging-a-recipe",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#debugging-a-recipe",
    "title": "1 - Feature Engineering",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g.Â hash_rec) can be estimated manually with a function called prep(). It is analogous to fit(). See TMwR section 16.4\n\n\n\n\nAnother function (bake()) is analogous to predict(), and gives you the processed data back.\n\n\n\n\nThe tidy() function can be used to get specific results from the recipe."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#example",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#example",
    "title": "1 - Feature Engineering",
    "section": "Example  ",
    "text": "Example  \n\nhash_rec_fit &lt;- prep(hash_rec)\n\n# Get the transformation coefficient\ntidy(hash_rec_fit, number = 1)\n\n# Get the processed data\nbake(hash_rec_fit, hotel_train %&gt;% slice(1:3), contains(\"_agent_\"))"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#more-on-recipes",
    "href": "archive/2023-09-posit-conf/advanced-02-feature-engineering.html#more-on-recipes",
    "title": "1 - Feature Engineering",
    "section": "More on recipes",
    "text": "More on recipes\n\nOnce fit() is called on a workflow, changing the model does not re-fit the recipe.\n\n\n\nA list of all known steps is at https://www.tidymodels.org/find/recipes/.\n\n\n\n\nSome steps can be skipped when using predict().\n\n\n\n\nThe order of the steps matters."
  },
  {
    "objectID": "archive/2023-07-nyr/index.html",
    "href": "archive/2023-07-nyr/index.html",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels. This workshop provides an introduction to machine learning with R using the tidymodels framework, a collection of packages for modeling and machine learning using tidyverse principles. We will build, evaluate, compare, and tune predictive models. Along the way, weâ€™ll learn about key concepts in machine learning including overfitting, resampling, and feature engineering. Learners will gain knowledge about good predictive modeling practices, as well as hands-on experience using tidymodels packages like parsnip, rsample, recipes, yardstick, tune, and workflows."
  },
  {
    "objectID": "archive/2023-07-nyr/index.html#welcome",
    "href": "archive/2023-07-nyr/index.html#welcome",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels. This workshop provides an introduction to machine learning with R using the tidymodels framework, a collection of packages for modeling and machine learning using tidyverse principles. We will build, evaluate, compare, and tune predictive models. Along the way, weâ€™ll learn about key concepts in machine learning including overfitting, resampling, and feature engineering. Learners will gain knowledge about good predictive modeling practices, as well as hands-on experience using tidymodels packages like parsnip, rsample, recipes, yardstick, tune, and workflows."
  },
  {
    "objectID": "archive/2023-07-nyr/index.html#is-this-workshop-for-me",
    "href": "archive/2023-07-nyr/index.html#is-this-workshop-for-me",
    "title": "Machine learning with tidymodels",
    "section": "Is this workshop for me? ",
    "text": "Is this workshop for me? \nThis course assumes intermediate R knowledge. This workshop is for you if:\n\nYou can use the magrittr pipe %&gt;% and/or native pipe |&gt;\nYou are familiar with functions from dplyr, tidyr, and ggplot2\nYou can read data into R, transform and reshape data, and make a wide variety of graphs\n\nWe expect participants to have some exposure to basic statistical concepts, but NOT intermediate or expert familiarity with modeling or machine learning."
  },
  {
    "objectID": "archive/2023-07-nyr/index.html#preparation",
    "href": "archive/2023-07-nyr/index.html#preparation",
    "title": "Machine learning with tidymodels",
    "section": "Preparation",
    "text": "Preparation\nPlease join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2022.02), available at https://www.rstudio.com/download\nThe following R packages, which you can install from the R console:\n\n\n# First, install the pak package: \n\ninstall.packages(\"pak\")\n\n# Then the packages for both days\npkgs &lt;- \n  c(\"bonsai\", \"doParallel\", \"finetune\", \"lightgbm\", \"lme4\", \"plumber\", \n    \"probably\", \"ranger\", \"rpart\", \"rpart.plot\", \"stacks\", \"textrecipes\", \n    \"tidymodels\", \"tidymodels/modeldatatoo\", \"vetiver\")\npak::pak(pkgs)"
  },
  {
    "objectID": "archive/2023-07-nyr/index.html#slides",
    "href": "archive/2023-07-nyr/index.html#slides",
    "title": "Machine learning with tidymodels",
    "section": "Slides",
    "text": "Slides\nThese slides are designed to use with live teaching and are published for workshop participantsâ€™ convenience. There are not meant as standalone learning materials. For that, we recommend tidymodels.org and Tidy Modeling with R.\n\nIntroduction to tidymodels\n\n01: Introduction\n02: Your data budget\n03: What makes a model?\n04: Evaluating models\n\n\n\nAdvanced tidymodels\n\n01: Feature engineering using recipes\n02: Tuning hyperparameters (grid search)\n03: Grid search via racing\n04: Iterative search\n\n\n\nExtra content (time permitting)\n\nTransit Case Study (includes stacking)\nEffect encodings\n\nThereâ€™s also a page for slide annotations; these are extra notes for selected slides."
  },
  {
    "objectID": "archive/2023-07-nyr/index.html#code",
    "href": "archive/2023-07-nyr/index.html#code",
    "title": "Machine learning with tidymodels",
    "section": "Code",
    "text": "Code\nQuarto files (version 1.4.104) for working along are available on GitHub. (Donâ€™t worry if you havenâ€™t used Quarto before; it will feel familiar to R Markdown users.)"
  },
  {
    "objectID": "archive/2023-07-nyr/index.html#past-workshops",
    "href": "archive/2023-07-nyr/index.html#past-workshops",
    "title": "Machine learning with tidymodels",
    "section": "Past workshops",
    "text": "Past workshops\n\nJuly 2022 at rstudio::conf()\nAugust 2022 in Reykjavik\nJuly 2023 at the New York R Conference"
  },
  {
    "objectID": "archive/2023-07-nyr/index.html#acknowledgments",
    "href": "archive/2023-07-nyr/index.html#acknowledgments",
    "title": "Machine learning with tidymodels",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including the slides, is made with Quarto. Please submit an issue on the GitHub repo for this workshop if you find something that could be fixed or improved."
  },
  {
    "objectID": "archive/2023-07-nyr/index.html#reuse-and-licensing",
    "href": "archive/2023-07-nyr/index.html#reuse-and-licensing",
    "title": "Machine learning with tidymodels",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e.Â not an original creation and reused from another source), these educational materials are licensed under Creative Commons Attribution CC BY-SA 4.0."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-effect-encodings.html#previously---setup",
    "href": "archive/2023-07-nyr/extras-effect-encodings.html#previously---setup",
    "title": "Extras - Effect Encodings",
    "section": "Previously - Setup",
    "text": "Previously - Setup\n\n\n\nlibrary(tidymodels)\nlibrary(modeldatatoo)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\nset.seed(295)\nhotel_rates &lt;- \n  data_hotel_rates() %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date_num, -arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-effect-encodings.html#previously---data-usage",
    "href": "archive/2023-07-nyr/extras-effect-encodings.html#previously---data-usage",
    "title": "Extras - Effect Encodings",
    "section": "Previously - Data Usage",
    "text": "Previously - Data Usage\n\nset.seed(4028)\nhotel_split &lt;-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_tr &lt;- training(hotel_split)\nhotel_te &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_tr, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-effect-encodings.html#what-do-we-do-with-the-agent-and-company-data",
    "href": "archive/2023-07-nyr/extras-effect-encodings.html#what-do-we-do-with-the-agent-and-company-data",
    "title": "Extras - Effect Encodings",
    "section": "What do we do with the agent and company data?",
    "text": "What do we do with the agent and company data?\nThere are 98 unique agent values and 100 companies in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables ğŸ˜³\nlump agents and companies that rarely occur into an â€œotherâ€ group\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the agent and company columns with the estimated effect of that predictor"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-effect-encodings.html#per-agent-statistics",
    "href": "archive/2023-07-nyr/extras-effect-encodings.html#per-agent-statistics",
    "title": "Extras - Effect Encodings",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-effect-encodings.html#what-is-an-effect-encoding",
    "href": "archive/2023-07-nyr/extras-effect-encodings.html#what-is-an-effect-encoding",
    "title": "Extras - Effect Encodings",
    "section": "What is an effect encoding?",
    "text": "What is an effect encoding?\nWe replace the qualitativeâ€™s predictor data with their effect on the outcome.\n\n\nData before:\n\nbefore\n#&gt; # A tibble: 7 Ã— 3\n#&gt;   avg_price_per_room agent            .row\n#&gt;                &lt;dbl&gt; &lt;fct&gt;           &lt;int&gt;\n#&gt; 1               52.7 cynthia_worsley     1\n#&gt; 2               51.8 carlos_bryant       2\n#&gt; 3               53.8 lance_hitchcock     3\n#&gt; 4               51.8 lance_hitchcock     4\n#&gt; 5               46.8 cynthia_worsley     5\n#&gt; 6               54.7 charles_najera      6\n#&gt; 7               46.8 cynthia_worsley     7\n\n\nData after:\n\nafter\n#&gt; # A tibble: 7 Ã— 3\n#&gt;   avg_price_per_room agent  .row\n#&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1               52.7  88.5     1\n#&gt; 2               51.8  89.5     2\n#&gt; 3               53.8  79.8     3\n#&gt; 4               51.8  79.8     4\n#&gt; 5               46.8  88.5     5\n#&gt; 6               54.7 109.      6\n#&gt; 7               46.8  88.5     7\n\n\nThe agent column is replaced with an estimate of the ADR."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-effect-encodings.html#per-agent-statistics-again",
    "href": "archive/2023-07-nyr/extras-effect-encodings.html#per-agent-statistics-again",
    "title": "Extras - Effect Encodings",
    "section": "Per-agent statistics again",
    "text": "Per-agent statistics again\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood statistical methods for estimating these means use partial pooling.\nPooling borrows strength across agents and shrinks extreme values towards the mean for agents with very few transations\nThe embed package has recipe steps for effect encodings.\n\n\n\nPartial pooling gives better estimates for agents with fewer reservations by shrinking the estimate to the overall ADR mean"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-effect-encodings.html#partial-pooling",
    "href": "archive/2023-07-nyr/extras-effect-encodings.html#partial-pooling",
    "title": "Extras - Effect Encodings",
    "section": "Partial pooling",
    "text": "Partial pooling"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-effect-encodings.html#agent-effects",
    "href": "archive/2023-07-nyr/extras-effect-encodings.html#agent-effects",
    "title": "Extras - Effect Encodings",
    "section": "Agent effects  ",
    "text": "Agent effects  \n\nlibrary(embed)\n\nhotel_effect_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;% \n  step_YeoJohnson(lead_time) %&gt;%\n  step_lencode_mixed(agent, company, outcome = vars(avg_price_per_room)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\n\nIt is very important to appropriately validate the effect encoding step to make sure that we are not overfitting."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-effect-encodings.html#effect-encoding-results",
    "href": "archive/2023-07-nyr/extras-effect-encodings.html#effect-encoding-results",
    "title": "Extras - Effect Encodings",
    "section": "Effect encoding results    ",
    "text": "Effect encoding results    \n\nhotel_effect_wflow &lt;-\n  workflow() %&gt;%\n  add_model(linear_reg()) %&gt;% \n  update_recipe(hotel_effect_rec)\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\nhotel_effect_res &lt;-\n  hotel_effect_wflow %&gt;%\n  fit_resamples(hotel_rs, metrics = reg_metrics)\n\ncollect_metrics(hotel_effect_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   17.8      10 0.236   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.867    10 0.00377 Preprocessor1_Model1\n\nSlightly worse but it can handle new agents (if they occur)."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#previously---setup",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#previously---setup",
    "title": "4 - Iterative Search",
    "section": "Previously - Setup",
    "text": "Previously - Setup\n\n\n\nlibrary(tidymodels)\nlibrary(modeldatatoo)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\nset.seed(295)\nhotel_rates &lt;- \n  data_hotel_rates() %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date_num, -arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#previously---data-usage",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#previously---data-usage",
    "title": "4 - Iterative Search",
    "section": "Previously - Data Usage",
    "text": "Previously - Data Usage\n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_tr &lt;- training(hotel_split)\nhotel_te &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_tr, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#previously---boosting-model",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#previously---boosting-model",
    "title": "4 - Iterative Search",
    "section": "Previously - Boosting Model",
    "text": "Previously - Boosting Model\n\nhotel_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) %&gt;%\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) %&gt;%\n  step_zv(all_predictors())\n\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune(), min_n = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\")\n\nlgbm_wflow &lt;- workflow(hotel_rec, lgbm_spec)\n\nlgbm_param &lt;-\n  lgbm_wflow %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#iterative-search",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#iterative-search",
    "title": "4 - Iterative Search",
    "section": "Iterative Search",
    "text": "Iterative Search\nInstead of pre-defining a grid of candidate points, we can model our current results to predict what the next candidate point should be.\n\nSuppose that we are only tuning the learning rate in our boosted tree.\n\nWe could do something like:\nmae_pred &lt;- lm(mae ~ learn_rate, data = resample_results)\nand use this to predict and rank new learning rate candidates."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#iterative-search-1",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#iterative-search-1",
    "title": "4 - Iterative Search",
    "section": "Iterative Search",
    "text": "Iterative Search\nA linear model probably isnâ€™t the best choice though (more in a minute).\nTo illustrate the process, we resampled a large grid of learning rate values for our data to show what the relationship is between MAE and learning rate.\nNow suppose that we used a grid of three points in the parameter range for learning rateâ€¦"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#a-large-grid",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#a-large-grid",
    "title": "4 - Iterative Search",
    "section": "A Large Grid",
    "text": "A Large Grid"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#a-three-point-grid",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#a-three-point-grid",
    "title": "4 - Iterative Search",
    "section": "A Three Point Grid",
    "text": "A Three Point Grid"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#gaussian-processes-and-optimization",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#gaussian-processes-and-optimization",
    "title": "4 - Iterative Search",
    "section": "Gaussian Processes and Optimization",
    "text": "Gaussian Processes and Optimization\nWe can make a â€œmeta-modelâ€ with a small set of historical performance results.\nGaussian Processes (GP) models are a good choice to model performance.\n\nIt is a Bayesian model so we are using Bayesian Optimization (BO).\nFor regression, we can assume that our data are multivariate normal.\nWe also define a covariance function for the variance relationship between data points. A common one is:\n\n\\[\\operatorname{cov}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\exp\\left(-\\frac{1}{2}|\\boldsymbol{x}_i - \\boldsymbol{x}_j|^2\\right) + \\sigma^2_{ij}\\]\n\nGPs are good because\n\nthey are flexible regression models (in the sense that splines are flexible).\nwe need to get mean and variance predictions (and they are Bayesian)\ntheir variability is based on spatial distances.\n\nSome people use random forests (with conformal variance estimates) or other methods but GPs are most popular."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#predicting-candidates",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#predicting-candidates",
    "title": "4 - Iterative Search",
    "section": "Predicting Candidates",
    "text": "Predicting Candidates\nThe GP model can take candidate tuning parameter combinations as inputs and make predictions for performance (e.g.Â MAE)\n\nThe mean performance\nThe variance of performance\n\nThe variance is mostly driven by spatial variability (the previous equation).\nThe predicted variance is zero at locations of actual data points and becomes very high when far away from any observed data."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#your-turn",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#your-turn",
    "title": "4 - Iterative Search",
    "section": "Your turn",
    "text": "Your turn\n\n\nYour GP makes predictions on two new candidate tuning parameters.\nWe want to minimize MAE.\nWhich should we choose?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#gp-fit-ribbon-is-mean---1sd",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#gp-fit-ribbon-is-mean---1sd",
    "title": "4 - Iterative Search",
    "section": "GP Fit (ribbon is mean +/- 1SD)",
    "text": "GP Fit (ribbon is mean +/- 1SD)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#choosing-new-candidates",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#choosing-new-candidates",
    "title": "4 - Iterative Search",
    "section": "Choosing New Candidates",
    "text": "Choosing New Candidates\nThis isnâ€™t a very good fit but we can still use it.\nHow can we use the outputs to choose the next point to measure?\n\nAcquisition functions take the predicted mean and variance and use them to balance:\n\nexploration: new candidates should explore new areas.\nexploitation: new candidates must stay near existing values.\n\nExploration focuses on the variance, exploitation is about the mean."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#acquisition-functions",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#acquisition-functions",
    "title": "4 - Iterative Search",
    "section": "Acquisition Functions",
    "text": "Acquisition Functions\nWeâ€™ll use an acquisition function to select a new candidate.\nThe most popular method appears to be expected improvement (EI) above the current best results.\n\nZero at existing data points.\nThe expected improvement is integrated over all possible improvement (â€œexpectedâ€ in the probability sense).\n\nWe would probably pick the point with the largest EI as the next point.\n(There are other functions beyond EI.)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#expected-improvement",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#expected-improvement",
    "title": "4 - Iterative Search",
    "section": "Expected Improvement",
    "text": "Expected Improvement"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#iteration",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#iteration",
    "title": "4 - Iterative Search",
    "section": "Iteration",
    "text": "Iteration\nOnce we pick the candidate point, we measure performance for it (e.g.Â resampling).\n\nAnother GP is fit, EI is recomputed, and so on.\n\nWe stop when we have completed the allowed number of iterations or if we donâ€™t see any improvement after a pre-set number of attempts."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#gp-fit-with-four-points",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#gp-fit-with-four-points",
    "title": "4 - Iterative Search",
    "section": "GP Fit with four points",
    "text": "GP Fit with four points"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#expected-improvement-1",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#expected-improvement-1",
    "title": "4 - Iterative Search",
    "section": "Expected Improvement",
    "text": "Expected Improvement"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#gp-evolution",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#gp-evolution",
    "title": "4 - Iterative Search",
    "section": "GP Evolution",
    "text": "GP Evolution"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#expected-improvement-evolution",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#expected-improvement-evolution",
    "title": "4 - Iterative Search",
    "section": "Expected Improvement Evolution",
    "text": "Expected Improvement Evolution"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#bo-in-tidymodels",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#bo-in-tidymodels",
    "title": "4 - Iterative Search",
    "section": "BO in tidymodels",
    "text": "BO in tidymodels\nWeâ€™ll use a function called tune_bayes() that has very similar syntax to tune_grid().\n\nIt has an additional initial argument for the initial set of performance estimates and parameter combinations for the GP model."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#initial-grid-points",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#initial-grid-points",
    "title": "4 - Iterative Search",
    "section": "Initial grid points",
    "text": "Initial grid points\ninitial can be the results of another tune_*() function or an integer (in which case tune_grid() is used under to hood to make such an initial set of results).\n\nWeâ€™ll run the optimization more than once, so letâ€™s make an initial grid of results to serve as the substrate for the BO.\nI suggest at least the number of tuning parameters plus two as the initial grid for BO."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#an-initial-grid",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#an-initial-grid",
    "title": "4 - Iterative Search",
    "section": "An Initial Grid",
    "text": "An Initial Grid\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\nset.seed(12)\ninit_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = nrow(lgbm_param) + 2,\n    param_info = lgbm_param,\n    metrics = reg_metrics\n  )\n\nshow_best(init_res, metric = \"mae\")\n#&gt; # A tibble: 5 Ã— 11\n#&gt;   trees min_n   learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config             \n#&gt;   &lt;int&gt; &lt;int&gt;        &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1   390    10 0.0139                 13             62 mae     standard    11.9    10   0.208 Preprocessor1_Model1\n#&gt; 2   718    31 0.00112                72             25 mae     standard    29.1    10   0.325 Preprocessor4_Model1\n#&gt; 3  1236    22 0.0000261              11             17 mae     standard    51.8    10   0.416 Preprocessor7_Model1\n#&gt; 4  1044    25 0.00000832             34             12 mae     standard    52.8    10   0.424 Preprocessor5_Model1\n#&gt; 5  1599     7 0.0000000402          254            179 mae     standard    53.2    10   0.427 Preprocessor6_Model1"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#bo-using-tidymodels",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#bo-using-tidymodels",
    "title": "4 - Iterative Search",
    "section": "BO using tidymodels",
    "text": "BO using tidymodels\n\nset.seed(15)\nlgbm_bayes_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_bayes(\n    resamples = hotel_rs,\n    initial = init_res,     # &lt;- initial results\n    iter = 20,\n    param_info = lgbm_param,\n    metrics = reg_metrics\n  )\n\nshow_best(lgbm_bayes_res, metric = \"mae\")\n#&gt; # A tibble: 5 Ã— 12\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config .iter\n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n#&gt; 1  1665     2     0.0593           12             59 mae     standard    10.1    10   0.173 Iter13     13\n#&gt; 2  1179     2     0.0552          161            121 mae     standard    10.2    10   0.147 Iter7       7\n#&gt; 3  1609     6     0.0592          186            192 mae     standard    10.2    10   0.195 Iter17     17\n#&gt; 4  1352     6     0.0799          217             46 mae     standard    10.3    10   0.211 Iter4       4\n#&gt; 5  1647     4     0.0819           12            240 mae     standard    10.3    10   0.198 Iter20     20"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#plotting-bo-results",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#plotting-bo-results",
    "title": "4 - Iterative Search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(lgbm_bayes_res, metric = \"mae\")"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#plotting-bo-results-1",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#plotting-bo-results-1",
    "title": "4 - Iterative Search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"parameters\")"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#plotting-bo-results-2",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#plotting-bo-results-2",
    "title": "4 - Iterative Search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"performance\")"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#enhance",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#enhance",
    "title": "4 - Iterative Search",
    "section": "ENHANCE",
    "text": "ENHANCE\n\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"performance\") +\n  ylim(c(9.5, 14))"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#your-turn-1",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#your-turn-1",
    "title": "4 - Iterative Search",
    "section": "Your turn",
    "text": "Your turn\nLetâ€™s try a different acquisition function: conf_bound(kappa).\nWeâ€™ll use the objective argument to set it.\nChoose your own kappa value:\n\nLarger values will explore the space more.\nâ€œLargeâ€ values are usually less than one.\n\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#notes",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#notes",
    "title": "4 - Iterative Search",
    "section": "Notes",
    "text": "Notes\n\nStopping tune_bayes() will return the current results.\nParallel processing can still be used to more efficiently measure each candidate point.\nThere are a lot of other iterative methods that you can use.\nThe finetune package also has functions for simulated annealing search."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#finalizing-the-model",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#finalizing-the-model",
    "title": "4 - Iterative Search",
    "section": "Finalizing the Model",
    "text": "Finalizing the Model\nLetâ€™s say that weâ€™ve tried a lot of different models and we like our lightgbm model the most.\nWhat do we do now?\n\nFinalize the workflow by choosing the values for the tuning parameters.\nFit the model on the entire training set.\nVerify performance using the test set.\nDocument and publish the model(?)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#locking-down-the-tuning-parameters",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#locking-down-the-tuning-parameters",
    "title": "4 - Iterative Search",
    "section": "Locking Down the Tuning Parameters",
    "text": "Locking Down the Tuning Parameters\nWe can take the results of the Bayesian optimization and accept the best results:\n\nbest_param &lt;- select_best(lgbm_bayes_res, metric = \"mae\")\nfinal_wflow &lt;- \n  lgbm_wflow %&gt;% \n  finalize_workflow(best_param)\nfinal_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Recipe\n#&gt; Model: boost_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; 4 Recipe Steps\n#&gt; \n#&gt; â€¢ step_YeoJohnson()\n#&gt; â€¢ step_dummy_hash()\n#&gt; â€¢ step_dummy_hash()\n#&gt; â€¢ step_zv()\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Boosted Tree Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1665\n#&gt;   min_n = 2\n#&gt;   learn_rate = 0.0592557571004946\n#&gt; \n#&gt; Computational engine: lightgbm"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#the-final-fit",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#the-final-fit",
    "title": "4 - Iterative Search",
    "section": "The Final Fit",
    "text": "The Final Fit\nWe can use individual functions:\nfinal_fit &lt;- final_wflow %&gt;% fit(data = hotel_tr)\n\n# then predict() or augment() \n# then compute metrics\n\nRemember that there is also a convenience function to do all of this:\n\nset.seed(3893)\nfinal_res &lt;- final_wflow %&gt;% last_fit(hotel_split, metrics = reg_metrics)\nfinal_res\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits              id               .metrics         .notes           .predictions         .workflow \n#&gt;   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;           &lt;list&gt;               &lt;list&gt;    \n#&gt; 1 &lt;split [3749/1251]&gt; train/test split &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1,251 Ã— 4]&gt; &lt;workflow&gt;"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-04-iterative.html#test-set-results",
    "href": "archive/2023-07-nyr/advanced-04-iterative.html#test-set-results",
    "title": "4 - Iterative Search",
    "section": "Test Set Results",
    "text": "Test Set Results\n\n\n\nfinal_res %&gt;% \n  collect_predictions() %&gt;% \n  cal_plot_regression(\n    truth = avg_price_per_room, \n    estimate = .pred, \n    alpha = 1 / 4)\n\nTest set performance:\n\nfinal_res %&gt;% collect_metrics()\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard      10.5   Preprocessor1_Model1\n#&gt; 2 rsq     standard       0.937 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#previously---setup",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#previously---setup",
    "title": "2 - Tuning Hyperparameters",
    "section": "Previously - Setup ",
    "text": "Previously - Setup \n\n\n\nlibrary(tidymodels)\nlibrary(modeldatatoo)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\n\n\nset.seed(295)\nhotel_rates &lt;- \n  data_hotel_rates() %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date_num, -arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#previously---data-usage",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#previously---data-usage",
    "title": "2 - Tuning Hyperparameters",
    "section": "Previously - Data Usage ",
    "text": "Previously - Data Usage \n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_tr &lt;- training(hotel_split)\nhotel_te &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_tr, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#previously---feature-engineering",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#previously---feature-engineering",
    "title": "2 - Tuning Hyperparameters",
    "section": "Previously - Feature engineering  ",
    "text": "Previously - Feature engineering  \n\nlibrary(textrecipes)\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  # Defaults to 32 signed indicator columns\n  step_dummy_hash(agent) %&gt;%\n  step_dummy_hash(company) %&gt;%\n  # Regular indicators for the others\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#tuning-parameters",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#tuning-parameters",
    "title": "2 - Tuning Hyperparameters",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#optimize-tuning-parameters",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#optimize-tuning-parameters",
    "title": "2 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#tagging-parameters-for-tuning",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#tagging-parameters-for-tuning",
    "title": "2 - Tuning Hyperparameters",
    "section": "Tagging parameters for tuning ",
    "text": "Tagging parameters for tuning \nWith tidymodels, you can mark the parameters that you want to optimize with a value of tune().\n\nThe function itself just returnsâ€¦ itself:\n\ntune()\n#&gt; tune()\nstr(tune())\n#&gt;  language tune()\n\n# optionally add a label\ntune(\"I hope that the workshop is going well\")\n#&gt; tune(\"I hope that the workshop is going well\")\n\n\nFor exampleâ€¦"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#optimizing-the-hash-features",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#optimizing-the-hash-features",
    "title": "2 - Tuning Hyperparameters",
    "section": "Optimizing the hash features   ",
    "text": "Optimizing the hash features   \nOur new recipe is:\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) %&gt;%\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) %&gt;%\n  step_zv(all_predictors())\n\n\nWe will be using a tree-based model in a minute.\n\nThe other categorical predictors are left as-is.\nThatâ€™s why there is no step_dummy()."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#boosted-trees",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#boosted-trees",
    "title": "2 - Tuning Hyperparameters",
    "section": "Boosted Trees",
    "text": "Boosted Trees\nThese are popular ensemble methods that build a sequence of tree models.\n\nEach tree uses the results of the previous tree to better predict samples, especially those that have been poorly predicted.\n\nEach tree in the ensemble is saved and new samples are predicted using a weighted average of the votes of each tree in the ensemble.\n\nWeâ€™ll focus on the popular lightgbm implementation."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "title": "2 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\nSome possible parameters:\n\nmtry: The number of predictors randomly sampled at each split (in \\([1, ncol(x)]\\) or \\((0, 1]\\)).\ntrees: The number of trees (\\([1, \\infty]\\), but usually up to thousands)\nmin_n: The number of samples needed to further split (\\([1, n]\\)).\nlearn_rate: The rate that each tree adapts from previous iterations (\\((0, \\infty]\\), usual maximum is 0.1).\nstop_iter: The number of iterations of boosting where no improvement was shown before stopping (\\([1, trees]\\))"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#boosted-tree-tuning-parameters-1",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#boosted-tree-tuning-parameters-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\nTBH it is usually not difficult to optimize these models.\n\nOften, there are multiple candidate tuning parameter combinations that have very good results.\n\nTo demonstrate simple concepts, weâ€™ll look at optimizing the number of trees in the ensemble (between 1 and 100) and the learning rate (\\(10^{-5}\\) to \\(10^{-1}\\))."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#boosted-tree-tuning-parameters-2",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#boosted-tree-tuning-parameters-2",
    "title": "2 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters   ",
    "text": "Boosted Tree Tuning Parameters   \nWeâ€™ll need to load the bonsai package. This has the information needed to use lightgbm\n\nlibrary(bonsai)\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\")\n\nlgbm_wflow &lt;- workflow(hash_rec, lgbm_spec)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search ğŸ’  which tests a pre-defined set of candidate values\nIterative search ğŸŒ€ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#grid-search",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#grid-search",
    "title": "2 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nA small grid of points trying to minimize the error via learning rate:"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#grid-search-1",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#grid-search-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nIn reality we would probably sample the space more densely:"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#iterative-search",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#iterative-search",
    "title": "2 - Tuning Hyperparameters",
    "section": "Iterative Search",
    "text": "Iterative Search\nWe could start with a few points and search the space:"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#parameters",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#parameters",
    "title": "2 - Tuning Hyperparameters",
    "section": "Parameters",
    "text": "Parameters\n\nThe tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\nThe extract_parameter_set_dials() function extracts these tuning parameters and the info.\n\n\nGrids\n\nCreate your grid manually or automatically.\nThe grid_*() functions can make a grid.\n\n\n\nMost basic (but very effective) way to tune models"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#create-a-grid",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#create-a-grid",
    "title": "2 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nlgbm_wflow %&gt;% \n  extract_parameter_set_dials()\n#&gt; Collection of 4 parameters for tuning\n#&gt; \n#&gt;    identifier       type    object\n#&gt;         trees      trees nparam[+]\n#&gt;    learn_rate learn_rate nparam[+]\n#&gt;    agent hash  num_terms nparam[+]\n#&gt;  company hash  num_terms nparam[+]\n\n# Individual functions: \ntrees()\n#&gt; # Trees (quantitative)\n#&gt; Range: [1, 2000]\nlearn_rate()\n#&gt; Learning Rate (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-10, -1]\n\n\nA parameter set can be updated (e.g.Â to change the ranges)."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#create-a-grid-1",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#create-a-grid-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\n\n\nset.seed(12)\ngrid &lt;- \n  lgbm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#&gt; # A tibble: 25 Ã— 4\n#&gt;    trees    learn_rate `agent hash` `company hash`\n#&gt;    &lt;int&gt;         &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1  1629 0.00000440             524           1454\n#&gt;  2  1746 0.0000000751          1009           2865\n#&gt;  3    53 0.0000180             2313            367\n#&gt;  4   442 0.000000445            347            460\n#&gt;  5  1413 0.0000000208          3232            553\n#&gt;  6  1488 0.0000578             3692            639\n#&gt;  7   906 0.000385               602            332\n#&gt;  8  1884 0.00000000101         1127            567\n#&gt;  9  1812 0.0239                 961           1183\n#&gt; 10   393 0.000000117            487           1783\n#&gt; # â„¹ 15 more rows\n\n\n\n\nA space-filling design tends to perform better than random grids.\nSpace-filling designs are also usually more efficient than regular grids."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#your-turn",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#your-turn",
    "title": "2 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a grid for our tunable workflow.\nTry creating a regular grid.\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#create-a-regular-grid",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#create-a-regular-grid",
    "title": "2 - Tuning Hyperparameters",
    "section": "Create a regular grid  ",
    "text": "Create a regular grid  \n\nset.seed(12)\ngrid &lt;- \n  lgbm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 4)\n\ngrid\n#&gt; # A tibble: 256 Ã— 4\n#&gt;    trees   learn_rate `agent hash` `company hash`\n#&gt;    &lt;int&gt;        &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1     1 0.0000000001          256            256\n#&gt;  2   667 0.0000000001          256            256\n#&gt;  3  1333 0.0000000001          256            256\n#&gt;  4  2000 0.0000000001          256            256\n#&gt;  5     1 0.0000001             256            256\n#&gt;  6   667 0.0000001             256            256\n#&gt;  7  1333 0.0000001             256            256\n#&gt;  8  2000 0.0000001             256            256\n#&gt;  9     1 0.0001                256            256\n#&gt; 10   667 0.0001                256            256\n#&gt; # â„¹ 246 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#your-turn-1",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#your-turn-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\n\nWhat advantage would a regular grid have?"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#update-parameter-ranges",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#update-parameter-ranges",
    "title": "2 - Tuning Hyperparameters",
    "section": "Update parameter ranges  ",
    "text": "Update parameter ranges  \n\nlgbm_param &lt;- \n  lgbm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  update(trees = trees(c(1L, 100L)),\n         learn_rate = learn_rate(c(-5, -1)))\n\nset.seed(712)\ngrid &lt;- \n  lgbm_param %&gt;% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#&gt; # A tibble: 25 Ã— 4\n#&gt;    trees learn_rate `agent hash` `company hash`\n#&gt;    &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1    75  0.000312          2991           1250\n#&gt;  2     4  0.0000337          899           3088\n#&gt;  3    15  0.0295             520           1578\n#&gt;  4     8  0.0997            1256           3592\n#&gt;  5    80  0.000622           419            258\n#&gt;  6    70  0.000474          2499           1089\n#&gt;  7    35  0.000165           287           2376\n#&gt;  8    64  0.00137            389            359\n#&gt;  9    58  0.0000250          616            881\n#&gt; 10    84  0.0639            2311           2635\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#the-results",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#the-results",
    "title": "2 - Tuning Hyperparameters",
    "section": "The results  ",
    "text": "The results  \n\n\ngrid %&gt;% \n  ggplot(aes(trees, learn_rate)) +\n  geom_point(size = 4) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\nNote that the learning rates are uniform on the log-10 scale."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#choosing-tuning-parameters",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#choosing-tuning-parameters",
    "title": "2 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters     ",
    "text": "Choosing tuning parameters     \nLetâ€™s take our previous model and tune more parameters:\n\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune(),  min_n = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\")\n\nlgbm_wflow &lt;- workflow(hash_rec, lgbm_spec)\n\n# Update the feature hash ranges (log-2 units)\nlgbm_param &lt;-\n  lgbm_wflow %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#grid-search-3",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#grid-search-3",
    "title": "2 - Tuning Hyperparameters",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nset.seed(9)\nctrl &lt;- control_grid(save_pred = TRUE)\n\nlgbm_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = 25,\n    # The options below are not required by default\n    param_info = lgbm_param, \n    control = ctrl,\n    metrics = reg_metrics\n  )\n\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#grid-search-4",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#grid-search-4",
    "title": "2 - Tuning Hyperparameters",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nlgbm_res \n#&gt; # Tuning results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics          .notes           .predictions        \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;              \n#&gt;  1 &lt;split [3372/377]&gt; Fold01 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,425 Ã— 9]&gt;\n#&gt;  2 &lt;split [3373/376]&gt; Fold02 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  3 &lt;split [3373/376]&gt; Fold03 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  4 &lt;split [3373/376]&gt; Fold04 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  5 &lt;split [3373/376]&gt; Fold05 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  6 &lt;split [3374/375]&gt; Fold06 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,375 Ã— 9]&gt;\n#&gt;  7 &lt;split [3375/374]&gt; Fold07 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,350 Ã— 9]&gt;\n#&gt;  8 &lt;split [3376/373]&gt; Fold08 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,325 Ã— 9]&gt;\n#&gt;  9 &lt;split [3376/373]&gt; Fold09 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,325 Ã— 9]&gt;\n#&gt; 10 &lt;split [3376/373]&gt; Fold10 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,325 Ã— 9]&gt;"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#grid-results",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#grid-results",
    "title": "2 - Tuning Hyperparameters",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(lgbm_res)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#tuning-results",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#tuning-results",
    "title": "2 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(lgbm_res)\n#&gt; # A tibble: 50 Ã— 11\n#&gt;    trees min_n learn_rate `agent hash` `company hash` .metric .estimator   mean     n std_err .config              \n#&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1   298    19   4.15e- 9          222             36 mae     standard   53.2      10 0.427   Preprocessor01_Model1\n#&gt;  2   298    19   4.15e- 9          222             36 rsq     standard    0.811    10 0.00785 Preprocessor01_Model1\n#&gt;  3  1394     5   5.82e- 6           28             21 mae     standard   52.9      10 0.424   Preprocessor02_Model1\n#&gt;  4  1394     5   5.82e- 6           28             21 rsq     standard    0.810    10 0.00857 Preprocessor02_Model1\n#&gt;  5   774    12   4.41e- 2           27             95 mae     standard   10.5      10 0.175   Preprocessor03_Model1\n#&gt;  6   774    12   4.41e- 2           27             95 rsq     standard    0.939    10 0.00381 Preprocessor03_Model1\n#&gt;  7  1342     7   6.84e-10           71             17 mae     standard   53.2      10 0.427   Preprocessor04_Model1\n#&gt;  8  1342     7   6.84e-10           71             17 rsq     standard    0.810    10 0.00903 Preprocessor04_Model1\n#&gt;  9   669    39   8.62e- 7          141            145 mae     standard   53.2      10 0.426   Preprocessor05_Model1\n#&gt; 10   669    39   8.62e- 7          141            145 rsq     standard    0.808    10 0.00661 Preprocessor05_Model1\n#&gt; # â„¹ 40 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#tuning-results-1",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#tuning-results-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(lgbm_res, summarize = FALSE)\n#&gt; # A tibble: 500 Ã— 10\n#&gt;    id     trees min_n    learn_rate `agent hash` `company hash` .metric .estimator .estimate .config              \n#&gt;    &lt;chr&gt;  &lt;int&gt; &lt;int&gt;         &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1 Fold01   298    19 0.00000000415          222             36 mae     standard      51.8   Preprocessor01_Model1\n#&gt;  2 Fold01   298    19 0.00000000415          222             36 rsq     standard       0.834 Preprocessor01_Model1\n#&gt;  3 Fold02   298    19 0.00000000415          222             36 mae     standard      52.1   Preprocessor01_Model1\n#&gt;  4 Fold02   298    19 0.00000000415          222             36 rsq     standard       0.801 Preprocessor01_Model1\n#&gt;  5 Fold03   298    19 0.00000000415          222             36 mae     standard      52.2   Preprocessor01_Model1\n#&gt;  6 Fold03   298    19 0.00000000415          222             36 rsq     standard       0.784 Preprocessor01_Model1\n#&gt;  7 Fold04   298    19 0.00000000415          222             36 mae     standard      51.7   Preprocessor01_Model1\n#&gt;  8 Fold04   298    19 0.00000000415          222             36 rsq     standard       0.828 Preprocessor01_Model1\n#&gt;  9 Fold05   298    19 0.00000000415          222             36 mae     standard      55.2   Preprocessor01_Model1\n#&gt; 10 Fold05   298    19 0.00000000415          222             36 rsq     standard       0.850 Preprocessor01_Model1\n#&gt; # â„¹ 490 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#choose-a-parameter-combination",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#choose-a-parameter-combination",
    "title": "2 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nshow_best(lgbm_res, metric = \"rsq\")\n#&gt; # A tibble: 5 Ã— 11\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1  1890    10    0.0159           115            174 rsq     standard   0.940    10 0.00369 Preprocessor12_Model1\n#&gt; 2   774    12    0.0441            27             95 rsq     standard   0.939    10 0.00381 Preprocessor03_Model1\n#&gt; 3  1638    36    0.0409            15            120 rsq     standard   0.938    10 0.00346 Preprocessor16_Model1\n#&gt; 4   963    23    0.00556          157             13 rsq     standard   0.930    10 0.00358 Preprocessor06_Model1\n#&gt; 5   590     5    0.00320           85             73 rsq     standard   0.905    10 0.00505 Preprocessor24_Model1"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \nCreate your own tibble for final parameters or use one of the tune::select_*() functions:\n\nlgbm_best &lt;- select_best(lgbm_res, metric = \"mae\")\nlgbm_best\n#&gt; # A tibble: 1 Ã— 6\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;                \n#&gt; 1   774    12     0.0441           27             95 Preprocessor03_Model1"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#checking-calibration",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#checking-calibration",
    "title": "2 - Tuning Hyperparameters",
    "section": "Checking Calibration  ",
    "text": "Checking Calibration  \n\n\nlibrary(probably)\nlgbm_res %&gt;%\n  collect_predictions(\n    parameters = lgbm_best\n  ) %&gt;%\n  cal_plot_regression(\n    truth = avg_price_per_room,\n    estimate = .pred,\n    alpha = 1 / 3\n  )"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#running-in-parallel",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#running-in-parallel",
    "title": "2 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\n\n\n\nGrid search, combined with resampling, requires fitting a lot of models!\nThese models donâ€™t depend on one another and can be run in parallel.\n\nWe can use a parallel backend to do this:\n\ncores &lt;- parallelly::availableCores(logical = FALSE)\ncl &lt;- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#running-in-parallel-1",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#running-in-parallel-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\nFaceted on the expensiveness of preprocessing used."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#early-stopping-for-boosted-trees",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#early-stopping-for-boosted-trees",
    "title": "2 - Tuning Hyperparameters",
    "section": "Early stopping for boosted trees",
    "text": "Early stopping for boosted trees\nWe have directly optimized the number of trees as a tuning parameter.\nInstead we could\n\nSet the number of trees to a single large number.\nStop adding trees when performance gets worse.\n\nThis is known as â€œearly stoppingâ€ and there is a parameter for that: stop_iter.\nEarly stopping has a potential to decrease the tuning time."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#your-turn-2",
    "href": "archive/2023-07-nyr/advanced-02-tuning-hyperparameters.html#your-turn-2",
    "title": "2 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\n\nSet trees = 2000 and tune the stop_iter parameter.\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2023-07-nyr/08-wrapping-up.html#your-turn",
    "href": "archive/2023-07-nyr/08-wrapping-up.html#your-turn",
    "title": "8 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-07-nyr/08-wrapping-up.html#resources-to-keep-learning",
    "href": "archive/2023-07-nyr/08-wrapping-up.html#resources-to-keep-learning",
    "title": "8 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://smltar.com/\n\n\n\nFollow us on Twitter and at the tidyverse blog for updates!"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nHow do you fit a linear model in R?\nHow many different ways can you think of?\n\n\n\nâˆ’+\n03:00\n\n\n\n\n\nlm for linear model\nglm for generalized linear model (e.g.Â logistic regression)\nglmnet for regularized regression\nkeras for regression using TensorFlow\nstan for Bayesian regression\nspark for large data sets"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-1",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-1",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg()\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\n\nModels have default engines"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-2",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-2",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-3",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-3",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg() %&gt;%\n  set_engine(\"glmnet\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glmnet"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-4",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-4",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg() %&gt;%\n  set_engine(\"stan\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: stan"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-5",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-5",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-6",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-6",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree()\n#&gt; Decision Tree Model Specification (unknown mode)\n#&gt; \n#&gt; Computational engine: rpart\n\n\nSome models have a default mode"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-7",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-7",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree() %&gt;% \n  set_mode(\"classification\")\n#&gt; Decision Tree Model Specification (classification)\n#&gt; \n#&gt; Computational engine: rpart\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-8",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#to-specify-a-model-8",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn-1",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn-1",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_spec chunk in your .qmd.\nEdit this code to use a different model.\n\n\n\nâˆ’+\n05:00\n\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#models-well-be-using-today",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#models-well-be-using-today",
    "title": "3 - What makes a model?",
    "section": "Models weâ€™ll be using today",
    "text": "Models weâ€™ll be using today\n\nLogistic regression\nDecision trees"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#logistic-regression",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#logistic-regression",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#logistic-regression-1",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#logistic-regression-1",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#logistic-regression-2",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#logistic-regression-2",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogit of outcome probability modeled as linear combination of predictors:\n\n\\(log(\\frac{p}{1 - p}) = \\beta_0 + \\beta_1\\cdot \\text{distance}\\)\n\nFind a sigmoid line that separates the two classes"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#decision-trees",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#decision-trees",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#decision-trees-1",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#decision-trees-1",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#decision-trees-2",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#decision-trees-2",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "title": "3 - What makes a model?",
    "section": "All models are wrong, but some are useful!",
    "text": "All models are wrong, but some are useful!\n\n\nLogistic regression\n\n\n\n\n\n\n\n\n\n\nDecision trees"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "title": "3 - What makes a model?",
    "section": "Workflows bind preprocessors and models",
    "text": "Workflows bind preprocessors and models\n\n\nExplain that PCA that is a preprocessor / dimensionality reduction, used to decorrelate data"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#what-is-wrong-with-this",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#what-is-wrong-with-this",
    "title": "3 - What makes a model?",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#why-a-workflow",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#why-a-workflow",
    "title": "3 - What makes a model?",
    "section": "Why a workflow()? ",
    "text": "Why a workflow()? \n\n\nWorkflows handle new data better than base R tools in terms of new factor levels\n\n\n\n\nYou can use other preprocessors besides formulas (more on feature engineering tomorrow!)\n\n\n\n\nThey can help organize your work when working with multiple models\n\n\n\n\nMost importantly, a workflow captures the entire modeling process: fit() and predict() apply to the preprocessing steps in addition to the actual model fit\n\n\nTwo ways workflows handle levels better than base R:\n\nEnforces that new levels are not allowed at prediction time (this is an optional check that can be turned off)\nRestores missing levels that were present at fit time, but happen to be missing at prediction time (like, if your â€œnewâ€ data just doesnâ€™t have an instance of that level)"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#a-model-workflow-1",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#a-model-workflow-1",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"classification\")\n\ntree_spec %&gt;% \n  fit(tip ~ ., data = taxi_train) \n#&gt; parsnip model object\n#&gt; \n#&gt; n= 7045 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 7045 2069 yes (0.70631654 0.29368346)  \n#&gt;    2) company=Chicago Independents,City Service,Sun Taxi,Taxicab Insurance Agency Llc,other 4328  744 yes (0.82809612 0.17190388)  \n#&gt;      4) distance&lt; 4.615 2365  254 yes (0.89260042 0.10739958) *\n#&gt;      5) distance&gt;=4.615 1963  490 yes (0.75038207 0.24961793)  \n#&gt;       10) distance&gt;=12.565 1069   81 yes (0.92422825 0.07577175) *\n#&gt;       11) distance&lt; 12.565 894  409 yes (0.54250559 0.45749441)  \n#&gt;         22) company=Chicago Independents,Sun Taxi,Taxicab Insurance Agency Llc 278   71 yes (0.74460432 0.25539568) *\n#&gt;         23) company=City Service,other 616  278 no (0.45129870 0.54870130)  \n#&gt;           46) distance&lt; 7.205 178   59 yes (0.66853933 0.33146067) *\n#&gt;           47) distance&gt;=7.205 438  159 no (0.36301370 0.63698630) *\n#&gt;    3) company=Flash Cab,Taxi Affiliation Services 2717 1325 yes (0.51232978 0.48767022)  \n#&gt;      6) distance&lt; 3.235 1331  391 yes (0.70623591 0.29376409) *\n#&gt;      7) distance&gt;=3.235 1386  452 no (0.32611833 0.67388167)  \n#&gt;       14) distance&gt;=12.39 344   90 yes (0.73837209 0.26162791) *\n#&gt;       15) distance&lt; 12.39 1042  198 no (0.19001919 0.80998081) *"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#a-model-workflow-2",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#a-model-workflow-2",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"classification\")\n\nworkflow() %&gt;%\n  add_formula(tip ~ .) %&gt;%\n  add_model(tree_spec) %&gt;%\n  fit(data = taxi_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; tip ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 7045 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 7045 2069 yes (0.70631654 0.29368346)  \n#&gt;    2) company=Chicago Independents,City Service,Sun Taxi,Taxicab Insurance Agency Llc,other 4328  744 yes (0.82809612 0.17190388)  \n#&gt;      4) distance&lt; 4.615 2365  254 yes (0.89260042 0.10739958) *\n#&gt;      5) distance&gt;=4.615 1963  490 yes (0.75038207 0.24961793)  \n#&gt;       10) distance&gt;=12.565 1069   81 yes (0.92422825 0.07577175) *\n#&gt;       11) distance&lt; 12.565 894  409 yes (0.54250559 0.45749441)  \n#&gt;         22) company=Chicago Independents,Sun Taxi,Taxicab Insurance Agency Llc 278   71 yes (0.74460432 0.25539568) *\n#&gt;         23) company=City Service,other 616  278 no (0.45129870 0.54870130)  \n#&gt;           46) distance&lt; 7.205 178   59 yes (0.66853933 0.33146067) *\n#&gt;           47) distance&gt;=7.205 438  159 no (0.36301370 0.63698630) *\n#&gt;    3) company=Flash Cab,Taxi Affiliation Services 2717 1325 yes (0.51232978 0.48767022)  \n#&gt;      6) distance&lt; 3.235 1331  391 yes (0.70623591 0.29376409) *\n#&gt;      7) distance&gt;=3.235 1386  452 no (0.32611833 0.67388167)  \n#&gt;       14) distance&gt;=12.39 344   90 yes (0.73837209 0.26162791) *\n#&gt;       15) distance&lt; 12.39 1042  198 no (0.19001919 0.80998081) *"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#a-model-workflow-3",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#a-model-workflow-3",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"classification\")\n\nworkflow(tip ~ ., tree_spec) %&gt;% \n  fit(data = taxi_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; tip ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 7045 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 7045 2069 yes (0.70631654 0.29368346)  \n#&gt;    2) company=Chicago Independents,City Service,Sun Taxi,Taxicab Insurance Agency Llc,other 4328  744 yes (0.82809612 0.17190388)  \n#&gt;      4) distance&lt; 4.615 2365  254 yes (0.89260042 0.10739958) *\n#&gt;      5) distance&gt;=4.615 1963  490 yes (0.75038207 0.24961793)  \n#&gt;       10) distance&gt;=12.565 1069   81 yes (0.92422825 0.07577175) *\n#&gt;       11) distance&lt; 12.565 894  409 yes (0.54250559 0.45749441)  \n#&gt;         22) company=Chicago Independents,Sun Taxi,Taxicab Insurance Agency Llc 278   71 yes (0.74460432 0.25539568) *\n#&gt;         23) company=City Service,other 616  278 no (0.45129870 0.54870130)  \n#&gt;           46) distance&lt; 7.205 178   59 yes (0.66853933 0.33146067) *\n#&gt;           47) distance&gt;=7.205 438  159 no (0.36301370 0.63698630) *\n#&gt;    3) company=Flash Cab,Taxi Affiliation Services 2717 1325 yes (0.51232978 0.48767022)  \n#&gt;      6) distance&lt; 3.235 1331  391 yes (0.70623591 0.29376409) *\n#&gt;      7) distance&gt;=3.235 1386  452 no (0.32611833 0.67388167)  \n#&gt;       14) distance&gt;=12.39 344   90 yes (0.73837209 0.26162791) *\n#&gt;       15) distance&lt; 12.39 1042  198 no (0.19001919 0.80998081) *"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn-2",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn-2",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_wflow chunk in your .qmd.\nEdit this code to make a workflow with your own model of choice.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#predict-with-your-model",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#predict-with-your-model",
    "title": "3 - What makes a model?",
    "section": "Predict with your model  ",
    "text": "Predict with your model  \nHow do you use your new tree_fit model?\n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"classification\")\n\ntree_fit &lt;-\n  workflow(tip ~ ., tree_spec) %&gt;% \n  fit(data = taxi_train)"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn-3",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn-3",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\npredict(tree_fit, new_data = taxi_test)\nWhat do you get?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn-4",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn-4",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\naugment(tree_fit, new_data = taxi_test)\nWhat do you get?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#understand-your-model",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#understand-your-model",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#understand-your-model-1",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#understand-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nlibrary(rpart.plot)\ntree_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot(roundint = FALSE)\n\nYou can extract_*() several components of your fitted workflow.\n\nroundint = FALSE is only to quiet a warning"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#understand-your-model-2",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#understand-your-model-2",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nYou can use your fitted workflow for model and/or prediction explanations:\n\n\n\noverall variable importance, such as with the vip package\n\n\n\n\nflexible model explainers, such as with the DALEXtra package\n\n\n\nLearn more at https://www.tmwr.org/explain.html"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn-5",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn-5",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nExtract the model engine object from your fitted workflow.\nâš ï¸ Never predict() with any extracted components!\n\n\n\nâˆ’+\n05:00\n\n\n\n\nAfterward, ask what kind of object people got from the extraction, and what they did with it (e.g.Â give it to summary(), plot(), broom::tidy() ). Live code along"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#deploying-a-model",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#deploying-a-model",
    "title": "3 - What makes a model?",
    "section": "Deploying a model ",
    "text": "Deploying a model \nHow do you use your new tree_fit model in production?\n\nlibrary(vetiver)\nv &lt;- vetiver_model(tree_fit, \"taxi\")\nv\n#&gt; \n#&gt; â”€â”€ taxi â”€ &lt;bundled_workflow&gt; model for deployment \n#&gt; A rpart classification modeling workflow using 6 features\n\nLearn more at https://vetiver.rstudio.com"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#deploy-your-model-1",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#deploy-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Deploy your model ",
    "text": "Deploy your model \nHow do you use your new model tree_fit in production?\n\nlibrary(plumber)\npr() %&gt;%\n  vetiver_api(v)\n#&gt; # Plumber router with 2 endpoints, 4 filters, and 1 sub-router.\n#&gt; # Use `pr_run()` on this object to start the API.\n#&gt; â”œâ”€â”€[queryString]\n#&gt; â”œâ”€â”€[body]\n#&gt; â”œâ”€â”€[cookieParser]\n#&gt; â”œâ”€â”€[sharedSecret]\n#&gt; â”œâ”€â”€/logo\n#&gt; â”‚  â”‚ # Plumber static router serving from directory: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/vetiver\n#&gt; â”œâ”€â”€/ping (GET)\n#&gt; â””â”€â”€/predict (POST)\n\nLearn more at https://vetiver.rstudio.com\n\nLive-code making a prediction"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn-6",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#your-turn-6",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the vetiver chunk in your .qmd.\nCheck out the automated visual documentation.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-07-nyr/03-what-makes-a-model.html#the-whole-game---status-update",
    "href": "archive/2023-07-nyr/03-what-makes-a-model.html#the-whole-game---status-update",
    "title": "3 - What makes a model?",
    "section": "The whole game - status update",
    "text": "The whole game - status update\n\n\nStress that fitting a model on the entire training set was only for illustrating how to fit a model"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#who-are-you",
    "href": "archive/2023-07-nyr/01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %&gt;% or base R |&gt; pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have exposure to basic statistical concepts\nYou do not need intermediate or expert familiarity with modeling or ML"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#who-are-tidymodels",
    "href": "archive/2023-07-nyr/01-introduction.html#who-are-tidymodels",
    "title": "1 - Introduction",
    "section": "Who are tidymodels?",
    "text": "Who are tidymodels?\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\nMany thanks to Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill, and DesirÃ©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#asking-for-help",
    "href": "archive/2023-07-nyr/01-introduction.html#asking-for-help",
    "title": "1 - Introduction",
    "section": "Asking for help",
    "text": "Asking for help\n\nğŸŸª â€œIâ€™m stuck and need help!â€\n\n\nğŸŸ© â€œI finished the exerciseâ€"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#section",
    "href": "archive/2023-07-nyr/01-introduction.html#section",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#tentative-plan-for-this-workshop",
    "href": "archive/2023-07-nyr/01-introduction.html#tentative-plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Tentative plan for this workshop",
    "text": "Tentative plan for this workshop\n\n\n\nToday:\n\nYour data budget\nWhat makes a model\nEvaluating models\n\n\n\n\nTomorrow:\n\nFeature engineering\nTuning hyperparameters\nRacing methods\nIterative search methods"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#section-1",
    "href": "archive/2023-07-nyr/01-introduction.html#section-1",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors ğŸ‘‹\n\nCheck Slack (#ml-ws-2023) for an RStudio Cloud link."
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#what-is-machine-learning",
    "href": "archive/2023-07-nyr/01-introduction.html#what-is-machine-learning",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nhttps://xkcd.com/1838/"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#what-is-machine-learning-1",
    "href": "archive/2023-07-nyr/01-introduction.html#what-is-machine-learning-1",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#what-is-machine-learning-2",
    "href": "archive/2023-07-nyr/01-introduction.html#what-is-machine-learning-2",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#your-turn",
    "href": "archive/2023-07-nyr/01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\n\n\nHow are statistics and machine learning related?\nHow are they similar? Different?\n\n\n\nâˆ’+\n03:00\n\n\n\n\nthe â€œtwo culturesâ€\nmodel first vs.Â data first\ninference vs.Â prediction"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#what-is-tidymodels",
    "href": "archive/2023-07-nyr/01-introduction.html#what-is-tidymodels",
    "title": "1 - Introduction",
    "section": "What is tidymodels? ",
    "text": "What is tidymodels? \n\nlibrary(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.1.0 â”€â”€\n#&gt; âœ” broom        1.0.5          âœ” rsample      1.1.1.9000\n#&gt; âœ” dials        1.2.0          âœ” tibble       3.2.1     \n#&gt; âœ” dplyr        1.1.2          âœ” tidyr        1.3.0     \n#&gt; âœ” infer        1.0.4          âœ” tune         1.1.1.9001\n#&gt; âœ” modeldata    1.1.0          âœ” workflows    1.1.3     \n#&gt; âœ” parsnip      1.1.0.9003     âœ” workflowsets 1.0.1     \n#&gt; âœ” purrr        1.0.1          âœ” yardstick    1.2.0.9001\n#&gt; âœ” recipes      1.0.6\n#&gt; â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\n#&gt; âœ– purrr::discard() masks scales::discard()\n#&gt; âœ– dplyr::filter()  masks stats::filter()\n#&gt; âœ– dplyr::lag()     masks stats::lag()\n#&gt; âœ– recipes::step()  masks stats::step()\n#&gt; â€¢ Use tidymodels_prefer() to resolve common conflicts."
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#the-whole-game",
    "href": "archive/2023-07-nyr/01-introduction.html#the-whole-game",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\nPart of any modelling process is\n\nSplitting your data into training and test set\nUsing a resampling scheme\nFitting models\nAssessing performance\nChoosing a model\nFitting and assessing the final model"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#the-whole-game-1",
    "href": "archive/2023-07-nyr/01-introduction.html#the-whole-game-1",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#the-whole-game-2",
    "href": "archive/2023-07-nyr/01-introduction.html#the-whole-game-2",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\n\nStress that we are not fitting a model on the entire training set other than for illustrative purposes in deck 2."
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#the-whole-game-3",
    "href": "archive/2023-07-nyr/01-introduction.html#the-whole-game-3",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#the-whole-game-4",
    "href": "archive/2023-07-nyr/01-introduction.html#the-whole-game-4",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#the-whole-game-5",
    "href": "archive/2023-07-nyr/01-introduction.html#the-whole-game-5",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#the-whole-game-6",
    "href": "archive/2023-07-nyr/01-introduction.html#the-whole-game-6",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#the-whole-game-7",
    "href": "archive/2023-07-nyr/01-introduction.html#the-whole-game-7",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#lets-install-some-packages",
    "href": "archive/2023-07-nyr/01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Letâ€™s install some packages",
    "text": "Letâ€™s install some packages\nIf you are using your own laptop instead of RStudio Cloud:\n\ninstall.packages(\"pak\")\n\npkgs &lt;- c(\"bonsai\", \"doParallel\", \"embed\", \"finetune\", \"lightgbm\", \"lme4\", \n          \"parallelly\", \"plumber\", \"probably\", \"ranger\", \"rpart\", \"rpart.plot\", \n          \"stacks\", \"textrecipes\", \"tidymodels\", \"tidymodels/modeldatatoo\", \n          \"vetiver\")\npak::pak(pkgs)\n\n\nCheck Slack (#ml-ws-2023) for an RStudio Cloud link."
  },
  {
    "objectID": "archive/2023-07-nyr/01-introduction.html#our-versions",
    "href": "archive/2023-07-nyr/01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\nbonsai (0.2.1.9000, Github (tidymodels/bonsai@aab79), broom (1.0.5, local), dials (1.2.0, CRAN), doParallel (1.0.17, CRAN), dplyr (1.1.2, CRAN), embed (1.0.0, CRAN), finetune (1.1.0.9000, Github (tidymodels/finetune@52d), ggplot2 (3.4.2, CRAN), lightgbm (3.3.5, CRAN), lme4 (1.1-33, CRAN), modeldata (1.1.0, CRAN), modeldatatoo (0.1.0.9000, Github (tidymodels/modeldatatoo), parallelly (1.36.0, CRAN), parsnip (1.1.0.9003, Github (tidymodels/parsnip@e627), plumber (1.2.1, CRAN), probably (1.0.2, CRAN), purrr (1.0.1, CRAN), ranger (0.15.1, CRAN), recipes (1.0.6, CRAN), rpart (4.1.19, CRAN), rpart.plot (3.1.1, CRAN), rsample (1.1.1.9000, Github (tidymodels/rsample@afc4), scales (1.2.1, CRAN), stacks (1.0.2.9000, local), textrecipes (1.0.2, CRAN), tibble (3.2.1, CRAN), tidymodels (1.1.0, CRAN), tidyr (1.3.0, CRAN), tune (1.1.1.9001, Github (tidymodels/tune@fea8b02), vetiver (0.2.0, CRAN), workflows (1.1.3, CRAN), workflowsets (1.0.1, CRAN), yardstick (1.2.0.9001, Github (tidymodels/yardstick@6c), and Quarto (1.3.433)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html",
    "href": "archive/2022-08-Reykjavik-City/annotations.html",
    "title": "Annotations",
    "section": "",
    "text": "This page contains annotations for selected slides.\nThereâ€™s a lot that we want to tell you. We donâ€™t want people to have to frantically scribble down things that we say that are not on the slides.\nWeâ€™ve added sections to this document with longer explanations and links to other resources.\n\n\n\nThis is a pretty complex data usage scheme. That is mostly because of the validation set. In every other case, the situation is much more simple.\nThe important point here is that: tidymodels does most of this work for you. In other words, you donâ€™t have to directly specify which data are being used where.\nIn a later section, we will talk about methods of resampling. These methods are like repeated validation sets. As an example, the popular 10-fold cross-validation method is one such type of resampling. Validation sets are special cases of resampling where there is a single â€œresampleâ€.\nMost types of resampling use multiple hold-out sets of samples from the training set. In those cases, a diagram for data usage here would look like\n\n\n\n\n\n\n\n\n\nIn this case there is just â€œtestingâ€ and â€œtrainingâ€. Once the final model is determined, the entire training set is used for the last fit.\nThis is the process that will be used for the tree frog data."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#section",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#section",
    "title": "Annotations",
    "section": "",
    "text": "This page contains annotations for selected slides.\nThereâ€™s a lot that we want to tell you. We donâ€™t want people to have to frantically scribble down things that we say that are not on the slides.\nWeâ€™ve added sections to this document with longer explanations and links to other resources."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#finalize-and-verify",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#finalize-and-verify",
    "title": "Annotations",
    "section": "",
    "text": "This is a pretty complex data usage scheme. That is mostly because of the validation set. In every other case, the situation is much more simple.\nThe important point here is that: tidymodels does most of this work for you. In other words, you donâ€™t have to directly specify which data are being used where.\nIn a later section, we will talk about methods of resampling. These methods are like repeated validation sets. As an example, the popular 10-fold cross-validation method is one such type of resampling. Validation sets are special cases of resampling where there is a single â€œresampleâ€.\nMost types of resampling use multiple hold-out sets of samples from the training set. In those cases, a diagram for data usage here would look like\n\n\n\n\n\n\n\n\n\nIn this case there is just â€œtestingâ€ and â€œtrainingâ€. Once the final model is determined, the entire training set is used for the last fit.\nThis is the process that will be used for the tree frog data."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#data-splitting-and-spending",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#data-splitting-and-spending",
    "title": "Annotations",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nWhat does set.seed() do?\nWeâ€™ll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random).\nThink of PRN as a box that takes a starting value (the â€œseedâ€) that produces random numbers using that starting value as an input into its process.\nIf we know a seed value, we can reproduce our â€œrandomâ€ numbers. To use a different set of random numbers, choose a different seed value.\nFor example:\n\nset.seed(1)\nrunif(3)\n\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\n# Get a new set of random numbers:\nset.seed(2)\nrunif(3)\n\n#&gt; [1] 0.1848823 0.7023740 0.5733263\n\n# We can reproduce the old ones with the same seed\nset.seed(1)\nrunif(3)\n\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\n\nIf we donâ€™t set the seed, R uses the clock time and the process ID to create a seed. This isnâ€™t reproducible.\nSince we want our code to be reproducible, we set the seeds before random numbers are used.\nIn theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same and we donâ€™t get reproducible results.\nThe value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to â€œspread the randomness aroundâ€. It is basically:\n\ncat(paste0(\"set.seed(\", sample.int(10000, 5), \")\", collapse = \"\\n\"))\n\n#&gt; set.seed(9725)\n#&gt; set.seed(8462)\n#&gt; set.seed(4050)\n#&gt; set.seed(8789)\n#&gt; set.seed(1301)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#what-is-wrong-with-this",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#what-is-wrong-with-this",
    "title": "Annotations",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?\nIf we treat the preprocessing as a separate task, it raises the risk that we might accidentally overfit to the data at hand.\nFor example, someone might estimate something from the entire data set (such as the principle components) and treat that data as if it were known (and not estimated). Depending on the what was done with the data, consequences in doing that could be:\n\nYour performance metrics are slightly-to-moderately optimistic (e.g.Â you might think your accuracy is 85% when it is actually 75%)\nA consequential component of the analysis is not right and the model just doesnâ€™t work.\n\nThe big issue here is that you wonâ€™t be able to figure this out until you get a new piece of data, such as the test set.\nA really good example of this is in â€˜Selection bias in gene extraction on the basis of microarray gene-expression dataâ€™. The authors re-analyze a previous publication and show that the original researchers did not include feature selection in the workflow. Because of that, their performance statistics were extremely optimistic. In one case, they could do the original analysis on complete noise and still achieve zero errors.\nGenerally speaking, this problem is referred to as data leakage. Some other references:\n\nOverfitting to Predictors and External Validation\nAre We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning\nNavigating the pitfalls of applying machine learning in genomics\nA review of feature selection techniques in bioinformatics\nOn Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#where-are-the-fitted-models",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#where-are-the-fitted-models",
    "title": "Annotations",
    "section": "Where are the fitted models?",
    "text": "Where are the fitted models?\nThe primary purpose of resampling is to estimate model performance. The models are almost never needed again.\nAlso, if the data set is large, the model object may require a lot of memory to save so, by default, we donâ€™t keep them.\nFor more advanced use cases, you can extract and save them. See:\n\nhttps://www.tmwr.org/resampling.html#extract\nhttps://www.tidymodels.org/learn/models/coefficients/ (an example)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#the-final-fit",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#the-final-fit",
    "title": "Annotations",
    "section": "The final fit",
    "text": "The final fit\nSince our data spending scheme created the resamples from the training set, last_fit() will use all of the training data to fit the final workflow.\nAs shown in the Whole Game slides, there is a slightly different scheme used when we have a validation set (instead of multiple resamples like 10-fold CV)."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#using-a-workflow",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#using-a-workflow",
    "title": "Annotations",
    "section": "Using a workflow",
    "text": "Using a workflow\nWhatâ€™s going on with the\n\nprediction from a rank-deficient fit may be misleading\n\nwarnings?\nFor linear regression, a computation is used called matrix inversion. The matrix in question is called the â€œmodel matrixâ€ and it contains the predictor set for the training data.\nMatrix inversion can fail if two or more columns:\n\nare identical, or\nadd up to some other column.\n\nThese situations are called linear dependencies.\nWhen this happens, lm() is pretty tolerant. It does not fail but does not compute regression coefficients for a minimal number of predictors involved in the dependency (and issues the warning above).\nFor these data, there is this dependency:\nshooter_type_{level} = shooter_{level}\nHere is what is happening: since the player shooting only ever plays a single position, their indicators sum up (row-wide) to the same data as the sum of the shooter type indicators.\nIn other words, if you know the playerâ€™s name, you know their position too. This is a perfect redundency in the data.\nThe way to avoid this problem is to use step_lincomb(all_numeric_predictors()) in the recipe. This step removes the minimum number of columns to avoid the issue.\nJust in case you ever want to figure out what the specific issues are, this code might help:\n\n# Get the exact data set used to fit the model. For a recipe, that is:\nprocessed_data &lt;- \n  nhl_indicators %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL, all_predictors())\n\n# Let's capture their names:\npred_names &lt;- names(processed_data)\n\n# caret has a function to determine the variables involved in each dependency\nissues &lt;- caret::findLinearCombos(processed_data)\n# Convert the column index to names:\nissues_vars &lt;- purrr::map(issues$linearCombos, ~ pred_names[.x])\n\n# caret proposes removing these columns to get rid of the issue. The choice\n# is pretty arbitrary: \nremoved_names &lt;- pred_names[issues$remove]\n\ntl;dr\nLinear regression detects some redundancies in the predictor set. We can ignore the warnings since lm() can deal with it or use step_lincomb() to avoid the warnings."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#per-player-statistics",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#per-player-statistics",
    "title": "Annotations",
    "section": "Per-player statistics",
    "text": "Per-player statistics\nThe effect encoding method essentially takes the effect of a variable, like player, and makes a data column for that effect. In our example, the ability of a player to have an on-goal shot is quantified by a model and then added as a data column to be used in the model.\nSuppose NHL rookie Max has a single shot in the data and it was on goal. If we used a naive estimate for Maxâ€™s effect, the model is being told that Max should have a 100% chance of being on goal. Thatâ€™s a very poor estimate since it is from a single data point.\nContrast this with seasoned player Davis, who has taken 250 shots and 75% of these were on goal. Davisâ€™s proportion is more predictive because it is estimated with better data (i.e., more total shots). Partial pooling leverages the entire data set and can borrow strength from all of the players. It is a common tool in Bayesian estimation and non-Bayesian mixed models. If a playerâ€™s data is of good quality, the partial pooling effect estimate is closer to the raw proportion. Maxâ€™s data is not great and is â€œshrunkâ€ towards the center of the overall on goal proportion. Since there is so little known about Maxâ€™s shot history, this is a better effect estimate (until more data is available for him).\nThe Stan documentation has a pretty good vignette on this: https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html\nAlso, Bayes Rules! has a nice section on this: https://www.bayesrulesbook.com/chapter-15.html\nIf the outcome were numeric, the effect would be the mean of the outcome per player. In this case, partial pooling is very similar to the Jamesâ€“Stein estimator: https://en.wikipedia.org/wiki/Jamesâ€“Stein_estimator"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#player-effects",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#player-effects",
    "title": "Annotations",
    "section": "Player effects",
    "text": "Player effects\nEffect encoding might result in a somewhat circular argument: the column is more likely to be important to the model since it is the output of a separate model. The risk here is that we might over-fit the effect to the data. For this reason, it is super important to make sure that we verify that we arenâ€™t overfitting by checking with resampling (or a validation set).\nPartial pooling somewhat lowers the risk of overfitting since it tends to correct for players with small sample sizes. It canâ€™t correct for improper data usage or data leakage though."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#angle",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#angle",
    "title": "Annotations",
    "section": "Angle",
    "text": "Angle\nAbout geometryâ€¦\nThe coordinates for the rink are centered at (0, 0) and the goal lines are both 89 ft from center. The center of the goal on the left is at (-89, 0) and the right-hand goal is centered at (89, 0).\nFor angle to center of the goal mouth, the formula is\n\\[a = \\tan^{-1}\\left(\\frac{y}{x}\\right)\\] This is in radian units and we can convert to degrees using\n\\[a = \\frac{180}{\\pi}\\tan^{-1}\\left(\\frac{y}{x}\\right)\\] For the angle to the goal, we need to alter \\(x\\) and use x* = (89 - abs(coord_x)) instead."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#update-parameter-ranges",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#update-parameter-ranges",
    "title": "Annotations",
    "section": "Update parameter ranges",
    "text": "Update parameter ranges\nIn about 90% of the cases, the dials function that you use to update the parameter range has the same name as the argument. For example, if you were to update the mtry parameter in a random forests model, the code would look like\n\nparameter_object %&gt;% \n  update(mtry = mtry(c(1, 100)))\n\nIn our case, the argument name is deg_free but we update it with spline_degree().\ndeg_free represents the general concept of degrees of freedom and could be associated with many different things. For example, if we ever had an argument that was the number of degrees of freedom for a \\(t\\) distribution, we would call that argument deg_free.\nFor splines, we probably want a wider range for the degrees of freedom. We made a specialized function called spline_degree() to be used in these cases.\nHow can you tell when this happens? There is a helper function called tunable() and that gives information on how we make the default ranges for parameters. There is a column in these objects names call_info:\n\nlibrary(tidymodels)\nns_tunable &lt;- \n  recipe(mpg ~ ., data = mtcars) %&gt;% \n  step_ns(dis, deg_free = tune()) %&gt;% \n  tunable()\n\nns_tunable\n\n#&gt; # A tibble: 1 Ã— 5\n#&gt;   name     call_info        source component component_id\n#&gt;   &lt;chr&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;       \n#&gt; 1 deg_free &lt;named list [3]&gt; recipe step_ns   ns_P1Tjg\n\nns_tunable$call_info\n\n#&gt; [[1]]\n#&gt; [[1]]$pkg\n#&gt; [1] \"dials\"\n#&gt; \n#&gt; [[1]]$fun\n#&gt; [1] \"spline_degree\"\n#&gt; \n#&gt; [[1]]$range\n#&gt; [1]  1 15"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#boosted-tree-tuning-parameters",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#boosted-tree-tuning-parameters",
    "title": "Annotations",
    "section": "Boosted tree tuning parameters",
    "text": "Boosted tree tuning parameters\nWhen deciding on the number of boosting iterations, there are two main strategies:\n\nDirectly tune it (trees = tune())\nSet it to one value and tune the number of early stopping iterations (trees = 500, stop_iter = tune()).\n\nEarly stopping is when we monitor the performance of the model. If the model doesnâ€™t make any improvements for stop_iter iterations, training stops.\nHereâ€™s an example where, after eleven iterations, performance starts to get worse.\n\n\n\n\n\n\n\n\n\nThis is likely due to over-fitting so we stop the model at eleven boosting iterations.\nEarly stopping usually has good results and takes far less time.\nWe could an engine argument called validation here. Thatâ€™s not an argument to any function in the xgboost package.\nparsnip has its own wrapper around (xgboost::xgb.train()) called xgb_train(). We use that here and it has a validation argument.\nHow would you know that? There are a few different ways:\n\nLook at the documentation in ?boost_tree and click on the xgboost entry in the engine list.\nCheck out the pkgdown reference website https://parsnip.tidymodels.org/reference/index.html\nRun the translate() function on the parsnip specification object.\n\nThe first two options are best since they tell you a lot more about the particularities of each model engine (there are a lot for xgboost)."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#the-final-fit-to-the-nhl-data",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#the-final-fit-to-the-nhl-data",
    "title": "Annotations",
    "section": "The final fit to the NHL data",
    "text": "The final fit to the NHL data\nRecall that last_fit() uses the objects produced by initial_split() to determine what data are used for the final model fit and which are used as the test set.\nFor the validation set, last_fit() will use the non-testing data to create the final model fit. This includes the training and validation set.\nThere is no agreement in the community on whether this is the best approach or if we should just use the training set. There are good arguments either way.\nIf you only want to use the training set for the final model, you can do this via:\n\ntraining_data &lt;- nhl_val$splits[[1]] %&gt;% analysis()\n\n# Use `fit()` to train the model on just the training set\nfinal_glm_spline_wflow &lt;- \n  glm_spline_wflow %&gt;% \n  fit(data = training_data)\n\n# Create test set predictions\ntest_set_pred &lt;- augment(final_glm_spline_wflow, nhl_test)\n\n# Setup and compute the test set metrics\ncls_metrics &lt;- metric_set(roc_auc, accuracy)\n\ntest_res &lt;- \n  test_set_pred %&gt;% \n  cls_metrics(on_goal, estimate = .pred_class, .pred_yes)\ntest_res"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#explain-yourself",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#explain-yourself",
    "title": "Annotations",
    "section": "Explain yourself",
    "text": "Explain yourself\nSome other resources:\n\nTMwR chapter Explaining Models and Predictions\nExplanatory Model Analysis book\nInterpretable Machine Learning book\nDefinitions, methods, and applications in interpretable machine learning"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/annotations.html#a-tidymodels-explainer",
    "href": "archive/2022-08-Reykjavik-City/annotations.html#a-tidymodels-explainer",
    "title": "Annotations",
    "section": "A tidymodels explainer",
    "text": "A tidymodels explainer\nFor our example, the angle was an original predictor. Recall that we made spline terms from this predictor, so there are derived features such as angle_ns_1 and so on.\nOriginal versus derived doesnâ€™t affect local explainers since we are focused on a single prediction.\nFor global explainers, we should decide between:\n\nexplaining the overall affect of angle (lumping all its features into one importance score), or\nexplaining the effect of each term in the model (including angle_ns_1 and so on).\n\nThe choice depends on what you want. For example, if we have an original date predictor and make features for month and year, is it more informative to know if date is important (overall) or exactly how the date is important? You might want to look at it both ways."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#chicago-l-train-data",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#chicago-l-train-data",
    "title": "7 - Case Study on Transportation",
    "section": "Chicago L-Train data",
    "text": "Chicago L-Train data\nSeveral years worth of pre-pandemic data were assembled to try to predict the daily number of people entering the Clark and Lake elevated (â€œLâ€) train station in Chicago.\nMore information:\n\nSeveral Chapters in Feature Engineering and Selection.\n\nStart with Section 4.1\nSee Section 1.3\n\nVideo: The Global Pandemic Ruined My Favorite Data Set"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#predictors",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#predictors",
    "title": "7 - Case Study on Transportation",
    "section": "Predictors",
    "text": "Predictors\n\nthe 14-day lagged ridership at this and other stations (units: thousands of rides/day)\nweather data\nhome/away game schedules for Chicago teams\nthe date\n\nThe data are in modeldata. See ?Chicago."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#l-train-locations",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#l-train-locations",
    "title": "7 - Case Study on Transportation",
    "section": "L Train Locations",
    "text": "L Train Locations"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#your-turn-explore-the-data",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#your-turn-explore-the-data",
    "title": "7 - Case Study on Transportation",
    "section": "Your turn: Explore the Data",
    "text": "Your turn: Explore the Data\nTake a look at these data for a few minutes and see if you can find any interesting characteristics in the predictors or the outcome.\n\nlibrary(tidymodels)\nlibrary(rules)\ndata(\"Chicago\")\ndim(Chicago)\n#&gt; [1] 5698   50\nstations\n#&gt;  [1] \"Austin\"           \"Quincy_Wells\"     \"Belmont\"          \"Archer_35th\"     \n#&gt;  [5] \"Oak_Park\"         \"Western\"          \"Clark_Lake\"       \"Clinton\"         \n#&gt;  [9] \"Merchandise_Mart\" \"Irving_Park\"      \"Washington_Wells\" \"Harlem\"          \n#&gt; [13] \"Monroe\"           \"Polk\"             \"Ashland\"          \"Kedzie\"          \n#&gt; [17] \"Addison\"          \"Jefferson_Park\"   \"Montrose\"         \"California\"\n\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#splitting-with-chicago-data",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#splitting-with-chicago-data",
    "title": "7 - Case Study on Transportation",
    "section": "Splitting with Chicago data ",
    "text": "Splitting with Chicago data \nLetâ€™s put the last two weeks of data into the test set. initial_time_split() can be used for this purpose:\n\ndata(Chicago)\n\nchi_split &lt;- initial_time_split(Chicago, prop = 1 - (14/nrow(Chicago)))\nchi_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;5684/14/5698&gt;\n\nchi_train &lt;- training(chi_split)\nchi_test  &lt;- testing(chi_split)\n\n## training\nnrow(chi_train)\n#&gt; [1] 5684\n \n## testing\nnrow(chi_test)\n#&gt; [1] 14"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#time-series-resampling",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#time-series-resampling",
    "title": "7 - Case Study on Transportation",
    "section": "Time series resampling",
    "text": "Time series resampling\nOur Chicago data is over time. Regular cross-validation, which uses random sampling, may not be the best idea.\nWe can emulate our training/test split by making similar resamples.\n\nFold 1: Take the first X years of data as the analysis set, the next 2 weeks as the assessment set.\nFold 2: Take the first X years + 2 weeks of data as the analysis set, the next 2 weeks as the assessment set.\nand so on"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#rolling-forecast-origin-resampling",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#rolling-forecast-origin-resampling",
    "title": "7 - Case Study on Transportation",
    "section": "Rolling forecast origin resampling",
    "text": "Rolling forecast origin resampling\n\n\nThis image shows overlapping assessment sets. We will use non-overlapping data but it could be done wither way."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#times-series-resampling",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#times-series-resampling",
    "title": "7 - Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n\n\n\n\n  )\n\nUse the date column to find the date data."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#times-series-resampling-1",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#times-series-resampling-1",
    "title": "7 - Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n\n\n\n  )\n\nOur units will be weeks."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#times-series-resampling-2",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#times-series-resampling-2",
    "title": "7 - Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15  \n    \n    \n  )\n\nEvery analysis set has 15 years of data"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#times-series-resampling-3",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#times-series-resampling-3",
    "title": "7 - Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n\n  )\n\nEvery assessment set has 2 weeks of data"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#times-series-resampling-4",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#times-series-resampling-4",
    "title": "7 - Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n    step = 2 \n  )\n\nIncrement by 2 weeks so that there are no overlapping assessment sets.\n\nchi_rs$splits[[1]] %&gt;% assessment() %&gt;% pluck(\"date\") %&gt;% range()\n#&gt; [1] \"2016-01-07\" \"2016-01-20\"\nchi_rs$splits[[2]] %&gt;% assessment() %&gt;% pluck(\"date\") %&gt;% range()\n#&gt; [1] \"2016-01-21\" \"2016-02-03\""
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#our-resampling-object",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#our-resampling-object",
    "title": "7 - Case Study on Transportation",
    "section": "Our resampling object ",
    "text": "Our resampling object \n\n\n\nchi_rs\n#&gt; # Sliding period resampling \n#&gt; # A tibble: 16 Ã— 2\n#&gt;    splits            id     \n#&gt;    &lt;list&gt;            &lt;chr&gt;  \n#&gt;  1 &lt;split [5463/14]&gt; Slice01\n#&gt;  2 &lt;split [5467/14]&gt; Slice02\n#&gt;  3 &lt;split [5467/14]&gt; Slice03\n#&gt;  4 &lt;split [5467/14]&gt; Slice04\n#&gt;  5 &lt;split [5467/14]&gt; Slice05\n#&gt;  6 &lt;split [5467/14]&gt; Slice06\n#&gt;  7 &lt;split [5467/14]&gt; Slice07\n#&gt;  8 &lt;split [5467/14]&gt; Slice08\n#&gt;  9 &lt;split [5467/14]&gt; Slice09\n#&gt; 10 &lt;split [5467/14]&gt; Slice10\n#&gt; 11 &lt;split [5467/14]&gt; Slice11\n#&gt; 12 &lt;split [5467/14]&gt; Slice12\n#&gt; 13 &lt;split [5467/14]&gt; Slice13\n#&gt; 14 &lt;split [5467/14]&gt; Slice14\n#&gt; 15 &lt;split [5467/14]&gt; Slice15\n#&gt; 16 &lt;split [5467/11]&gt; Slice16\n\n\n\n\nWe will fit 16 models on 16 slightly different analysis sets.\nEach will produce a separate performance metrics.\nWe will average the 16 metrics to get the resampling estimate of that statistic."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#feature-engineering-with-recipes",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#feature-engineering-with-recipes",
    "title": "7 - Case Study on Transportation",
    "section": "Feature engineering with recipes ",
    "text": "Feature engineering with recipes \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train)\n\nBased on the formula, the function assigns columns to roles of â€œoutcomeâ€ or â€œpredictorâ€"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#a-recipe",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#a-recipe",
    "title": "7 - Case Study on Transportation",
    "section": "A recipe",
    "text": "A recipe\n\nsummary(chi_rec)\n#&gt; # A tibble: 50 Ã— 4\n#&gt;    variable         type      role      source  \n#&gt;    &lt;chr&gt;            &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 Austin           &lt;chr [2]&gt; predictor original\n#&gt;  2 Quincy_Wells     &lt;chr [2]&gt; predictor original\n#&gt;  3 Belmont          &lt;chr [2]&gt; predictor original\n#&gt;  4 Archer_35th      &lt;chr [2]&gt; predictor original\n#&gt;  5 Oak_Park         &lt;chr [2]&gt; predictor original\n#&gt;  6 Western          &lt;chr [2]&gt; predictor original\n#&gt;  7 Clark_Lake       &lt;chr [2]&gt; predictor original\n#&gt;  8 Clinton          &lt;chr [2]&gt; predictor original\n#&gt;  9 Merchandise_Mart &lt;chr [2]&gt; predictor original\n#&gt; 10 Irving_Park      &lt;chr [2]&gt; predictor original\n#&gt; # â„¹ 40 more rows"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#a-recipe---work-with-dates",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#a-recipe---work-with-dates",
    "title": "7 - Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) \n\nThis creates three new columns in the data based on the date. Note that the day-of-the-week column is a factor."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#a-recipe---work-with-dates-1",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#a-recipe---work-with-dates-1",
    "title": "7 - Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %&gt;% \n  step_holiday(date) \n\nAdd indicators for major holidays. Specific holidays, especially those non-USA, can also be generated.\nAt this point, we donâ€™t need date anymore. Instead of deleting it (there is a step for that) we will change its role to be an identification variable.\n\nWe might want to change the role (instead of removing the column) because it will stay in the data set (even when resampled) and might be useful for diagnosing issues."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#a-recipe---work-with-dates-2",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#a-recipe---work-with-dates-2",
    "title": "7 - Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %&gt;% \n  step_holiday(date) %&gt;% \n  update_role(date, new_role = \"id\") %&gt;%\n  update_role_requirements(role = \"id\", bake = TRUE)\n\ndate is still in the data set but tidymodels knows not to treat it as an analysis column.\nupdate_role_requirements() is needed to make sure that this column is required when making new data points."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#a-recipe---remove-constant-columns",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#a-recipe---remove-constant-columns",
    "title": "7 - Case Study on Transportation",
    "section": "A recipe - remove constant columns ",
    "text": "A recipe - remove constant columns \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %&gt;% \n  step_holiday(date) %&gt;% \n  update_role(date, new_role = \"id\") %&gt;%\n  update_role_requirements(role = \"id\", bake = TRUE) %&gt;% \n  step_zv(all_nominal_predictors())"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#a-recipe---handle-correlations",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#a-recipe---handle-correlations",
    "title": "7 - Case Study on Transportation",
    "section": "A recipe - handle correlations ",
    "text": "A recipe - handle correlations \nThe station columns have a very high degree of correlation.\nWe might want to decorrelated them with principle component analysis to help the model fits go more easily.\nThe vector stations contains all station names and can be used to identify all the relevant columns.\n\nchi_pca_rec &lt;- \n  chi_rec %&gt;% \n  step_normalize(all_of(!!stations)) %&gt;% \n  step_pca(all_of(!!stations), num_comp = tune())\n\nWeâ€™ll tune the number of PCA components for (default) values of one to four."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#make-some-models",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#make-some-models",
    "title": "7 - Case Study on Transportation",
    "section": "Make some models     ",
    "text": "Make some models     \nLetâ€™s try three models. The first one requires the rules package (loaded earlier).\n\ncb_spec &lt;- cubist_rules(committees = 25, neighbors = tune())\nmars_spec &lt;- mars(prod_degree = tune()) %&gt;% set_mode(\"regression\")\nlm_spec &lt;- linear_reg()\n\nchi_set &lt;- \n  workflow_set(\n    list(pca = chi_pca_rec, basic = chi_rec), \n    list(cubist = cb_spec, mars = mars_spec, lm = lm_spec)\n  ) %&gt;% \n  # Evaluate models using mean absolute errors\n  option_add(metrics = metric_set(mae))\n\n\nBriefly talk about Cubist being a (sort of) boosted rule-based model and MARS being a nonlinear regression model. Both incorporate feature selection nicely."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#process-them-on-the-resamples",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#process-them-on-the-resamples",
    "title": "7 - Case Study on Transportation",
    "section": "Process them on the resamples",
    "text": "Process them on the resamples\n\n# Set up some objects for stacking ensembles (in a few slides)\ngrid_ctrl &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\nchi_res &lt;- \n  chi_set %&gt;% \n  workflow_map(\n    resamples = chi_rs,\n    grid = 10,\n    control = grid_ctrl,\n    verbose = TRUE,\n    seed = 12\n  )"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#how-do-the-results-look",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#how-do-the-results-look",
    "title": "7 - Case Study on Transportation",
    "section": "How do the results look?",
    "text": "How do the results look?\n\nrank_results(chi_res)\n#&gt; # A tibble: 31 Ã— 9\n#&gt;    wflow_id     .config              .metric  mean std_err     n preprocessor model   rank\n#&gt;    &lt;chr&gt;        &lt;chr&gt;                &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;  &lt;int&gt;\n#&gt;  1 pca_cubist   Preprocessor1_Model1 mae     0.798   0.104    16 recipe       cubisâ€¦     1\n#&gt;  2 pca_cubist   Preprocessor3_Model3 mae     0.978   0.110    16 recipe       cubisâ€¦     2\n#&gt;  3 pca_cubist   Preprocessor4_Model2 mae     0.983   0.122    16 recipe       cubisâ€¦     3\n#&gt;  4 pca_cubist   Preprocessor4_Model1 mae     0.991   0.127    16 recipe       cubisâ€¦     4\n#&gt;  5 pca_cubist   Preprocessor3_Model2 mae     0.991   0.113    16 recipe       cubisâ€¦     5\n#&gt;  6 pca_cubist   Preprocessor2_Model2 mae     1.02    0.118    16 recipe       cubisâ€¦     6\n#&gt;  7 pca_cubist   Preprocessor1_Model3 mae     1.05    0.134    16 recipe       cubisâ€¦     7\n#&gt;  8 basic_cubist Preprocessor1_Model8 mae     1.07    0.115    16 recipe       cubisâ€¦     8\n#&gt;  9 basic_cubist Preprocessor1_Model7 mae     1.07    0.112    16 recipe       cubisâ€¦     9\n#&gt; 10 basic_cubist Preprocessor1_Model6 mae     1.07    0.114    16 recipe       cubisâ€¦    10\n#&gt; # â„¹ 21 more rows"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#plot-the-results",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#plot-the-results",
    "title": "7 - Case Study on Transportation",
    "section": "Plot the results  ",
    "text": "Plot the results  \n\nautoplot(chi_res)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#pull-out-specific-results",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#pull-out-specific-results",
    "title": "7 - Case Study on Transportation",
    "section": "Pull out specific results  ",
    "text": "Pull out specific results  \nWe can also pull out the specific tuning results and look at them:\n\nchi_res %&gt;% \n  extract_workflow_set_result(\"pca_cubist\") %&gt;% \n  autoplot()"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#why-choose-just-one-final_fit",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#why-choose-just-one-final_fit",
    "title": "7 - Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit? \nModel stacks generate predictions that are informed by several models."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#why-choose-just-one-final_fit-1",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#why-choose-just-one-final_fit-1",
    "title": "7 - Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#why-choose-just-one-final_fit-2",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#why-choose-just-one-final_fit-2",
    "title": "7 - Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#why-choose-just-one-final_fit-3",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#why-choose-just-one-final_fit-3",
    "title": "7 - Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#why-choose-just-one-final_fit-4",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#why-choose-just-one-final_fit-4",
    "title": "7 - Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#why-choose-just-one-final_fit-5",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#why-choose-just-one-final_fit-5",
    "title": "7 - Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#building-a-model-stack",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#building-a-model-stack",
    "title": "7 - Case Study on Transportation",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlibrary(stacks)\n\n\nDefine candidate members\nInitialize a data stack object\nAdd candidate ensemble members to the data stack\nEvaluate how to combine their predictions\nFit candidate ensemble members with non-zero stacking coefficients\nPredict on new data!"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#start-the-stack-and-add-members",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#start-the-stack-and-add-members",
    "title": "7 - Case Study on Transportation",
    "section": "Start the stack and add members ",
    "text": "Start the stack and add members \nCollect all of the resampling results for all model configurations.\n\nchi_stack &lt;- \n  stacks() %&gt;% \n  add_candidates(chi_res)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#estimate-weights-for-each-candidate",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#estimate-weights-for-each-candidate",
    "title": "7 - Case Study on Transportation",
    "section": "Estimate weights for each candidate ",
    "text": "Estimate weights for each candidate \nWhich configurations should be retained? Uses a penalized linear model:\n\nset.seed(122)\nchi_stack_res &lt;- blend_predictions(chi_stack)\n\nchi_stack_res\n#&gt; # A tibble: 5 Ã— 3\n#&gt;   member           type         weight\n#&gt;   &lt;chr&gt;            &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 pca_cubist_1_1   cubist_rules  0.343\n#&gt; 2 pca_cubist_3_2   cubist_rules  0.236\n#&gt; 3 basic_cubist_1_4 cubist_rules  0.189\n#&gt; 4 pca_lm_4_1       linear_reg    0.163\n#&gt; 5 pca_cubist_3_3   cubist_rules  0.109"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#how-did-it-do",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#how-did-it-do",
    "title": "7 - Case Study on Transportation",
    "section": "How did it do?  ",
    "text": "How did it do?  \nThe overall results of the penalized model:\n\nautoplot(chi_stack_res)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#what-does-it-use",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#what-does-it-use",
    "title": "7 - Case Study on Transportation",
    "section": "What does it use?  ",
    "text": "What does it use?  \n\nautoplot(chi_stack_res, type = \"weights\")"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#fit-the-required-candidate-models",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#fit-the-required-candidate-models",
    "title": "7 - Case Study on Transportation",
    "section": "Fit the required candidate models",
    "text": "Fit the required candidate models\nFor each model we retain in the stack, we need their model fit on the entire training set.\n\nchi_stack_res &lt;- fit_members(chi_stack_res)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#the-test-set-best-cubist-model",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#the-test-set-best-cubist-model",
    "title": "7 - Case Study on Transportation",
    "section": "The test set: best Cubist model  ",
    "text": "The test set: best Cubist model  \nWe can pull out the results and the workflow to fit the single best cubist model.\n\nbest_cubist &lt;- \n  chi_res %&gt;% \n  extract_workflow_set_result(\"pca_cubist\") %&gt;% \n  select_best()\n\ncubist_res &lt;- \n  chi_res %&gt;% \n  extract_workflow(\"pca_cubist\") %&gt;% \n  finalize_workflow(best_cubist) %&gt;% \n  last_fit(split = chi_split, metrics = metric_set(mae))"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#the-test-set-stack-ensemble",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#the-test-set-stack-ensemble",
    "title": "7 - Case Study on Transportation",
    "section": "The test set: stack ensemble",
    "text": "The test set: stack ensemble\nWe donâ€™t have last_fit() for stacks (yet) so we manually make predictions.\n\nstack_pred &lt;- \n  predict(chi_stack_res, chi_test) %&gt;% \n  bind_cols(chi_test)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#compare-the-results",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#compare-the-results",
    "title": "7 - Case Study on Transportation",
    "section": "Compare the results  ",
    "text": "Compare the results  \nSingle best versus the stack:\n\ncollect_metrics(cubist_res)\n#&gt; # A tibble: 1 Ã— 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard       0.670 Preprocessor1_Model1\n\nstack_pred %&gt;% mae(ridership, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       0.689"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#plot-the-test-set",
    "href": "archive/2022-08-Reykjavik-City/07-transit-case-study.html#plot-the-test-set",
    "title": "7 - Case Study on Transportation",
    "section": "Plot the test set  ",
    "text": "Plot the test set  \n\n\ncubist_res %&gt;% \n  collect_predictions() %&gt;% \n  ggplot(aes(ridership, .pred)) + \n  geom_point(alpha = 1 / 2) + \n  geom_abline(lty = 2, col = \"green\") + \n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#working-with-our-predictors",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#working-with-our-predictors",
    "title": "5 - Feature engineering",
    "section": "Working with our predictors",
    "text": "Working with our predictors\nWe might want to modify our predictors columns for a few reasons:\n\nThe model requires them in a different format (e.g.Â dummy variables for lm()).\nThe model needs certain data qualities (e.g.Â same units for K-NN).\nThe outcome is better predicted when one or more columns are transformed in some way (a.k.a â€œfeature engineeringâ€).\n\n\nThe first two reasons are fairly predictable (next page).\nThe last one depends on your modeling problem."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#what-is-feature-engineering",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#what-is-feature-engineering",
    "title": "5 - Feature engineering",
    "section": "What is feature engineering?",
    "text": "What is feature engineering?\nThink of a feature as some representation of a predictor that will be used in a model.\n\nExample representations:\n\nInteractions\nPolynomial expansions/splines\nPCA feature extraction\n\nThere are a lot of examples in Feature Engineering and Selection."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#example-dates",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#example-dates",
    "title": "5 - Feature engineering",
    "section": "Example: Dates",
    "text": "Example: Dates\nHow can we represent date columns for our model?\n\nWhen a date column is used in its native format, it is usually converted by an R model to an integer.\n\n\nIt can be re-engineered as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nIndicators for holidays\n\n\nThe main point is that we try to maximize performance with different versions of the predictors.\nMention that, for the Chicago data, the day or the week features are usually the most important ones in the model."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#general-definitions",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#general-definitions",
    "title": "5 - Feature engineering",
    "section": "General definitions ",
    "text": "General definitions \n\nData preprocessing steps allow your model to fit.\nFeature engineering steps help the model do the least work to predict the outcome as well as possible.\n\nThe recipes package can handle both!\nIn a little bit, weâ€™ll see successful (and unsuccessful) feature engineering methods for our example data.\n\nThese terms are often used interchangeably in the ML community but we want to distinguish them."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#the-nhl-data",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#the-nhl-data",
    "title": "5 - Feature engineering",
    "section": "The NHL data ğŸ’",
    "text": "The NHL data ğŸ’\n\nFrom Pittsburgh Penguins games, 8,915 shots\nData from the 2015-2016 season\n\n\nLetâ€™s predict whether a shot is on-goal (a goal or blocked by goaltender) or not."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#case-study",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#case-study",
    "title": "5 - Feature engineering",
    "section": "Case study",
    "text": "Case study\n\nlibrary(tidymodels)\nlibrary(ongoal)\n\ntidymodels_prefer()\n\nglimpse(season_2015)\n#&gt; Rows: 8,915\n#&gt; Columns: 13\n#&gt; $ on_goal         &lt;fct&gt; no, no, yes, no, yes, no, no, yes, no, no, no, yes, noâ€¦\n#&gt; $ period          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â€¦\n#&gt; $ game_seconds    &lt;dbl&gt; 13, 36, 47, 92, 99, 125, 179, 220, 252, 309, 413, 427,â€¦\n#&gt; $ strength        &lt;fct&gt; even, even, even, even, even, even, even, even, even, â€¦\n#&gt; $ home_skaters    &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, â€¦\n#&gt; $ away_skaters    &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, â€¦\n#&gt; $ goaltender      &lt;fct&gt; marc_andre_fleury, antti_niemi, antti_niemi, antti_nieâ€¦\n#&gt; $ goal_difference &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1,â€¦\n#&gt; $ shooter         &lt;fct&gt; evgeni_malkin, valeri_nichushkin, phil_kessel, beau_beâ€¦\n#&gt; $ shooter_type    &lt;fct&gt; center, right_wing, center, right_wing, center, defensâ€¦\n#&gt; $ coord_x         &lt;dbl&gt; -66, -49, 64, 65, 80, 42, -55, 62, -67, -58, 76, 56, -â€¦\n#&gt; $ coord_y         &lt;dbl&gt; -11, -21, -31, -21, 13, 31, -19, 15, -9, -16, -8, 25, â€¦\n#&gt; $ extra_attacker  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#data-splitting-strategy",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#data-splitting-strategy",
    "title": "5 - Feature engineering",
    "section": "Data splitting strategy",
    "text": "Data splitting strategy"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#why-a-validation-set",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#why-a-validation-set",
    "title": "5 - Feature engineering",
    "section": "Why a validation set?",
    "text": "Why a validation set?\nRecall that resampling gives us performance measures without using the test set.\nItâ€™s important to get good resampling statistics (e.g.Â \\(R^2\\)).\n\nThat usually means having enough data to estimate performance.\n\nWhen you have â€œa lotâ€ of data, a validation set can be an efficient way to do this."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#splitting-the-nhl-data",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#splitting-the-nhl-data",
    "title": "5 - Feature engineering",
    "section": "Splitting the NHL data ",
    "text": "Splitting the NHL data \n\nset.seed(23)\nnhl_split &lt;- initial_split(season_2015, prop = 3/4)\nnhl_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;6686/2229/8915&gt;\n\nnhl_train_and_val &lt;- training(nhl_split)\nnhl_test  &lt;- testing(nhl_split)\n\n## not testing\nnrow(nhl_train_and_val)\n#&gt; [1] 6686\n \n## testing\nnrow(nhl_test)\n#&gt; [1] 2229"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#validation-split",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#validation-split",
    "title": "5 - Feature engineering",
    "section": "Validation split ",
    "text": "Validation split \nSince there are a lot of observations, weâ€™ll use a validation set:\n\nset.seed(234)\nnhl_val &lt;- validation_split(nhl_train_and_val, prop = 0.80)\nnhl_val\n#&gt; # Validation Set Split (0.8/0.2)  \n#&gt; # A tibble: 1 Ã— 2\n#&gt;   splits              id        \n#&gt;   &lt;list&gt;              &lt;chr&gt;     \n#&gt; 1 &lt;split [5348/1338]&gt; validation\n\n\nRemember that a validation split is a type of resample."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#your-turn",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#your-turn",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nLetâ€™s explore the training set data.\nUse the function plot_nhl_shots() for nice spatial plots of the data.\n\n\n\nnhl_train &lt;- analysis(nhl_val$splits[[1]])\n\nset.seed(100)\nnhl_train %&gt;% \n  sample_n(200) %&gt;%\n  plot_nhl_shots(emphasis = shooter_type)\n\n\n\n\n\n\n\n\n\n\n\n\n08:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#prepare-your-data-for-modeling",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#prepare-your-data-for-modeling",
    "title": "5 - Feature engineering",
    "section": "Prepare your data for modeling ",
    "text": "Prepare your data for modeling \n\nThe recipes package is an extensible framework for pipeable sequences of feature engineering steps that provide preprocessing tools to be applied to data.\n\n\n\nStatistical parameters for the steps can be estimated from an initial data set and then applied to other data sets.\n\n\n\n\nThe resulting processed output can be used as inputs for statistical or machine learning models."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#a-first-recipe",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#a-first-recipe",
    "title": "5 - Feature engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train)\n\n\n\nThe recipe() function assigns columns to roles of â€œoutcomeâ€ or â€œpredictorâ€ using the formula"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#a-first-recipe-1",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#a-first-recipe-1",
    "title": "5 - Feature engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nsummary(nhl_rec)\n#&gt; # A tibble: 13 Ã— 4\n#&gt;    variable        type      role      source  \n#&gt;    &lt;chr&gt;           &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 period          &lt;chr [2]&gt; predictor original\n#&gt;  2 game_seconds    &lt;chr [2]&gt; predictor original\n#&gt;  3 strength        &lt;chr [3]&gt; predictor original\n#&gt;  4 home_skaters    &lt;chr [2]&gt; predictor original\n#&gt;  5 away_skaters    &lt;chr [2]&gt; predictor original\n#&gt;  6 goaltender      &lt;chr [3]&gt; predictor original\n#&gt;  7 goal_difference &lt;chr [2]&gt; predictor original\n#&gt;  8 shooter         &lt;chr [3]&gt; predictor original\n#&gt;  9 shooter_type    &lt;chr [3]&gt; predictor original\n#&gt; 10 coord_x         &lt;chr [2]&gt; predictor original\n#&gt; 11 coord_y         &lt;chr [2]&gt; predictor original\n#&gt; 12 extra_attacker  &lt;chr [2]&gt; predictor original\n#&gt; 13 on_goal         &lt;chr [3]&gt; outcome   original"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#create-indicator-variables",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#create-indicator-variables",
    "title": "5 - Feature engineering",
    "section": "Create indicator variables ",
    "text": "Create indicator variables \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\n\nFor any factor or character predictors, make binary indicators.\nThere are many recipe steps that can convert categorical predictors to numeric columns."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#filter-out-constant-columns",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#filter-out-constant-columns",
    "title": "5 - Feature engineering",
    "section": "Filter out constant columns ",
    "text": "Filter out constant columns \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())\n\n\nIn case there is a factor level that was never observed in the training data (resulting in a column of all 0s), we can delete any zero-variance predictors that have a single unique value.\n\nNote that the selector chooses all columns with a role of â€œpredictorâ€"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#normalization",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#normalization",
    "title": "5 - Feature engineering",
    "section": "Normalization ",
    "text": "Normalization \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\n\n\nThis centers and scales the numeric predictors.\nThe recipe will use the training set to estimate the means and standard deviations of the data.\n\n\n\n\nAll data the recipe is applied to will be normalized using those statistics (there is no re-estimation)."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#reduce-correlation",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#reduce-correlation",
    "title": "5 - Feature engineering",
    "section": "Reduce correlation ",
    "text": "Reduce correlation \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_corr(all_numeric_predictors(), threshold = 0.9)\n\n\nTo deal with highly correlated predictors, find the minimum set of predictor columns that make the pairwise correlations less than the threshold."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#other-possible-steps",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#other-possible-steps",
    "title": "5 - Feature engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_pca(all_numeric_predictors())\n\n\nPCA feature extractionâ€¦"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#other-possible-steps-1",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#other-possible-steps-1",
    "title": "5 - Feature engineering",
    "section": "Other possible steps  ",
    "text": "Other possible steps  \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  embed::step_umap(all_numeric_predictors(), outcome = on_goal)\n\n\nA fancy machine learning supervised dimension reduction techniqueâ€¦\n\nNote that this uses the outcome, and it is from an extension package"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#other-possible-steps-2",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#other-possible-steps-2",
    "title": "5 - Feature engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_ns(coord_y, coord_x, deg_free = 10)\n\n\nNonlinear transforms like natural splines, and so on!"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#your-turn-1",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#your-turn-1",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a recipe() for the on-goal data to :\n\ncreate one-hot indicator variables\nremove zero-variance variables\n\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#minimal-recipe",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#minimal-recipe",
    "title": "5 - Feature engineering",
    "section": "Minimal recipe ",
    "text": "Minimal recipe \n\nnhl_indicators &lt;-\n  recipe(on_goal ~ ., data = nhl_train) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#using-a-workflow",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#using-a-workflow",
    "title": "5 - Feature engineering",
    "section": "Using a workflow    ",
    "text": "Using a workflow    \n\nset.seed(9)\n\nnhl_glm_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(nhl_indicators) %&gt;%\n  add_model(logistic_reg())\n \nctrl &lt;- control_resamples(save_pred = TRUE)\nnhl_glm_res &lt;-\n  nhl_glm_wflow %&gt;%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_glm_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.756     1      NA Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.753     1      NA Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#your-turn-2",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#your-turn-2",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() to fit your workflow with a recipe.\nCollect the predictions from the results.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#holdout-predictions",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#holdout-predictions",
    "title": "5 - Feature engineering",
    "section": "Holdout predictions    ",
    "text": "Holdout predictions    \n\n# Since we used `save_pred = TRUE`\nglm_val_pred &lt;- collect_predictions(nhl_glm_res)\nglm_val_pred %&gt;% slice(1:7)\n#&gt; # A tibble: 7 Ã— 7\n#&gt;   id             .pred_yes .pred_no  .row .pred_class on_goal .config           \n#&gt;   &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;   &lt;chr&gt;             \n#&gt; 1 validation 0.00844          0.992     2 no          no      Preprocessor1_Modâ€¦\n#&gt; 2 validation 0.0864           0.914    10 no          no      Preprocessor1_Modâ€¦\n#&gt; 3 validation 0.799            0.201    22 yes         yes     Preprocessor1_Modâ€¦\n#&gt; 4 validation 0.615            0.385    24 yes         yes     Preprocessor1_Modâ€¦\n#&gt; 5 validation 0.787            0.213    31 yes         yes     Preprocessor1_Modâ€¦\n#&gt; 6 validation 0.872            0.128    39 yes         yes     Preprocessor1_Modâ€¦\n#&gt; 7 validation 0.00000000903    1.00     40 no          no      Preprocessor1_Modâ€¦"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#two-class-data-1",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#two-class-data-1",
    "title": "5 - Feature engineering",
    "section": "Two class data",
    "text": "Two class data\nThese definitions assume that we know the threshold for converting â€œsoftâ€ probability predictions into â€œhardâ€ class predictions.\n\nIs a 50% threshold good?\nWhat happens if we say that we need to be 80% sure to declare an event?\n\nsensitivity â¬‡ï¸, specificity â¬†ï¸\n\n\n\nWhat happens for a 20% threshold?\n\nsensitivity â¬†ï¸, specificity â¬‡ï¸"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#varying-the-threshold",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#varying-the-threshold",
    "title": "5 - Feature engineering",
    "section": "Varying the threshold",
    "text": "Varying the threshold"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#roc-curves",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#roc-curves",
    "title": "5 - Feature engineering",
    "section": "ROC curves",
    "text": "ROC curves\nTo make an ROC (receiver operator characteristic) curve, we:\n\ncalculate the sensitivity and specificity for all possible thresholds\nplot false positive rate (x-axis) versus true positive rate (y-axis)\n\n\nWe can use the area under the ROC curve as a classification metric:\n\nROC AUC = 1 ğŸ’¯\nROC AUC = 1/2 ğŸ˜¢\n\n\nROC curves are insensitive to class imbalance."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#roc-curves-1",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#roc-curves-1",
    "title": "5 - Feature engineering",
    "section": "ROC curves ",
    "text": "ROC curves \n\n# Assumes _first_ factor level is event; there are options to change that\nroc_curve_points &lt;- glm_val_pred %&gt;% roc_curve(truth = on_goal, .pred_yes)\nroc_curve_points %&gt;% slice(1, 50, 100)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .threshold specificity sensitivity\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 -Inf            0            1    \n#&gt; 2    0.00143      0.0412       0.968\n#&gt; 3    0.00988      0.127        0.968\n\nglm_val_pred %&gt;% roc_auc(truth = on_goal, .pred_yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.753"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#roc-curve-plot",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#roc-curve-plot",
    "title": "5 - Feature engineering",
    "section": "ROC curve plot ",
    "text": "ROC curve plot \n\nautoplot(roc_curve_points)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#your-turn-3",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#your-turn-3",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCompute and plot an ROC curve for your current model.\nWhat data are being used for this ROC curve plot?\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#what-do-we-do-with-the-player-data",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#what-do-we-do-with-the-player-data",
    "title": "5 - Feature engineering",
    "section": "What do we do with the player data? ğŸ’",
    "text": "What do we do with the player data? ğŸ’\nThere are 598 unique player values in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables ğŸ˜³\nlump players who rarely shoot into an â€œotherâ€ group\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the shooter column with the estimated effect of that predictor\n\n\n\nLetâ€™s look at othering then effect encodings."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#per-player-statistics",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#per-player-statistics",
    "title": "5 - Feature engineering",
    "section": "Per-player statistics",
    "text": "Per-player statistics"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#collapsing-factor-levels",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#collapsing-factor-levels",
    "title": "5 - Feature engineering",
    "section": "Collapsing factor levels",
    "text": "Collapsing factor levels\nThere is a recipe step that will redefine factor levels based on the their frequency in the training set:\n\nnhl_other_rec &lt;-\n  recipe(on_goal ~ ., data = nhl_train) %&gt;%\n  # Any player with &lt;= 0.01% of shots is set to \"other\"\n  step_other(shooter, threshold = 0.001) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nUsing this code, 402 players (out of 598) were collapsed into â€œotherâ€ based on the training set.\nWe could try to optimize the threshold for collapsing (see the next set of slides on model tuning)."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#does-othering-help",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#does-othering-help",
    "title": "5 - Feature engineering",
    "section": "Does othering help?",
    "text": "Does othering help?\n\nnhl_other_wflow &lt;-\n  nhl_glm_wflow %&gt;%\n  update_recipe(nhl_other_rec)\n\nnhl_other_res &lt;-\n  nhl_other_wflow %&gt;%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_other_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.778     1      NA Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.804     1      NA Preprocessor1_Model1\n\nA little better ROC AUC and much faster to complete.\nNow letâ€™s look at a more sophisticated tool called effect encodings."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#what-is-an-effect-encoding",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#what-is-an-effect-encoding",
    "title": "5 - Feature engineering",
    "section": "What is an effect encoding?",
    "text": "What is an effect encoding?\nWe replace the qualitativeâ€™s predictor data with their effect on the outcome.\n\n\nData before:\n\nbefore\n#&gt; # A tibble: 7 Ã— 3\n#&gt;   on_goal shooter        .row\n#&gt;   &lt;fct&gt;   &lt;fct&gt;         &lt;int&gt;\n#&gt; 1 yes     sidney_crosby     1\n#&gt; 2 yes     mike_hoffman      2\n#&gt; 3 yes     ian_cole          3\n#&gt; 4 yes     matt_cullen       4\n#&gt; 5 yes     kris_letang       5\n#&gt; 6 no      nazem_kadri       6\n#&gt; 7 yes     david_perron      7\n\n\nData after:\n\nafter\n#&gt; # A tibble: 7 Ã— 3\n#&gt;   on_goal shooter  .row\n#&gt;   &lt;fct&gt;     &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 yes       0.220     1\n#&gt; 2 yes       0.302     2\n#&gt; 3 yes       0.261     3\n#&gt; 4 yes       0.463     4\n#&gt; 5 yes       0.197     5\n#&gt; 6 no        0.302     6\n#&gt; 7 yes       0.400     7\n\n\nThe shooter column is replaced with the log-odds of being on goal.\n\nAs a reminder:\n\\[\\text{log-odds} = log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)\\]\nwhere \\(\\hat{p}\\) is the on goal rate estimate.\nFor logistic regression, this is what the predictors are modeling. The log-odds are more likely to be linear with the outcome."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#per-player-statistics-again",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#per-player-statistics-again",
    "title": "5 - Feature engineering",
    "section": "Per-player statistics again",
    "text": "Per-player statistics again\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood statistical methods for estimating these rates use partial pooling.\nPooling borrows strength across players and shrinks extreme values (e.g.Â zero or one) towards the mean for players with very few shots.\nThe embed package has recipe steps for effect encodings.\n\n\n\nPartial pooling gives better estimates for players with fewer shots by shrinking the estimate to the overall on-goal rate (56.5%)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#partial-pooling",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#partial-pooling",
    "title": "5 - Feature engineering",
    "section": "Partial pooling",
    "text": "Partial pooling"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#player-effects",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#player-effects",
    "title": "5 - Feature engineering",
    "section": "Player effects  ",
    "text": "Player effects  \n\nlibrary(embed)\n\nnhl_effect_rec &lt;-\n  recipe(on_goal ~ ., data = nhl_train) %&gt;%\n  step_lencode_mixed(shooter, goaltender, outcome = vars(on_goal)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\n\nIt is very important to appropriately validate the effect encoding step to make sure that we are not overfitting."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#recipes-are-estimated",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#recipes-are-estimated",
    "title": "5 - Feature engineering",
    "section": "Recipes are estimated ",
    "text": "Recipes are estimated \nPreprocessing steps in a recipe use the training set to compute quantities.\n\nWhat kind of quantities are computed for preprocessing?\n\nLevels of a factor\nWhether a column has zero variance\nNormalization\nFeature extraction\nEffect encodings\n\n\n\nWhen a recipe is part of a workflow, this estimation occurs when fit() is called."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#effect-encoding-results",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#effect-encoding-results",
    "title": "5 - Feature engineering",
    "section": "Effect encoding results    ",
    "text": "Effect encoding results    \n\nnhl_effect_wflow &lt;-\n  nhl_glm_wflow %&gt;%\n  update_recipe(nhl_effect_rec)\n\nnhl_effect_res &lt;-\n  nhl_effect_wflow %&gt;%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_effect_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.791     1      NA Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.805     1      NA Preprocessor1_Model1\n\nBetter and it can handle new players (if they occur)."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#angle",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#angle",
    "title": "5 - Feature engineering",
    "section": "Angle",
    "text": "Angle\n\nnhl_angle_rec &lt;-\n  nhl_effect_rec %&gt;%\n  step_mutate(\n    angle = abs( atan2(abs(coord_y), (89 - coord_x) ) * (180 / pi) )\n  )\n\n\n\nNote the danger of using step_mutate() â€“ easy to have data leakage"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#shot-from-the-defensive-zone",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#shot-from-the-defensive-zone",
    "title": "5 - Feature engineering",
    "section": "Shot from the defensive zone",
    "text": "Shot from the defensive zone\n\nnhl_zone_rec &lt;-\n  nhl_angle_rec %&gt;%\n  step_mutate(\n    defensive_zone = ifelse(coord_x &lt;= -25.5, 1, 0)\n  )"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#behind-goal-line",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#behind-goal-line",
    "title": "5 - Feature engineering",
    "section": "Behind goal line",
    "text": "Behind goal line\n\nnhl_behind_rec &lt;-\n  nhl_zone_rec %&gt;%\n  step_mutate(\n    behind_goal_line = ifelse(coord_x &gt;= 89, 1, 0)\n  )"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#fit-different-recipes",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#fit-different-recipes",
    "title": "5 - Feature engineering",
    "section": "Fit different recipes    ",
    "text": "Fit different recipes    \nA workflow set can cross models and/or preprocessors and then resample them en masse.\n\nno_coord_rec &lt;- \n  nhl_indicators %&gt;% \n  step_rm(starts_with(\"coord\"))\n\nset.seed(9)\n\nnhl_glm_set_res &lt;-\n  workflow_set(\n    list(`1_no_coord` = no_coord_rec,   `2_other` = nhl_other_rec, \n         `3_effects`  = nhl_effect_rec, `4_angle` = nhl_angle_rec, \n         `5_zone`     = nhl_zone_rec,   `6_bgl`   = nhl_behind_rec),\n    list(logistic = logistic_reg())\n  ) %&gt;%\n  workflow_map(fn = \"fit_resamples\", resamples = nhl_val, verbose = TRUE, control = ctrl)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#your-turn-4",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#your-turn-4",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a workflow set with 2 or 3 recipes.\n(Consider using recipes weâ€™ve already created.)\nUse workflow_map() to resample the workflow set.\n\n\n\n08:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#compare-recipes",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#compare-recipes",
    "title": "5 - Feature engineering",
    "section": "Compare recipes",
    "text": "Compare recipes\n\nlibrary(forcats)\ncollect_metrics(nhl_glm_set_res) %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  mutate(\n    features = gsub(\"_logistic\", \"\", wflow_id), \n    features = fct_reorder(features, mean)\n  ) %&gt;%\n  ggplot(aes(x = mean, y = features)) +\n  geom_point(size = 3) +\n  labs(y = NULL, x = \"ROC AUC (validation set)\")"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#compare-recipes-1",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#compare-recipes-1",
    "title": "5 - Feature engineering",
    "section": "Compare recipes",
    "text": "Compare recipes"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#debugging-a-recipe",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#debugging-a-recipe",
    "title": "5 - Feature engineering",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g.Â encoded_players) can be estimated manually with a function called prep(). It is analogous to fit(). See TMwR section 16.4\n\n\n\n\nAnother function (bake()) is analogous to predict(), and gives you the processed data back.\n\n\n\n\nThe tidy() function can be used to get specific results from the recipe."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#example",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#example",
    "title": "5 - Feature engineering",
    "section": "Example",
    "text": "Example\n\nnhl_angle_fit &lt;- prep(nhl_angle_rec)\n\ntidy(nhl_angle_fit, number = 1) %&gt;% slice(1:4)\n#&gt; # A tibble: 4 Ã— 4\n#&gt;   level           value terms   id                 \n#&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;              \n#&gt; 1 aaron_ekblad    0.250 shooter lencode_mixed_qBdjK\n#&gt; 2 adam_clendening 0.202 shooter lencode_mixed_qBdjK\n#&gt; 3 adam_cracknell  0.288 shooter lencode_mixed_qBdjK\n#&gt; 4 adam_henrique   0.221 shooter lencode_mixed_qBdjK\n\nbake(nhl_angle_fit, nhl_train %&gt;% slice(1:3), starts_with(\"coord\"), angle, shooter)\n#&gt; # A tibble: 3 Ã— 4\n#&gt;   coord_x coord_y angle shooter\n#&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1      67     -37  59.3   0.220\n#&gt; 2      55      -7  11.6   0.302\n#&gt; 3      42     -18  21.0   0.261"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#more-on-recipes",
    "href": "archive/2022-08-Reykjavik-City/05-feature-engineering.html#more-on-recipes",
    "title": "5 - Feature engineering",
    "section": "More on recipes",
    "text": "More on recipes\n\nOnce fit() is called on a workflow, changing the model does not re-fit the recipe.\n\n\n\nA list of all known steps is at https://www.tidymodels.org/find/recipes/.\n\n\n\n\nSome steps can be skipped when using predict().\n\n\n\n\nThe order of the steps matters."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nHow do you fit a linear model in R?\nHow many different ways can you think of?\n\n\n\n03:00\n\n\n\n\n\nlm for linear model\nglm for generalized linear model (e.g.Â logistic regression)\nglmnet for regularized regression\nkeras for regression using TensorFlow\nstan for Bayesian regression\nspark for large data sets"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-1",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-1",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlinear_reg()\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\nModels have default engines"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-2",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-2",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-3",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-3",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlinear_reg() %&gt;%\n  set_engine(\"glmnet\")\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: glmnet"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-4",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-4",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlinear_reg() %&gt;%\n  set_engine(\"stan\")\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: stan"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-5",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-5",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-6",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-6",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree()\n#&gt; Decision Tree Model Specification (unknown mode)\n#&gt; \n#&gt; Computational engine: rpart\n\n\nSome models have a default mode"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-7",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-7",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree() %&gt;% \n  set_mode(\"regression\")\n#&gt; Decision Tree Model Specification (regression)\n#&gt; \n#&gt; Computational engine: rpart\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-8",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#to-specify-a-model-8",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn-1",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn-1",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_spec chunk in your .qmd.\nEdit this code so it creates a different model.\n\n\n\n05:00\n\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#models-well-be-using-today",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#models-well-be-using-today",
    "title": "3 - What makes a model?",
    "section": "Models weâ€™ll be using today",
    "text": "Models weâ€™ll be using today\n\nLinear regression\nDecision trees"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#linear-regression",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#linear-regression",
    "title": "3 - What makes a model?",
    "section": "Linear regression",
    "text": "Linear regression"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#linear-regression-1",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#linear-regression-1",
    "title": "3 - What makes a model?",
    "section": "Linear regression",
    "text": "Linear regression"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#linear-regression-2",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#linear-regression-2",
    "title": "3 - What makes a model?",
    "section": "Linear regression",
    "text": "Linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome modeled as linear combination of predictors:\n\n\\(\\mbox{latency} = \\beta_0 + \\beta_1\\cdot\\mbox{age} + \\epsilon\\)\n\nFind a line that minimizes the mean squared error (MSE)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#decision-trees",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#decision-trees",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#decision-trees-1",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#decision-trees-1",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#decision-trees-2",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#decision-trees-2",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "title": "3 - What makes a model?",
    "section": "All models are wrong, but some are useful!",
    "text": "All models are wrong, but some are useful!\n\n\nLinear regression\n\n\n\n\n\n\n\n\n\n\nDecision trees"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "title": "3 - What makes a model?",
    "section": "Workflows bind preprocessors and models",
    "text": "Workflows bind preprocessors and models\n\n\nExplain that PCA that is a preprocessor / dimensionality reduction, used to decorrelate data"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#what-is-wrong-with-this",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#what-is-wrong-with-this",
    "title": "3 - What makes a model?",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#why-a-workflow",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#why-a-workflow",
    "title": "3 - What makes a model?",
    "section": "Why a workflow()? ",
    "text": "Why a workflow()? \n\n\nWorkflows handle new data better than base R tools in terms of new factor levels\n\n\n\n\nYou can use other preprocessors besides formulas (more on feature engineering tomorrow!)\n\n\n\n\nThey can help organize your work when working with multiple models\n\n\n\n\nMost importantly, a workflow captures the entire modeling process: fit() and predict() apply to the preprocessing steps in addition to the actual model fit\n\n\nTwo ways workflows handle levels better than base R:\n\nEnforces that new levels are not allowed at prediction time (this is an optional check that can be turned off)\nRestores missing levels that were present at fit time, but happen to be missing at prediction time (like, if your â€œnewâ€ data just doesnâ€™t have an instance of that level)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#a-model-workflow-1",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#a-model-workflow-1",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"regression\")\n\ntree_spec %&gt;% \n  fit(latency ~ ., data = frog_train) \n#&gt; parsnip model object\n#&gt; \n#&gt; n= 456 \n#&gt; \n#&gt; node), split, n, deviance, yval\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 456 2197966.00  92.90351  \n#&gt;    2) age&gt;=4.947975 256  252347.40  60.89844  \n#&gt;      4) treatment=control 131   91424.06  48.42748 *\n#&gt;      5) treatment=gentamicin 125  119197.90  73.96800 *\n#&gt;    3) age&lt; 4.947975 200 1347741.00 133.87000  \n#&gt;      6) treatment=control 140  986790.70 118.25710  \n#&gt;       12) reflex=mid,full 129  754363.70 111.56590 *\n#&gt;       13) reflex=low 11  158918.20 196.72730 *\n#&gt;      7) treatment=gentamicin 60  247194.60 170.30000  \n#&gt;       14) age&lt; 4.664439 30  102190.20 147.83330  \n#&gt;         28) age&gt;=4.566638 22   53953.86 129.77270 *\n#&gt;         29) age&lt; 4.566638 8   21326.00 197.50000 *\n#&gt;       15) age&gt;=4.664439 30  114719.40 192.76670 *"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#a-model-workflow-2",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#a-model-workflow-2",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"regression\")\n\nworkflow() %&gt;%\n  add_formula(latency ~ .) %&gt;%\n  add_model(tree_spec) %&gt;%\n  fit(data = frog_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; latency ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 456 \n#&gt; \n#&gt; node), split, n, deviance, yval\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 456 2197966.00  92.90351  \n#&gt;    2) age&gt;=4.947975 256  252347.40  60.89844  \n#&gt;      4) treatment=control 131   91424.06  48.42748 *\n#&gt;      5) treatment=gentamicin 125  119197.90  73.96800 *\n#&gt;    3) age&lt; 4.947975 200 1347741.00 133.87000  \n#&gt;      6) treatment=control 140  986790.70 118.25710  \n#&gt;       12) reflex=mid,full 129  754363.70 111.56590 *\n#&gt;       13) reflex=low 11  158918.20 196.72730 *\n#&gt;      7) treatment=gentamicin 60  247194.60 170.30000  \n#&gt;       14) age&lt; 4.664439 30  102190.20 147.83330  \n#&gt;         28) age&gt;=4.566638 22   53953.86 129.77270 *\n#&gt;         29) age&lt; 4.566638 8   21326.00 197.50000 *\n#&gt;       15) age&gt;=4.664439 30  114719.40 192.76670 *"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#a-model-workflow-3",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#a-model-workflow-3",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"regression\")\n\nworkflow(latency ~ ., tree_spec) %&gt;% \n  fit(data = frog_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; latency ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 456 \n#&gt; \n#&gt; node), split, n, deviance, yval\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 456 2197966.00  92.90351  \n#&gt;    2) age&gt;=4.947975 256  252347.40  60.89844  \n#&gt;      4) treatment=control 131   91424.06  48.42748 *\n#&gt;      5) treatment=gentamicin 125  119197.90  73.96800 *\n#&gt;    3) age&lt; 4.947975 200 1347741.00 133.87000  \n#&gt;      6) treatment=control 140  986790.70 118.25710  \n#&gt;       12) reflex=mid,full 129  754363.70 111.56590 *\n#&gt;       13) reflex=low 11  158918.20 196.72730 *\n#&gt;      7) treatment=gentamicin 60  247194.60 170.30000  \n#&gt;       14) age&lt; 4.664439 30  102190.20 147.83330  \n#&gt;         28) age&gt;=4.566638 22   53953.86 129.77270 *\n#&gt;         29) age&lt; 4.566638 8   21326.00 197.50000 *\n#&gt;       15) age&gt;=4.664439 30  114719.40 192.76670 *"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn-2",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn-2",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_wflow chunk in your .qmd.\nEdit this code so it uses a linear model.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#predict-with-your-model",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#predict-with-your-model",
    "title": "3 - What makes a model?",
    "section": "Predict with your model  ",
    "text": "Predict with your model  \nHow do you use your new tree_fit model?\n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"regression\")\n\ntree_fit &lt;-\n  workflow(latency ~ ., tree_spec) %&gt;% \n  fit(data = frog_train)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn-3",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn-3",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\npredict(tree_fit, new_data = frog_test)\nWhat do you get?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn-4",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn-4",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\naugment(tree_fit, new_data = frog_test)\nWhat do you get?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#understand-your-model",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#understand-your-model",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#understand-your-model-1",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#understand-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nlibrary(rpart.plot)\ntree_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot(roundint = FALSE)\n\nYou can extract_*() several components of your fitted workflow.\n\nroundint = FALSE is only to quiet a warning"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#understand-your-model-2",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#understand-your-model-2",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nYou can use your fitted workflow for model and/or prediction explanations:\n\n\n\noverall variable importance, such as with the vip package\n\n\n\n\nflexible model explainers, such as with the DALEXtra package\n\n\n\nLearn more at https://www.tmwr.org/explain.html"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn-5",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn-5",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nExtract the model engine object from your fitted linear workflow.\nâš ï¸ Never predict() with any extracted components!\n\n\n\n05:00\n\n\n\n\nAfterward, ask what kind of object people got from the extraction, and what they did with it (e.g.Â give it to summary(), plot(), broom::tidy() ). Live code along"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#deploying-a-model",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#deploying-a-model",
    "title": "3 - What makes a model?",
    "section": "Deploying a model ",
    "text": "Deploying a model \nHow do you use your new tree_fit model in production?\n\nlibrary(vetiver)\nv &lt;- vetiver_model(tree_fit, \"frog_hatching\")\nv\n#&gt; \n#&gt; â”€â”€ frog_hatching â”€ &lt;bundled_workflow&gt; model for deployment \n#&gt; A rpart regression modeling workflow using 4 features\n\nLearn more at https://vetiver.rstudio.com"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#deploy-your-model-1",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#deploy-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Deploy your model ",
    "text": "Deploy your model \nHow do you use your new model tree_fit in production?\n\nlibrary(plumber)\npr() %&gt;%\n  vetiver_api(v)\n#&gt; # Plumber router with 3 endpoints, 4 filters, and 1 sub-router.\n#&gt; # Use `pr_run()` on this object to start the API.\n#&gt; â”œâ”€â”€[queryString]\n#&gt; â”œâ”€â”€[body]\n#&gt; â”œâ”€â”€[cookieParser]\n#&gt; â”œâ”€â”€[sharedSecret]\n#&gt; â”œâ”€â”€/logo\n#&gt; â”‚  â”‚ # Plumber static router serving from directory: /Users/emilhvitfeldt/Library/R/arm64/4.2/library/vetiver\n#&gt; â”œâ”€â”€/metadata (GET)\n#&gt; â”œâ”€â”€/ping (GET)\n#&gt; â””â”€â”€/predict (POST)\n\nLearn more at https://vetiver.rstudio.com\n\nLive-code making a prediction"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn-6",
    "href": "archive/2022-08-Reykjavik-City/03-what-makes-a-model.html#your-turn-6",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the vetiver chunk in your .qmd.\nCheck out the automated visual documentation.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#who-are-you",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %&gt;% or base R |&gt; pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have exposure to basic statistical concepts\nYou do not need intermediate or expert familiarity with modeling or ML"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#who-are-tidymodels",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#who-are-tidymodels",
    "title": "1 - Introduction",
    "section": "Who are tidymodels?",
    "text": "Who are tidymodels?\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\nMany thanks to Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill, and DesirÃ©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#asking-for-help",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#asking-for-help",
    "title": "1 - Introduction",
    "section": "Asking for help",
    "text": "Asking for help\n\nğŸŸª â€œIâ€™m stuck and need help!â€\n\n\nğŸŸ© â€œI finished the exerciseâ€"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#section",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#section",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#tentative-plan-for-this-workshop",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#tentative-plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Tentative plan for this workshop",
    "text": "Tentative plan for this workshop\n\nToday:\n\nYour data budget\nWhat makes a model\nEvaluating models\n\nTomorrow:\n\nFeature engineering\nTuning hyperparameters\nTransportation case study\nWrapping up!"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#section-1",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#section-1",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors ğŸ‘‹\n\n Log in to RStudio Cloud here (free):\nbit.ly/tidymodels-iceland-2022"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#what-is-machine-learning",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#what-is-machine-learning",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nhttps://xkcd.com/1838/"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#what-is-machine-learning-1",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#what-is-machine-learning-1",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#what-is-machine-learning-2",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#what-is-machine-learning-2",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#your-turn",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\n\n\nHow are statistics and machine learning related?\nHow are they similar? Different?\n\n\n\n03:00\n\n\n\n\nthe â€œtwo culturesâ€\nmodel first vs.Â data first\ninference vs.Â prediction"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#what-is-tidymodels",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#what-is-tidymodels",
    "title": "1 - Introduction",
    "section": "What is tidymodels? ",
    "text": "What is tidymodels? \n\nlibrary(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.1.0 â”€â”€\n#&gt; âœ” broom        1.0.5     âœ” rsample      1.1.1\n#&gt; âœ” dials        1.2.0     âœ” tibble       3.2.1\n#&gt; âœ” dplyr        1.1.2     âœ” tidyr        1.3.0\n#&gt; âœ” infer        1.0.4     âœ” tune         1.1.1\n#&gt; âœ” modeldata    1.1.0     âœ” workflows    1.1.3\n#&gt; âœ” parsnip      1.1.0     âœ” workflowsets 1.0.1\n#&gt; âœ” purrr        1.0.1     âœ” yardstick    1.2.0\n#&gt; âœ” recipes      1.0.6\n#&gt; â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\n#&gt; âœ– purrr::discard() masks scales::discard()\n#&gt; âœ– dplyr::filter()  masks stats::filter()\n#&gt; âœ– dplyr::lag()     masks stats::lag()\n#&gt; âœ– recipes::step()  masks stats::step()\n#&gt; â€¢ Learn how to get started at https://www.tidymodels.org/start/"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#the-whole-game",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#the-whole-game",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\nTomorrow we will walk through a case study in detail to illustrate feature engineering and model tuning.\nToday we will walk through the analysis at a higher level to show the model development process as a whole and give you an introduction to the data set.\nThe data are from the NHL where we want to predict whether a shot was on-goal or not! ğŸ’\nItâ€™s a good example to show how model development works."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#shots-on-goal",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#shots-on-goal",
    "title": "1 - Introduction",
    "section": "Shots on goal",
    "text": "Shots on goal"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#data-spending",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#data-spending",
    "title": "1 - Introduction",
    "section": "Data spending",
    "text": "Data spending"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#a-first-model",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#a-first-model",
    "title": "1 - Introduction",
    "section": "A first model",
    "text": "A first model"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#starting-point-logistic-regression",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#starting-point-logistic-regression",
    "title": "1 - Introduction",
    "section": "Starting point: logistic regression",
    "text": "Starting point: logistic regression\n\nWeâ€™ll start by using basic logistic regression to predict our binary outcome.\nOur first model will have 12 simple predictor columns.\nOne initial question: there are 632 players taking shots.\nFor logistic regression, do we convert these to binary indicators (a.k.a. â€œdummiesâ€)?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#basic-features-inc-dummy-variables",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#basic-features-inc-dummy-variables",
    "title": "1 - Introduction",
    "section": "Basic features (inc dummy variables)",
    "text": "Basic features (inc dummy variables)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#different-player-encoding",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#different-player-encoding",
    "title": "1 - Introduction",
    "section": "Different player encoding",
    "text": "Different player encoding"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#what-about-location",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#what-about-location",
    "title": "1 - Introduction",
    "section": "What about location",
    "text": "What about location\nThe previous models used the x/y coordinates.\nAre there better ways to represent shot location?\nHow can we make location more usable for the model?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#add-shot-angle",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#add-shot-angle",
    "title": "1 - Introduction",
    "section": "Add shot angle?",
    "text": "Add shot angle?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#add-shot-from-defensive-zone",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#add-shot-from-defensive-zone",
    "title": "1 - Introduction",
    "section": "Add shot from defensive zone?",
    "text": "Add shot from defensive zone?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#add-shot-behind-goal-line",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#add-shot-behind-goal-line",
    "title": "1 - Introduction",
    "section": "Add shot behind goal line?",
    "text": "Add shot behind goal line?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#nonlinear-terms-for-angle-and-distance",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#nonlinear-terms-for-angle-and-distance",
    "title": "1 - Introduction",
    "section": "Nonlinear terms for angle and distance",
    "text": "Nonlinear terms for angle and distance"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#try-another-model",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#try-another-model",
    "title": "1 - Introduction",
    "section": "Try another model",
    "text": "Try another model"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#switch-to-boosting-and-basic-features",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#switch-to-boosting-and-basic-features",
    "title": "1 - Introduction",
    "section": "Switch to boosting and basic features",
    "text": "Switch to boosting and basic features"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#boosting-with-location-features",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#boosting-with-location-features",
    "title": "1 - Introduction",
    "section": "Boosting with location features",
    "text": "Boosting with location features"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#choose-wisely",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#choose-wisely",
    "title": "1 - Introduction",
    "section": "Choose wiselyâ€¦",
    "text": "Choose wiselyâ€¦"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#finalize-and-verify",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#finalize-and-verify",
    "title": "1 - Introduction",
    "section": "Finalize and verify",
    "text": "Finalize and verify"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#and-so-on",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#and-so-on",
    "title": "1 - Introduction",
    "section": "â€¦ and so on",
    "text": "â€¦ and so on\nOnce we find an acceptable model and feature set, the process is to\n\nConfirm our results on the test set.\nDocument the data and model development process.\nDeploy, monitor, etc."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#lets-install-some-packages",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Letâ€™s install some packages",
    "text": "Letâ€™s install some packages\nIf you are using your own laptop instead of RStudio Cloud:\n\ninstall.packages(c(\"Cubist\", \"DALEXtra\", \"doParallel\", \"earth\", \"embed\", \n                   \"forcats\", \"lme4\", \"parallelly\", \"ranger\", \"remotes\", \"rpart\", \n                   \"rpart.plot\", \"rules\", \"stacks\", \"tidymodels\",\n                   \"vetiver\", \"xgboost\"))\n\nremotes::install_github(\"topepo/ongoal@hockeyR\")\n\n\n\n Or log in to RStudio Cloud:\nbit.ly/tidymodels-iceland-2022"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/01-introduction.html#our-versions",
    "href": "archive/2022-08-Reykjavik-City/01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\nbroom (1.0.5, CRAN), DALEX (2.4.3, CRAN), DALEXtra (2.2.1, CRAN), dials (1.2.0, CRAN), doParallel (1.0.17, CRAN), dplyr (1.1.2, CRAN), embed (1.1.1, CRAN), ggplot2 (3.4.2, CRAN (R 4.2.1)), modeldata (1.1.0, CRAN), ongoal (0.0.4, Github (topepo/ongoal@2532b2404), parsnip (1.1.0, CRAN), purrr (1.0.1, CRAN), ranger (0.15.1, CRAN), recipes (1.0.6, CRAN), rpart (4.1.16, CRAN (R 4.2.1)), rpart.plot (3.1.1, CRAN), rsample (1.1.1, CRAN), scales (1.2.1, CRAN), stacks (1.0.2, CRAN), tibble (3.2.1, CRAN (R 4.2.1)), tidymodels (1.1.0, CRAN), tidyr (1.3.0, CRAN), tune (1.1.1, CRAN), vetiver (0.2.1, CRAN), workflows (1.1.3, CRAN), workflowsets (1.0.1, CRAN), xgboost (1.7.5.1, CRAN), and yardstick (1.2.0, CRAN)\nQuarto: 1.3.433"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html",
    "href": "archive/2022-07-RStudio-conf/annotations.html",
    "title": "Annotations",
    "section": "",
    "text": "This page contains annotations for selected slides.\nThereâ€™s a lot that we want to tell you. We donâ€™t want people to have to frantically scribble down things that we say that are not on the slides.\nWeâ€™ve added sections to this document with longer explanations and links to other resources.\n\n\n\nThis is a pretty complex data usage scheme. That is mostly because of the validation set. In every other case, the situation is much more simple.\nThe important point here is that: tidymodels does most of this work for you. In other words, you donâ€™t have to directly specify which data are being used where.\nIn a later section, we will talk about methods of resampling. These methods are like repeated validation sets. As an example, the popular 10-fold cross-validation method is one such type of resampling. Validation sets are special cases of resampling where there is a single â€œresampleâ€.\nMost types of resampling use multiple hold-out sets of samples from the training set. In those cases, a diagram for data usage here would look like\n\n\n\n\n\n\n\n\n\nIn this case there is just â€œtestingâ€ and â€œtrainingâ€. Once the final model is determined, the entire training set is used for the last fit.\nThis is the process that will be used for the tree frog data."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#section",
    "href": "archive/2022-07-RStudio-conf/annotations.html#section",
    "title": "Annotations",
    "section": "",
    "text": "This page contains annotations for selected slides.\nThereâ€™s a lot that we want to tell you. We donâ€™t want people to have to frantically scribble down things that we say that are not on the slides.\nWeâ€™ve added sections to this document with longer explanations and links to other resources."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#finalize-and-verify",
    "href": "archive/2022-07-RStudio-conf/annotations.html#finalize-and-verify",
    "title": "Annotations",
    "section": "",
    "text": "This is a pretty complex data usage scheme. That is mostly because of the validation set. In every other case, the situation is much more simple.\nThe important point here is that: tidymodels does most of this work for you. In other words, you donâ€™t have to directly specify which data are being used where.\nIn a later section, we will talk about methods of resampling. These methods are like repeated validation sets. As an example, the popular 10-fold cross-validation method is one such type of resampling. Validation sets are special cases of resampling where there is a single â€œresampleâ€.\nMost types of resampling use multiple hold-out sets of samples from the training set. In those cases, a diagram for data usage here would look like\n\n\n\n\n\n\n\n\n\nIn this case there is just â€œtestingâ€ and â€œtrainingâ€. Once the final model is determined, the entire training set is used for the last fit.\nThis is the process that will be used for the tree frog data."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#data-splitting-and-spending",
    "href": "archive/2022-07-RStudio-conf/annotations.html#data-splitting-and-spending",
    "title": "Annotations",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nWhat does set.seed() do?\nWeâ€™ll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random).\nThink of PRN as a box that takes a starting value (the â€œseedâ€) that produces random numbers using that starting value as an input into its process.\nIf we know a seed value, we can reproduce our â€œrandomâ€ numbers. To use a different set of random numbers, choose a different seed value.\nFor example:\n\nset.seed(1)\nrunif(3)\n\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\n# Get a new set of random numbers:\nset.seed(2)\nrunif(3)\n\n#&gt; [1] 0.1848823 0.7023740 0.5733263\n\n# We can reproduce the old ones with the same seed\nset.seed(1)\nrunif(3)\n\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\n\nIf we donâ€™t set the seed, R uses the clock time and the process ID to create a seed. This isnâ€™t reproducible.\nSince we want our code to be reproducible, we set the seeds before random numbers are used.\nIn theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same and we donâ€™t get reproducible results.\nThe value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to â€œspread the randomness aroundâ€. It is basically:\n\ncat(paste0(\"set.seed(\", sample.int(10000, 5), \")\", collapse = \"\\n\"))\n\n#&gt; set.seed(9725)\n#&gt; set.seed(8462)\n#&gt; set.seed(4050)\n#&gt; set.seed(8789)\n#&gt; set.seed(1301)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#what-is-wrong-with-this",
    "href": "archive/2022-07-RStudio-conf/annotations.html#what-is-wrong-with-this",
    "title": "Annotations",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?\nIf we treat the preprocessing as a separate task, it raises the risk that we might accidentally overfit to the data at hand.\nFor example, someone might estimate something from the entire data set (such as the principle components) and treat that data as if it were known (and not estimated). Depending on the what was done with the data, consequences in doing that could be:\n\nYour performance metrics are slightly-to-moderately optimistic (e.g.Â you might think your accuracy is 85% when it is actually 75%)\nA consequential component of the analysis is not right and the model just doesnâ€™t work.\n\nThe big issue here is that you wonâ€™t be able to figure this out until you get a new piece of data, such as the test set.\nA really good example of this is in â€˜Selection bias in gene extraction on the basis of microarray gene-expression dataâ€™. The authors re-analyze a previous publication and show that the original researchers did not include feature selection in the workflow. Because of that, their performance statistics were extremely optimistic. In one case, they could do the original analysis on complete noise and still achieve zero errors.\nGenerally speaking, this problem is referred to as data leakage. Some other references:\n\nOverfitting to Predictors and External Validation\nAre We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning\nNavigating the pitfalls of applying machine learning in genomics\nA review of feature selection techniques in bioinformatics\nOn Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#where-are-the-fitted-models",
    "href": "archive/2022-07-RStudio-conf/annotations.html#where-are-the-fitted-models",
    "title": "Annotations",
    "section": "Where are the fitted models?",
    "text": "Where are the fitted models?\nThe primary purpose of resampling is to estimate model performance. The models are almost never needed again.\nAlso, if the data set is large, the model object may require a lot of memory to save so, by default, we donâ€™t keep them.\nFor more advanced use cases, you can extract and save them. See:\n\nhttps://www.tmwr.org/resampling.html#extract\nhttps://www.tidymodels.org/learn/models/coefficients/ (an example)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#the-final-fit",
    "href": "archive/2022-07-RStudio-conf/annotations.html#the-final-fit",
    "title": "Annotations",
    "section": "The final fit",
    "text": "The final fit\nSince our data spending scheme created the resamples from the training set, last_fit() will use all of the training data to fit the final workflow.\nAs shown in the Whole Game slides, there is a slightly different scheme used when we have a validation set (instead of multiple resamples like 10-fold CV)."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#per-player-statistics",
    "href": "archive/2022-07-RStudio-conf/annotations.html#per-player-statistics",
    "title": "Annotations",
    "section": "Per-player statistics",
    "text": "Per-player statistics\nThe effect encoding method essentially takes the effect of a variable, like player, and makes a data column for that effect. In our example, the ability of a player to have an on-goal shot is quantified by a model and then added as a data column to be used in the model.\nSuppose NHL rookie Max has a single shot in the data and it was on goal. If we used a naive estimate for Maxâ€™s effect, the model is being told that Max should have a 100% chance of being on goal. Thatâ€™s a very poor estimate since it is from a single data point.\nContrast this with seasoned player Davis, who has taken 250 shots and 75% of these were on goal. Davisâ€™s proportion is more predictive because it is estimated with better data (i.e., more total shots). Partial pooling leverages the entire data set and can borrow strength from all of the players. It is a common tool in Bayesian estimation and non-Bayesian mixed models. If a playerâ€™s data is of good quality, the partial pooling effect estimate is closer to the raw proportion. Maxâ€™s data is not great and is â€œshrunkâ€ towards the center of the overall on goal proportion. Since there is so little known about Maxâ€™s shot history, this is a better effect estimate (until more data is available for him).\nThe Stan documentation has a pretty good vignette on this: https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html\nAlso, Bayes Rules! has a nice section on this: https://www.bayesrulesbook.com/chapter-15.html\nIf the outcome were numeric, the effect would be the mean of the outcome per player. In this case, partial pooling is very similar to the Jamesâ€“Stein estimator: https://en.wikipedia.org/wiki/Jamesâ€“Stein_estimator"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#player-effects",
    "href": "archive/2022-07-RStudio-conf/annotations.html#player-effects",
    "title": "Annotations",
    "section": "Player effects",
    "text": "Player effects\nEffect encoding might result in a somewhat circular argument: the column is more likely to be important to the model since it is the output of a separate model. The risk here is that we might over-fit the effect to the data. For this reason, it is super important to make sure that we verify that we arenâ€™t overfitting by checking with resampling (or a validation set).\nPartial pooling somewhat lowers the risk of overfitting since it tends to correct for players with small sample sizes. It canâ€™t correct for improper data usage or data leakage though."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#angle",
    "href": "archive/2022-07-RStudio-conf/annotations.html#angle",
    "title": "Annotations",
    "section": "Angle",
    "text": "Angle\nAbout these formulasâ€¦\nThe coordinates for the rink are centered at (0, 0) and the goal lines are both 89 ft from center. The center of the goal on the left is at (-89, 0) and the right-hand goal is centered at (89, 0).\nFor the distance to center ice, the formula is\n\\[d_{center} = \\sqrt{x^2 + y^2}\\]\nWe want distance to the goal line(s), so we first use x* = (89 - abs(coord_x)) to make the side of the rink irrelevant and then compute\n\\[d_{goal} = \\sqrt{x^{*2} + y^2}\\]\nIn the code, we log the distance value to help its distribution become more symmetric.\nFor angle to center, the formula is\n\\[a = \\tan^{-1}\\left(\\frac{y}{x}\\right)\\]\nThis is in radian units and we can convert to degrees using\n\\[a = \\frac{180}{\\pi}\\tan^{-1}\\left(\\frac{y}{x}\\right)\\]\nFor the angle to the goal, we need to alter \\(x\\) and \\(y\\) again. Weâ€™ll use \\(x^*\\) from above and also use the absolute value of \\(y\\) so that the degrees range from 0 to 180."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#update-parameter-ranges",
    "href": "archive/2022-07-RStudio-conf/annotations.html#update-parameter-ranges",
    "title": "Annotations",
    "section": "Update parameter ranges",
    "text": "Update parameter ranges\nIn about 90% of the cases, the dials function that you use to update the parameter range has the same name as the argument. For example, if you were to update the mtry parameter in a random forests model, the code would look like\n\nparameter_object %&gt;% \n  update(mtry = mtry(c(1, 100)))\n\nIn our case, the argument name is deg_free but we update it with spline_degree().\ndeg_free represents the general concept of degrees of freedom and could be associated with many different things. For example, if we ever had an argument that was the number of degrees of freedom for a \\(t\\) distribution, we would call that argument deg_free.\nFor splines, we probably want a wider range for the degrees of freedom. We made a specialized function called spline_degree() to be used in these cases.\nHow can you tell when this happens? There is a helper function called tunable() and that gives information on how we make the default ranges for parameters. There is a column in these objects names call_info:\n\nlibrary(tidymodels)\nns_tunable &lt;- \n  recipe(mpg ~ ., data = mtcars) %&gt;% \n  step_ns(dis, deg_free = tune()) %&gt;% \n  tunable()\n\nns_tunable\n\n#&gt; # A tibble: 1 Ã— 5\n#&gt;   name     call_info        source component component_id\n#&gt;   &lt;chr&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;       \n#&gt; 1 deg_free &lt;named list [3]&gt; recipe step_ns   ns_P1Tjg\n\nns_tunable$call_info\n\n#&gt; [[1]]\n#&gt; [[1]]$pkg\n#&gt; [1] \"dials\"\n#&gt; \n#&gt; [[1]]$fun\n#&gt; [1] \"spline_degree\"\n#&gt; \n#&gt; [[1]]$range\n#&gt; [1]  1 15"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#spline-grid-search",
    "href": "archive/2022-07-RStudio-conf/annotations.html#spline-grid-search",
    "title": "Annotations",
    "section": "Spline grid search",
    "text": "Spline grid search\nWhatâ€™s going on with the\n\nprediction from a rank-deficient fit may be misleading\n\nwarnings?\nFor linear regression, a computation is used called matrix inversion. The matrix in question is called the â€œmodel matrixâ€ and it contains the predictor set for the training data.\nMatrix inversion can fail if two or more columns:\n\nare identical, or\nadd up to some other column.\n\nThese situations are called linear dependencies.\nWhen this happens, lm() is pretty tolerant. It does not fail but does not compute regression coefficients for a minimal number of predictors involved in the dependency (and issues the warning above).\nFor these data, there are three dependencies between:\n\ndefense_team_PIT and offense_team_PIT\nstrength_short_handed, player_diff, and strength_power_play\nyear, month_Oct, month_Nov, and month_Dec\n\nThe first one is easy to explain. For each row, when one these two PIT column has a one, the other must have a zero. The linear regression intercept is represented in the model matrix as a column of all ones. The dependency is\n(Intercept) = defense_team_PIT + offense_team_PIT\nThe way to avoid this problem is to use step_lincomb(all_numeric_predictors()) in the recipe. This step removes the minimum number of columns to avoid the issue.\ntl;dr\nLinear regression detects some redundancies in the predictor set. We can ignore the warnings since lm() can deal with it or use step_lincomb() to avoid the warnings."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#boosted-tree-tuning-parameters",
    "href": "archive/2022-07-RStudio-conf/annotations.html#boosted-tree-tuning-parameters",
    "title": "Annotations",
    "section": "Boosted tree tuning parameters",
    "text": "Boosted tree tuning parameters\nWhen deciding on the number of boosting iterations, there are two main strategies:\n\nDirectly tune it (trees = tune())\nSet it to one value and tune the number of early stopping iterations (trees = 500, stop_iter = tune()).\n\nEarly stopping is when we monitor the performance of the model. If the model doesnâ€™t make any improvements for stop_iter iterations, training stops.\nHereâ€™s an example where, after eleven iterations, performance starts to get worse.\n\n\n\n\n\n\n\n\n\nThis is likely due to over-fitting so we stop the model at eleven boosting iterations.\nEarly stopping usually has good results and takes far less time."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#boosted-tree-code",
    "href": "archive/2022-07-RStudio-conf/annotations.html#boosted-tree-code",
    "title": "Annotations",
    "section": "Boosted tree code",
    "text": "Boosted tree code\nWe set an engine argument called validation here. Thatâ€™s not an argument to any function in the xgboost package.\nparsnip has its own wrapper around (xgboost::xgb.train()) called xgb_train(). We use that here and it has a validation argument.\nHow would you know that? There are a few different ways:\n\nLook at the documentation in ?boost_tree and click on the xgboost entry in the engine list.\nCheck out the pkgdown reference website https://parsnip.tidymodels.org/reference/index.html\nRun the translate() function on the parsnip specification object.\n\nThe first two options are best since they tell you a lot more about the particularities of each model engine (there are a lot for xgboost)."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#the-final-fit-to-the-nhl-data",
    "href": "archive/2022-07-RStudio-conf/annotations.html#the-final-fit-to-the-nhl-data",
    "title": "Annotations",
    "section": "The final fit to the NHL data",
    "text": "The final fit to the NHL data\nRecall that last_fit() uses the objects produced by initial_split() to determine what data are used for the final model fit and which are used as the test set.\nFor the validation set, last_fit() will use the non-testing data to create the final model fit. This includes the training and validation set.\nThere is no agreement in the community on whether this is the best approach or if we should just use the training set. There are good arguments either way.\nIf you only want to use the training set for the final model, you can do this via:\n\ntraining_data &lt;- nhl_val$splits[[1]] %&gt;% analysis()\n\n# Use `fit()` to train the model on just the training set\nfinal_glm_spline_wflow &lt;- \n  glm_spline_wflow %&gt;% \n  fit(data = training_data)\n\n# Create test set predictions\ntest_set_pred &lt;- augment(final_glm_spline_wflow, nhl_test)\n\n# Setup and compute the test set metrics\ncls_metrics &lt;- metric_set(roc_auc, accuracy)\n\ntest_res &lt;- \n  test_set_pred %&gt;% \n  cls_metrics(on_goal, estimate = .pred_class, .pred_yes)\ntest_res"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#explain-yourself",
    "href": "archive/2022-07-RStudio-conf/annotations.html#explain-yourself",
    "title": "Annotations",
    "section": "Explain yourself",
    "text": "Explain yourself\nSome other resources:\n\nTMwR chapter Explaining Models and Predictions\nExplanatory Model Analysis book\nInterpretable Machine Learning book\nDefinitions, methods, and applications in interpretable machine learning"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/annotations.html#a-tidymodels-explainer",
    "href": "archive/2022-07-RStudio-conf/annotations.html#a-tidymodels-explainer",
    "title": "Annotations",
    "section": "A tidymodels explainer",
    "text": "A tidymodels explainer\nFor our example, the angle was an original predictor. Recall that we made spline terms from this predictor, so there are derived features such as angle_ns_1 and so on.\nOriginal versus derived doesnâ€™t affect local explainers since we are focused on a single prediction.\nFor global explainers, we should decide between:\n\nexplaining the overall affect of angle (lumping all its features into one importance score), or\nexplaining the effect of each term in the model (including angle_ns_1 and so on).\n\nThe choice depends on what you want. For example, if we have an original date predictor and make features for month and year, is it more informative to know if date is important (overall) or exactly how the date is important? You might want to look at it both ways."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-parameters",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#optimize-tuning-parameters",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#optimize-tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search ğŸ’  which tests a pre-defined set of candidate values\nIterative search ğŸŒ€ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choosing-tuning-parameters",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choosing-tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters    ",
    "text": "Choosing tuning parameters    \nLetâ€™s take our previous recipe and add a few changes:\n\nglm_rec &lt;-\n  recipe(on_goal ~ ., data = nhl_train) %&gt;%\n  step_lencode_mixed(player, outcome = vars(on_goal)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),\n    distance = log(distance),\n    behind_goal_line = ifelse(abs(coord_x) &gt;= 89, 1, 0)\n  ) %&gt;%\n  step_rm(coord_x, coord_y) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_ns(angle, deg_free = tune(\"angle\")) %&gt;%\n  step_ns(distance, deg_free = tune(\"distance\")) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n\nLetâ€™s tune() our spline terms!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choosing-tuning-parameters-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choosing-tuning-parameters-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters    ",
    "text": "Choosing tuning parameters    \nLetâ€™s take our previous recipe and add a few changes:\n\nglm_spline_wflow &lt;-\n  workflow() %&gt;%\n  add_model(logistic_reg()) %&gt;%\n  add_recipe(glm_rec)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#section",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#section",
    "title": "6 - Tuning Hyperparameters",
    "section": "",
    "text": "Splines replace the existing numeric predictor with a set of columns that allow a model to emulate a flexible, nonlinear relationship.\nMore spline terms = more â€œwigglyâ€, i.e.Â flexibly model a nonlinear relationship\nHow many spline terms? This is called degrees of freedom\n2 and 5 look like they underfit; 20 and 100 look like they overfit"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#splines-and-nonlinear-relationships",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#splines-and-nonlinear-relationships",
    "title": "6 - Tuning Hyperparameters",
    "section": "Splines and nonlinear relationships",
    "text": "Splines and nonlinear relationships\n\n\nOur hockey data exhibits nonlinear relationships\nWe can model nonlinearity like this via a model (later this afternoon) or feature engineering\nHow do we decide how â€œwigglyâ€ or flexible to make our spline features? TUNING"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#grid-search",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#grid-search",
    "title": "6 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nParameters\n\nThe tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\nThe extract_parameter_set_dials() function extracts these tuning parameters and the info.\n\n\nGrids\n\nCreate your grid manually or automatically.\nThe grid_*() functions can make a grid.\n\n\n\nMost basic (but very effective) way to tune models"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#create-a-grid",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#create-a-grid",
    "title": "6 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nglm_spline_wflow %&gt;% \n  extract_parameter_set_dials()\n#&gt; Collection of 2 parameters for tuning\n#&gt; \n#&gt;  identifier     type    object\n#&gt;       angle deg_free nparam[+]\n#&gt;    distance deg_free nparam[+]\n\n\nA parameter set can be updated (e.g.Â to change the ranges)."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#create-a-grid-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#create-a-grid-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\n\n\nset.seed(2)\ngrid &lt;- \n  glm_spline_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#&gt; # A tibble: 23 Ã— 2\n#&gt;    angle distance\n#&gt;    &lt;int&gt;    &lt;int&gt;\n#&gt;  1    12        4\n#&gt;  2    15        8\n#&gt;  3     6       14\n#&gt;  4    10        5\n#&gt;  5    12       12\n#&gt;  6     7        8\n#&gt;  7    14        3\n#&gt;  8    14       13\n#&gt;  9    11       12\n#&gt; 10     8       11\n#&gt; # â€¦ with 13 more rows\n#&gt; # â„¹ Use `print(n = ...)` to see more rows\n\n\n\n\nA space-filling design like this tends to perform better than random grids.\nSpace-filling designs are also usually more efficient than regular grids."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a grid for our tunable workflow.\nTry creating a regular grid.\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#create-a-grid-2",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#create-a-grid-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nset.seed(2)\ngrid &lt;- \n  glm_spline_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 25)\n\ngrid\n#&gt; # A tibble: 225 Ã— 2\n#&gt;    angle distance\n#&gt;    &lt;int&gt;    &lt;int&gt;\n#&gt;  1     1        1\n#&gt;  2     2        1\n#&gt;  3     3        1\n#&gt;  4     4        1\n#&gt;  5     5        1\n#&gt;  6     6        1\n#&gt;  7     7        1\n#&gt;  8     8        1\n#&gt;  9     9        1\n#&gt; 10    10        1\n#&gt; # â€¦ with 215 more rows\n#&gt; # â„¹ Use `print(n = ...)` to see more rows\n\n\nNote that even though we requested 25x25=625 rows, we only got 15x15=225 back, since the deg_free parameters only have a range of 1-&gt;15."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#update-parameter-ranges",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#update-parameter-ranges",
    "title": "6 - Tuning Hyperparameters",
    "section": "Update parameter ranges  ",
    "text": "Update parameter ranges  \n\nset.seed(2)\ngrid &lt;- \n  glm_spline_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  update(angle = spline_degree(c(2L, 20L)),\n         distance = spline_degree(c(2L, 20L))) %&gt;% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#&gt; # A tibble: 24 Ã— 2\n#&gt;    angle distance\n#&gt;    &lt;int&gt;    &lt;int&gt;\n#&gt;  1    16        6\n#&gt;  2    20       11\n#&gt;  3     8       19\n#&gt;  4    14        7\n#&gt;  5    16       17\n#&gt;  6    10       11\n#&gt;  7    19        5\n#&gt;  8    18       17\n#&gt;  9    15       16\n#&gt; 10    11       15\n#&gt; # â€¦ with 14 more rows\n#&gt; # â„¹ Use `print(n = ...)` to see more rows\n\n\nEven though angle is a deg_free parameter in step_ns(), we donâ€™t use the dials deg_free() object here. We have a special spline_degree() function that has better defaults for splines."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#the-results",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#the-results",
    "title": "6 - Tuning Hyperparameters",
    "section": "The results  ",
    "text": "The results  \n\n\ngrid %&gt;% \n  ggplot(aes(angle, distance)) +\n  geom_point(size = 4)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#spline-grid-search",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#spline-grid-search",
    "title": "6 - Tuning Hyperparameters",
    "section": "Spline grid search   ",
    "text": "Spline grid search   \n\nset.seed(9)\nctrl &lt;- control_grid(save_pred = TRUE, parallel_over = \"everything\")\n\nglm_spline_res &lt;-\n  glm_spline_wflow %&gt;%\n  tune_grid(resamples = nhl_val, grid = grid, control = ctrl)\n\nglm_spline_res\n#&gt; # Tuning results\n#&gt; # Validation Set Split (0.8/0.2)  \n#&gt; # A tibble: 1 Ã— 5\n#&gt;   splits              id         .metrics          .notes            .predictions         \n#&gt;   &lt;list&gt;              &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;            &lt;list&gt;               \n#&gt; 1 &lt;split [7288/1822]&gt; validation &lt;tibble [48 Ã— 6]&gt; &lt;tibble [24 Ã— 3]&gt; &lt;tibble [43,728 Ã— 8]&gt;\n#&gt; \n#&gt; There were issues with some computations:\n#&gt; \n#&gt;   - Warning(s) x24: prediction from a rank-deficient fit may be misleading\n#&gt; \n#&gt; Run `show_notes(.Last.tune.result)` for more information.\n\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nTune our glm_wflow.\nWhat happens if you donâ€™t supply a grid argument to tune_grid()?\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#grid-results",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#grid-results",
    "title": "6 - Tuning Hyperparameters",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(glm_spline_res)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(glm_spline_res)\n#&gt; # A tibble: 48 Ã— 8\n#&gt;    angle distance .metric  .estimator  mean     n std_err .config              \n#&gt;    &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1    16        6 accuracy binary     0.610     1      NA Preprocessor01_Model1\n#&gt;  2    16        6 roc_auc  binary     0.649     1      NA Preprocessor01_Model1\n#&gt;  3    20       11 accuracy binary     0.613     1      NA Preprocessor02_Model1\n#&gt;  4    20       11 roc_auc  binary     0.649     1      NA Preprocessor02_Model1\n#&gt;  5     8       19 accuracy binary     0.619     1      NA Preprocessor03_Model1\n#&gt;  6     8       19 roc_auc  binary     0.652     1      NA Preprocessor03_Model1\n#&gt;  7    14        7 accuracy binary     0.610     1      NA Preprocessor04_Model1\n#&gt;  8    14        7 roc_auc  binary     0.650     1      NA Preprocessor04_Model1\n#&gt;  9    16       17 accuracy binary     0.617     1      NA Preprocessor05_Model1\n#&gt; 10    16       17 roc_auc  binary     0.649     1      NA Preprocessor05_Model1\n#&gt; # â€¦ with 38 more rows\n#&gt; # â„¹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(glm_spline_res, summarize = FALSE)\n#&gt; # A tibble: 48 Ã— 7\n#&gt;    id         angle distance .metric  .estimator .estimate .config              \n#&gt;    &lt;chr&gt;      &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1 validation    16        6 accuracy binary         0.610 Preprocessor01_Model1\n#&gt;  2 validation    16        6 roc_auc  binary         0.649 Preprocessor01_Model1\n#&gt;  3 validation    20       11 accuracy binary         0.613 Preprocessor02_Model1\n#&gt;  4 validation    20       11 roc_auc  binary         0.649 Preprocessor02_Model1\n#&gt;  5 validation     8       19 accuracy binary         0.619 Preprocessor03_Model1\n#&gt;  6 validation     8       19 roc_auc  binary         0.652 Preprocessor03_Model1\n#&gt;  7 validation    14        7 accuracy binary         0.610 Preprocessor04_Model1\n#&gt;  8 validation    14        7 roc_auc  binary         0.650 Preprocessor04_Model1\n#&gt;  9 validation    16       17 accuracy binary         0.617 Preprocessor05_Model1\n#&gt; 10 validation    16       17 roc_auc  binary         0.649 Preprocessor05_Model1\n#&gt; # â€¦ with 38 more rows\n#&gt; # â„¹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choose-a-parameter-combination",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choose-a-parameter-combination",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nshow_best(glm_spline_res, metric = \"roc_auc\")\n#&gt; # A tibble: 5 Ã— 8\n#&gt;   angle distance .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1     5        8 roc_auc binary     0.653     1      NA Preprocessor12_Model1\n#&gt; 2     6        9 roc_auc binary     0.653     1      NA Preprocessor17_Model1\n#&gt; 3     3       15 roc_auc binary     0.653     1      NA Preprocessor13_Model1\n#&gt; 4     3       13 roc_auc binary     0.652     1      NA Preprocessor11_Model1\n#&gt; 5     5       19 roc_auc binary     0.652     1      NA Preprocessor24_Model1"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \nCreate your own tibble for final parameters or use one of the tune::select_*() functions:\n\nselect_best(glm_spline_res, metric = \"roc_auc\")\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   angle distance .config              \n#&gt;   &lt;int&gt;    &lt;int&gt; &lt;chr&gt;                \n#&gt; 1     5        8 Preprocessor12_Model1\n\n\nThis best result has:\n\nlow-degree spline for angle (less â€œwigglyâ€, less complex)\nhigher-degree spline for distance (more â€œwigglyâ€, more complex)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-2",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nTry an alternative selection strategy.\nRead the docs for select_by_pct_loss().\nTry choosing a model that has a simpler (less â€œwigglyâ€) relationship for distance.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choose-a-parameter-combination-2",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#choose-a-parameter-combination-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nselect_best(glm_spline_res, metric = \"roc_auc\")\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   angle distance .config              \n#&gt;   &lt;int&gt;    &lt;int&gt; &lt;chr&gt;                \n#&gt; 1     5        8 Preprocessor12_Model1\nselect_by_pct_loss(glm_spline_res, distance, metric = \"roc_auc\")\n#&gt; # A tibble: 1 Ã— 10\n#&gt;   angle distance .metric .estimator  mean     n std_err .config               .best .loss\n#&gt;   &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    13        4 roc_auc binary     0.646     1      NA Preprocessor20_Model1 0.653 0.984"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-trees-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-trees-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted trees ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ",
    "text": "Boosted trees ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\n\nEnsemble many decision tree models\n\n\nReview how a decision tree model works:\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#single-decision-tree",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#single-decision-tree",
    "title": "6 - Tuning Hyperparameters",
    "section": "Single decision tree",
    "text": "Single decision tree"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-trees-2",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-trees-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted trees ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ",
    "text": "Boosted trees ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\nBoosting methods fit a sequence of tree-based models.\n\n\nEach tree is dependent on the one before and tries to compensate for any poor results in the previous trees.\nThis is like gradient-based steepest ascent methods from calculus."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted tree tuning parameters",
    "text": "Boosted tree tuning parameters\nMost modern boosting methods have a lot of tuning parameters!\n\n\nFor tree growth and pruning (min_n, max_depth, etc)\nFor boosting (trees, stop_iter, learn_rate)\n\n\n\nWeâ€™ll use early stopping to stop boosting when a few iterations produce consecutively worse results."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#comparing-tree-ensembles",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#comparing-tree-ensembles",
    "title": "6 - Tuning Hyperparameters",
    "section": "Comparing tree ensembles",
    "text": "Comparing tree ensembles\n\n\nRandom forest\n\nIndependent trees\nBootstrapped data\nNo pruning\n1000â€™s of trees\n\n\nBoosting\n\nDependent trees\nDifferent case weights\nTune tree parameters\nFar fewer trees\n\n\nThe general consensus for tree-based models is, in terms of performance: boosting &gt; random forest &gt; bagging &gt; single trees."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-tree-code",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#boosted-tree-code",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted tree code",
    "text": "Boosted tree code\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = 500, min_n = tune(), stop_iter = tune(), tree_depth = tune(),\n    learn_rate = tune(), loss_reduction = tune()\n  ) %&gt;%\n  set_mode(\"classification\") %&gt;% \n  set_engine(\"xgboost\", validation = 1/10) # &lt;- for better early stopping\n\nxgb_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_lencode_mixed(player, outcome = vars(on_goal)) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nxgb_wflow &lt;- \n  workflow() %&gt;% \n  add_model(xgb_spec) %&gt;% \n  add_recipe(xgb_rec)\n\n\nvalidation is an argument to parsnip::xgb_train(), not directly to xgboost. It generates a validation set that is used by xgboost when evaluating model performance. It is eventually assigned to xgb.train(watchlist = list(validation = data)).\nSee translate(xgb_spec) to see where it is passed to parsnip::xgb_train()."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-3",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-3",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate your boosted tree workflow.\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#running-in-parallel",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#running-in-parallel",
    "title": "6 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\n\n\n\nGrid search, combined with resampling, requires fitting a lot of models!\nThese models donâ€™t depend on one another and can be run in parallel.\n\nWe can use a parallel backend to do this:\n\ncores &lt;- parallel::detectCores(logical = FALSE)\ncl &lt;- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#running-in-parallel-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#running-in-parallel-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\nFaceted on the expensiveness of preprocessing used."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning ",
    "text": "Tuning \nThis will take some time to run â³\n\nset.seed(9)\n\nxgb_res &lt;-\n  xgb_wflow %&gt;%\n  tune_grid(resamples = nhl_val, grid = 15, control = ctrl) # automatic grid now!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-4",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-4",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nStart tuning the boosted tree model!\nWe wonâ€™t wait for everyoneâ€™s tuning to finish, but take this time to get it started before we move on.\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results-2",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\nxgb_res\n#&gt; # Tuning results\n#&gt; # Validation Set Split (0.8/0.2)  \n#&gt; # A tibble: 1 Ã— 5\n#&gt;   splits              id         .metrics          .notes           .predictions          \n#&gt;   &lt;list&gt;              &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;           &lt;list&gt;                \n#&gt; 1 &lt;split [7288/1822]&gt; validation &lt;tibble [30 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [27,330 Ã— 11]&gt;"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results-3",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#tuning-results-3",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\nautoplot(xgb_res)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#again-with-the-location-features",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#again-with-the-location-features",
    "title": "6 - Tuning Hyperparameters",
    "section": "Again with the location features",
    "text": "Again with the location features\n\ncoord_rec &lt;- \n  xgb_rec %&gt;%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi)),\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),\n    distance = log(distance),\n    behind_goal_line = ifelse(abs(coord_x) &gt;= 89, 1, 0)\n  ) %&gt;% \n  step_rm(coord_x, coord_y)\n\nxgb_coord_wflow &lt;- \n  workflow() %&gt;% \n  add_model(xgb_spec) %&gt;% \n  add_recipe(coord_rec)\n\nset.seed(9)\nxgb_coord_res &lt;-\n  xgb_coord_wflow %&gt;%\n  tune_grid(resamples = nhl_val, grid = 20, control = ctrl)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#did-the-machine-figure-it-out",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#did-the-machine-figure-it-out",
    "title": "6 - Tuning Hyperparameters",
    "section": "Did the machine figure it out?",
    "text": "Did the machine figure it out?\n\nshow_best(xgb_res, metric = \"roc_auc\")\n#&gt; # A tibble: 5 Ã— 11\n#&gt;   min_n tree_depth learn_rate loss_reduction stop_iter .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1    19          2    0.311         1.46e- 4        14 roc_auc binary     0.633     1      NA Preprocessor1_Model12\n#&gt; 2    11          3    0.0255        5.61e- 7        19 roc_auc binary     0.628     1      NA Preprocessor1_Model05\n#&gt; 3    27          5    0.0120        6.04e- 6        12 roc_auc binary     0.627     1      NA Preprocessor1_Model07\n#&gt; 4    25         14    0.0379        3.62e- 5         8 roc_auc binary     0.626     1      NA Preprocessor1_Model13\n#&gt; 5    31         11    0.00585       1.02e-10         7 roc_auc binary     0.625     1      NA Preprocessor1_Model08\n\nshow_best(xgb_coord_res, metric = \"roc_auc\")\n#&gt; # A tibble: 5 Ã— 11\n#&gt;   min_n tree_depth learn_rate loss_reduction stop_iter .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1    30         13     0.0578  0.00000000141        18 roc_auc binary     0.648     1      NA Preprocessor1_Model07\n#&gt; 2    39         12     0.0803  0.000411             10 roc_auc binary     0.643     1      NA Preprocessor1_Model12\n#&gt; 3    14          2     0.146   0.00244              19 roc_auc binary     0.642     1      NA Preprocessor1_Model14\n#&gt; 4    26         15     0.0365  2.51                 17 roc_auc binary     0.642     1      NA Preprocessor1_Model11\n#&gt; 5    35          5     0.101   0.0000000784         13 roc_auc binary     0.641     1      NA Preprocessor1_Model17"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#compare-models",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#compare-models",
    "title": "6 - Tuning Hyperparameters",
    "section": "Compare models",
    "text": "Compare models\nBest logistic regression results:\n\nglm_spline_res %&gt;% \n  show_best(metric = \"roc_auc\", n = 1) %&gt;% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#&gt; # A tibble: 1 Ã— 6\n#&gt;   .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1 roc_auc binary     0.653     1      NA Preprocessor12_Model1\n\n\nBest boosting results:\n\nxgb_coord_res %&gt;% \n  show_best(metric = \"roc_auc\", n = 1) %&gt;% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#&gt; # A tibble: 1 Ã— 6\n#&gt;   .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1 roc_auc binary     0.648     1      NA Preprocessor1_Model07"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-5",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-5",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCan you get better ROC results with xgboost?\nTry increasing learn_rate beyond the original range.\n\n\n\n20:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#updating-the-workflow",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#updating-the-workflow",
    "title": "6 - Tuning Hyperparameters",
    "section": "Updating the workflow  ",
    "text": "Updating the workflow  \n\nbest_auc &lt;- select_best(glm_spline_res, metric = \"roc_auc\")\nbest_auc\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   angle distance .config              \n#&gt;   &lt;int&gt;    &lt;int&gt; &lt;chr&gt;                \n#&gt; 1     5        8 Preprocessor12_Model1\n\nglm_spline_wflow &lt;-\n  glm_spline_wflow %&gt;% \n  finalize_workflow(best_auc)\n\nglm_spline_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Recipe\n#&gt; Model: logistic_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; 8 Recipe Steps\n#&gt; \n#&gt; â€¢ step_lencode_mixed()\n#&gt; â€¢ step_dummy()\n#&gt; â€¢ step_mutate()\n#&gt; â€¢ step_rm()\n#&gt; â€¢ step_zv()\n#&gt; â€¢ step_ns()\n#&gt; â€¢ step_ns()\n#&gt; â€¢ step_normalize()\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#the-final-fit-to-the-nhl-data",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#the-final-fit-to-the-nhl-data",
    "title": "6 - Tuning Hyperparameters",
    "section": "The final fit to the NHL data  ",
    "text": "The final fit to the NHL data  \n\ntest_res &lt;- \n  glm_spline_wflow %&gt;% \n  last_fit(split = nhl_split)\n\ntest_res\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits              id               .metrics         .notes           .predictions         .workflow \n#&gt;   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;           &lt;list&gt;               &lt;list&gt;    \n#&gt; 1 &lt;split [9110/3037]&gt; train/test split &lt;tibble [2 Ã— 4]&gt; &lt;tibble [1 Ã— 3]&gt; &lt;tibble [3,037 Ã— 6]&gt; &lt;workflow&gt;\n#&gt; \n#&gt; There were issues with some computations:\n#&gt; \n#&gt;   - Warning(s) x1: prediction from a rank-deficient fit may be misleading\n#&gt; \n#&gt; Run `show_notes(.Last.tune.result)` for more information.\n\n\nRemember that last_fit() fits one time with the combined training and validation set, then evaluates one time with the testing set."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-6",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-6",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nFinalize your workflow with the best parameters.\nCreate a final fit.\n\n\n\n08:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#estimates-of-roc-auc",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#estimates-of-roc-auc",
    "title": "6 - Tuning Hyperparameters",
    "section": "Estimates of ROC AUC ",
    "text": "Estimates of ROC AUC \nValidation results from tuning:\n\nglm_spline_res %&gt;% \n  show_best(metric = \"roc_auc\", n = 1) %&gt;% \n  select(.metric, mean, n, std_err)\n#&gt; # A tibble: 1 Ã— 4\n#&gt;   .metric  mean     n std_err\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1 roc_auc 0.653     1      NA\n\n\nTest set results:\n\ntest_res %&gt;% collect_metrics()\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric  .estimator .estimate .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary         0.616 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary         0.656 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#final-fitted-workflow",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#final-fitted-workflow",
    "title": "6 - Tuning Hyperparameters",
    "section": "Final fitted workflow",
    "text": "Final fitted workflow\nExtract the final fitted workflow, fit using the training set:\n\nfinal_glm_spline_wflow &lt;- \n  test_res %&gt;% \n  extract_workflow()\n\n# use this object to predict or deploy\npredict(final_glm_spline_wflow, nhl_test[1:3,])\n#&gt; # A tibble: 3 Ã— 1\n#&gt;   .pred_class\n#&gt;   &lt;fct&gt;      \n#&gt; 1 no         \n#&gt; 2 yes        \n#&gt; 3 no"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#next-steps",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#next-steps",
    "title": "6 - Tuning Hyperparameters",
    "section": "Next steps",
    "text": "Next steps\n\nDocument the model.\n\n\n\nDeploy the model.\n\n\n\n\nCreate an applicability domain model to help monitor our data over time.\n\n\n\n\nUse explainers to characterize the model and the predictions."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-yourself",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-yourself",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain yourself",
    "text": "Explain yourself\nThere are two categories of model explanations, global and local.\n\n\nGlobal model explanations provide an overall understanding aggregated over a whole set of observations.\nLocal model explanations provide information about a prediction for a single observation.\n\n\n\nYou can also build global model explanations by aggregating local model explanations."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#a-tidymodels-explainer",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#a-tidymodels-explainer",
    "title": "6 - Tuning Hyperparameters",
    "section": "A tidymodels explainer",
    "text": "A tidymodels explainer\nWe can build explainers using:\n\noriginal, basic predictors\nderived features\n\n\nlibrary(DALEXtra)\n\nglm_explainer &lt;- explain_tidymodels(\n  final_glm_spline_wflow,\n  data = dplyr::select(nhl_train, -on_goal),\n  # DALEX required an integer for factors:\n  y = as.integer(nhl_train$on_goal),\n  verbose = FALSE\n)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-the-x-coordinates",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-the-x-coordinates",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain the x coordinates",
    "text": "Explain the x coordinates\nWith our explainer, letâ€™s create partial dependence profiles:\n\nset.seed(123)\npdp_coord_x &lt;- model_profile(\n  glm_explainer,\n  variables = \"coord_x\",\n  N = 500,\n  groups = \"position\"\n)\n\n\nYou can use the default plot() method or create your own visualization."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-the-x-coordinates-1",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-the-x-coordinates-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain the x coordinates",
    "text": "Explain the x coordinates"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-the-x-coordinates-2",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#explain-the-x-coordinates-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain the x coordinates",
    "text": "Explain the x coordinates"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-7",
    "href": "archive/2022-07-RStudio-conf/06-tuning-hyperparameters.html#your-turn-7",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate an explainer for our glm model.\nTry grouping by another variable, like game_type or dow.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\naugment(tree_fit, new_data = frog_test) %&gt;%\n  metrics(latency, .pred)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      59.2  \n#&gt; 2 rsq     standard       0.380\n#&gt; 3 mae     standard      40.2\n\n\n\nRMSE: difference between the predicted and observed values â¬‡ï¸\n\\(R^2\\): squared correlation between the predicted and observed values â¬†ï¸\nMAE: similar to RMSE, but mean absolute error â¬‡ï¸"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\naugment(tree_fit, new_data = frog_test) %&gt;%\n  rmse(latency, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard        59.2"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\naugment(tree_fit, new_data = frog_test) %&gt;%\n  group_by(reflex) %&gt;%\n  rmse(latency, .pred)\n#&gt; # A tibble: 3 Ã— 4\n#&gt;   reflex .metric .estimator .estimate\n#&gt;   &lt;fct&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 low    rmse    standard        94.3\n#&gt; 2 mid    rmse    standard       101. \n#&gt; 3 full   rmse    standard        51.2"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#metrics-for-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\nfrog_metrics &lt;- metric_set(rmse, msd)\naugment(tree_fit, new_data = frog_test) %&gt;%\n  frog_metrics(latency, .pred)\n#&gt; # A tibble: 2 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      59.2  \n#&gt; 2 msd     standard      -0.908"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-1",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸",
    "text": "Dangers of overfitting âš ï¸"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-2",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸",
    "text": "Dangers of overfitting âš ï¸"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-3",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\ntree_fit %&gt;%\n  augment(frog_train)\n#&gt; # A tibble: 456 Ã— 6\n#&gt;    treatment  reflex   age t_o_d     latency .pred\n#&gt;    &lt;chr&gt;      &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 control    full    5.42 morning        33  39.8\n#&gt;  2 control    full    5.38 morning        19  66.7\n#&gt;  3 control    full    5.38 morning         2  66.7\n#&gt;  4 control    full    5.44 morning        39  39.8\n#&gt;  5 control    full    5.41 morning        42  39.8\n#&gt;  6 control    full    4.75 afternoon      20  59.8\n#&gt;  7 control    full    4.95 night          31  83.1\n#&gt;  8 control    full    5.42 morning        21  39.8\n#&gt;  9 gentamicin full    5.39 morning        30  64.6\n#&gt; 10 control    full    4.55 afternoon      43 174. \n#&gt; # â€¦ with 446 more rows\n#&gt; # â„¹ Use `print(n = ...)` to see more rows\n\nWe call this â€œresubstitutionâ€ or â€œrepredicting the training setâ€"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-4",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-4",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\ntree_fit %&gt;%\n  augment(frog_train) %&gt;%\n  rmse(latency, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard        49.4\n\nWe call this a â€œresubstitution estimateâ€"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-5",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-5",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\ntree_fit %&gt;%\n  augment(frog_train) %&gt;%\n  rmse(latency, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard        49.4"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-6",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-6",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\ntree_fit %&gt;%\n  augment(frog_train) %&gt;%\n  rmse(latency, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard        49.4\n\n\n\ntree_fit %&gt;%\n  augment(frog_test) %&gt;%\n  rmse(latency, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard        59.2\n\n\n\nâš ï¸ Remember that weâ€™re demonstrating overfitting\n\n\nâš ï¸ Donâ€™t use the test set until the end of your modeling analysis"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse augment() and metrics() to compute a regression metric like mae().\nCompute the metrics for both training and testing data.\nNotice the evidence of overfitting! âš ï¸\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-7",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#dangers-of-overfitting-7",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\ntree_fit %&gt;%\n  augment(frog_train) %&gt;%\n  metrics(latency, .pred)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      49.4  \n#&gt; 2 rsq     standard       0.494\n#&gt; 3 mae     standard      33.4\n\n\n\ntree_fit %&gt;%\n  augment(frog_test) %&gt;%\n  metrics(latency, .pred)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      59.2  \n#&gt; 2 rsq     standard       0.380\n#&gt; 3 mae     standard      40.2\n\n\n\nWhat if we want to compare more models?\n\n\nAnd/or more model configurations?\n\n\nAnd we want to understand if these are important differences?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-1",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-1",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nIf we use 10 folds, what percent of the training data\n\nends up in analysis\nends up in assessment\n\nfor each fold?\n\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-2",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(frog_train) # v = 10 is default\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits           id    \n#&gt;    &lt;list&gt;           &lt;chr&gt; \n#&gt;  1 &lt;split [410/46]&gt; Fold01\n#&gt;  2 &lt;split [410/46]&gt; Fold02\n#&gt;  3 &lt;split [410/46]&gt; Fold03\n#&gt;  4 &lt;split [410/46]&gt; Fold04\n#&gt;  5 &lt;split [410/46]&gt; Fold05\n#&gt;  6 &lt;split [410/46]&gt; Fold06\n#&gt;  7 &lt;split [411/45]&gt; Fold07\n#&gt;  8 &lt;split [411/45]&gt; Fold08\n#&gt;  9 &lt;split [411/45]&gt; Fold09\n#&gt; 10 &lt;split [411/45]&gt; Fold10"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-3",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWhat is in this?\n\nfrog_folds &lt;- vfold_cv(frog_train)\nfrog_folds$splits[1:3]\n#&gt; [[1]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;410/46/456&gt;\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;410/46/456&gt;\n#&gt; \n#&gt; [[3]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;410/46/456&gt;\n\n\nTalk about a list column, storing non-atomic types in dataframe"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-4",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-4",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(frog_train, v = 5)\n#&gt; #  5-fold cross-validation \n#&gt; # A tibble: 5 Ã— 2\n#&gt;   splits           id   \n#&gt;   &lt;list&gt;           &lt;chr&gt;\n#&gt; 1 &lt;split [364/92]&gt; Fold1\n#&gt; 2 &lt;split [365/91]&gt; Fold2\n#&gt; 3 &lt;split [365/91]&gt; Fold3\n#&gt; 4 &lt;split [365/91]&gt; Fold4\n#&gt; 5 &lt;split [365/91]&gt; Fold5"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-5",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-5",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(frog_train, strata = latency)\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits           id    \n#&gt;    &lt;list&gt;           &lt;chr&gt; \n#&gt;  1 &lt;split [408/48]&gt; Fold01\n#&gt;  2 &lt;split [408/48]&gt; Fold02\n#&gt;  3 &lt;split [408/48]&gt; Fold03\n#&gt;  4 &lt;split [409/47]&gt; Fold04\n#&gt;  5 &lt;split [411/45]&gt; Fold05\n#&gt;  6 &lt;split [412/44]&gt; Fold06\n#&gt;  7 &lt;split [412/44]&gt; Fold07\n#&gt;  8 &lt;split [412/44]&gt; Fold08\n#&gt;  9 &lt;split [412/44]&gt; Fold09\n#&gt; 10 &lt;split [412/44]&gt; Fold10\n\n\nStratification often helps, with very little downside"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-6",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#cross-validation-6",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWeâ€™ll use this setup:\n\nset.seed(123)\nfrog_folds &lt;- vfold_cv(frog_train, v = 10, strata = latency)\nfrog_folds\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits           id    \n#&gt;    &lt;list&gt;           &lt;chr&gt; \n#&gt;  1 &lt;split [408/48]&gt; Fold01\n#&gt;  2 &lt;split [408/48]&gt; Fold02\n#&gt;  3 &lt;split [408/48]&gt; Fold03\n#&gt;  4 &lt;split [409/47]&gt; Fold04\n#&gt;  5 &lt;split [411/45]&gt; Fold05\n#&gt;  6 &lt;split [412/44]&gt; Fold06\n#&gt;  7 &lt;split [412/44]&gt; Fold07\n#&gt;  8 &lt;split [412/44]&gt; Fold08\n#&gt;  9 &lt;split [412/44]&gt; Fold09\n#&gt; 10 &lt;split [412/44]&gt; Fold10\n\n\nSet the seed when creating resamples"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#fit-our-model-to-the-resamples",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#fit-our-model-to-the-resamples",
    "title": "4 - Evaluating models",
    "section": "Fit our model to the resamples",
    "text": "Fit our model to the resamples\n\ntree_res &lt;- fit_resamples(tree_wflow, frog_folds)\ntree_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 4\n#&gt;    splits           id     .metrics         .notes          \n#&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n#&gt;  1 &lt;split [408/48]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  2 &lt;split [408/48]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  3 &lt;split [408/48]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  4 &lt;split [409/47]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  5 &lt;split [411/45]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  6 &lt;split [412/44]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  7 &lt;split [412/44]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  8 &lt;split [412/44]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  9 &lt;split [412/44]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt; 10 &lt;split [412/44]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluating-model-performance",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluating-model-performance",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\ntree_res %&gt;%\n  collect_metrics()\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard   59.6      10  2.31   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.305    10  0.0342 Preprocessor1_Model1\n\n\nWe can reliably measure performance using only the training data ğŸ‰"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#comparing-metrics",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#comparing-metrics",
    "title": "4 - Evaluating models",
    "section": "Comparing metrics ",
    "text": "Comparing metrics \nHow do the metrics from resampling compare to the metrics from training and testing?\n\n\n\ntree_res %&gt;%\n  collect_metrics() %&gt;% \n  select(.metric, mean, n)\n#&gt; # A tibble: 2 Ã— 3\n#&gt;   .metric   mean     n\n#&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 rmse    59.6      10\n#&gt; 2 rsq      0.305    10\n\n\nThe RMSE previously was\n\n49.36 for the training set\n59.16 for test set\n\n\n\nRemember that:\nâš ï¸ the training set gives you overly optimistic metrics\nâš ï¸ the test set is precious"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluating-model-performance-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluating-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nctrl_frog &lt;- control_resamples(save_pred = TRUE)\ntree_res &lt;- fit_resamples(tree_wflow, frog_folds, control = ctrl_frog)\n\ntree_preds &lt;- collect_predictions(tree_res)\ntree_preds\n#&gt; # A tibble: 456 Ã— 5\n#&gt;    id     .pred  .row latency .config             \n#&gt;    &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt;  1 Fold01  39.6     1      33 Preprocessor1_Model1\n#&gt;  2 Fold01  72.1     3       2 Preprocessor1_Model1\n#&gt;  3 Fold01  63.8     9      30 Preprocessor1_Model1\n#&gt;  4 Fold01  72.1    13      46 Preprocessor1_Model1\n#&gt;  5 Fold01  43.3    28      11 Preprocessor1_Model1\n#&gt;  6 Fold01  61.7    35      41 Preprocessor1_Model1\n#&gt;  7 Fold01  39.6    51      43 Preprocessor1_Model1\n#&gt;  8 Fold01 134.     70      20 Preprocessor1_Model1\n#&gt;  9 Fold01  70.6    74      21 Preprocessor1_Model1\n#&gt; 10 Fold01  39.6   106      14 Preprocessor1_Model1\n#&gt; # â€¦ with 446 more rows\n#&gt; # â„¹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#section-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#section-3",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "tree_preds %&gt;% \n  ggplot(aes(latency, .pred, color = id)) + \n  geom_abline(lty = 2, col = \"gray\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#where-are-the-fitted-models",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#where-are-the-fitted-models",
    "title": "4 - Evaluating models",
    "section": "Where are the fitted models? ",
    "text": "Where are the fitted models? \n\ntree_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits           id     .metrics         .notes           .predictions     \n#&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;           \n#&gt;  1 &lt;split [408/48]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [48 Ã— 4]&gt;\n#&gt;  2 &lt;split [408/48]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [48 Ã— 4]&gt;\n#&gt;  3 &lt;split [408/48]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [48 Ã— 4]&gt;\n#&gt;  4 &lt;split [409/47]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [47 Ã— 4]&gt;\n#&gt;  5 &lt;split [411/45]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [45 Ã— 4]&gt;\n#&gt;  6 &lt;split [412/44]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt;  7 &lt;split [412/44]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt;  8 &lt;split [412/44]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt;  9 &lt;split [412/44]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt; 10 &lt;split [412/44]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n\n\nğŸ—‘ï¸"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#bootstrapping",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#bootstrapping",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#bootstrapping-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#bootstrapping-1",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(3214)\nbootstraps(frog_train)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 25 Ã— 2\n#&gt;    splits            id         \n#&gt;    &lt;list&gt;            &lt;chr&gt;      \n#&gt;  1 &lt;split [456/163]&gt; Bootstrap01\n#&gt;  2 &lt;split [456/166]&gt; Bootstrap02\n#&gt;  3 &lt;split [456/173]&gt; Bootstrap03\n#&gt;  4 &lt;split [456/177]&gt; Bootstrap04\n#&gt;  5 &lt;split [456/166]&gt; Bootstrap05\n#&gt;  6 &lt;split [456/163]&gt; Bootstrap06\n#&gt;  7 &lt;split [456/164]&gt; Bootstrap07\n#&gt;  8 &lt;split [456/165]&gt; Bootstrap08\n#&gt;  9 &lt;split [456/170]&gt; Bootstrap09\n#&gt; 10 &lt;split [456/177]&gt; Bootstrap10\n#&gt; # â€¦ with 15 more rows\n#&gt; # â„¹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-2",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCreate:\n\nbootstrap folds (change times from the default)\nvalidation set (use the reference guide to find the function)\n\nDonâ€™t forget to set a seed when you resample!\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#bootstrapping-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#bootstrapping-2",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(322)\nbootstraps(frog_train, times = 10)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits            id         \n#&gt;    &lt;list&gt;            &lt;chr&gt;      \n#&gt;  1 &lt;split [456/173]&gt; Bootstrap01\n#&gt;  2 &lt;split [456/168]&gt; Bootstrap02\n#&gt;  3 &lt;split [456/170]&gt; Bootstrap03\n#&gt;  4 &lt;split [456/164]&gt; Bootstrap04\n#&gt;  5 &lt;split [456/176]&gt; Bootstrap05\n#&gt;  6 &lt;split [456/156]&gt; Bootstrap06\n#&gt;  7 &lt;split [456/166]&gt; Bootstrap07\n#&gt;  8 &lt;split [456/168]&gt; Bootstrap08\n#&gt;  9 &lt;split [456/167]&gt; Bootstrap09\n#&gt; 10 &lt;split [456/170]&gt; Bootstrap10"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#validation-set",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#validation-set",
    "title": "4 - Evaluating models",
    "section": "Validation set ",
    "text": "Validation set \n\nset.seed(853)\nvalidation_split(frog_train, strata = latency)\n#&gt; # Validation Set Split (0.75/0.25)  using stratification \n#&gt; # A tibble: 1 Ã— 2\n#&gt;   splits            id        \n#&gt;   &lt;list&gt;            &lt;chr&gt;     \n#&gt; 1 &lt;split [340/116]&gt; validation\n\n\nA validation set is just another type of resample"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#random-forest-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#random-forest-1",
    "title": "4 - Evaluating models",
    "section": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ",
    "text": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\n\nEnsemble many decision tree models\nAll the trees vote! ğŸ—³ï¸\nBootstrap aggregating + random predictor sampling\n\n\n\nOften works well without tuning hyperparameters (more on this tomorrow!), as long as there are enough trees"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#create-a-random-forest-model",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#create-a-random-forest-model",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_spec &lt;- rand_forest(trees = 1000, mode = \"regression\")\nrf_spec\n#&gt; Random Forest Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#create-a-random-forest-model-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#create-a-random-forest-model-1",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_wflow &lt;- workflow(latency ~ ., rf_spec)\nrf_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; latency ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-3",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() and rf_wflow to:\n\nkeep predictions\ncompute metrics\nplot true vs.Â predicted values\n\n\n\n\n08:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluating-model-performance-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluating-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nctrl_frog &lt;- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\n\nset.seed(2)\nrf_res &lt;- fit_resamples(rf_wflow, frog_folds, control = ctrl_frog)\ncollect_metrics(rf_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard   55.9      10  1.71   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.370    10  0.0306 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#section-5",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#section-5",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "collect_predictions(rf_res) %&gt;% \n  ggplot(aes(latency, .pred, color = id)) + \n  geom_abline(lty = 2, col = \"gray\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#how-can-we-compare-multiple-model-workflows-at-once",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#how-can-we-compare-multiple-model-workflows-at-once",
    "title": "4 - Evaluating models",
    "section": "How can we compare multiple model workflows at once?",
    "text": "How can we compare multiple model workflows at once?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluate-a-workflow-set",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluate-a-workflow-set",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec))\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result    \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluate-a-workflow-set-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluate-a-workflow-set-1",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec)) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = frog_folds)\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result   \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluate-a-workflow-set-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#evaluate-a-workflow-set-2",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec)) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = frog_folds) %&gt;%\n  rank_results()\n#&gt; # A tibble: 4 Ã— 9\n#&gt;   wflow_id              .config .metric   mean std_err     n preprâ€¦Â¹ model  rank\n#&gt;   &lt;chr&gt;                 &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 formula_rand_forest   Preproâ€¦ rmse    55.8    1.71      10 formula randâ€¦     1\n#&gt; 2 formula_rand_forest   Preproâ€¦ rsq      0.371  0.0301    10 formula randâ€¦     1\n#&gt; 3 formula_decision_tree Preproâ€¦ rmse    59.6    2.31      10 formula deciâ€¦     2\n#&gt; 4 formula_decision_tree Preproâ€¦ rsq      0.305  0.0342    10 formula deciâ€¦     2\n#&gt; # â€¦ with abbreviated variable name Â¹â€‹preprocessor\n\nThe first metric of the metric set is used for ranking. Use rank_metric to change that.\n\nLots more available with workflow sets, like collect_metrics(), autoplot() methods, and more!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-4",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-4",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nWhen do you think a workflow set would be useful?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#the-final-fit",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#the-final-fit",
    "title": "4 - Evaluating models",
    "section": "The final fit ",
    "text": "The final fit \nSuppose that we are happy with our random forest model.\nLetâ€™s fit the model on the training set and verify our performance using the test set.\n\nWeâ€™ve shown you fit() and predict() (+ augment()) but there is a shortcut:\n\n# frog_split has train + test info\nfinal_fit &lt;- last_fit(rf_wflow, frog_split) \n\nfinal_fit\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits            id               .metrics .notes   .predictions .workflow \n#&gt;   &lt;list&gt;            &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n#&gt; 1 &lt;split [456/116]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#what-is-in-final_fit",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#what-is-in-final_fit",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_metrics(final_fit)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard      57.1   Preprocessor1_Model1\n#&gt; 2 rsq     standard       0.420 Preprocessor1_Model1\n\n\nThese are metrics computed with the test set"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#what-is-in-final_fit-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#what-is-in-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_predictions(final_fit)\n#&gt; # A tibble: 116 Ã— 5\n#&gt;    id               .pred  .row latency .config             \n#&gt;    &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt;  1 train/test split  43.5     1      22 Preprocessor1_Model1\n#&gt;  2 train/test split 104.      3     106 Preprocessor1_Model1\n#&gt;  3 train/test split  76.2     6      39 Preprocessor1_Model1\n#&gt;  4 train/test split  42.4     8      50 Preprocessor1_Model1\n#&gt;  5 train/test split  43.5    10      63 Preprocessor1_Model1\n#&gt;  6 train/test split  43.1    14      25 Preprocessor1_Model1\n#&gt;  7 train/test split  51.5    16      48 Preprocessor1_Model1\n#&gt;  8 train/test split 160.     17      91 Preprocessor1_Model1\n#&gt;  9 train/test split  50.9    32      11 Preprocessor1_Model1\n#&gt; 10 train/test split 171.     33     109 Preprocessor1_Model1\n#&gt; # â€¦ with 106 more rows\n#&gt; # â„¹ Use `print(n = ...)` to see more rows\n\n\nThese are predictions for the test set"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#section-6",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#section-6",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "collect_predictions(final_fit) %&gt;%\n  ggplot(aes(latency, .pred)) + \n  geom_abline(lty = 2, col = \"deeppink4\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#what-is-in-final_fit-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#what-is-in-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\nextract_workflow(final_fit)\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; latency ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n#&gt; \n#&gt; Type:                             Regression \n#&gt; Number of trees:                  1000 \n#&gt; Sample size:                      456 \n#&gt; Number of independent variables:  4 \n#&gt; Mtry:                             2 \n#&gt; Target node size:                 5 \n#&gt; Variable importance mode:         none \n#&gt; Splitrule:                        variance \n#&gt; OOB prediction error (MSE):       3124.583 \n#&gt; R squared (OOB):                  0.3531813\n\n\nUse this for prediction on new data, like for deploying"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-5",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#your-turn-5",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nEnd of the day discussion!\nWhich model do you think you would decide to use?\nWhat surprised you the most?\nWhat is one thing you are looking forward to for tomorrow?\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit? \nModel stacks generate predictions that are informed by several models."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-3",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-4",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-4",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-5",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#why-choose-just-one-final_fit-5",
    "title": "4 - Evaluating models",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlibrary(stacks)\n\n\nDefine candidate members\nInitialize a data stack object\nIteratively add candidate ensemble members to the data stack\nEvaluate how to combine their predictions\nFit candidate ensemble members with non-zero stacking coefficients\nPredict on new data!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-1",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-1",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nstack_ctrl &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-2",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-2",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nDefine candidate members\n\nStart out with a linear regression:\n\nlr_res &lt;- \n  # define model spec\n  linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  # add to workflow\n  workflow(preprocessor = latency ~ .) %&gt;%\n  # fit to resamples\n  fit_resamples(frog_folds, control = stack_ctrl)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-3",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-3",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlr_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits           id     .metrics         .notes           .predictions     \n#&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;           \n#&gt;  1 &lt;split [408/48]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [48 Ã— 4]&gt;\n#&gt;  2 &lt;split [408/48]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [48 Ã— 4]&gt;\n#&gt;  3 &lt;split [408/48]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [48 Ã— 4]&gt;\n#&gt;  4 &lt;split [409/47]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [47 Ã— 4]&gt;\n#&gt;  5 &lt;split [411/45]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [45 Ã— 4]&gt;\n#&gt;  6 &lt;split [412/44]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt;  7 &lt;split [412/44]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt;  8 &lt;split [412/44]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt;  9 &lt;split [412/44]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt; 10 &lt;split [412/44]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-4",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-4",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \nThen, a random forest:\n\nrf_res &lt;- \n  # define model spec\n  rand_forest() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  # add to workflow\n  workflow(preprocessor = latency ~ .) %&gt;%\n  # fit to resamples\n  fit_resamples(frog_folds, control = stack_ctrl)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-5",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-5",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nrf_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits           id     .metrics         .notes           .predictions     \n#&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;           \n#&gt;  1 &lt;split [408/48]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [48 Ã— 4]&gt;\n#&gt;  2 &lt;split [408/48]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [48 Ã— 4]&gt;\n#&gt;  3 &lt;split [408/48]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [48 Ã— 4]&gt;\n#&gt;  4 &lt;split [409/47]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [47 Ã— 4]&gt;\n#&gt;  5 &lt;split [411/45]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [45 Ã— 4]&gt;\n#&gt;  6 &lt;split [412/44]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt;  7 &lt;split [412/44]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt;  8 &lt;split [412/44]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt;  9 &lt;split [412/44]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt; 10 &lt;split [412/44]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-6",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-6",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nInitialize a data stack object\n\n\n\nfrog_st &lt;- stacks()\n\nfrog_st\n#&gt; # A data stack with 0 model definitions and 0 candidate members."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-7",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-7",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nIteratively add candidate ensemble members to the data stack\n\n\nfrog_st &lt;- frog_st %&gt;%\n  add_candidates(lr_res) %&gt;%\n  add_candidates(rf_res)\n\nfrog_st\n#&gt; # A data stack with 2 model definitions and 2 candidate members:\n#&gt; #   lr_res: 1 model configuration\n#&gt; #   rf_res: 1 model configuration\n#&gt; # Outcome: latency (numeric)\n\nTomorrow weâ€™ll discuss tuning parameters where there are different configurations of models (e.g.Â 10 different variations of the random forest model).\nThese configurations can greatly improve the performance of the stacking ensemble."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-8",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-8",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nEvaluate how to combine their predictions\n\n\nfrog_st_res &lt;- frog_st %&gt;%\n  blend_predictions()\n\nfrog_st_res\n#&gt; # A tibble: 2 Ã— 3\n#&gt;   member     type        weight\n#&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;\n#&gt; 1 rf_res_1_1 rand_forest  0.635\n#&gt; 2 lr_res_1_1 linear_reg   0.344"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-9",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-9",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nFit candidate ensemble members with non-zero stacking coefficients\n\n\nfrog_st_res &lt;- frog_st_res %&gt;%\n  fit_members()\n\nfrog_st_res\n#&gt; # A tibble: 2 Ã— 3\n#&gt;   member     type        weight\n#&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;\n#&gt; 1 rf_res_1_1 rand_forest  0.635\n#&gt; 2 lr_res_1_1 linear_reg   0.344"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-10",
    "href": "archive/2022-07-RStudio-conf/04-evaluating-models.html#building-a-model-stack-10",
    "title": "4 - Evaluating models",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nPredict on new data!\n\n\n\nfrog_test %&gt;%\n  select(latency) %&gt;%\n  bind_cols(\n    predict(frog_st_res, frog_test)\n  ) %&gt;%\n  ggplot(aes(latency, .pred)) + \n  geom_abline(lty = 2, \n              col = \"deeppink4\", \n              size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching-1",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching-1",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\nRed-eyed tree frog embryos can hatch earlier than their normal ~7 days if they detect potential predator threat!\nType ?stacks::tree_frogs to learn more about this dataset, including references.\nWe are using a slightly modified version from stacks.\n\n\nlibrary(tidymodels)\n\ndata(\"tree_frogs\", package = \"stacks\")\ntree_frogs &lt;- tree_frogs %&gt;%\n  mutate(t_o_d = factor(t_o_d),\n         age = age / 86400) %&gt;%\n  filter(!is.na(latency)) %&gt;%\n  select(-c(clutch, hatched))"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching-2",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching-2",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\n\n\nN = 572\nA numeric outcome, latency\n4 other variables\n\ntreatment, reflex, and t_o_d are nominal predictors\nage is a numeric predictor\n\n\n\n\n\n\n\nlatency: How long it took the frog to hatch after being stimulated - i.e.Â after being poked by a blunt probe (in seconds).\ntreatment: Whether or not they got gentamicin, a compound that knocks out the embryoâ€™s lateral line (a sensory organ).\nreflex: A measure of ear function (low, mid, full)\nt_o_d: Time that the stimulus was applied (morning, afternoon, night)\nage: Age at the time it was stimulated (in days)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching-3",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-on-tree-frog-hatching-3",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\ntree_frogs\n#&gt; # A tibble: 572 Ã— 5\n#&gt;    treatment  reflex   age t_o_d     latency\n#&gt;    &lt;chr&gt;      &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;\n#&gt;  1 control    full    5.40 morning        22\n#&gt;  2 control    low     4.18 night         360\n#&gt;  3 control    full    4.65 afternoon     106\n#&gt;  4 control    mid     4.14 night         180\n#&gt;  5 control    full    4.6  afternoon      60\n#&gt;  6 gentamicin full    5.36 morning        39\n#&gt;  7 control    full    4.56 afternoon     214\n#&gt;  8 control    full    5.43 morning        50\n#&gt;  9 control    full    4.63 afternoon     224\n#&gt; 10 control    full    5.40 morning        63\n#&gt; # â€¦ with 562 more rows\n#&gt; # â„¹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nFor machine learning, we typically split data into training and test sets:\n\n\nThe training set is used to estimate model parameters.\nThe test set is used to find an independent assessment of model performance.\n\n\n\nDo not ğŸš« use the test set during training."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-1",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-1",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-2",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-2",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\n\nSpending too much data in training prevents us from computing a good assessment of predictive performance.\n\n\n\nSpending too much data in testing prevents us from computing a good estimate of model parameters."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#your-turn",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nWhen is a good time to split your data?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-3",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-3",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nfrog_split &lt;- initial_split(tree_frogs)\nfrog_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;429/143/572&gt;\n\n\nHow much data in training vs testing? This function uses a good default, but this depends on your specific goal/data We will talk about more powerful ways of splitting, like stratification, later"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#accessing-the-data",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#accessing-the-data",
    "title": "2 - Your data budget",
    "section": "Accessing the data ",
    "text": "Accessing the data \n\nfrog_train &lt;- training(frog_split)\nfrog_test &lt;- testing(frog_split)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#the-training-set",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#the-training-set",
    "title": "2 - Your data budget",
    "section": "The training set",
    "text": "The training set\n\nfrog_train\n#&gt; # A tibble: 429 Ã— 5\n#&gt;    treatment  reflex   age t_o_d     latency\n#&gt;    &lt;chr&gt;      &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;\n#&gt;  1 control    full    5.36 morning        36\n#&gt;  2 gentamicin full    5.37 morning        72\n#&gt;  3 gentamicin full    4.65 afternoon     141\n#&gt;  4 control    full    5.42 morning        27\n#&gt;  5 control    full    5.43 morning        27\n#&gt;  6 gentamicin full    5.38 morning        73\n#&gt;  7 gentamicin full    5.42 morning        68\n#&gt;  8 gentamicin full    4.75 afternoon     124\n#&gt;  9 control    full    5.00 night          62\n#&gt; 10 control    full    5.39 morning        25\n#&gt; # â€¦ with 419 more rows\n#&gt; # â„¹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#the-test-set",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#the-test-set",
    "title": "2 - Your data budget",
    "section": "The test set ",
    "text": "The test set \n\nfrog_test\n#&gt; # A tibble: 143 Ã— 5\n#&gt;    treatment  reflex   age t_o_d     latency\n#&gt;    &lt;chr&gt;      &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;\n#&gt;  1 control    full    5.40 morning        22\n#&gt;  2 control    low     4.18 night         360\n#&gt;  3 control    full    4.63 afternoon     224\n#&gt;  4 gentamicin full    4.75 afternoon     158\n#&gt;  5 control    mid     4.22 night          91\n#&gt;  6 gentamicin full    4.89 night         301\n#&gt;  7 control    full    5.38 morning         2\n#&gt;  8 control    full    4.80 afternoon      56\n#&gt;  9 control    full    5.36 morning        11\n#&gt; 10 control    full    5.40 morning        64\n#&gt; # â€¦ with 133 more rows\n#&gt; # â„¹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#your-turn-1",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#your-turn-1",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nSplit your data so 20% is held out for the test set.\nTry out different values in set.seed() to see how the results change.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-4",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#data-splitting-and-spending-4",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nfrog_split &lt;- initial_split(tree_frogs, prop = 0.8)\nfrog_train &lt;- training(frog_split)\nfrog_test &lt;- testing(frog_split)\n\nnrow(frog_train)\n#&gt; [1] 457\nnrow(frog_test)\n#&gt; [1] 115"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#section-1",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#section-1",
    "title": "2 - Your data budget",
    "section": "",
    "text": "We will use this tomorrow"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#your-turn-2",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#your-turn-2",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nExplore the frog_train data on your own!\n\nWhatâ€™s the distribution of the outcome, latency?\nWhatâ€™s the distribution of numeric variables like age?\nHow does latency differ across the categorical variables?\n\n\n\n\n08:00\n\n\n\n\nMake a plot or summary and then share with neighbor"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#section-3",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#section-3",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(latency)) +\n  geom_histogram(bins = 20)\n\n\n\nThis histogram brings up a concern. What if in our training set we get unlucky and sample few or none of these large values? That could mean that our model wouldnâ€™t be able to predict such values. Letâ€™s come back to that!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#section-4",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#section-4",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(latency, treatment, fill = treatment)) +\n  geom_boxplot(alpha = 0.5, show.legend = FALSE)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#section-5",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#section-5",
    "title": "2 - Your data budget",
    "section": "",
    "text": "frog_train %&gt;%\n  ggplot(aes(latency, reflex, fill = reflex)) +\n  geom_boxplot(alpha = 0.3, show.legend = FALSE)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#section-6",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#section-6",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(age, latency, color = reflex)) +\n  geom_point(alpha = .8, size = 2)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#section-7",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#section-7",
    "title": "2 - Your data budget",
    "section": "",
    "text": "Stratified sampling would split within each quartile\n\nBased on our exploration, we realized that stratifying by latency might help get a consistent distribution. For instance, weâ€™d include high and low latency in both the test and training"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/02-data-budget.html#stratification",
    "href": "archive/2022-07-RStudio-conf/02-data-budget.html#stratification",
    "title": "2 - Your data budget",
    "section": "Stratification",
    "text": "Stratification\nUse strata = latency\n\nset.seed(123)\nfrog_split &lt;- initial_split(tree_frogs, prop = 0.8, strata = latency)\nfrog_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;456/116/572&gt;\n\n\nStratification often helps, with very little downside"
  },
  {
    "objectID": "slides/intro-extra-workflowsets.html#how-can-we-compare-multiple-model-workflows-at-once",
    "href": "slides/intro-extra-workflowsets.html#how-can-we-compare-multiple-model-workflows-at-once",
    "title": "Extras - workflowsets",
    "section": "How can we compare multiple model workflows at once?",
    "text": "How can we compare multiple model workflows at once?"
  },
  {
    "objectID": "slides/intro-extra-workflowsets.html#evaluate-a-workflow-set",
    "href": "slides/intro-extra-workflowsets.html#evaluate-a-workflow-set",
    "title": "Extras - workflowsets",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nwf_set &lt;- workflow_set(list(forested ~ .), list(tree_spec, rf_spec))\nwf_set\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result    \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "slides/intro-extra-workflowsets.html#evaluate-a-workflow-set-1",
    "href": "slides/intro-extra-workflowsets.html#evaluate-a-workflow-set-1",
    "title": "Extras - workflowsets",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nwf_set_fit &lt;- wf_set |&gt;\n  workflow_map(\"fit_resamples\", resamples = forested_folds)\nwf_set_fit\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result   \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "slides/intro-extra-workflowsets.html#evaluate-a-workflow-set-2",
    "href": "slides/intro-extra-workflowsets.html#evaluate-a-workflow-set-2",
    "title": "Extras - workflowsets",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nwf_set_fit |&gt;\n  rank_results()\n#&gt; # A tibble: 6 Ã— 9\n#&gt;   wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n#&gt;   &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n#&gt; 1 formula_rand_forâ€¦ pre0_mâ€¦ accuraâ€¦ 0.753 0.00462    10 formula      randâ€¦     1\n#&gt; 2 formula_rand_forâ€¦ pre0_mâ€¦ brier_â€¦ 0.167 0.00312    10 formula      randâ€¦     1\n#&gt; 3 formula_rand_forâ€¦ pre0_mâ€¦ roc_auc 0.757 0.00994    10 formula      randâ€¦     1\n#&gt; 4 formula_decisionâ€¦ pre0_mâ€¦ accuraâ€¦ 0.704 0.00653    10 formula      deciâ€¦     2\n#&gt; 5 formula_decisionâ€¦ pre0_mâ€¦ brier_â€¦ 0.214 0.00349    10 formula      deciâ€¦     2\n#&gt; 6 formula_decisionâ€¦ pre0_mâ€¦ roc_auc 0.692 0.00496    10 formula      deciâ€¦     2\n\nThe first metric of the metric set is used for ranking. Use rank_metric to change that.\n\nLots more available with workflow sets, like collect_metrics(), autoplot() methods, and more!"
  },
  {
    "objectID": "slides/intro-extra-workflowsets.html#your-turn",
    "href": "slides/intro-extra-workflowsets.html#your-turn",
    "title": "Extras - workflowsets",
    "section": "Your turn",
    "text": "Your turn\n\nWhen do you think a workflow set would be useful?\nDiscuss with your neighbors!\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/intro-06-wrapping-up.html#your-turn",
    "href": "slides/intro-06-wrapping-up.html#your-turn",
    "title": "6 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/intro-06-wrapping-up.html#resources-to-keep-learning",
    "href": "slides/intro-06-wrapping-up.html#resources-to-keep-learning",
    "title": "6 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://tidymodels.aml4td.org/\n\n\n\n\nhttps://smltar.com/\n\n\n\n\nhttps://feaz-book.com/\n\n\n\nFollow us on Bluesky, Mastodon and at the tidyverse blog for updates!"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#looking-at-predictions",
    "href": "slides/intro-04-evaluating-models.html#looking-at-predictions",
    "title": "4 - Evaluating models",
    "section": "Looking at predictions",
    "text": "Looking at predictions\n\naugment(forested_fit, new_data = forested_train)\n#&gt; # A tibble: 8,749 Ã— 22\n#&gt;    .pred_class .pred_Yes .pred_No forested  year elevation eastness roughness\n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 Yes             0.931   0.0690 Yes       1997        66       82        10\n#&gt;  2 Yes             0.983   0.0172 No        1997       284      -99        58\n#&gt;  3 Yes             0.960   0.0401 Yes       2022       130       86        15\n#&gt;  4 Yes             0.870   0.130  Yes       2021       202      -55         3\n#&gt;  5 Yes             0.823   0.177  Yes       1995        75      -89         1\n#&gt;  6 Yes             0.758   0.242  No        1995       110      -53         5\n#&gt;  7 Yes             0.823   0.177  Yes       2022       111       73        12\n#&gt;  8 No              0.467   0.533  Yes       1997       230       96        14\n#&gt;  9 Yes             0.983   0.0172 Yes       2002       160      -88        13\n#&gt; 10 Yes             0.871   0.129  Yes       2020        39        9         6\n#&gt; # â„¹ 8,739 more rows\n#&gt; # â„¹ 14 more variables: tree_no_tree &lt;fct&gt;, dew_temp &lt;dbl&gt;, precip_annual &lt;dbl&gt;,\n#&gt; #   temp_annual_mean &lt;dbl&gt;, temp_annual_min &lt;dbl&gt;, temp_annual_max &lt;dbl&gt;,\n#&gt; #   temp_january_min &lt;dbl&gt;, vapor_min &lt;dbl&gt;, vapor_max &lt;dbl&gt;,\n#&gt; #   canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, land_type &lt;fct&gt;, county &lt;fct&gt;"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#confusion-matrix",
    "href": "slides/intro-04-evaluating-models.html#confusion-matrix",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#confusion-matrix-1",
    "href": "slides/intro-04-evaluating-models.html#confusion-matrix-1",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix \n\naugment(forested_fit, new_data = forested_train) |&gt;\n  conf_mat(truth = forested, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction  Yes   No\n#&gt;        Yes 5884  869\n#&gt;        No   438 1558"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#confusion-matrix-2",
    "href": "slides/intro-04-evaluating-models.html#confusion-matrix-2",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix \n\naugment(forested_fit, new_data = forested_train) |&gt;\n  conf_mat(truth = forested, estimate = .pred_class) |&gt;\n  autoplot(type = \"heatmap\")"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#your-turn",
    "href": "slides/intro-04-evaluating-models.html#your-turn",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\n\nHow would you combine TP, TN, FP, and FN into one number to describe how often the model is correct?\n\n\n\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#metrics-for-model-performance",
    "href": "slides/intro-04-evaluating-models.html#metrics-for-model-performance",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  accuracy(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.851"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#dangers-of-accuracy",
    "href": "slides/intro-04-evaluating-models.html#dangers-of-accuracy",
    "title": "4 - Evaluating models",
    "section": "Dangers of accuracy ",
    "text": "Dangers of accuracy \nWe need to be careful of using accuracy() since it can give â€œgoodâ€ performance by only predicting one way with imbalanced data:\n\naugment(forested_fit, new_data = forested_train) %&gt;%\n  mutate(.pred_class = factor(\"Yes\", levels = c(\"Yes\", \"No\"))) %&gt;%\n  accuracy(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.723"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#metrics-for-model-performance-1",
    "href": "slides/intro-04-evaluating-models.html#metrics-for-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  sensitivity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.931"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#metrics-for-model-performance-2",
    "href": "slides/intro-04-evaluating-models.html#metrics-for-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  sensitivity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.931\n\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  specificity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 specificity binary         0.642"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#metrics-for-model-performance-3",
    "href": "slides/intro-04-evaluating-models.html#metrics-for-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \nWe can use metric_set() to combine multiple calculations into one\n\nforested_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  forested_metrics(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy    binary         0.851\n#&gt; 2 specificity binary         0.642\n#&gt; 3 sensitivity binary         0.931"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#metrics-for-model-performance-4",
    "href": "slides/intro-04-evaluating-models.html#metrics-for-model-performance-4",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \nMetrics and metric sets work with grouped data frames!\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  group_by(tree_no_tree) |&gt;\n  accuracy(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   tree_no_tree .metric  .estimator .estimate\n#&gt;   &lt;fct&gt;        &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 Tree         accuracy binary         0.875\n#&gt; 2 No tree      accuracy binary         0.807"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#your-turn-1",
    "href": "slides/intro-04-evaluating-models.html#your-turn-1",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nApply the forested_metrics metric set to augment()\noutput grouped by tree_no_tree.\nDo any metrics differ substantially between groups?\n\n\n\nâˆ’+\n05:00\n\n\n\n\nThe specificity for \"Tree\" is a good bit lower than it is for \"No tree\".\nSpecificity is the proportion of negatives that are correctly identified as negatives. â€œNegativeâ€ is the non-event level of the outcome, i.e.Â â€œnon-forested.â€ So, when this index classifies the plot as having a tree, the model does not do well at correctly identifying the plot as non-forested when it is indeed non-forested."
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#two-class-data",
    "href": "slides/intro-04-evaluating-models.html#two-class-data",
    "title": "4 - Evaluating models",
    "section": "Two class data",
    "text": "Two class data\nThese metrics assume that we know the threshold for converting â€œsoftâ€ probability predictions into â€œhardâ€ class predictions.\n\nIs a 50% threshold good?\nWhat happens if we say that we need to be 80% sure to declare an event?\n\nsensitivity â¬‡ï¸, specificity â¬†ï¸\n\n\n\nWhat happens for a 20% threshold?\n\nsensitivity â¬†ï¸, specificity â¬‡ï¸"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#varying-the-threshold",
    "href": "slides/intro-04-evaluating-models.html#varying-the-threshold",
    "title": "4 - Evaluating models",
    "section": "Varying the threshold",
    "text": "Varying the threshold"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#roc-curves",
    "href": "slides/intro-04-evaluating-models.html#roc-curves",
    "title": "4 - Evaluating models",
    "section": "ROC curves",
    "text": "ROC curves\n\n\nFor an ROC (receiver operator characteristic) curve, we plot\n\nthe false positive rate (1 - specificity) on the x-axis\nthe true positive rate (sensitivity) on the y-axis\n\nwith sensitivity and specificity calculated at all possible thresholds.\n\n\n\n\n\n\n\n\n\n\n\n\nROC curves are insensitive to class imbalance."
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#roc-curves-1",
    "href": "slides/intro-04-evaluating-models.html#roc-curves-1",
    "title": "4 - Evaluating models",
    "section": "ROC curves",
    "text": "ROC curves\n\n\nWe can use the area under the ROC curve as a classification metric:\n\nROC AUC = 1 ğŸ’¯\nROC AUC = 1/2 ğŸ˜¢"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#roc-curves-2",
    "href": "slides/intro-04-evaluating-models.html#roc-curves-2",
    "title": "4 - Evaluating models",
    "section": "ROC curves ",
    "text": "ROC curves \n\n# Assumes _first_ factor level is event; there are options to change that\naugment(forested_fit, new_data = forested_train) |&gt; \n  roc_curve(truth = forested, .pred_Yes) |&gt;\n  slice(1, 20, 50)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .threshold specificity sensitivity\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1   -Inf           0           1    \n#&gt; 2      0.143       0.267       0.990\n#&gt; 3      0.385       0.571       0.951\n\naugment(forested_fit, new_data = forested_train) |&gt; \n  roc_auc(truth = forested, .pred_Yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.881"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#roc-curve-plot",
    "href": "slides/intro-04-evaluating-models.html#roc-curve-plot",
    "title": "4 - Evaluating models",
    "section": "ROC curve plot ",
    "text": "ROC curve plot \n\n\naugment(forested_fit, \n        new_data = forested_train) |&gt; \n  roc_curve(truth = forested, \n            .pred_Yes) |&gt;\n  autoplot()"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#your-turn-2",
    "href": "slides/intro-04-evaluating-models.html#your-turn-2",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCompute and plot an ROC curve for your current model.\nWhat data are being used for this ROC curve plot?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#brier-score",
    "href": "slides/intro-04-evaluating-models.html#brier-score",
    "title": "4 - Evaluating models",
    "section": "Brier score",
    "text": "Brier score\n\naugment(forested_fit, new_data = forested_train) |&gt; \n  brier_class(truth = forested, .pred_Yes) \n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary         0.113\n\n\nSmaller values are better, for binary classification the â€œbad model thresholdâ€ is about 0.25."
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#brier-score-1",
    "href": "slides/intro-04-evaluating-models.html#brier-score-1",
    "title": "4 - Evaluating models",
    "section": "Brier score",
    "text": "Brier score\n\naugment(forested_fit, new_data = forested_train) |&gt; \n  brier_class(truth = forested, .pred_Yes) \n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary         0.113\n\n\nSmaller values are better, for binary classification the â€œbad model thresholdâ€ is about 0.25."
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#separation-vs-calibration",
    "href": "slides/intro-04-evaluating-models.html#separation-vs-calibration",
    "title": "4 - Evaluating models",
    "section": "Separation vs calibration",
    "text": "Separation vs calibration\n\n\nThe ROC captures separation.\n\n\n\n\n\n\n\n\n\n\nThe Brier score captures calibration.\n\n\n\n\n\n\n\n\n\n\n\n\nGood separation: the densities donâ€™t overlap.\nGood calibration: the calibration line follows the diagonal.\n\nCalibration plot: We bin observations according to predicted probability. In the bin for 20%-30% predicted prob, we should see an event rate of ~25% if the model is well-calibrated."
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-1",
    "href": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-1",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting",
    "text": "Dangers of overfitting\n\n\nAML4TD"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-2",
    "href": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-2",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸",
    "text": "Dangers of overfitting âš ï¸"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-3",
    "href": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-3",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\nforested_fit |&gt;\n  augment(forested_train)\n#&gt; # A tibble: 8,749 Ã— 22\n#&gt;    .pred_class .pred_Yes .pred_No forested  year elevation eastness roughness\n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 Yes             0.931   0.0690 Yes       1997        66       82        10\n#&gt;  2 Yes             0.983   0.0172 No        1997       284      -99        58\n#&gt;  3 Yes             0.960   0.0401 Yes       2022       130       86        15\n#&gt;  4 Yes             0.870   0.130  Yes       2021       202      -55         3\n#&gt;  5 Yes             0.823   0.177  Yes       1995        75      -89         1\n#&gt;  6 Yes             0.758   0.242  No        1995       110      -53         5\n#&gt;  7 Yes             0.823   0.177  Yes       2022       111       73        12\n#&gt;  8 No              0.467   0.533  Yes       1997       230       96        14\n#&gt;  9 Yes             0.983   0.0172 Yes       2002       160      -88        13\n#&gt; 10 Yes             0.871   0.129  Yes       2020        39        9         6\n#&gt; # â„¹ 8,739 more rows\n#&gt; # â„¹ 14 more variables: tree_no_tree &lt;fct&gt;, dew_temp &lt;dbl&gt;, precip_annual &lt;dbl&gt;,\n#&gt; #   temp_annual_mean &lt;dbl&gt;, temp_annual_min &lt;dbl&gt;, temp_annual_max &lt;dbl&gt;,\n#&gt; #   temp_january_min &lt;dbl&gt;, vapor_min &lt;dbl&gt;, vapor_max &lt;dbl&gt;,\n#&gt; #   canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, land_type &lt;fct&gt;, county &lt;fct&gt;\n\nWe call this â€œresubstitutionâ€ or â€œrepredicting the training setâ€"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-4",
    "href": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-4",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\nforested_fit |&gt;\n  augment(forested_train) |&gt;\n  accuracy(forested, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.851\n\nWe call this a â€œresubstitution estimateâ€"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-5",
    "href": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-5",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\nforested_fit |&gt;\n  augment(forested_train) |&gt;\n  accuracy(forested, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.851"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-6",
    "href": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-6",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\nforested_fit |&gt;\n  augment(forested_train) |&gt;\n  accuracy(forested, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.851\n\n\n\nforested_fit |&gt;\n  augment(forested_test) |&gt;\n  accuracy(forested, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.711\n\n\n\nâš ï¸ Remember that weâ€™re demonstrating overfitting\n\n\nâš ï¸ Donâ€™t use the test set until the end of your modeling analysis"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#your-turn-3",
    "href": "slides/intro-04-evaluating-models.html#your-turn-3",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse augment() and a metric function to compute a classification metric like roc_auc().\nCompute the metrics for both training and testing data to demonstrate overfitting!\nNotice the evidence of overfitting! âš ï¸\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-7",
    "href": "slides/intro-04-evaluating-models.html#dangers-of-overfitting-7",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\nforested_fit |&gt;\n  augment(forested_train) |&gt;\n  roc_auc(forested, .pred_Yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.881\n\n\n\nforested_fit |&gt;\n  augment(forested_test) |&gt;\n  roc_auc(forested, .pred_Yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.703\n\n\n\nWhat if we want to compare more models?\n\n\nAnd/or more model configurations?\n\n\nAnd we want to understand if these are important differences?"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#cross-validation",
    "href": "slides/intro-04-evaluating-models.html#cross-validation",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#cross-validation-1",
    "href": "slides/intro-04-evaluating-models.html#cross-validation-1",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#your-turn-4",
    "href": "slides/intro-04-evaluating-models.html#your-turn-4",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nIf we use 10 folds, what percent of the training data\n\nends up in analysis\nends up in assessment\n\nfor each fold?\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#cross-validation-2",
    "href": "slides/intro-04-evaluating-models.html#cross-validation-2",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(forested_train) # v = 10 is default\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [7874/875]&gt; Fold01\n#&gt;  2 &lt;split [7874/875]&gt; Fold02\n#&gt;  3 &lt;split [7874/875]&gt; Fold03\n#&gt;  4 &lt;split [7874/875]&gt; Fold04\n#&gt;  5 &lt;split [7874/875]&gt; Fold05\n#&gt;  6 &lt;split [7874/875]&gt; Fold06\n#&gt;  7 &lt;split [7874/875]&gt; Fold07\n#&gt;  8 &lt;split [7874/875]&gt; Fold08\n#&gt;  9 &lt;split [7874/875]&gt; Fold09\n#&gt; 10 &lt;split [7875/874]&gt; Fold10"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#cross-validation-3",
    "href": "slides/intro-04-evaluating-models.html#cross-validation-3",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWhat is in this?\n\nforested_folds &lt;- vfold_cv(forested_train)\nforested_folds$splits[1:3]\n#&gt; [[1]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;7874/875/8749&gt;\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;7874/875/8749&gt;\n#&gt; \n#&gt; [[3]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;7874/875/8749&gt;\n\n\nTalk about a list column, storing non-atomic types in dataframe"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#cross-validation-4",
    "href": "slides/intro-04-evaluating-models.html#cross-validation-4",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(forested_train, v = 5)\n#&gt; #  5-fold cross-validation \n#&gt; # A tibble: 5 Ã— 2\n#&gt;   splits              id   \n#&gt;   &lt;list&gt;              &lt;chr&gt;\n#&gt; 1 &lt;split [6999/1750]&gt; Fold1\n#&gt; 2 &lt;split [6999/1750]&gt; Fold2\n#&gt; 3 &lt;split [6999/1750]&gt; Fold3\n#&gt; 4 &lt;split [6999/1750]&gt; Fold4\n#&gt; 5 &lt;split [7000/1749]&gt; Fold5"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#cross-validation-5",
    "href": "slides/intro-04-evaluating-models.html#cross-validation-5",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWeâ€™ll use this setup:\n\nset.seed(123)\nforested_folds &lt;- vfold_cv(forested_train, v = 10)\nforested_folds\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [7874/875]&gt; Fold01\n#&gt;  2 &lt;split [7874/875]&gt; Fold02\n#&gt;  3 &lt;split [7874/875]&gt; Fold03\n#&gt;  4 &lt;split [7874/875]&gt; Fold04\n#&gt;  5 &lt;split [7874/875]&gt; Fold05\n#&gt;  6 &lt;split [7874/875]&gt; Fold06\n#&gt;  7 &lt;split [7874/875]&gt; Fold07\n#&gt;  8 &lt;split [7874/875]&gt; Fold08\n#&gt;  9 &lt;split [7874/875]&gt; Fold09\n#&gt; 10 &lt;split [7875/874]&gt; Fold10\n\n\nSet the seed when creating resamples"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#fit-our-model-to-the-resamples",
    "href": "slides/intro-04-evaluating-models.html#fit-our-model-to-the-resamples",
    "title": "4 - Evaluating models",
    "section": "Fit our model to the resamples",
    "text": "Fit our model to the resamples\n\nforested_res &lt;- fit_resamples(forested_wflow, forested_folds)\nforested_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 4\n#&gt;    splits             id     .metrics         .notes          \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n#&gt;  1 &lt;split [7874/875]&gt; Fold01 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt;\n#&gt;  2 &lt;split [7874/875]&gt; Fold02 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt;\n#&gt;  3 &lt;split [7874/875]&gt; Fold03 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt;\n#&gt;  4 &lt;split [7874/875]&gt; Fold04 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt;\n#&gt;  5 &lt;split [7874/875]&gt; Fold05 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt;\n#&gt;  6 &lt;split [7874/875]&gt; Fold06 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt;\n#&gt;  7 &lt;split [7874/875]&gt; Fold07 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt;\n#&gt;  8 &lt;split [7874/875]&gt; Fold08 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt;\n#&gt;  9 &lt;split [7874/875]&gt; Fold09 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt;\n#&gt; 10 &lt;split [7875/874]&gt; Fold10 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt;"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#evaluating-model-performance",
    "href": "slides/intro-04-evaluating-models.html#evaluating-model-performance",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nforested_res |&gt;\n  collect_metrics()\n#&gt; # A tibble: 3 Ã— 6\n#&gt;   .metric     .estimator  mean     n std_err .config        \n#&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1 accuracy    binary     0.704    10 0.00653 pre0_mod0_post0\n#&gt; 2 brier_class binary     0.214    10 0.00349 pre0_mod0_post0\n#&gt; 3 roc_auc     binary     0.692    10 0.00496 pre0_mod0_post0\n\n\ncollect_metrics() is one of a suite of collect_*() functions that can be used to work with columns of tuning results. Most columns in a tuning result prefixed with . have a corresponding collect_*() function with options for common summaries.\n\n\nWe can reliably measure performance using only the training data ğŸ‰"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#comparing-metrics",
    "href": "slides/intro-04-evaluating-models.html#comparing-metrics",
    "title": "4 - Evaluating models",
    "section": "Comparing metrics ",
    "text": "Comparing metrics \nHow do the metrics from resampling compare to the metrics from training and testing?\n\n\n\nforested_res |&gt;\n  collect_metrics() |&gt; \n  select(.metric, mean, n)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric      mean     n\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 accuracy    0.704    10\n#&gt; 2 brier_class 0.214    10\n#&gt; 3 roc_auc     0.692    10\n\n\nThe ROC AUC previously was\n\n0.88 for the training set\n0.7 for test set\n\n\n\nRemember that:\nâš ï¸ the training set gives you overly optimistic metrics\nâš ï¸ the test set is precious"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#evaluating-model-performance-1",
    "href": "slides/intro-04-evaluating-models.html#evaluating-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nctrl_forested &lt;- control_resamples(save_pred = TRUE)\nforested_res &lt;- fit_resamples(forested_wflow, forested_folds, control = ctrl_forested)\n\nforested_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [7874/875]&gt; Fold01 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [7874/875]&gt; Fold02 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [7874/875]&gt; Fold03 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [7874/875]&gt; Fold04 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [7874/875]&gt; Fold05 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [7874/875]&gt; Fold06 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [7874/875]&gt; Fold07 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [7874/875]&gt; Fold08 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [7874/875]&gt; Fold09 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [7875/874]&gt; Fold10 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#evaluating-model-performance-2",
    "href": "slides/intro-04-evaluating-models.html#evaluating-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nforested_preds &lt;- collect_predictions(forested_res)\nforested_preds\n#&gt; # A tibble: 8,749 Ã— 7\n#&gt;    .pred_class .pred_Yes .pred_No id     forested  .row .config        \n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;fct&gt;    &lt;int&gt; &lt;chr&gt;          \n#&gt;  1 Yes             0.953   0.0473 Fold01 No           2 pre0_mod0_post0\n#&gt;  2 Yes             0.6     0.4    Fold01 No           6 pre0_mod0_post0\n#&gt;  3 Yes             0.848   0.152  Fold01 Yes          7 pre0_mod0_post0\n#&gt;  4 Yes             0.941   0.0588 Fold01 Yes         36 pre0_mod0_post0\n#&gt;  5 Yes             0.895   0.105  Fold01 No          38 pre0_mod0_post0\n#&gt;  6 Yes             1       0      Fold01 Yes         58 pre0_mod0_post0\n#&gt;  7 No              0.187   0.812  Fold01 No          69 pre0_mod0_post0\n#&gt;  8 Yes             0.905   0.0952 Fold01 Yes         71 pre0_mod0_post0\n#&gt;  9 Yes             0.907   0.0930 Fold01 Yes         74 pre0_mod0_post0\n#&gt; 10 Yes             0.904   0.0962 Fold01 Yes         80 pre0_mod0_post0\n#&gt; # â„¹ 8,739 more rows"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#evaluating-model-performance-3",
    "href": "slides/intro-04-evaluating-models.html#evaluating-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nforested_preds |&gt; \n  group_by(id) |&gt;\n  forested_metrics(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 30 Ã— 4\n#&gt;    id     .metric  .estimator .estimate\n#&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt;  1 Fold01 accuracy binary         0.729\n#&gt;  2 Fold02 accuracy binary         0.685\n#&gt;  3 Fold03 accuracy binary         0.687\n#&gt;  4 Fold04 accuracy binary         0.711\n#&gt;  5 Fold05 accuracy binary         0.736\n#&gt;  6 Fold06 accuracy binary         0.670\n#&gt;  7 Fold07 accuracy binary         0.699\n#&gt;  8 Fold08 accuracy binary         0.710\n#&gt;  9 Fold09 accuracy binary         0.699\n#&gt; 10 Fold10 accuracy binary         0.719\n#&gt; # â„¹ 20 more rows"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#where-are-the-fitted-models",
    "href": "slides/intro-04-evaluating-models.html#where-are-the-fitted-models",
    "title": "4 - Evaluating models",
    "section": "Where are the fitted models? ",
    "text": "Where are the fitted models? \n\nforested_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [7874/875]&gt; Fold01 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [7874/875]&gt; Fold02 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [7874/875]&gt; Fold03 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [7874/875]&gt; Fold04 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [7874/875]&gt; Fold05 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [7874/875]&gt; Fold06 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [7874/875]&gt; Fold07 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [7874/875]&gt; Fold08 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [7874/875]&gt; Fold09 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [7875/874]&gt; Fold10 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;\n\n\nğŸ—‘ï¸"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#bootstrapping",
    "href": "slides/intro-04-evaluating-models.html#bootstrapping",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#bootstrapping-1",
    "href": "slides/intro-04-evaluating-models.html#bootstrapping-1",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(3214)\nbootstraps(forested_train)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 25 Ã— 2\n#&gt;    splits              id         \n#&gt;    &lt;list&gt;              &lt;chr&gt;      \n#&gt;  1 &lt;split [8749/3218]&gt; Bootstrap01\n#&gt;  2 &lt;split [8749/3264]&gt; Bootstrap02\n#&gt;  3 &lt;split [8749/3220]&gt; Bootstrap03\n#&gt;  4 &lt;split [8749/3208]&gt; Bootstrap04\n#&gt;  5 &lt;split [8749/3230]&gt; Bootstrap05\n#&gt;  6 &lt;split [8749/3197]&gt; Bootstrap06\n#&gt;  7 &lt;split [8749/3193]&gt; Bootstrap07\n#&gt;  8 &lt;split [8749/3226]&gt; Bootstrap08\n#&gt;  9 &lt;split [8749/3243]&gt; Bootstrap09\n#&gt; 10 &lt;split [8749/3233]&gt; Bootstrap10\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#the-whole-game---status-update",
    "href": "slides/intro-04-evaluating-models.html#the-whole-game---status-update",
    "title": "4 - Evaluating models",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#your-turn-5",
    "href": "slides/intro-04-evaluating-models.html#your-turn-5",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCreate:\n\nMonte Carlo Cross-Validation sets\nvalidation set\n\n(use the reference guide to find the functions)\nDonâ€™t forget to set a seed when you resample!\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#monte-carlo-cross-validation",
    "href": "slides/intro-04-evaluating-models.html#monte-carlo-cross-validation",
    "title": "4 - Evaluating models",
    "section": "Monte Carlo Cross-Validation ",
    "text": "Monte Carlo Cross-Validation \n\nset.seed(322)\nmc_cv(forested_train, times = 10)\n#&gt; # Monte Carlo cross-validation (0.75/0.25) with 10 resamples \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits              id        \n#&gt;    &lt;list&gt;              &lt;chr&gt;     \n#&gt;  1 &lt;split [6561/2188]&gt; Resample01\n#&gt;  2 &lt;split [6561/2188]&gt; Resample02\n#&gt;  3 &lt;split [6561/2188]&gt; Resample03\n#&gt;  4 &lt;split [6561/2188]&gt; Resample04\n#&gt;  5 &lt;split [6561/2188]&gt; Resample05\n#&gt;  6 &lt;split [6561/2188]&gt; Resample06\n#&gt;  7 &lt;split [6561/2188]&gt; Resample07\n#&gt;  8 &lt;split [6561/2188]&gt; Resample08\n#&gt;  9 &lt;split [6561/2188]&gt; Resample09\n#&gt; 10 &lt;split [6561/2188]&gt; Resample10"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#validation-set",
    "href": "slides/intro-04-evaluating-models.html#validation-set",
    "title": "4 - Evaluating models",
    "section": "Validation set ",
    "text": "Validation set \n\nset.seed(853)\nforested_val_split &lt;- initial_validation_split(forested)\nvalidation_set(forested_val_split)\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   splits              id        \n#&gt;   &lt;list&gt;              &lt;chr&gt;     \n#&gt; 1 &lt;split [4264/1421]&gt; validation\n\n\nA validation set is just another type of resample"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#random-forest-1",
    "href": "slides/intro-04-evaluating-models.html#random-forest-1",
    "title": "4 - Evaluating models",
    "section": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ",
    "text": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\n\nEnsemble many decision tree models\nAll the trees vote! ğŸ—³ï¸\nBootstrap aggregating + random predictor sampling\n\n\n\nOften works well without tuning hyperparameters (more on this later!), as long as there are enough trees"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#create-a-random-forest-model",
    "href": "slides/intro-04-evaluating-models.html#create-a-random-forest-model",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_spec &lt;- rand_forest(trees = 1000, mode = \"classification\")\nrf_spec\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#create-a-random-forest-model-1",
    "href": "slides/intro-04-evaluating-models.html#create-a-random-forest-model-1",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_wflow &lt;- workflow(forested ~ ., rf_spec)\nrf_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#your-turn-6",
    "href": "slides/intro-04-evaluating-models.html#your-turn-6",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() and rf_wflow to:\n\nkeep predictions\ncompute metrics\n\n\n\n\nâˆ’+\n08:00"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#evaluating-model-performance-4",
    "href": "slides/intro-04-evaluating-models.html#evaluating-model-performance-4",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nctrl_forested &lt;- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\n\nset.seed(2)\nrf_res &lt;- fit_resamples(rf_wflow, forested_folds, control = ctrl_forested)\ncollect_metrics(rf_res)\n#&gt; # A tibble: 3 Ã— 6\n#&gt;   .metric     .estimator  mean     n std_err .config        \n#&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1 accuracy    binary     0.755    10 0.00482 pre0_mod0_post0\n#&gt; 2 brier_class binary     0.167    10 0.00321 pre0_mod0_post0\n#&gt; 3 roc_auc     binary     0.757    10 0.0103  pre0_mod0_post0"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#the-whole-game---status-update-1",
    "href": "slides/intro-04-evaluating-models.html#the-whole-game---status-update-1",
    "title": "4 - Evaluating models",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#the-final-fit",
    "href": "slides/intro-04-evaluating-models.html#the-final-fit",
    "title": "4 - Evaluating models",
    "section": "The final fit ",
    "text": "The final fit \nSuppose that we are happy with our random forest model.\nLetâ€™s fit the model on the training set and verify our performance using the test set.\n\nWeâ€™ve shown you fit() and predict() (+ augment()) but there is a shortcut:\n\n# forested_split has train + test info\nfinal_fit &lt;- last_fit(rf_wflow, forested_split) \n\nfinal_fit\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits              id               .metrics .notes   .predictions .workflow \n#&gt;   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n#&gt; 1 &lt;split [8749/2188]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#what-is-in-final_fit",
    "href": "slides/intro-04-evaluating-models.html#what-is-in-final_fit",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_metrics(final_fit)\n#&gt; # A tibble: 3 Ã— 4\n#&gt;   .metric     .estimator .estimate .config        \n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1 accuracy    binary         0.761 pre0_mod0_post0\n#&gt; 2 roc_auc     binary         0.761 pre0_mod0_post0\n#&gt; 3 brier_class binary         0.162 pre0_mod0_post0\n\n\nThese are metrics computed with the test set"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#what-is-in-final_fit-1",
    "href": "slides/intro-04-evaluating-models.html#what-is-in-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_predictions(final_fit)\n#&gt; # A tibble: 2,188 Ã— 7\n#&gt;    .pred_class .pred_Yes .pred_No id               forested  .row .config       \n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;            &lt;fct&gt;    &lt;int&gt; &lt;chr&gt;         \n#&gt;  1 Yes             0.957   0.0429 train/test split Yes          4 pre0_mod0_posâ€¦\n#&gt;  2 Yes             0.810   0.190  train/test split Yes          8 pre0_mod0_posâ€¦\n#&gt;  3 Yes             0.826   0.174  train/test split Yes         10 pre0_mod0_posâ€¦\n#&gt;  4 Yes             0.842   0.158  train/test split Yes         19 pre0_mod0_posâ€¦\n#&gt;  5 Yes             0.863   0.137  train/test split Yes         23 pre0_mod0_posâ€¦\n#&gt;  6 Yes             0.585   0.415  train/test split No          28 pre0_mod0_posâ€¦\n#&gt;  7 Yes             0.847   0.153  train/test split Yes         34 pre0_mod0_posâ€¦\n#&gt;  8 Yes             0.605   0.395  train/test split No          35 pre0_mod0_posâ€¦\n#&gt;  9 Yes             0.964   0.0362 train/test split Yes         38 pre0_mod0_posâ€¦\n#&gt; 10 Yes             0.926   0.0744 train/test split Yes         40 pre0_mod0_posâ€¦\n#&gt; # â„¹ 2,178 more rows"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#what-is-in-final_fit-2",
    "href": "slides/intro-04-evaluating-models.html#what-is-in-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\nextract_workflow(final_fit)\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  1000 \n#&gt; Sample size:                      8749 \n#&gt; Number of independent variables:  18 \n#&gt; Mtry:                             4 \n#&gt; Target node size:                 10 \n#&gt; Variable importance mode:         none \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.1671606\n\n\nUse this for prediction on new data, like for deploying"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#the-whole-game",
    "href": "slides/intro-04-evaluating-models.html#the-whole-game",
    "title": "4 - Evaluating models",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#data-on-forests-in-georgia",
    "href": "slides/intro-02-data-budget.html#data-on-forests-in-georgia",
    "title": "2 - Your data budget",
    "section": "Data on forests in Georgia",
    "text": "Data on forests in Georgia\n\n\n\nThe U.S. Forest Service maintains ML models to predict whether a plot of land is â€œforested.â€\nThis classification is important for all sorts of research, legislation, and land management purposes.\nPlots are typically remeasured every 10 years and this dataset contains the most recent measurement per plot.\nType ?forested_ga to learn more about this dataset, including references.\n\n\n\n\n\nCredit: https://www.svgrepo.com/svg/251793/forest-mountain"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#data-on-forests-in-georgia-1",
    "href": "slides/intro-02-data-budget.html#data-on-forests-in-georgia-1",
    "title": "2 - Your data budget",
    "section": "Data on forests in Georgia",
    "text": "Data on forests in Georgia\n\n\n\nN = 10937 plots of land, one from each of 10937 6000-acre hexagons in Georgia.\nA nominal outcome, forested, with levels \"Yes\" and \"No\", measured â€œon-the-ground.â€\n18 remotely-sensed and easily-accessible predictors:\n\nnumeric variables based on weather and topography.\nnominal variables based on classifications from other governmental orgs.\n\n\n\n\n\n\nCredit: https://www.svgrepo.com/svg/67614/forest\n\n\n\nThose nominal variables are classifications similar to â€œforestedâ€ but from other agencies. e.g.Â land_type is from the European Space Agency, and is a remotely-sensed 3-class distribution based on predictions for how the land is used. (The data also includes a county variable, which is not itself from a model.)"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#checklist-for-predictors",
    "href": "slides/intro-02-data-budget.html#checklist-for-predictors",
    "title": "2 - Your data budget",
    "section": "Checklist for predictors",
    "text": "Checklist for predictors\n\nIs it ethical to use this variable? (Or even legal?)\nWill this variable be available at prediction time?\nDoes this variable contribute to explainability?\n\n\n\nre: ethics â€“ what issues might arise from releasing the true lat and lon? In reality, these lat and lon are slightly jittered to help ensure trust with landowners who allow surveyers to come take measurements."
  },
  {
    "objectID": "slides/intro-02-data-budget.html#data-on-forests-in-georgia-2",
    "href": "slides/intro-02-data-budget.html#data-on-forests-in-georgia-2",
    "title": "2 - Your data budget",
    "section": "Data on forests in Georgia",
    "text": "Data on forests in Georgia\n\nlibrary(tidymodels)\nlibrary(forested)\n\nforested_ga\n#&gt; # A tibble: 10,937 Ã— 19\n#&gt;    forested  year elevation eastness roughness tree_no_tree dew_temp\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt;\n#&gt;  1 Yes       2007        14        0         0 No tree          13.9\n#&gt;  2 Yes       2007        66      -53        10 Tree             13.8\n#&gt;  3 Yes       2006        59      -82         6 No tree          13.5\n#&gt;  4 Yes       2007       116      -78        20 Tree             12.3\n#&gt;  5 Yes       2006       283       63        13 Tree             10.0\n#&gt;  6 Yes       2007       250       63        14 Tree             10.8\n#&gt;  7 Yes       2007        58       31         1 No tree          13.8\n#&gt;  8 Yes       2023       140       56        11 Tree             12.2\n#&gt;  9 Yes       2024       118       72        17 Tree             12.2\n#&gt; 10 Yes       2024       217      -46        13 Tree             12.4\n#&gt; # â„¹ 10,927 more rows\n#&gt; # â„¹ 12 more variables: precip_annual &lt;dbl&gt;, temp_annual_mean &lt;dbl&gt;,\n#&gt; #   temp_annual_min &lt;dbl&gt;, temp_annual_max &lt;dbl&gt;, temp_january_min &lt;dbl&gt;,\n#&gt; #   vapor_min &lt;dbl&gt;, vapor_max &lt;dbl&gt;, canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;,\n#&gt; #   land_type &lt;fct&gt;, county &lt;fct&gt;"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#data-splitting-and-spending",
    "href": "slides/intro-02-data-budget.html#data-splitting-and-spending",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nFor machine learning, we typically split data into training and test sets:\n\n\nThe training set is used to estimate model parameters.\nThe test set is used to find an independent assessment of model performance.\n\n\n\nDo not ğŸš« use the test set during training."
  },
  {
    "objectID": "slides/intro-02-data-budget.html#data-splitting-and-spending-1",
    "href": "slides/intro-02-data-budget.html#data-splitting-and-spending-1",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#data-splitting-and-spending-2",
    "href": "slides/intro-02-data-budget.html#data-splitting-and-spending-2",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\n\nSpending too much data in training prevents us from computing a good assessment of predictive performance.\n\n\n\nSpending too much data in testing prevents us from computing a good estimate of model parameters."
  },
  {
    "objectID": "slides/intro-02-data-budget.html#your-turn",
    "href": "slides/intro-02-data-budget.html#your-turn",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nWhen is a good time to split your data?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#the-initial-split",
    "href": "slides/intro-02-data-budget.html#the-initial-split",
    "title": "2 - Your data budget",
    "section": "The initial split ",
    "text": "The initial split \n\nset.seed(123)\nforested_split &lt;- initial_split(forested_ga)\nforested_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;8202/2735/10937&gt;\n\n\nHow much data in training vs testing? This function uses a good default, but this depends on your specific goal/data"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#what-is-set.seed",
    "href": "slides/intro-02-data-budget.html#what-is-set.seed",
    "title": "2 - Your data budget",
    "section": "What is set.seed()?",
    "text": "What is set.seed()?\nTo create that split of the data, R generates â€œpseudo-randomâ€ numbers: while they are made to behave like random numbers, their generation is deterministic given a â€œseedâ€.\nThis allows us to reproduce results by setting that seed.\nWhich seed you pick doesnâ€™t matter, as long as you donâ€™t try a bunch of seeds and pick the one that gives you the best performance."
  },
  {
    "objectID": "slides/intro-02-data-budget.html#accessing-the-data",
    "href": "slides/intro-02-data-budget.html#accessing-the-data",
    "title": "2 - Your data budget",
    "section": "Accessing the data ",
    "text": "Accessing the data \n\nforested_train &lt;- training(forested_split)\nforested_test &lt;- testing(forested_split)"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#the-training-set",
    "href": "slides/intro-02-data-budget.html#the-training-set",
    "title": "2 - Your data budget",
    "section": "The training set",
    "text": "The training set\n\nforested_train\n#&gt; # A tibble: 8,202 Ã— 19\n#&gt;    forested  year elevation eastness roughness tree_no_tree dew_temp\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt;\n#&gt;  1 Yes       1997        66       82        10 Tree            12.2 \n#&gt;  2 No        1997       284      -99        58 Tree            10.3 \n#&gt;  3 Yes       2022       130       86        15 Tree            11.8 \n#&gt;  4 Yes       2021       202      -55         3 Tree            10.7 \n#&gt;  5 Yes       1995        75      -89         1 Tree            13.8 \n#&gt;  6 No        1995       110      -53         5 Tree            12.4 \n#&gt;  7 Yes       2022       111       73        12 Tree            11.5 \n#&gt;  8 Yes       1997       230       96        14 Tree             9.98\n#&gt;  9 Yes       2002       160      -88        13 Tree            11.1 \n#&gt; 10 Yes       2020        39        9         6 Tree            13.9 \n#&gt; # â„¹ 8,192 more rows\n#&gt; # â„¹ 12 more variables: precip_annual &lt;dbl&gt;, temp_annual_mean &lt;dbl&gt;,\n#&gt; #   temp_annual_min &lt;dbl&gt;, temp_annual_max &lt;dbl&gt;, temp_january_min &lt;dbl&gt;,\n#&gt; #   vapor_min &lt;dbl&gt;, vapor_max &lt;dbl&gt;, canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;,\n#&gt; #   land_type &lt;fct&gt;, county &lt;fct&gt;"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#the-training-set-1",
    "href": "slides/intro-02-data-budget.html#the-training-set-1",
    "title": "2 - Your data budget",
    "section": "The training set",
    "text": "The training set\n\nforested_train |&gt;\n  select(where(is.factor))\n#&gt; # A tibble: 8,202 Ã— 4\n#&gt;    forested tree_no_tree land_type           county    \n#&gt;    &lt;fct&gt;    &lt;fct&gt;        &lt;fct&gt;               &lt;fct&gt;     \n#&gt;  1 Yes      Tree         Tree                Muscogee  \n#&gt;  2 No       Tree         Tree                Polk      \n#&gt;  3 Yes      Tree         Tree                Hancock   \n#&gt;  4 Yes      Tree         Tree                Oglethorpe\n#&gt;  5 Yes      Tree         Tree                Berrien   \n#&gt;  6 No       Tree         Non-tree vegetation Dooly     \n#&gt;  7 Yes      Tree         Tree                Columbia  \n#&gt;  8 Yes      Tree         Tree                Walker    \n#&gt;  9 Yes      Tree         Tree                Greene    \n#&gt; 10 Yes      Tree         Tree                Wayne     \n#&gt; # â„¹ 8,192 more rows\n\n\n\nThe tree_no_tree and land_type variables are both outputs from other machine learning models. Companies make their own classifications of forest vs.Â non-forest based on satellite images and then sell them. Those classifications are available at prediction time; the FIA will happily spend the few thousand dollars to buy the data for the whole country."
  },
  {
    "objectID": "slides/intro-02-data-budget.html#the-test-set",
    "href": "slides/intro-02-data-budget.html#the-test-set",
    "title": "2 - Your data budget",
    "section": "The test set ",
    "text": "The test set \nğŸ™ˆ\n\nThere are 2735 rows and 19 columns in the test set."
  },
  {
    "objectID": "slides/intro-02-data-budget.html#your-turn-1",
    "href": "slides/intro-02-data-budget.html#your-turn-1",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nSplit your data so 20% is held out for the test set.\nTry out different values in set.seed() to see how the results change.\n\nWe recommend using the .qmd files in the classwork/ folder for code exercises. They set you up with the code from the slides.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#data-splitting-and-spending-3",
    "href": "slides/intro-02-data-budget.html#data-splitting-and-spending-3",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nforested_split &lt;- initial_split(forested_ga, prop = 0.8)\nforested_train &lt;- training(forested_split)\nforested_test &lt;- testing(forested_split)\n\nnrow(forested_train)\n#&gt; [1] 8749\nnrow(forested_test)\n#&gt; [1] 2188"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#your-turn-2",
    "href": "slides/intro-02-data-budget.html#your-turn-2",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nExplore the forested_train data on your own!\n\nWhatâ€™s the distribution of the outcome, forested?\nWhatâ€™s the distribution of numeric variables like precip_annual?\nHow does the distribution of forested differ across the categorical variables?\n\n\n\n\nâˆ’+\n08:00\n\n\n\n\nMake a plot or summary and then share with neighbor"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#section-1",
    "href": "slides/intro-02-data-budget.html#section-1",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train |&gt; \n  ggplot(aes(x = forested)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#section-2",
    "href": "slides/intro-02-data-budget.html#section-2",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train |&gt; \n  ggplot(aes(x = forested, fill = tree_no_tree)) +\n  geom_bar()"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#section-3",
    "href": "slides/intro-02-data-budget.html#section-3",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train |&gt; \n  ggplot(aes(x = precip_annual, fill = forested, group = forested)) +\n  geom_histogram(position = \"identity\", alpha = .7)"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#section-4",
    "href": "slides/intro-02-data-budget.html#section-4",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train |&gt; \n  ggplot(aes(x = precip_annual, fill = forested, group = forested)) +\n  geom_histogram(position = \"fill\")"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#section-5",
    "href": "slides/intro-02-data-budget.html#section-5",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train |&gt; \n  ggplot(aes(x = lon, y = lat, col = forested)) +\n  geom_point()"
  },
  {
    "objectID": "slides/intro-02-data-budget.html#the-whole-game---status-update",
    "href": "slides/intro-02-data-budget.html#the-whole-game---status-update",
    "title": "2 - Your data budget",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#chicago-l-train-data",
    "href": "slides/extras-transit-case-study.html#chicago-l-train-data",
    "title": "Case Study on Transportation",
    "section": "Chicago L-Train data",
    "text": "Chicago L-Train data\nSeveral years worth of pre-pandemic data were assembled to try to predict the daily number of people entering the Clark and Lake elevated (â€œLâ€) train station in Chicago.\nMore information:\n\nSeveral Chapters in Feature Engineering and Selection.\n\nStart with Section 4.1\nSee Section 1.3\n\nVideo: The Global Pandemic Ruined My Favorite Data Set"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#predictors",
    "href": "slides/extras-transit-case-study.html#predictors",
    "title": "Case Study on Transportation",
    "section": "Predictors",
    "text": "Predictors\n\nthe 14-day lagged ridership at this and other stations (units: thousands of rides/day)\nweather data\nhome/away game schedules for Chicago teams\nthe date\n\nThe data are in modeldata. See ?Chicago."
  },
  {
    "objectID": "slides/extras-transit-case-study.html#l-train-locations",
    "href": "slides/extras-transit-case-study.html#l-train-locations",
    "title": "Case Study on Transportation",
    "section": "L Train Locations",
    "text": "L Train Locations"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#your-turn-explore-the-data",
    "href": "slides/extras-transit-case-study.html#your-turn-explore-the-data",
    "title": "Case Study on Transportation",
    "section": "Your turn: Explore the Data",
    "text": "Your turn: Explore the Data\nTake a look at these data for a few minutes and see if you can find any interesting characteristics in the predictors or the outcome.\n\nlibrary(tidymodels)\nlibrary(rules)\ndata(\"Chicago\")\ndim(Chicago)\n#&gt; [1] 5698   50\nstations\n#&gt;  [1] \"Austin\"           \"Quincy_Wells\"     \"Belmont\"          \"Archer_35th\"     \n#&gt;  [5] \"Oak_Park\"         \"Western\"          \"Clark_Lake\"       \"Clinton\"         \n#&gt;  [9] \"Merchandise_Mart\" \"Irving_Park\"      \"Washington_Wells\" \"Harlem\"          \n#&gt; [13] \"Monroe\"           \"Polk\"             \"Ashland\"          \"Kedzie\"          \n#&gt; [17] \"Addison\"          \"Jefferson_Park\"   \"Montrose\"         \"California\"\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#splitting-with-chicago-data",
    "href": "slides/extras-transit-case-study.html#splitting-with-chicago-data",
    "title": "Case Study on Transportation",
    "section": "Splitting with Chicago data ",
    "text": "Splitting with Chicago data \nLetâ€™s put the last two weeks of data into the test set. initial_time_split() can be used for this purpose:\n\ndata(Chicago)\n\nchi_split &lt;- initial_time_split(Chicago, prop = 1 - (14/nrow(Chicago)))\nchi_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;5684/14/5698&gt;\n\nchi_train &lt;- training(chi_split)\nchi_test  &lt;- testing(chi_split)\n\n## training\nnrow(chi_train)\n#&gt; [1] 5684\n \n## testing\nnrow(chi_test)\n#&gt; [1] 14"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#time-series-resampling",
    "href": "slides/extras-transit-case-study.html#time-series-resampling",
    "title": "Case Study on Transportation",
    "section": "Time series resampling",
    "text": "Time series resampling\nOur Chicago data is over time. Regular cross-validation, which uses random sampling, may not be the best idea.\nWe can emulate our training/test split by making similar resamples.\n\nFold 1: Take the first X years of data as the analysis set, the next 2 weeks as the assessment set.\nFold 2: Take the first X years + 2 weeks of data as the analysis set, the next 2 weeks as the assessment set.\nand so on"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#rolling-forecast-origin-resampling",
    "href": "slides/extras-transit-case-study.html#rolling-forecast-origin-resampling",
    "title": "Case Study on Transportation",
    "section": "Rolling forecast origin resampling",
    "text": "Rolling forecast origin resampling\n\n\nThis image shows overlapping assessment sets. We will use non-overlapping data but it could be done wither way."
  },
  {
    "objectID": "slides/extras-transit-case-study.html#times-series-resampling",
    "href": "slides/extras-transit-case-study.html#times-series-resampling",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train |&gt;\n  sliding_period(\n    index = \"date\",  \n\n\n\n\n  )\n\nUse the date column to find the date data."
  },
  {
    "objectID": "slides/extras-transit-case-study.html#times-series-resampling-1",
    "href": "slides/extras-transit-case-study.html#times-series-resampling-1",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train |&gt;\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n\n\n\n  )\n\nOur units will be weeks."
  },
  {
    "objectID": "slides/extras-transit-case-study.html#times-series-resampling-2",
    "href": "slides/extras-transit-case-study.html#times-series-resampling-2",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train |&gt;\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15  \n    \n    \n  )\n\nEvery analysis set has 15 years of data"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#times-series-resampling-3",
    "href": "slides/extras-transit-case-study.html#times-series-resampling-3",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train |&gt;\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n\n  )\n\nEvery assessment set has 2 weeks of data"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#times-series-resampling-4",
    "href": "slides/extras-transit-case-study.html#times-series-resampling-4",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train |&gt;\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n    step = 2 \n  )\n\nIncrement by 2 weeks so that there are no overlapping assessment sets.\n\nchi_rs$splits[[1]] |&gt; assessment() |&gt; pluck(\"date\") |&gt; range()\n#&gt; [1] \"2016-01-07\" \"2016-01-20\"\nchi_rs$splits[[2]] |&gt; assessment() |&gt; pluck(\"date\") |&gt; range()\n#&gt; [1] \"2016-01-21\" \"2016-02-03\""
  },
  {
    "objectID": "slides/extras-transit-case-study.html#our-resampling-object",
    "href": "slides/extras-transit-case-study.html#our-resampling-object",
    "title": "Case Study on Transportation",
    "section": "Our resampling object ",
    "text": "Our resampling object \n\n\n\nchi_rs\n#&gt; # Sliding period resampling \n#&gt; # A tibble: 16 Ã— 2\n#&gt;    splits            id     \n#&gt;    &lt;list&gt;            &lt;chr&gt;  \n#&gt;  1 &lt;split [5463/14]&gt; Slice01\n#&gt;  2 &lt;split [5467/14]&gt; Slice02\n#&gt;  3 &lt;split [5467/14]&gt; Slice03\n#&gt;  4 &lt;split [5467/14]&gt; Slice04\n#&gt;  5 &lt;split [5467/14]&gt; Slice05\n#&gt;  6 &lt;split [5467/14]&gt; Slice06\n#&gt;  7 &lt;split [5467/14]&gt; Slice07\n#&gt;  8 &lt;split [5467/14]&gt; Slice08\n#&gt;  9 &lt;split [5467/14]&gt; Slice09\n#&gt; 10 &lt;split [5467/14]&gt; Slice10\n#&gt; 11 &lt;split [5467/14]&gt; Slice11\n#&gt; 12 &lt;split [5467/14]&gt; Slice12\n#&gt; 13 &lt;split [5467/14]&gt; Slice13\n#&gt; 14 &lt;split [5467/14]&gt; Slice14\n#&gt; 15 &lt;split [5467/14]&gt; Slice15\n#&gt; 16 &lt;split [5467/11]&gt; Slice16\n\n\n\n\nWe will fit 16 models on 16 slightly different analysis sets.\nEach will produce a separate performance metrics.\nWe will average the 16 metrics to get the resampling estimate of that statistic."
  },
  {
    "objectID": "slides/extras-transit-case-study.html#feature-engineering-with-recipes",
    "href": "slides/extras-transit-case-study.html#feature-engineering-with-recipes",
    "title": "Case Study on Transportation",
    "section": "Feature engineering with recipes ",
    "text": "Feature engineering with recipes \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train)\n\nBased on the formula, the function assigns columns to roles of â€œoutcomeâ€ or â€œpredictorâ€"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#a-recipe",
    "href": "slides/extras-transit-case-study.html#a-recipe",
    "title": "Case Study on Transportation",
    "section": "A recipe",
    "text": "A recipe\n\nsummary(chi_rec)\n#&gt; # A tibble: 50 Ã— 4\n#&gt;    variable         type      role      source  \n#&gt;    &lt;chr&gt;            &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 Austin           &lt;chr [2]&gt; predictor original\n#&gt;  2 Quincy_Wells     &lt;chr [2]&gt; predictor original\n#&gt;  3 Belmont          &lt;chr [2]&gt; predictor original\n#&gt;  4 Archer_35th      &lt;chr [2]&gt; predictor original\n#&gt;  5 Oak_Park         &lt;chr [2]&gt; predictor original\n#&gt;  6 Western          &lt;chr [2]&gt; predictor original\n#&gt;  7 Clark_Lake       &lt;chr [2]&gt; predictor original\n#&gt;  8 Clinton          &lt;chr [2]&gt; predictor original\n#&gt;  9 Merchandise_Mart &lt;chr [2]&gt; predictor original\n#&gt; 10 Irving_Park      &lt;chr [2]&gt; predictor original\n#&gt; # â„¹ 40 more rows"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#a-recipe---work-with-dates",
    "href": "slides/extras-transit-case-study.html#a-recipe---work-with-dates",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) |&gt; \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) \n\nThis creates three new columns in the data based on the date. Note that the day-of-the-week column is a factor."
  },
  {
    "objectID": "slides/extras-transit-case-study.html#a-recipe---work-with-dates-1",
    "href": "slides/extras-transit-case-study.html#a-recipe---work-with-dates-1",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) |&gt; \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) |&gt; \n  step_holiday(date) \n\nAdd indicators for major holidays. Specific holidays, especially those non-USA, can also be generated.\nAt this point, we donâ€™t need date anymore. Instead of deleting it (there is a step for that) we will change its role to be an identification variable.\n\nWe might want to change the role (instead of removing the column) because it will stay in the data set (even when resampled) and might be useful for diagnosing issues."
  },
  {
    "objectID": "slides/extras-transit-case-study.html#a-recipe---work-with-dates-2",
    "href": "slides/extras-transit-case-study.html#a-recipe---work-with-dates-2",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) |&gt; \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) |&gt; \n  step_holiday(date) |&gt; \n  update_role(date, new_role = \"id\") |&gt;\n  update_role_requirements(role = \"id\", bake = TRUE)\n\ndate is still in the data set but tidymodels knows not to treat it as an analysis column.\nupdate_role_requirements() is needed to make sure that this column is required when making new data points. The help page has a good discussion about the nuances."
  },
  {
    "objectID": "slides/extras-transit-case-study.html#a-recipe---remove-constant-columns",
    "href": "slides/extras-transit-case-study.html#a-recipe---remove-constant-columns",
    "title": "Case Study on Transportation",
    "section": "A recipe - remove constant columns ",
    "text": "A recipe - remove constant columns \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) |&gt; \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) |&gt; \n  step_holiday(date) |&gt; \n  update_role(date, new_role = \"id\") |&gt;\n  update_role_requirements(role = \"id\", bake = TRUE) |&gt; \n  step_zv(all_nominal_predictors())"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#a-recipe---handle-correlations",
    "href": "slides/extras-transit-case-study.html#a-recipe---handle-correlations",
    "title": "Case Study on Transportation",
    "section": "A recipe - handle correlations ",
    "text": "A recipe - handle correlations \nThe station columns have a very high degree of correlation.\nWe might want to decorrelated them with principle component analysis to help the model fits go more easily.\nThe vector stations contains all station names and can be used to identify all the relevant columns.\n\nchi_pca_rec &lt;- \n  chi_rec |&gt; \n  step_normalize(all_of(!!stations)) |&gt; \n  step_pca(all_of(!!stations), num_comp = tune())\n\nWeâ€™ll tune the number of PCA components for (default) values of one to four."
  },
  {
    "objectID": "slides/extras-transit-case-study.html#make-some-models",
    "href": "slides/extras-transit-case-study.html#make-some-models",
    "title": "Case Study on Transportation",
    "section": "Make some models     ",
    "text": "Make some models     \nLetâ€™s try three models. The first one requires the rules package (loaded earlier).\n\ncb_spec &lt;- cubist_rules(committees = 25, neighbors = tune())\nmars_spec &lt;- mars(prod_degree = tune()) |&gt; set_mode(\"regression\")\nlm_spec &lt;- linear_reg()\n\nchi_set &lt;- \n  workflow_set(\n    list(pca = chi_pca_rec, basic = chi_rec), \n    list(cubist = cb_spec, mars = mars_spec, lm = lm_spec)\n  ) |&gt; \n  # Evaluate models using mean absolute errors\n  option_add(metrics = metric_set(mae))\n\n\nBriefly talk about Cubist being a (sort of) boosted rule-based model and MARS being a nonlinear regression model. Both incorporate feature selection nicely."
  },
  {
    "objectID": "slides/extras-transit-case-study.html#process-them-on-the-resamples",
    "href": "slides/extras-transit-case-study.html#process-them-on-the-resamples",
    "title": "Case Study on Transportation",
    "section": "Process them on the resamples",
    "text": "Process them on the resamples\n\n# Set up some objects for stacking ensembles (in a few slides)\ngrid_ctrl &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\nchi_res &lt;- \n  chi_set |&gt; \n  workflow_map(\n    resamples = chi_rs,\n    grid = 10,\n    control = grid_ctrl,\n    verbose = TRUE,\n    seed = 12\n  )"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#how-do-the-results-look",
    "href": "slides/extras-transit-case-study.html#how-do-the-results-look",
    "title": "Case Study on Transportation",
    "section": "How do the results look?",
    "text": "How do the results look?\n\nrank_results(chi_res)\n#&gt; # A tibble: 34 Ã— 9\n#&gt;    wflow_id     .config          .metric  mean std_err     n preprocessor model       rank\n#&gt;    &lt;chr&gt;        &lt;chr&gt;            &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;      &lt;int&gt;\n#&gt;  1 pca_cubist   pre2_mod01_post0 mae     0.804   0.101    16 recipe       cubist_ruâ€¦     1\n#&gt;  2 basic_cubist pre0_mod1_post0  mae     0.901   0.115    16 recipe       cubist_ruâ€¦     2\n#&gt;  3 pca_cubist   pre3_mod10_post0 mae     0.985   0.120    16 recipe       cubist_ruâ€¦     3\n#&gt;  4 pca_cubist   pre4_mod08_post0 mae     0.990   0.121    16 recipe       cubist_ruâ€¦     4\n#&gt;  5 pca_cubist   pre2_mod07_post0 mae     1.00    0.119    16 recipe       cubist_ruâ€¦     5\n#&gt;  6 pca_cubist   pre4_mod05_post0 mae     1.02    0.131    16 recipe       cubist_ruâ€¦     6\n#&gt;  7 pca_cubist   pre1_mod09_post0 mae     1.03    0.121    16 recipe       cubist_ruâ€¦     7\n#&gt;  8 pca_cubist   pre1_mod06_post0 mae     1.07    0.137    16 recipe       cubist_ruâ€¦     8\n#&gt;  9 basic_cubist pre0_mod9_post0  mae     1.07    0.115    16 recipe       cubist_ruâ€¦     9\n#&gt; 10 basic_cubist pre0_mod8_post0  mae     1.07    0.114    16 recipe       cubist_ruâ€¦    10\n#&gt; # â„¹ 24 more rows"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#plot-the-results",
    "href": "slides/extras-transit-case-study.html#plot-the-results",
    "title": "Case Study on Transportation",
    "section": "Plot the results  ",
    "text": "Plot the results  \n\nautoplot(chi_res)"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#pull-out-specific-results",
    "href": "slides/extras-transit-case-study.html#pull-out-specific-results",
    "title": "Case Study on Transportation",
    "section": "Pull out specific results  ",
    "text": "Pull out specific results  \nWe can also pull out the specific tuning results and look at them:\n\nchi_res |&gt; \n  extract_workflow_set_result(\"pca_cubist\") |&gt; \n  autoplot()"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#why-choose-just-one-final_fit",
    "href": "slides/extras-transit-case-study.html#why-choose-just-one-final_fit",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit? \nModel stacks generate predictions that are informed by several models."
  },
  {
    "objectID": "slides/extras-transit-case-study.html#why-choose-just-one-final_fit-1",
    "href": "slides/extras-transit-case-study.html#why-choose-just-one-final_fit-1",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#why-choose-just-one-final_fit-2",
    "href": "slides/extras-transit-case-study.html#why-choose-just-one-final_fit-2",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#why-choose-just-one-final_fit-3",
    "href": "slides/extras-transit-case-study.html#why-choose-just-one-final_fit-3",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#why-choose-just-one-final_fit-4",
    "href": "slides/extras-transit-case-study.html#why-choose-just-one-final_fit-4",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#why-choose-just-one-final_fit-5",
    "href": "slides/extras-transit-case-study.html#why-choose-just-one-final_fit-5",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#building-a-model-stack",
    "href": "slides/extras-transit-case-study.html#building-a-model-stack",
    "title": "Case Study on Transportation",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlibrary(stacks)\n\n\nDefine candidate members\nInitialize a data stack object\nAdd candidate ensemble members to the data stack\nEvaluate how to combine their predictions\nFit candidate ensemble members with non-zero stacking coefficients\nPredict on new data!"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#start-the-stack-and-add-members",
    "href": "slides/extras-transit-case-study.html#start-the-stack-and-add-members",
    "title": "Case Study on Transportation",
    "section": "Start the stack and add members ",
    "text": "Start the stack and add members \nCollect all of the resampling results for all model configurations.\n\nchi_stack &lt;- \n  stacks() |&gt; \n  add_candidates(chi_res)"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#estimate-weights-for-each-candidate",
    "href": "slides/extras-transit-case-study.html#estimate-weights-for-each-candidate",
    "title": "Case Study on Transportation",
    "section": "Estimate weights for each candidate ",
    "text": "Estimate weights for each candidate \nWhich configurations should be retained? Uses a penalized linear model:\n\nset.seed(122)\nchi_stack_res &lt;- blend_predictions(chi_stack, penalty = 10^seq(-6, -1, length.out = 25))\n\nchi_stack_res\n#&gt; # A tibble: 5 Ã— 3\n#&gt;   member             type         weight\n#&gt;   &lt;chr&gt;              &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 pca_cubist_2_01_0  cubist_rules 0.358 \n#&gt; 2 basic_cubist_0_1_0 cubist_rules 0.270 \n#&gt; 3 pca_cubist_2_07_0  cubist_rules 0.202 \n#&gt; 4 basic_cubist_0_6_0 cubist_rules 0.144 \n#&gt; 5 pca_lm_1_0_0       linear_reg   0.0381"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#how-did-it-do",
    "href": "slides/extras-transit-case-study.html#how-did-it-do",
    "title": "Case Study on Transportation",
    "section": "How did it do?  ",
    "text": "How did it do?  \nThe overall results of the penalized model:\n\nautoplot(chi_stack_res)"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#what-does-it-use",
    "href": "slides/extras-transit-case-study.html#what-does-it-use",
    "title": "Case Study on Transportation",
    "section": "What does it use?  ",
    "text": "What does it use?  \n\nautoplot(chi_stack_res, type = \"weights\")"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#fit-the-required-candidate-models",
    "href": "slides/extras-transit-case-study.html#fit-the-required-candidate-models",
    "title": "Case Study on Transportation",
    "section": "Fit the required candidate models",
    "text": "Fit the required candidate models\nFor each model we retain in the stack, we need their model fit on the entire training set.\n\nchi_stack_res &lt;- fit_members(chi_stack_res)"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#the-test-set-best-cubist-model",
    "href": "slides/extras-transit-case-study.html#the-test-set-best-cubist-model",
    "title": "Case Study on Transportation",
    "section": "The test set: best Cubist model  ",
    "text": "The test set: best Cubist model  \nWe can pull out the results and the workflow to fit the single best cubist model.\n\nbest_cubist &lt;- \n  chi_res |&gt; \n  extract_workflow_set_result(\"pca_cubist\") |&gt; \n  select_best()\n\ncubist_res &lt;- \n  chi_res |&gt; \n  extract_workflow(\"pca_cubist\") |&gt; \n  finalize_workflow(best_cubist) |&gt; \n  last_fit(split = chi_split, metrics = metric_set(mae))"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#the-test-set-stack-ensemble",
    "href": "slides/extras-transit-case-study.html#the-test-set-stack-ensemble",
    "title": "Case Study on Transportation",
    "section": "The test set: stack ensemble",
    "text": "The test set: stack ensemble\nWe donâ€™t have last_fit() for stacks (yet) so we manually make predictions.\n\nstack_pred &lt;- \n  predict(chi_stack_res, chi_test) |&gt; \n  bind_cols(chi_test)"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#compare-the-results",
    "href": "slides/extras-transit-case-study.html#compare-the-results",
    "title": "Case Study on Transportation",
    "section": "Compare the results  ",
    "text": "Compare the results  \nSingle best versus the stack:\n\ncollect_metrics(cubist_res)\n#&gt; # A tibble: 1 Ã— 4\n#&gt;   .metric .estimator .estimate .config        \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1 mae     standard       0.669 pre0_mod0_post0\n\nstack_pred |&gt; mae(ridership, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       0.647"
  },
  {
    "objectID": "slides/extras-transit-case-study.html#plot-the-test-set",
    "href": "slides/extras-transit-case-study.html#plot-the-test-set",
    "title": "Case Study on Transportation",
    "section": "Plot the test set  ",
    "text": "Plot the test set  \n\n\nlibrary(probably)\ncubist_res |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(ridership, .pred)) + \n  geom_point(alpha = 1 / 2) + \n  geom_abline(lty = 2, col = \"green\") + \n  coord_obs_pred()"
  },
  {
    "objectID": "slides/annotations.html#section",
    "href": "slides/annotations.html#section",
    "title": "Annotations",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€\nThis page contains annotations for selected slides.\nThereâ€™s a lot that we want to tell you. We donâ€™t want people to have to frantically scribble down things that we say that are not on the slides.\nWeâ€™ve added sections to this document with longer explanations and links to other resources."
  },
  {
    "objectID": "slides/annotations.html#data-splitting-and-spending",
    "href": "slides/annotations.html#data-splitting-and-spending",
    "title": "Annotations",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nMore about the initial data split can be found in Chapter 3 of Applied Machine Learning for Tabular Data (AML4TD).\nIn particular, a three-way split into training, validation, and testing set can be done via\n\nset.seed(123)\ninitial_validation_split(forested, prop = c(0.6, 0.2))\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;4264/1421/1422/7107&gt;"
  },
  {
    "objectID": "slides/annotations.html#what-is-set.seed",
    "href": "slides/annotations.html#what-is-set.seed",
    "title": "Annotations",
    "section": "What is set.seed()?",
    "text": "What is set.seed()?\nWhat does set.seed() do?\nWeâ€™ll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random).\nThink of PRN as a box that takes a starting value (the â€œseedâ€) that produces random numbers using that starting value as an input into its process.\nIf we know a seed value, we can reproduce our â€œrandomâ€ numbers. To use a different set of random numbers, choose a different seed value.\nFor example:\n\nset.seed(1)\nrunif(3)\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\n# Get a new set of random numbers:\nset.seed(2)\nrunif(3)\n#&gt; [1] 0.1848823 0.7023740 0.5733263\n\n# We can reproduce the old ones with the same seed\nset.seed(1)\nrunif(3)\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\nIf we donâ€™t set the seed, R uses the clock time and the process ID to create a seed. This isnâ€™t reproducible.\nSince we want our code to be reproducible, we set the seeds before random numbers are used.\nIn theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same, and we donâ€™t get reproducible results.\nThe value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to â€œspread the randomness aroundâ€. It is basically:\n\ncat(paste0(\"set.seed(\", sample.int(10000, 5), \")\", collapse = \"\\n\"))\n#&gt; set.seed(9725)\n#&gt; set.seed(8462)\n#&gt; set.seed(4050)\n#&gt; set.seed(8789)\n#&gt; set.seed(1301)"
  },
  {
    "objectID": "slides/annotations.html#understand-your-model",
    "href": "slides/annotations.html#understand-your-model",
    "title": "Annotations",
    "section": "Understand your model",
    "text": "Understand your model\nOn the next slide, weâ€™ve abbreviated the code necessary to make this plot. If you want to replicate the plot exactly, youâ€™ll need:\nlibrary(rpart.plot)\nsplit_fun &lt;- function(x, labs, digits, varlen, faclen) {\n  for (i in 1:length(labs)) {\n    if (grepl(\",\", labs[i])) {\n      parts &lt;- strsplit(labs[i], \",\")[[1]]\n      parts &lt;- trimws(parts)\n      if (length(parts) &gt; 5) {\n        parts &lt;- c(parts[1:5], \"...\")\n      }\n      labs[i] &lt;- paste(parts, collapse = \", \")\n    }\n    labs[i] &lt;- paste(strwrap(labs[i], width = 15), collapse = \"\\n\")\n  }\n  labs\n}\n\ntree_fit |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot(\n    roundint = FALSE,\n    type = 3,\n    clip.right.labs = FALSE,\n    split.fun = split_fun\n  )"
  },
  {
    "objectID": "slides/annotations.html#what-is-wrong-with-this",
    "href": "slides/annotations.html#what-is-wrong-with-this",
    "title": "Annotations",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?\nIf we treat the preprocessing as a separate task, it raises the risk that we might accidentally overfit to the data at hand.\nFor example, someone might estimate something from the entire data set (such as the principal components) and treat that data as if it were known (and not estimated). Depending on what was done with the data, the consequences of doing that could be:\n\nYour performance metrics are slightly-to-moderately optimistic (e.g., you might think your accuracy is 85% when it is actually 75%)\nA consequential component of the analysis is not right, and the model just doesnâ€™t work.\n\nThe big issue here is that you wonâ€™t be able to figure this out until you get a new piece of data, such as the test set.\nA really good example of this is in â€˜Selection bias in gene extraction on the basis of microarray gene-expression dataâ€™. The authors re-analyze a previous publication and show that the original researchers did not include feature selection in the workflow. Because of that, their performance statistics were extremely optimistic. In one case, they could do the original analysis on complete noise and still achieve zero errors.\nGenerally speaking, this problem is referred to as data leakage. Some other references:\n\nOverfitting to Predictors and External Validation\nAre We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning\nNavigating the pitfalls of applying machine learning in genomics\nA review of feature selection techniques in bioinformatics\nOn Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation"
  },
  {
    "objectID": "slides/annotations.html#brier-score",
    "href": "slides/annotations.html#brier-score",
    "title": "Annotations",
    "section": "Brier score",
    "text": "Brier score\nThe Brier score measures how close a model probability estimate is to its best possible value (i.e., zero or one).\nIn the best case, the model is perfect, and every prediction equals 0.0 or 1.0 (depending on the true class). In this case, the Brier score is zero.\nWhen the model is uninformative and there are two classes, the worst-case values range from 0.25 to about 0.50. Imagine that the model predicts the same noninformative prediction of 50% (basically â€œÂ¯\\(ãƒ„)/Â¯â€). In that case, every prediction is either \\((0.00 - 0.50)^2\\) or \\((1.00 - 0.50)^2\\). The average of those is 0.25.\nThere are many different ways a model can be bad, though, and some of these will produce Brier scores between 0.25 and 0.50."
  },
  {
    "objectID": "slides/annotations.html#dangers-of-overfitting",
    "href": "slides/annotations.html#dangers-of-overfitting",
    "title": "Annotations",
    "section": "Dangers of overfitting",
    "text": "Dangers of overfitting\nSee the â€œOverfittingâ€ chapter of AML4TD for more information."
  },
  {
    "objectID": "slides/annotations.html#where-are-the-fitted-models",
    "href": "slides/annotations.html#where-are-the-fitted-models",
    "title": "Annotations",
    "section": "Where are the fitted models?",
    "text": "Where are the fitted models?\nThe primary purpose of resampling is to estimate model performance. The models are almost never needed again.\nAlso, if the data set is large, the model object may require a lot of memory to save so, by default, we donâ€™t keep them.\nFor more advanced use cases, you can extract and save them. See:\n\nhttps://www.tmwr.org/resampling.html#extract\nhttps://www.tidymodels.org/learn/models/coefficients/ (an example)"
  },
  {
    "objectID": "slides/annotations.html#resampling-strategy",
    "href": "slides/annotations.html#resampling-strategy",
    "title": "Annotations",
    "section": "Resampling Strategy",
    "text": "Resampling Strategy\nThese data have a time component, and while not a typical time series data set, we have the option to use a time series resampling method.\nAn example is shown in the extra slides â€œCase Study on Transportationâ€.\nConsider the agent data. Cross-validation may not group all of an agentâ€™s data into the analysis or assessment sets. In this case, our analysis data might have future data that is later than the agentâ€™s data in the assessment set."
  },
  {
    "objectID": "slides/annotations.html#different-types-of-grids",
    "href": "slides/annotations.html#different-types-of-grids",
    "title": "Annotations",
    "section": "Different types of grids",
    "text": "Different types of grids\nMore on space-filling designs in Chapters 4 and 5 of Surrogates: Gaussian process modeling, design, and optimization for the applied sciences.\nThese designs also scale well as the number of tuning parameters grows. They are designed to fill the predictor space efficiently."
  },
  {
    "objectID": "slides/annotations.html#extract-and-update-parameters",
    "href": "slides/annotations.html#extract-and-update-parameters",
    "title": "Annotations",
    "section": "Extract and update parameters",
    "text": "Extract and update parameters\nIn about 90% of the cases, the dials function that you use to update the parameter range has the same name as the argument. For example, if you were to update the mtry parameter in a random forest model, the code would look like\n\nparameter_object |&gt; \n  update(mtry = mtry(c(1, 100)))\n\nIn some cases, the parameter function or its associated values differ from the argument name.\nFor example, with step_spline_naturall(), we might want to tune the deg_free argument (for the degrees of freedom of a spline function). In this case, the argument name is deg_free, but we update it with spline_degree().\ndeg_free represents the general concept of degrees of freedom and could be associated with many different things. For example, if we ever had an argument that was the number of degrees of freedom for a \\(t\\) distribution, we would call that argument deg_free.\nWe probably want a wider range of degrees of freedom for splines. To this end, we created a specialized function called spline_degree().\nHow can you tell when this happens? There is a helper function called tunable(), and that gives information on how we make the default ranges for parameters. There is a column in these objects names call_info:\n\nlibrary(tidymodels)\nns_tunable &lt;- \n  recipe(mpg ~ ., data = mtcars) |&gt; \n  step_spline_natural(dis, deg_free = tune()) |&gt; \n  tunable()\n\nns_tunable\n#&gt; # A tibble: 1 Ã— 5\n#&gt;   name     call_info        source component           component_id        \n#&gt;   &lt;chr&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;chr&gt;               &lt;chr&gt;               \n#&gt; 1 deg_free &lt;named list [3]&gt; recipe step_spline_natural spline_natural_P1Tjg\nns_tunable$call_info\n#&gt; [[1]]\n#&gt; [[1]]$pkg\n#&gt; [1] \"dials\"\n#&gt; \n#&gt; [[1]]$fun\n#&gt; [1] \"spline_degree\"\n#&gt; \n#&gt; [[1]]$range\n#&gt; [1]  2 15"
  },
  {
    "objectID": "slides/annotations.html#grid-search",
    "href": "slides/annotations.html#grid-search",
    "title": "Annotations",
    "section": "Grid Search",
    "text": "Grid Search\nThere might be some warnings that\n\nâ€œEarly stopping occurred at epoch 3 due to numerical overflow of the loss function.â€\n\ntorch can be very aggressive about moving in the direction of the gradient. In some cases, it moves to a space where the gradient is not well-behaved (e.g., flat in multiple directions, saddle point, etc).\nThis can cause abnormally large parameter values with magnitudes larger than double precision variables can hold.\nIn this case, brulee stops the optimization and returns the parameters from the last best iteration."
  },
  {
    "objectID": "slides/annotations.html#running-in-parallel",
    "href": "slides/annotations.html#running-in-parallel",
    "title": "Annotations",
    "section": "Running in parallel",
    "text": "Running in parallel\nWe usually leave one or two cores available to work with when we run in parallel.\nIf you are using an HPC system, be careful to use only the cores allocated to you.\nAlso, memory is duplicated for each worker. If your main R session is using 1GB of memory, using 10 workers requires 11GB.\nComparing these methods:\n\nThey usually perform about the same.\nfuture is more mature and is geared towards users and developers.\nmirai contains its own queuing system and can be more efficient in allocating work."
  },
  {
    "objectID": "slides/annotations.html#early-stopping-for-boosted-trees",
    "href": "slides/annotations.html#early-stopping-for-boosted-trees",
    "title": "Annotations",
    "section": "Early stopping for boosted trees",
    "text": "Early stopping for boosted trees\nWhen deciding on the number of boosting iterations, there are two main strategies:\n\nDirectly tune it (trees = tune())\nSet it to one value and tune the number of early stopping iterations (trees = 500, stop_iter = tune()).\n\nEarly stopping is when we monitor the performance of the model. If the model doesnâ€™t make any improvements for stop_iter iterations, training stops.\nHereâ€™s an example where, after eleven iterations, performance starts to get worse.\n\n\n\n\n\n\n\n\n\nThis is likely due to over-fitting so we stop the model at eleven boosting iterations.\nEarly stopping usually has good results and takes far less time.\nWe could an engine argument called validation here. Thatâ€™s not an argument to any function in the lightgbm package.\nbonsai has its own wrapper around (lightgbm::lgb.train()) called bonsai::train_lightgbm(). We use that here and it has a validation argument.\nHow would you know that? There are a few different ways:\n\nLook at the documentation in ?boost_tree and click on the lightgbm entry in the engine list.\nCheck out the pkgdown reference website https://parsnip.tidymodels.org/reference/index.html\nRun the translate() function on the parsnip specification object.\n\nThe first two options are best since they tell you a lot more about the particularities of each model engine (there are a lot for lightgbm)."
  },
  {
    "objectID": "slides/annotations.html#gaussian-processes-and-optimization",
    "href": "slides/annotations.html#gaussian-processes-and-optimization",
    "title": "Annotations",
    "section": "Gaussian Processes and Optimization",
    "text": "Gaussian Processes and Optimization\nSome other references for GPâ€™s:\n\nChapter 5 of Surrogates: Gaussian process modeling, design, and optimization for the applied sciences\nBayesian Optimization, Chapter 3 (pdf)\nGaussian Processes for Machine Learning (pdf)"
  },
  {
    "objectID": "slides/annotations.html#acquisition-functions",
    "href": "slides/annotations.html#acquisition-functions",
    "title": "Annotations",
    "section": "Acquisition Functions",
    "text": "Acquisition Functions\nMore references:\n\nChapter 7 of Surrogates: Gaussian process modeling, design, and optimization for the applied sciences\nBayesian Optimization, Chapter 6 (pdf)\nGaussian Processes for Machine Learning"
  },
  {
    "objectID": "slides/annotations.html#isomap-with-recipes",
    "href": "slides/annotations.html#isomap-with-recipes",
    "title": "Annotations",
    "section": "isomap with recipes",
    "text": "isomap with recipes\nThe results from Isomap, unlike UMAP, cannot be distorted in the same way. Here, the number of neighbors changes between the charts. Each of the charts is an application of Isomap on the same completely random data. The points are in different locations, but none of the charts appear to show any patterns."
  },
  {
    "objectID": "slides/annotations.html#per-agent-statistics",
    "href": "slides/annotations.html#per-agent-statistics",
    "title": "Annotations",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics\nThe effect encoding method essentially takes the effect of a variable, like an agent, and creates a data column for that effect. In our example, the agentâ€™s effect on the ADR is quantified by a model and then added as a data column to be used in the model.\nSuppose agent Max has a single reservation in the data, with an ADR of â‚¬200. If we use a naive estimate for Maxâ€™s effect, the model is told that Max should always produce an effect of â‚¬200. Thatâ€™s a very poor estimate since it is from a single data point.\nContrast this with seasoned agent Davis, who has taken 250 reservations with an average ADR of â‚¬100. Davisâ€™s mean is more predictive because it is estimated with better data (i.e., more total reservations). Partial pooling leverages the entire data set and can borrow strength from all of the agents. It is a common tool in Bayesian estimation and non-Bayesian mixed models. If an agentâ€™s data is of good quality, the partial pooling effect estimate is closer to the raw mean. Maxâ€™s data is not great and is â€œshrunkâ€ towards the center of the overall average. Since there is so little known about Maxâ€™s reservation history, this is a better effect estimate (until more data is available for him).\nThe Stan documentation has a pretty good vignette on this: https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html\nAlso, Bayes Rules! has a nice section on this: https://www.bayesrulesbook.com/chapter-15.html\nSince this example has a numeric outcome, partial pooling is very similar to the Jamesâ€“Stein estimator: https://en.wikipedia.org/wiki/Jamesâ€“Stein_estimator"
  },
  {
    "objectID": "slides/annotations.html#agent-effects",
    "href": "slides/annotations.html#agent-effects",
    "title": "Annotations",
    "section": "Agent effects",
    "text": "Agent effects\nEffect encoding might result in a somewhat circular argument: the column is more likely to be important to the model since it is the output of a separate model. The risk here is that we might overfit the effect to the data. For this reason, it is super important to verify that we arenâ€™t overfitting by checking with resampling (or a validation set).\nPartial pooling somewhat lowers the risk of overfitting since it tends to correct for agents with small sample sizes. However, it canâ€™t correct for improper data usage or data leakage."
  },
  {
    "objectID": "slides/annotations.html#a-recipe---handle-correlations",
    "href": "slides/annotations.html#a-recipe---handle-correlations",
    "title": "Annotations",
    "section": "A recipe - handle correlations",
    "text": "A recipe - handle correlations\nIn this code chunk, whatâ€™s the story with !!stations?\n\nchi_pca_rec &lt;- \n  chi_rec |&gt; \n  step_normalize(all_of(!!stations)) |&gt; \n  step_pca(all_of(!!stations), num_comp = tune())\n\nstations is a vector of names of 20 columns that we want to use in the steps. If the list were shorter, we could type them in (e.g., c(\"col1\", \"col2\"), etc.).\nThe vector lives in our global workspace, and if we are in parallel, the worker processes might not have access to stations. The !! (frequently said as â€œbang bangâ€) inserts the actual contents of the vector into the all_of() calls so that it looks like you just typed it in.\nThis means that the parallel process workers have a copy of the data in their reach, and the code will run without error."
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#startup",
    "href": "slides/advanced-07-feature-selection.html#startup",
    "title": "7 - Feature selection",
    "section": "Startup!  ",
    "text": "Startup!  \n\nlibrary(tidymodels)\nlibrary(important)\nlibrary(probably)\nlibrary(mirai)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\ndaemons(parallel::detectCores())"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#more-startup",
    "href": "slides/advanced-07-feature-selection.html#more-startup",
    "title": "7 - Feature selection",
    "section": "More startup! ",
    "text": "More startup! \n\n# Load our example data for this section\n\"https://raw.githubusercontent.com/tidymodels/\" |&gt; \n  paste0(\"workshops/main/slides/class_data.RData\") |&gt; \n  url() |&gt; \n  load()\n\nset.seed(429)\nsim_split &lt;- initial_split(class_data, prop = 0.75, strata = class)\nsim_train &lt;- training(sim_split)\nsim_test  &lt;- testing(sim_split)\n\nset.seed(523)\nsim_rs &lt;- vfold_cv(sim_train, v = 10, strata = class)"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#models-and-feature-selection",
    "href": "slides/advanced-07-feature-selection.html#models-and-feature-selection",
    "title": "7 - Feature selection",
    "section": "Models and feature selection",
    "text": "Models and feature selection\nSome models automatically remove predictors by never using them in the model:\n\ntree- and rule-based models\nsome regularized models (e.g., glmnet)\nmultivariate adaptive regression splines (MARS)\nRuleFit\nnot really ensembles though\n\nSometimes using irrelevant predictors hurts model performance."
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#effects-of-extra-predictors",
    "href": "slides/advanced-07-feature-selection.html#effects-of-extra-predictors",
    "title": "7 - Feature selection",
    "section": "Effects of extra predictors",
    "text": "Effects of extra predictors\n\n\nAML4TD"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#general-selection-methods",
    "href": "slides/advanced-07-feature-selection.html#general-selection-methods",
    "title": "7 - Feature selection",
    "section": "General selection methods",
    "text": "General selection methods\n\nwrappers: a sequential algorithm proposes feature subsets, fits the model with these subsets, and then determines a better subset from the results.\nfilters: screen predictors before adding them to the model.\n\ntidymodels doesnâ€™t have any wrappers (but see the caret documentation for them)\n\nThe new important package does have filters via recipes."
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#be-careful",
    "href": "slides/advanced-07-feature-selection.html#be-careful",
    "title": "7 - Feature selection",
    "section": "Be careful!!!",
    "text": "Be careful!!!\ntidymodels has always contained some â€œhidden guardrailsâ€ that should prevent practitioners from making subtle (but consequential) methodological mistakes.\n\nFeature selection is a good example. Based on the literature, it is easily done wrong.\n\nThe selection process should take place inside a resampling loop so that the workflow does not overfit the predictors.\n\nA&M (2002), AML4TD, FES"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#important",
    "href": "slides/advanced-07-feature-selection.html#important",
    "title": "7 - Feature selection",
    "section": "IMPORTANT ",
    "text": "IMPORTANT \nWe released two packages this year that enable supervised feature selection:\n\nfiltro: low-level scoring methods for predictors (e.g., importance).\nimportant: tools for permutation importance and recipes steps for supervised feature selection.\n\n\nLetâ€™s look at the help page for important::step_predictor_best()."
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#k-nearest-neighbors",
    "href": "slides/advanced-07-feature-selection.html#k-nearest-neighbors",
    "title": "7 - Feature selection",
    "section": "K-nearest neighbors     ",
    "text": "K-nearest neighbors     \n\nrec &lt;-\n  recipe(class ~ ., data = sim_train) |&gt;\n  step_predictor_best(\n    all_predictors(),\n    score = \"imp_rf\",\n    prop_terms = tune(),\n    id = \"filter\"\n  ) |&gt;\n  step_normalize(all_numeric_predictors())\n  \nknn_spec &lt;- \n  nearest_neighbor(neighbors = tune(), weight_func = tune()) |&gt; \n  set_mode(\"classification\")\n  \nthrsh_tlr &lt;-\n  tailor() |&gt;\n  adjust_probability_threshold(threshold = tune())"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#setup-the-workflow",
    "href": "slides/advanced-07-feature-selection.html#setup-the-workflow",
    "title": "7 - Feature selection",
    "section": "Setup the workflow  ",
    "text": "Setup the workflow  \n\nknn_wflow &lt;- workflow(rec, knn_spec, thrsh_tlr)\n\nknn_param &lt;-\n  knn_wflow |&gt;\n  extract_parameter_set_dials() |&gt;\n  update(\n    threshold = threshold(c(0.001, 0.1)),\n    neighbors = neighbors(c(1, 50))\n  )"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#tuning-results",
    "href": "slides/advanced-07-feature-selection.html#tuning-results",
    "title": "7 - Feature selection",
    "section": "Tuning results  ",
    "text": "Tuning results  \n\ncls_mtr &lt;- metric_set(brier_class, roc_auc, sensitivity, specificity)\nctrl &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(12)\nknn_res &lt;-\n  knn_wflow |&gt;\n  tune_grid(\n    resamples = sim_rs,\n    grid = 50,\n    control = ctrl,\n    metrics = cls_mtr,\n    param_info = knn_param\n  )"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#grid-results",
    "href": "slides/advanced-07-feature-selection.html#grid-results",
    "title": "7 - Feature selection",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(knn_res)"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#brier-results",
    "href": "slides/advanced-07-feature-selection.html#brier-results",
    "title": "7 - Feature selection",
    "section": "Brier results ",
    "text": "Brier results \n\nautoplot(knn_res, metric = \"brier_class\") + \n  facet_grid(. ~ name, scale = \"free_x\")"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#roc-curve-results",
    "href": "slides/advanced-07-feature-selection.html#roc-curve-results",
    "title": "7 - Feature selection",
    "section": "ROC curve results ",
    "text": "ROC curve results \n\nautoplot(knn_res, metric = \"roc_auc\") + \n  facet_grid(. ~ name, scale = \"free_x\")"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#sensitivityspecificity-results",
    "href": "slides/advanced-07-feature-selection.html#sensitivityspecificity-results",
    "title": "7 - Feature selection",
    "section": "Sensitivity/Specificity results ",
    "text": "Sensitivity/Specificity results \n\nautoplot(knn_res, metric = c(\"sensitivity\", \"specificity\"))"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#fit-the-model-and-get-filter-information",
    "href": "slides/advanced-07-feature-selection.html#fit-the-model-and-get-filter-information",
    "title": "7 - Feature selection",
    "section": "Fit the model and get filter information  ",
    "text": "Fit the model and get filter information  \n\nknn_fit &lt;- fit_best(knn_res, metric = \"brier_class\")\nfilter_info &lt;-\n    knn_fit |&gt;\n    extract_recipe() |&gt;\n    tidy(id = \"filter\")\n\nfilter_info\n#&gt; # A tibble: 30 Ã— 4\n#&gt;    terms        removed      score id    \n#&gt;    &lt;chr&gt;        &lt;lgl&gt;        &lt;dbl&gt; &lt;chr&gt; \n#&gt;  1 predictor_01 TRUE     0.00152   filter\n#&gt;  2 predictor_02 TRUE     0.00170   filter\n#&gt;  3 predictor_03 TRUE    -0.0000460 filter\n#&gt;  4 predictor_04 TRUE     0.000968  filter\n#&gt;  5 predictor_05 TRUE    -0.0000626 filter\n#&gt;  6 predictor_06 TRUE     0.000126  filter\n#&gt;  7 predictor_07 TRUE     0.000779  filter\n#&gt;  8 predictor_08 TRUE     0.00160   filter\n#&gt;  9 predictor_09 TRUE     0.00218   filter\n#&gt; 10 predictor_10 TRUE    -0.000302  filter\n#&gt; # â„¹ 20 more rows"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#the-truth-about-our-data",
    "href": "slides/advanced-07-feature-selection.html#the-truth-about-our-data",
    "title": "7 - Feature selection",
    "section": "The truth about our data",
    "text": "The truth about our data\nThe data were simulated and 15 out of 30 predictors were uninformative (and highly correlated). How did we do?\n\n\n\n\n\n\n\n\n\n\nnoise\nreal\n\n\n\n\nkept\n0\n2\n\n\nremoved\n15\n13\n\n\n\n\n\n\n\nselection sensitivity: 13.3%\nselection specificity: 100%\n\nIt was good at removing noise but not keeping the real predictors."
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#random-forest-importance-scores",
    "href": "slides/advanced-07-feature-selection.html#random-forest-importance-scores",
    "title": "7 - Feature selection",
    "section": "Random forest importance scores",
    "text": "Random forest importance scores\n\n\n# A \"truth\" column was added\nfilter_info |&gt;\n  mutate(\n    terms = factor(terms),\n    terms = reorder(terms, score)\n  ) |&gt;\n  ggplot(\n    aes(x = score, \n        y = terms, \n        fill = truth)\n  ) +\n  geom_bar(stat = \"identity\") + \n  labs(x = \"RF Importance\", y = NULL) + \n  scale_fill_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#the-simulation",
    "href": "slides/advanced-07-feature-selection.html#the-simulation",
    "title": "7 - Feature selection",
    "section": "The simulation",
    "text": "The simulation\nThe simulation system is documented here with method = \"caret\". The two most important predictors being retained correspond to:\n\n# In logit units: \n- 4 * two_factor_1 + 4 * two_factor_2 + 2 * two_factor_1 * two_factor_2 \n\n\nMost of the others are small linear effects and tree-based models are not great at modeling those.\n\nAlso, the noise predictors were simulated to have fairly high correlations with one another. That can often compromise random forest importance scores."
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#other-steps",
    "href": "slides/advanced-07-feature-selection.html#other-steps",
    "title": "7 - Feature selection",
    "section": "Other steps",
    "text": "Other steps\nThe important package has two other feature selection steps that can be used with multiple scores:\n\n\nstep_predictor_retain(): choose predictors based on a logical statement. Example:\n\nimp_rf &gt; 2 & cor_pearson &gt;= 0.75\n\n\n\nstep_predictor_desirability(): choose multiple scores to compute then use desirability functions to rank them:\n\ndesirability(\n  maximize(correlation),\n  maximize(imp_rf)\n)"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#manual-approach",
    "href": "slides/advanced-07-feature-selection.html#manual-approach",
    "title": "7 - Feature selection",
    "section": "Manual approach  ",
    "text": "Manual approach  \nWe already have our fitted model and, if we are happy with it:\n\ntest_pred &lt;- augment(knn_fit, sim_test)\ntest_pred |&gt; cls_mtr(class, estimate = .pred_class, .pred_event)\n#&gt; # A tibble: 4 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary        0.982 \n#&gt; 2 specificity binary        0.874 \n#&gt; 3 brier_class binary        0.0246\n#&gt; 4 roc_auc     binary        0.981\n\n\nResampling estimates:\n\n#&gt; # A tibble: 4 Ã— 4\n#&gt;   .metric       mean     n std_err\n#&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1 sensitivity 0.958     10 0.0154 \n#&gt; 2 specificity 0.867     10 0.00948\n#&gt; 3 brier_class 0.0349    10 0.00201\n#&gt; 4 roc_auc     0.968     10 0.00714"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#checking-approximate-calibration",
    "href": "slides/advanced-07-feature-selection.html#checking-approximate-calibration",
    "title": "7 - Feature selection",
    "section": "Checking (Approximate) Calibration  ",
    "text": "Checking (Approximate) Calibration  \n\n\n\ntest_pred|&gt;\n  cal_plot_windowed(\n    truth = class,\n    estimate = .pred_event,\n    window_size = 0.2,\n    step_size = 0.025,\n  )\n\n\nLooks alright. The small effective sample size (57 events) makes it pretty noisy."
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#automated-approach",
    "href": "slides/advanced-07-feature-selection.html#automated-approach",
    "title": "7 - Feature selection",
    "section": "Automated approach ",
    "text": "Automated approach \nSimilar to fit_best(), there is a convenience function that can be used to get the final model and the test set results.\n\nWe have to start with a finalized workflow (i.e., no tune() values):\n\nknn_best &lt;- select_best(knn_res, metric = \"brier_class\")\nknn_last_wflow &lt;- finalize_workflow(knn_wflow, knn_best)"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#automated-approach-1",
    "href": "slides/advanced-07-feature-selection.html#automated-approach-1",
    "title": "7 - Feature selection",
    "section": "Automated approach",
    "text": "Automated approach\nlast_fit() uses the original split object to fit, predict, and measure the model using the test set:\n\nknn_test_res &lt;- \n  knn_last_wflow |&gt; \n  last_fit(sim_split, metrics = cls_mtr)\n  \nknn_test_res\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits             id               .metrics .notes   .predictions .workflow \n#&gt;   &lt;list&gt;             &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n#&gt; 1 &lt;split [1499/501]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;"
  },
  {
    "objectID": "slides/advanced-07-feature-selection.html#automated-approach-2",
    "href": "slides/advanced-07-feature-selection.html#automated-approach-2",
    "title": "7 - Feature selection",
    "section": "Automated approach",
    "text": "Automated approach\nWe can pick out the parts that we want:\n\nknn_final_fit &lt;- knn_test_res |&gt; extract_workflow()\nknn_test_pred &lt;- knn_test_res |&gt; collect_predictions()\nknn_test_mtr  &lt;- knn_test_res |&gt; collect_metrics()\n\nknn_test_mtr\n#&gt; # A tibble: 4 Ã— 4\n#&gt;   .metric     .estimator .estimate .config        \n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1 sensitivity binary        0.982  pre0_mod0_post0\n#&gt; 2 specificity binary        0.874  pre0_mod0_post0\n#&gt; 3 brier_class binary        0.0246 pre0_mod0_post0\n#&gt; 4 roc_auc     binary        0.981  pre0_mod0_post0\n\nEasy peasy!"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#getting-set-up",
    "href": "slides/advanced-05-feature-engineering-part-two.html#getting-set-up",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Getting set up   ",
    "text": "Getting set up   \n\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(extrasteps)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#hotel-rates-data-set",
    "href": "slides/advanced-05-feature-engineering-part-two.html#hotel-rates-data-set",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Hotel rates data set",
    "text": "Hotel rates data set\nRegression data set for predicting the average daily rate for a room, for â€œResort Hotelâ€. The agent and company use random names.\n\nglimpse(hotel_rates)\n#&gt; Rows: 15,402\n#&gt; Columns: 28\n#&gt; $ avg_price_per_room             &lt;dbl&gt; 110.00, 74.00, 81.90, 81.00, 112.20, 90â€¦\n#&gt; $ lead_time                      &lt;dbl&gt; 241, 273, 248, 236, 243, 267, 94, 10, 1â€¦\n#&gt; $ stays_in_weekend_nights        &lt;dbl&gt; 0, 2, 2, 2, 4, 2, 4, 0, 0, 0, 0, 0, 0, â€¦\n#&gt; $ stays_in_week_nights           &lt;dbl&gt; 1, 5, 5, 5, 10, 5, 7, 1, 1, 1, 1, 1, 1,â€¦\n#&gt; $ adults                         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, â€¦\n#&gt; $ children                       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, â€¦\n#&gt; $ babies                         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n#&gt; $ meal                           &lt;fct&gt; bed_and_breakfast, bed_and_breakfast, bâ€¦\n#&gt; $ country                        &lt;fct&gt; prt, aus, gbr, prt, gbr, null, prt, espâ€¦\n#&gt; $ market_segment                 &lt;fct&gt; online_travel_agent, offline_travel_ageâ€¦\n#&gt; $ distribution_channel           &lt;fct&gt; ta_to, ta_to, ta_to, ta_to, ta_to, ta_tâ€¦\n#&gt; $ is_repeated_guest              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, â€¦\n#&gt; $ previous_cancellations         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n#&gt; $ previous_bookings_not_canceled &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, â€¦\n#&gt; $ reserved_room_type             &lt;fct&gt; a, a, a, a, a, a, f, e, h, a, a, g, a, â€¦\n#&gt; $ assigned_room_type             &lt;fct&gt; c, a, c, a, a, a, f, f, h, e, e, g, e, â€¦\n#&gt; $ booking_changes                &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, â€¦\n#&gt; $ agent                          &lt;fct&gt; devin_rivera_borrego, lia_nauth, jawharâ€¦\n#&gt; $ company                        &lt;fct&gt; not_applicable, not_applicable, not_appâ€¦\n#&gt; $ days_in_waiting_list           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n#&gt; $ customer_type                  &lt;fct&gt; transient, transient_party, transient, â€¦\n#&gt; $ required_car_parking_spaces    &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, â€¦\n#&gt; $ total_of_special_requests      &lt;dbl&gt; 1, 0, 0, 2, 0, 0, 1, 1, 0, 2, 2, 0, 2, â€¦\n#&gt; $ arrival_date                   &lt;date&gt; 2016-07-02, 2016-07-02, 2016-07-02, 20â€¦\n#&gt; $ arrival_date_num               &lt;dbl&gt; 2016.5, 2016.5, 2016.5, 2016.5, 2016.5,â€¦\n#&gt; $ near_christmas                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n#&gt; $ near_new_years                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n#&gt; $ historical_adr                 &lt;dbl&gt; 104.9811, 104.9811, 104.9811, 104.9811,â€¦\n\n\nAntonio, N., de Almeida, A., and Nunes, L. (2019). Hotel booking demand datasets. Data in Brief, 22, 41-49."
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#hotel-data-splitting",
    "href": "slides/advanced-05-feature-engineering-part-two.html#hotel-data-splitting",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Hotel data splitting ",
    "text": "Hotel data splitting \nGenerally, you should always do data splitting. We are doing it here explicitly because some artifacts of splitting data become useful later on.\n\nset.seed(1234)\nhotel_split &lt;- initial_split(hotel_rates)\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#your-turn",
    "href": "slides/advanced-05-feature-engineering-part-two.html#your-turn",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Your turn",
    "text": "Your turn\n\nLoad and explore the hotel_train data\nComes loaded with the modeldata package\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#nonlinear-predictors",
    "href": "slides/advanced-05-feature-engineering-part-two.html#nonlinear-predictors",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Nonlinear predictors",
    "text": "Nonlinear predictors"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#splines-1",
    "href": "slides/advanced-05-feature-engineering-part-two.html#splines-1",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Splines",
    "text": "Splines\nIt is a way to transform a single numeric predictor into multiple numeric predictors, with the hope that the new numeric predictors are more linearly related to the outcome.\nMostly needed with linear models, but it should rarely hurt to use it.\nIf youâ€™ve ever used geom_smooth() you have seen splines in action.\n\nFEAZ FES"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#splines-explained",
    "href": "slides/advanced-05-feature-engineering-part-two.html#splines-explained",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Splines explained",
    "text": "Splines explained\nA spline is a piecewise polynomial function.\nWe have 2 main parameters to worry about. Number of knots and the polynomial degree.\nThe domain of the predictor is split into k regions, with a knot between each, and a polynomial function is fit within each region, under the constraint that they touch each other at the knot."
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#knots-1-degree-1",
    "href": "slides/advanced-05-feature-engineering-part-two.html#knots-1-degree-1",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "knots: 1, degree: 1",
    "text": "knots: 1, degree: 1"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#knots-2-degree-1",
    "href": "slides/advanced-05-feature-engineering-part-two.html#knots-2-degree-1",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "knots: 2, degree: 1",
    "text": "knots: 2, degree: 1"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#knots-5-degree-1",
    "href": "slides/advanced-05-feature-engineering-part-two.html#knots-5-degree-1",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "knots: 5, degree: 1",
    "text": "knots: 5, degree: 1"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#knots-5-degree-2",
    "href": "slides/advanced-05-feature-engineering-part-two.html#knots-5-degree-2",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "knots: 5, degree: 2",
    "text": "knots: 5, degree: 2"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#knots-5-degree-3",
    "href": "slides/advanced-05-feature-engineering-part-two.html#knots-5-degree-3",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "knots: 5, degree: 3",
    "text": "knots: 5, degree: 3"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#knots-9-degree-3",
    "href": "slides/advanced-05-feature-engineering-part-two.html#knots-9-degree-3",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "knots: 9, degree: 3",
    "text": "knots: 9, degree: 3"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#b-spline-features-visualized---degree-3",
    "href": "slides/advanced-05-feature-engineering-part-two.html#b-spline-features-visualized---degree-3",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "B-Spline features visualized - degree: 3",
    "text": "B-Spline features visualized - degree: 3"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#splines-as-numbers",
    "href": "slides/advanced-05-feature-engineering-part-two.html#splines-as-numbers",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Splines as numbers",
    "text": "Splines as numbers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narrival_date_num\nSpline Feature 1\nSpline Feature 2\nSpline Feature 3\nSpline Feature 4\nSpline Feature 5\nSpline Feature 6\n\n\n\n\n2017.619\n0.00\n0.00\n0.00\n0.03\n0.35\n0.62\n\n\n2016.844\n0.15\n0.59\n0.26\n0.00\n0.00\n0.00\n\n\n2016.702\n0.51\n0.40\n0.05\n0.00\n0.00\n0.00\n\n\n2017.077\n0.00\n0.19\n0.67\n0.14\n0.00\n0.00\n\n\n2016.861\n0.13\n0.58\n0.29\n0.00\n0.00\n0.00\n\n\n2017.019\n0.00\n0.30\n0.62\n0.07\n0.00\n0.00\n\n\n2017.123\n0.00\n0.11\n0.66\n0.23\n0.00\n0.00"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#splines-pros-and-cons",
    "href": "slides/advanced-05-feature-engineering-part-two.html#splines-pros-and-cons",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Splines pros and cons",
    "text": "Splines pros and cons\n\n\nPros\n\nfast\neasy to use\nsemi-interpretable\n\n\nCons\n\nadds more columns\nwill need to select the number of columns\ncan be messy outside the range"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#your-turn-1",
    "href": "slides/advanced-05-feature-engineering-part-two.html#your-turn-1",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Your turn",
    "text": "Your turn\n\nApply B-splines to some variables using step_spline_b()\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#factors-with-many-categories",
    "href": "slides/advanced-05-feature-engineering-part-two.html#factors-with-many-categories",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Factors with many categories",
    "text": "Factors with many categories\n\n\n\nhotel_train |&gt;\n  count(country)\n#&gt; # A tibble: 88 Ã— 2\n#&gt;    country     n\n#&gt;    &lt;fct&gt;   &lt;int&gt;\n#&gt;  1 ago         7\n#&gt;  2 and         1\n#&gt;  3 are         3\n#&gt;  4 arg        14\n#&gt;  5 aus        31\n#&gt;  6 aut        76\n#&gt;  7 aze         2\n#&gt;  8 bel       184\n#&gt;  9 bgr         2\n#&gt; 10 bhs         1\n#&gt; # â„¹ 78 more rows\n\n\n\nhotel_train |&gt;\n  count(company)\n#&gt; # A tibble: 157 Ã— 2\n#&gt;    company                 n\n#&gt;    &lt;fct&gt;               &lt;int&gt;\n#&gt;  1 abdou_llc               2\n#&gt;  2 afework_llc             5\n#&gt;  3 alston_pbc              3\n#&gt;  4 battle_llc             23\n#&gt;  5 bennett_and_company     3\n#&gt;  6 berhanu_pbc            53\n#&gt;  7 biggers_llc             4\n#&gt;  8 blasingime_llc          1\n#&gt;  9 boddy_llc              16\n#&gt; 10 boles_pbc               3\n#&gt; # â„¹ 147 more rows\n\n\n\nhotel_train |&gt;\n  count(agent)\n#&gt; # A tibble: 119 Ã— 2\n#&gt;    agent                 n\n#&gt;    &lt;fct&gt;             &lt;int&gt;\n#&gt;  1 aaron_marquez         2\n#&gt;  2 alexander_drake    1117\n#&gt;  3 allen_her             1\n#&gt;  4 anas_el_bashir        1\n#&gt;  5 araseli_billy         1\n#&gt;  6 arhab_al_islam        7\n#&gt;  7 audray_tucker        38\n#&gt;  8 bernice_baltierra    35\n#&gt;  9 betzy_rodriguez      66\n#&gt; 10 brayan_guerrero       2\n#&gt; # â„¹ 109 more rows"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#how-do-we-handle-them",
    "href": "slides/advanced-05-feature-engineering-part-two.html#how-do-we-handle-them",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "How do we handle them?",
    "text": "How do we handle them?\n\nWe could:\n\nMake the full set of indicator variables ğŸ˜³\nLump agents and companies that rarely occur into an â€œotherâ€ group\nUse feature hashing to create a smaller set of indicator variables\nUse target encoding to replace the county, agent, and company columns with the estimated effect of that predictor"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#target-encoding-1",
    "href": "slides/advanced-05-feature-engineering-part-two.html#target-encoding-1",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Target encoding",
    "text": "Target encoding\nTarget encoding (also called mean encoding, likelihood encoding, impact encoding, or effect encoding) is a supervised trained method that turns a single categorical predictor into a single numeric predictor.\nIt is often used to deal with categorical predictors with many levels, although it works regardless.\nSince it uses the outcome to train it, you need to make sure to use cross-validation to avoid overfitting.\n\nFEAZ FES"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#target-encoding-motivation",
    "href": "slides/advanced-05-feature-engineering-part-two.html#target-encoding-motivation",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Target encoding motivation",
    "text": "Target encoding motivation\nYou have a numeric outcome and a categorical predictor. And you want to transform each value of the categorical predictor into a value that best represents the outcome?\n\n\n\nWe calculate the mean of the outcome within each level of the predictor, and use that as the new value.\n\n\n\n\n\n\nCaution\n\n\nDonâ€™t do just this! We are building up the method one thing at a time. Unregularized target encoding is really prone to overfitting.\n\n\n\n\n\nhotel_train |&gt;\n  summarise(\n    mean = mean(avg_price_per_room),\n    .by = agent\n  )\n#&gt; # A tibble: 119 Ã— 2\n#&gt;    agent                 mean\n#&gt;    &lt;fct&gt;                &lt;dbl&gt;\n#&gt;  1 alexander_drake      144. \n#&gt;  2 kaylae_maxedon        62.5\n#&gt;  3 michael_mcdole        60.9\n#&gt;  4 devin_rivera_borrego 126. \n#&gt;  5 james_richards        78.6\n#&gt;  6 estela_bonilla        41.9\n#&gt;  7 charles_najera       109. \n#&gt;  8 reema_el_tamer       118. \n#&gt;  9 jawhara_al_azad       90.1\n#&gt; 10 not_applicable        84.1\n#&gt; # â„¹ 109 more rows"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#target-encoding-handling-unseen-levels",
    "href": "slides/advanced-05-feature-engineering-part-two.html#target-encoding-handling-unseen-levels",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Target encoding handling unseen levels",
    "text": "Target encoding handling unseen levels\n\n\n\n\n\n\nCaution\n\n\nDonâ€™t look at the testing data set. This is done for educational purposes.\n\n\n\n\n\n\nhotel_train |&gt;\n  count(agent, .drop = FALSE)\n#&gt; # A tibble: 174 Ã— 2\n#&gt;    agent                   n\n#&gt;    &lt;fct&gt;               &lt;int&gt;\n#&gt;  1 aaron_marquez           2\n#&gt;  2 aayaat_al_farran        0\n#&gt;  3 alanah_cook             0\n#&gt;  4 alexander_drake      1117\n#&gt;  5 allen_her               1\n#&gt;  6 amirah_christian        0\n#&gt;  7 anas_el_bashir          1\n#&gt;  8 anna_beltran_moreno     0\n#&gt;  9 anna_choi               0\n#&gt; 10 araseli_billy           1\n#&gt; # â„¹ 164 more rows\n\n\n\nhotel_test |&gt;\n  count(agent, .drop = FALSE)\n#&gt; # A tibble: 174 Ã— 2\n#&gt;    agent                   n\n#&gt;    &lt;fct&gt;               &lt;int&gt;\n#&gt;  1 aaron_marquez           1\n#&gt;  2 aayaat_al_farran        0\n#&gt;  3 alanah_cook             0\n#&gt;  4 alexander_drake       367\n#&gt;  5 allen_her               1\n#&gt;  6 amirah_christian        0\n#&gt;  7 anas_el_bashir          0\n#&gt;  8 anna_beltran_moreno     0\n#&gt;  9 anna_choi               0\n#&gt; 10 araseli_billy           1\n#&gt; # â„¹ 164 more rows"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#target-encoding-handling-unseen-levels-1",
    "href": "slides/advanced-05-feature-engineering-part-two.html#target-encoding-handling-unseen-levels-1",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Target encoding handling unseen levels",
    "text": "Target encoding handling unseen levels\n\n\nCalculate the global mean of the outcome and use it for cases that arenâ€™t seen in the training data set.\n\n\nmean(hotel_train$avg_price_per_room)\n#&gt; [1] 104.6039\n\n\n\nhotel_train |&gt;\n  summarise(\n    mean = mean(avg_price_per_room),\n    .by = agent\n  )\n#&gt; # A tibble: 119 Ã— 2\n#&gt;    agent                 mean\n#&gt;    &lt;fct&gt;                &lt;dbl&gt;\n#&gt;  1 alexander_drake      144. \n#&gt;  2 kaylae_maxedon        62.5\n#&gt;  3 michael_mcdole        60.9\n#&gt;  4 devin_rivera_borrego 126. \n#&gt;  5 james_richards        78.6\n#&gt;  6 estela_bonilla        41.9\n#&gt;  7 charles_najera       109. \n#&gt;  8 reema_el_tamer       118. \n#&gt;  9 jawhara_al_azad       90.1\n#&gt; 10 not_applicable        84.1\n#&gt; # â„¹ 109 more rows"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#how-do-we-handle-low-counts",
    "href": "slides/advanced-05-feature-engineering-part-two.html#how-do-we-handle-low-counts",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "How do we handle low counts?",
    "text": "How do we handle low counts?\n\n\nSome of the levels have very low counts. We canâ€™t have the same confidence in those means as the means calculated on high counts.\nWe use the global mean to account for 0 occurrences. Let us adjust the calculated mean by the global mean depending on the counts.\n\n\nhotel_train |&gt;\n  summarise(\n    mean = mean(avg_price_per_room),\n    n = n(),\n    .by = agent\n  ) |&gt;\n  arrange(agent)\n#&gt; # A tibble: 119 Ã— 3\n#&gt;    agent              mean     n\n#&gt;    &lt;fct&gt;             &lt;dbl&gt; &lt;int&gt;\n#&gt;  1 aaron_marquez     118.      2\n#&gt;  2 alexander_drake   144.   1117\n#&gt;  3 allen_her          65       1\n#&gt;  4 anas_el_bashir     99       1\n#&gt;  5 araseli_billy      40       1\n#&gt;  6 arhab_al_islam     35       7\n#&gt;  7 audray_tucker      76.0    38\n#&gt;  8 bernice_baltierra  71.3    35\n#&gt;  9 betzy_rodriguez    84.0    66\n#&gt; 10 brayan_guerrero    37.5     2\n#&gt; # â„¹ 109 more rows"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#partial-pooling",
    "href": "slides/advanced-05-feature-engineering-part-two.html#partial-pooling",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Partial pooling",
    "text": "Partial pooling\nPartial pooling somewhat lowers the risk of overfitting since it tends to correct for agents with small sample sizes. It canâ€™t correct for improper data usage or data leakage, though."
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#partial-pooling-results",
    "href": "slides/advanced-05-feature-engineering-part-two.html#partial-pooling-results",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Partial pooling results",
    "text": "Partial pooling results"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#implentations",
    "href": "slides/advanced-05-feature-engineering-part-two.html#implentations",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Implentations",
    "text": "Implentations\nWe have described this method solely based on analytical calculations (step_lencode()), but you could arrive at similar numbers using a model-based approach by fitting a no-intercept generalized linear model. A hierarchical version would induce partial pooling.\n\nstep_lencode_glm()\nstep_lencode_bayes()\nstep_lencode_mixed()"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#target-encoding-in-recipes",
    "href": "slides/advanced-05-feature-engineering-part-two.html#target-encoding-in-recipes",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Target encoding in recipes  ",
    "text": "Target encoding in recipes  \n\nrecipe(avg_price_per_room ~ ., data = hotel_train) |&gt;\n  step_lencode(\n    agent, country, company,\n    outcome = vars(\"avg_price_per_room\"), smooth = TRUE,\n  ) |&gt;\n  prep() |&gt;\n  bake(NULL) |&gt;\n  select(agent, country, company)\n#&gt; # A tibble: 11,551 Ã— 3\n#&gt;    agent country company\n#&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 144.    108.     109.\n#&gt;  2  77.2    83.4    109.\n#&gt;  3  61.5    99.9    109.\n#&gt;  4 126.    108.     109.\n#&gt;  5 144.    108.     109.\n#&gt;  6  79.9    73.8    109.\n#&gt;  7 126.     99.9    109.\n#&gt;  8  69.8   108.     109.\n#&gt;  9 126.     99.9    109.\n#&gt; 10 144.    108.     109.\n#&gt; # â„¹ 11,541 more rows"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#your-turn-2",
    "href": "slides/advanced-05-feature-engineering-part-two.html#your-turn-2",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Your turn",
    "text": "Your turn\n\nApply target encoding to the data set, see how it affects different predictors, not just the ones we listed here.\n\nstep_lencode()\nstep_lencode_glm()\nstep_lencode_bayes()\nstep_lencode_mixed()\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#date-time-variables-1",
    "href": "slides/advanced-05-feature-engineering-part-two.html#date-time-variables-1",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Date time variables",
    "text": "Date time variables\nHow can we represent the date column arrival_date for our model?\n\nWhen we use a date column in its native format, most models in R convert it to an integer.\n\n\nWe can re-engineer it as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nIndicators for holidays\n\n\nThe main point is that we try to maximize performance with different versions of the predictors.\nMention that, for the Chicago data, the day or the week features are usually the most important ones in the model.\n\n\nFEAZ"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#your-turn-3",
    "href": "slides/advanced-05-feature-engineering-part-two.html#your-turn-3",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Your turn",
    "text": "Your turn\n\nExplore the arrival_date variable and its relation to avg_price_per_room\nThe lubridate package might provide helpful\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#arrival_date-date-features",
    "href": "slides/advanced-05-feature-engineering-part-two.html#arrival_date-date-features",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "arrival_date date features",
    "text": "arrival_date date features\nusing step_date( features = c(\"year\", \"month\", \"dow\", \"decimal\", \"mday\", \"doy\", \"week\", \"semester\", \"quarter\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narrival_date\nyear\nmonth\ndow\ndecimal\nmday\ndoy\nweek\nsemester\nquarter\n\n\n\n\n2016-08-30\n2016\nAug\nTue\n2016.661\n30\n243\n35\n2\n3\n\n\n2016-10-22\n2016\nOct\nSat\n2016.806\n22\n296\n43\n2\n4\n\n\n2016-12-17\n2016\nDec\nSat\n2016.959\n17\n352\n51\n2\n4\n\n\n2017-02-13\n2017\nFeb\nMon\n2017.118\n13\n44\n7\n1\n1\n\n\n2017-04-05\n2017\nApr\nWed\n2017.258\n5\n95\n14\n1\n2"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#arrival_date-date-features-1",
    "href": "slides/advanced-05-feature-engineering-part-two.html#arrival_date-date-features-1",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "arrival_date date features",
    "text": "arrival_date date features\nAdding label = FALSE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narrival_date\nyear\nmonth\ndow\ndecimal\nmday\ndoy\nweek\nsemester\nquarter\n\n\n\n\n2016-08-30\n2016\n8\n3\n2016.661\n30\n243\n35\n2\n3\n\n\n2016-10-22\n2016\n10\n7\n2016.806\n22\n296\n43\n2\n4\n\n\n2016-12-17\n2016\n12\n7\n2016.959\n17\n352\n51\n2\n4\n\n\n2017-02-13\n2017\n2\n2\n2017.118\n13\n44\n7\n1\n1\n\n\n2017-04-05\n2017\n4\n4\n2017.258\n5\n95\n14\n1\n2"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#other-recipes-steps",
    "href": "slides/advanced-05-feature-engineering-part-two.html#other-recipes-steps",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Other recipes steps",
    "text": "Other recipes steps\n\nstep_time()\n\nWorks the same as step_date() but for measurements smaller than day: hour, hour12, am/pm, minute, second, decimal_day.\n\nstep_holiday()\n\nAdds indicators for holidays. See timeDate::listHolidays() for supported holidays."
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#your-turn-4",
    "href": "slides/advanced-05-feature-engineering-part-two.html#your-turn-4",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Your turn",
    "text": "Your turn\n\nApply date steps to the arrival_date variable and try to see if we capture anything about avg_price_per_room\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#dates-as-numerics",
    "href": "slides/advanced-05-feature-engineering-part-two.html#dates-as-numerics",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "dates as numerics",
    "text": "dates as numerics"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#holidays-as-numerics",
    "href": "slides/advanced-05-feature-engineering-part-two.html#holidays-as-numerics",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "holidays as numerics",
    "text": "holidays as numerics"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#what-are-the-issues-with-these-features",
    "href": "slides/advanced-05-feature-engineering-part-two.html#what-are-the-issues-with-these-features",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "What are the issues with these features?",
    "text": "What are the issues with these features?\nThe numeric features make it easy to capture the end or beginning, but harder to do anything more granular.\nThe indicators mostly care about the day itself. No information about the lead-up or aftermath"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#time-events",
    "href": "slides/advanced-05-feature-engineering-part-two.html#time-events",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "time events",
    "text": "time events\nUsing extrasteps::step_time_event() and the almanac package, we can create useful time features.\n\nlibrary(almanac)\n\nrule_1 &lt;- weekly() |&gt;\n  recur_on_weekdays() |&gt;\n  rsetdiff(hol_christmas())\n\nrule_2 &lt;- monthly(since = \"2000-01-01\") |&gt;\n  recur_on_interval(3) |&gt;\n  recur_on_day_of_month(1)\n\nrule_3 &lt;- yearly(\"1997-06-05\") |&gt;\n  recur_on_day_of_week(\"Thursday\") |&gt;\n  recur_on_month_of_year(c(\"Jun\", \"July\", \"Aug\"))"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#step_time_event",
    "href": "slides/advanced-05-feature-engineering-part-two.html#step_time_event",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "step_time_event() ",
    "text": "step_time_event() \nCreate a list of rules (last slide) and pass them to the rules argument of extrasteps::step_time_event()\n\nrules &lt;- list(rule_1 = rule_1, rule_2 = rule_2, rule_3 = rule_3)\n\nrecipe(~arrival_date, data = hotel_rates) |&gt;\n  step_time_event(arrival_date, rules = rules)\n\n\nFEAZ"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#step_time_event-as-numerics",
    "href": "slides/advanced-05-feature-engineering-part-two.html#step_time_event-as-numerics",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "step_time_event() as numerics ",
    "text": "step_time_event() as numerics"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#non-indicator-time-events",
    "href": "slides/advanced-05-feature-engineering-part-two.html#non-indicator-time-events",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Non-indicator time events",
    "text": "Non-indicator time events\nThese features still have the issue that they only attach value to the date itself.\nWe can attach values based on how far we are away from those dates.\n\nstep_date_before()\nstep_date_after()\nstep_date_nearest()"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#step_date_before",
    "href": "slides/advanced-05-feature-engineering-part-two.html#step_date_before",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "step_date_before()",
    "text": "step_date_before()"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#step_date_before---inverse",
    "href": "slides/advanced-05-feature-engineering-part-two.html#step_date_before---inverse",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "step_date_before() - inverse",
    "text": "step_date_before() - inverse"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#step_date_after---inverse",
    "href": "slides/advanced-05-feature-engineering-part-two.html#step_date_after---inverse",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "step_date_after() - inverse",
    "text": "step_date_after() - inverse"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#step_date_nearest---inverse",
    "href": "slides/advanced-05-feature-engineering-part-two.html#step_date_nearest---inverse",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "step_date_nearest() - inverse",
    "text": "step_date_nearest() - inverse"
  },
  {
    "objectID": "slides/advanced-05-feature-engineering-part-two.html#datetime-features",
    "href": "slides/advanced-05-feature-engineering-part-two.html#datetime-features",
    "title": "5 - Feature engineering: splines, target encoding and dates",
    "section": "Datetime features",
    "text": "Datetime features\nAvoid crafting datetime features by hand if at all possible.\nDealing with uneven month lengths, leap days (leap seconds)\nOr tried to define any event that doesnâ€™t land on the same day of the week or date each year.\n\nThe first Sunday after the first full moon on or after the vernal equinox"
  },
  {
    "objectID": "slides/advanced-03-racing.html#startup",
    "href": "slides/advanced-03-racing.html#startup",
    "title": "3 - Racing",
    "section": "Startup!   ",
    "text": "Startup!   \n\nlibrary(tidymodels)\nlibrary(finetune)\nlibrary(bonsai)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)"
  },
  {
    "objectID": "slides/advanced-03-racing.html#more-startup",
    "href": "slides/advanced-03-racing.html#more-startup",
    "title": "3 - Racing",
    "section": "More startup! ",
    "text": "More startup! \n\n# Load our example data for this section\n\"https://raw.githubusercontent.com/tidymodels/\" |&gt; \n  paste0(\"workshops/main/slides/class_data.RData\") |&gt; \n  url() |&gt; \n  load()\n\nset.seed(429)\nsim_split &lt;- initial_split(class_data, prop = 0.75, strata = class)\nsim_train &lt;- training(sim_split)\nsim_test  &lt;- testing(sim_split)\n\nset.seed(523)\nsim_rs &lt;- vfold_cv(sim_train, v = 10, strata = class)"
  },
  {
    "objectID": "slides/advanced-03-racing.html#first-a-shameless-promotion",
    "href": "slides/advanced-03-racing.html#first-a-shameless-promotion",
    "title": "3 - Racing",
    "section": "First, a shameless promotion",
    "text": "First, a shameless promotion"
  },
  {
    "objectID": "slides/advanced-03-racing.html#making-grid-search-more-efficient",
    "href": "slides/advanced-03-racing.html#making-grid-search-more-efficient",
    "title": "3 - Racing",
    "section": "Making Grid Search More Efficient",
    "text": "Making Grid Search More Efficient\nPreviously, we evaluated 250 models (25 candidates times 10 resamples).\nWe can make this go faster using parallel processing.\n\nAlso, for some models, we can fit far fewer models than the number being evaluated.\n\nFor example, with boosted trees, a model with X trees can often predict on candidates with fewer than X trees (i.e., no retraining).\n\nThese strategies can lead to enormous speed-ups."
  },
  {
    "objectID": "slides/advanced-03-racing.html#model-racing",
    "href": "slides/advanced-03-racing.html#model-racing",
    "title": "3 - Racing",
    "section": "Model Racing",
    "text": "Model Racing\nRacing is an old tool that we can use to go even faster.\n\nEvaluate all of the candidate models, but only for a few resamples.\nDetermine which candidates have a low probability of being selected (cough, cough, tanh activation, cough).\nEliminate poor candidates.\nRepeat with next resample (until no more resamples remain).\n\nThis can result in fitting a small number of models.\nIt is not an iterative search; it is an adaptive grid search.\n\nTMwR, TMwR example, AML4TD"
  },
  {
    "objectID": "slides/advanced-03-racing.html#discarding-candidates",
    "href": "slides/advanced-03-racing.html#discarding-candidates",
    "title": "3 - Racing",
    "section": "Discarding Candidates",
    "text": "Discarding Candidates\nHow do we eliminate tuning parameter combinations?\nThere are a few methods to do so. Weâ€™ll use one based on analysis of variance (ANOVA).\nHoweverâ€¦ there is typically a large resampling effect in the results."
  },
  {
    "objectID": "slides/advanced-03-racing.html#resampling-results-non-racing",
    "href": "slides/advanced-03-racing.html#resampling-results-non-racing",
    "title": "3 - Racing",
    "section": "Resampling Results (Non-Racing)",
    "text": "Resampling Results (Non-Racing)\n\n\nHere are some realistic (but simulated) examples of two candidate models.\nAn error estimate is measured for each of 10 resamples.\n\nThe lines connect resamples.\n\nThere is usually a significant resample-to-resample effect (rank corr: 0.83)."
  },
  {
    "objectID": "slides/advanced-03-racing.html#are-candidates-different",
    "href": "slides/advanced-03-racing.html#are-candidates-different",
    "title": "3 - Racing",
    "section": "Are Candidates Different?",
    "text": "Are Candidates Different?\nOne way to evaluate these models is to do a paired t-test\n\nor a t-test on their differences matched by resamples\n\nWith \\(n = 10\\) resamples, the confidence interval for the difference in the model error is (0.99, 2.8), indicating that candidate number 2 has a smaller error."
  },
  {
    "objectID": "slides/advanced-03-racing.html#evaluating-differences-in-candidates",
    "href": "slides/advanced-03-racing.html#evaluating-differences-in-candidates",
    "title": "3 - Racing",
    "section": "Evaluating Differences in Candidates",
    "text": "Evaluating Differences in Candidates\n\n\nWhat if we were to have compared the candidates while we sequentially evaluated each resample?\nğŸ‘‰\n\nOne candidate shows superiority when 5 resamples have been evaluated."
  },
  {
    "objectID": "slides/advanced-03-racing.html#interim-analysis-of-results",
    "href": "slides/advanced-03-racing.html#interim-analysis-of-results",
    "title": "3 - Racing",
    "section": "Interim Analysis of Results",
    "text": "Interim Analysis of Results\nOne version of racing uses a mixed model ANOVA to construct one-sided confidence intervals for each candidate versus the current best.\nAny candidates whose bound does not include zero are discarded. Here is an animation.\nThe resamples are analyzed in a random order (so set the seed).\n\nKuhn (2014) has examples and simulations to show that the method works.\nThe finetune package has functions tune_race_anova() and tune_race_win_loss()."
  },
  {
    "objectID": "slides/advanced-03-racing.html#boosted-trees-1",
    "href": "slides/advanced-03-racing.html#boosted-trees-1",
    "title": "3 - Racing",
    "section": "Boosted Trees",
    "text": "Boosted Trees\nThese are popular ensemble methods that build a sequence of tree models.\n\nEach tree uses the results of the previous tree to better predict samples, especially those that have been poorly predicted.\n\nEach tree in the ensemble is saved, and new samples are predicted using a weighted average of its votes.\n\nWeâ€™ll focus on the popular lightgbm implementation."
  },
  {
    "objectID": "slides/advanced-03-racing.html#boosted-tree-tuning-parameters",
    "href": "slides/advanced-03-racing.html#boosted-tree-tuning-parameters",
    "title": "3 - Racing",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\nSome possible parameters:\n\nmtry: The number of predictors randomly sampled at each split (in \\([1, ncol(x)]\\) or \\((0, 1]\\)).\ntrees: The number of trees (\\([1, \\infty]\\), but usually up to thousands).\nmin_n: The number of samples needed to further split (\\([1, n]\\)).\nlearn_rate: The rate that each tree adapts from previous iterations (\\((0, \\infty]\\), usual maximum is 0.1).\nstop_iter: The number of iterations of boosting where no improvement was shown before stopping (\\([1, trees]\\))."
  },
  {
    "objectID": "slides/advanced-03-racing.html#boosted-tree-tuning-parameters-1",
    "href": "slides/advanced-03-racing.html#boosted-tree-tuning-parameters-1",
    "title": "3 - Racing",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\nTBH, it is usually not difficult to optimize these models.\n\n\n\nOften, there are multiple candidate tuning parameter regions with very good results.\nFor example: ğŸ‘‰\n\nTo demonstrate, weâ€™ll look at optimizing five of the tuning parameters."
  },
  {
    "objectID": "slides/advanced-03-racing.html#boosted-tree-tuning-parameters-2",
    "href": "slides/advanced-03-racing.html#boosted-tree-tuning-parameters-2",
    "title": "3 - Racing",
    "section": "Boosted Tree Tuning Parameters   ",
    "text": "Boosted Tree Tuning Parameters   \nWeâ€™ll need to load the bonsai package. This has the information needed to use lightgbm\n\nlibrary(bonsai)\n\nlgbm_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    learn_rate = tune(),\n    mtry = tune(),\n    min_n = tune(),\n    stop_iter = tune()\n  ) |&gt;\n  set_mode(\"classification\") |&gt;\n  # Turn off within-tree parallel processing; it's faster to run \n  # the resamples/configurations in parallel\n  set_engine(\"lightgbm\", num_threads = 1) \n\n# No preprocessing required:\nlgbm_wflow &lt;- workflow(class ~ ., lgbm_spec)"
  },
  {
    "objectID": "slides/advanced-03-racing.html#racing",
    "href": "slides/advanced-03-racing.html#racing",
    "title": "3 - Racing",
    "section": "Racing   ",
    "text": "Racing   \n\nlibrary(finetune)\n\n# Set this to true to demo\nctrl &lt;- control_race(verbose_elim = FALSE)\n\n# Optimizes on the first metric in the set\ncls_mtr &lt;- metric_set(brier_class, roc_auc, sensitivity, specificity)\n\nmirai::daemons(parallel::detectCores() - 1)\n\nset.seed(321)\nlgbm_res &lt;-\n  lgbm_wflow |&gt;\n  tune_race_anova(              # &lt;- very similar syntax to tune_grid()\n    resamples = sim_rs,\n    # Let's use a larger grid\n    grid = 50,\n    control = ctrl,\n    metrics = cls_mtr\n  )"
  },
  {
    "objectID": "slides/advanced-03-racing.html#racing-results",
    "href": "slides/advanced-03-racing.html#racing-results",
    "title": "3 - Racing",
    "section": "Racing Results ",
    "text": "Racing Results \n\nshow_best(lgbm_res, metric = \"brier_class\")\n#&gt; # A tibble: 1 Ã— 11\n#&gt;    mtry trees min_n learn_rate stop_iter .metric .estimator   mean     n std_err\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1     9  1836     9    0.00222         6 brier_â€¦ binary     0.0379    10 0.00238\n#&gt; # â„¹ 1 more variable: .config &lt;chr&gt;\n\n\n\nTimes using 10 cores: sequential: 605s, parallel: 92s, and parallel racing: 50s.\n\n\n\nParallel was 6.6-fold faster, and racing in parallel was 12.3-fold faster."
  },
  {
    "objectID": "slides/advanced-03-racing.html#racing-results-1",
    "href": "slides/advanced-03-racing.html#racing-results-1",
    "title": "3 - Racing",
    "section": "Racing Results ",
    "text": "Racing Results \n\n\nOnly 378 models were fit (out of 500).\nselect_best() never considers candidate models that did not get to the end of the race.\nThere is a helper function to see how candidate models were removed from consideration.\n\n\nplot_race(lgbm_res)"
  },
  {
    "objectID": "slides/advanced-03-racing.html#your-turn",
    "href": "slides/advanced-03-racing.html#your-turn",
    "title": "3 - Racing",
    "section": "Your turn",
    "text": "Your turn\n\nRun tune_race_anova() with a different seed and/or a different metric.\nDid you get the same or similar results?\n\n\n\n\nâˆ’+\n08:00"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#venue-information",
    "href": "slides/advanced-01-introduction.html#venue-information",
    "title": "1 - Introduction",
    "section": "Venue information",
    "text": "Venue information\n\nThere are gender neutral bathrooms located on floor LL2, next to Chicago A\nA meditation/prayer room is located on floor LL2 in Chicago A\nA lactation room is located on floor LL2 in Chicago B"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#workshop-policies",
    "href": "slides/advanced-01-introduction.html#workshop-policies",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease review the posit::conf code of conduct, which applies to all workshops: https://posit.co/code-of-conduct\nCoC site has info on how to report a problem (in person, email, phone)\nPlease do not photograph people wearing red lanyards"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#who-are-you",
    "href": "slides/advanced-01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %&gt;% or base R |&gt; pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have exposure to basic statistical concepts\nYou do need intermediate familiarity with modeling or ML\nYou have used some tidymodels packages\nYou have some experience with evaluating statistical models using resampling techniques"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#who-are-tidymodels",
    "href": "slides/advanced-01-introduction.html#who-are-tidymodels",
    "title": "1 - Introduction",
    "section": "Who are tidymodels?",
    "text": "Who are tidymodels?\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\n+ our TA today, Edgar Ruiz!\n\n\nMany thanks to Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill, and DesirÃ©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#asking-for-help",
    "href": "slides/advanced-01-introduction.html#asking-for-help",
    "title": "1 - Introduction",
    "section": "Asking for help",
    "text": "Asking for help\n\nğŸŸª â€œIâ€™m stuck and need help!â€\n\n\nğŸŸ© â€œI finished the exerciseâ€"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#discord",
    "href": "slides/advanced-01-introduction.html#discord",
    "title": "1 - Introduction",
    "section": "Discord ",
    "text": "Discord \n\npos.it/conf-event-portal (login)\nClick on â€œJoin Discord, the virtual networking platform!â€\nBrowse Channels -&gt; #workshop-feat-eng-tune"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#section-2",
    "href": "slides/advanced-01-introduction.html#section-2",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#section-3",
    "href": "slides/advanced-01-introduction.html#section-3",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#tentative-plan-for-this-workshop",
    "href": "slides/advanced-01-introduction.html#tentative-plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Tentative plan for this workshop",
    "text": "Tentative plan for this workshop\n\nModel optimization by tuning\n\nGrid search\nRacing\n\nFeature engineering with recipes\nPostprocessing\n\nFeature selection"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#section-4",
    "href": "slides/advanced-01-introduction.html#section-4",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors ğŸ‘‹"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#getting-the-materials",
    "href": "slides/advanced-01-introduction.html#getting-the-materials",
    "title": "1 - Introduction",
    "section": "Getting the materials",
    "text": "Getting the materials\n\nIf you are using Posit Cloud:\n Log in to Posit Cloud (free): TODO-ADD-LATER\n\nIf you are working locally:\n# local download\nusethis::use_course(\"tidymodels/workshops\", destdir = \"some_path\")\n\n# or fork via\nusethis::create_from_github(\"tidymodels/workshops\", fork = TRUE)"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#lets-install-some-packages",
    "href": "slides/advanced-01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Letâ€™s install some packages",
    "text": "Letâ€™s install some packages\nIf you are using your own laptop instead of Posit Cloud:\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"almanac\", \"betacal\", \"bonsai\", \"brulee\", \"C50\", \"Cubist\", \"desirability2\", \n    \"dimRed\", \"earth\", \"embed\", \"extrasteps\", \"finetune\", \"igraph\", \n    \"important\", \"irlba\", \"kknn\", \"lightgbm\", \"lme4\", \"mirai\", \"parallelly\", \n    \"plumber\", \"probably\", \"RANN\", \"rpart\", \"RSpectra\", \"rules\", \n    \"splines2\", \"stacks\", \"text2vec\", \"textrecipes\", \"tidymodels\", \n    \"uwot\", \"vetiver\")\n\ninstall.packages(pkgs)\n\nAlso, you should make sure that you have installed the newest version of a few packages. To check this, you can run:\n\nrlang::check_installed(\"tidymodels\", version = \"1.4.1\")\nrlang::check_installed(\"embed\", version = \"1.2.0\")"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#load-tidymodels",
    "href": "slides/advanced-01-introduction.html#load-tidymodels",
    "title": "1 - Introduction",
    "section": "Load tidymodels ",
    "text": "Load tidymodels \nWeâ€™re here to learn more about how to use the more advanced bits of tidymodels for supervised learning. Letâ€™s load the meta-package:\n\nlibrary(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.4.1 â”€â”€\n#&gt; âœ” broom        1.0.9          âœ” rsample      1.3.1     \n#&gt; âœ” dials        1.4.2          âœ” tailor       0.1.0.9000\n#&gt; âœ” dplyr        1.1.4          âœ” tidyr        1.3.1     \n#&gt; âœ” infer        1.0.9          âœ” tune         2.0.0     \n#&gt; âœ” modeldata    1.5.1          âœ” workflows    1.3.0     \n#&gt; âœ” parsnip      1.3.3          âœ” workflowsets 1.1.1     \n#&gt; âœ” purrr        1.1.0          âœ” yardstick    1.3.2     \n#&gt; âœ” recipes      1.3.1\n#&gt; â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\n#&gt; âœ– purrr::discard() masks scales::discard()\n#&gt; âœ– dplyr::filter()  masks stats::filter()\n#&gt; âœ– dplyr::lag()     masks stats::lag()\n#&gt; âœ– recipes::step()  masks stats::step()"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#resolve-naming-conflicts",
    "href": "slides/advanced-01-introduction.html#resolve-naming-conflicts",
    "title": "1 - Introduction",
    "section": "Resolve naming conflicts ",
    "text": "Resolve naming conflicts \nYou might want to run this function to avoid function name conflicts:\n\n\ntidymodels_prefer()\n\n\nTo get more details, use the quiet = FALSE option."
  },
  {
    "objectID": "slides/advanced-01-introduction.html#data-sets",
    "href": "slides/advanced-01-introduction.html#data-sets",
    "title": "1 - Introduction",
    "section": "Data sets",
    "text": "Data sets\nFor illustration, weâ€™ll use a few different data sets today:\n\nclass_data: a simulated set of data with a 1:10 class imbalance. Two classes, 30 predictors, and 2,000 data points.\nleaf_data: a real data set to identify plant species from their leaves. Thirty-two levels, 53 predictors, and 1,907 data points.\nhotel_data: a real data set for predicting the average cost per night. Numeric outcome, 27 predictors, and 15,402 data points.\n\n\n\nLetâ€™s get warmed up with the first data set."
  },
  {
    "objectID": "slides/advanced-01-introduction.html#imbalanced-data",
    "href": "slides/advanced-01-introduction.html#imbalanced-data",
    "title": "1 - Introduction",
    "section": "Imbalanced data",
    "text": "Imbalanced data\nThese data can be loaded from the GitHub repo:\n\n\n\"https://raw.githubusercontent.com/tidymodels/\" |&gt; \n  paste0(\"workshops/main/slides/class_data.RData\") |&gt; \n  url() |&gt; \n  load()\n\n\nThe outcome column is class with levels \"event\" and \"no_event\". Predictors are \"predictor_01\" to \"predictor_30\"."
  },
  {
    "objectID": "slides/advanced-01-introduction.html#your-turn",
    "href": "slides/advanced-01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\nLetâ€™s warm up by taking 8 minutes to explore the data.\n\nWeâ€™ll ask you to tell us something about these data that might be interesting for modeling.\n\n\n\nâˆ’+\n08:00"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#data-splitting",
    "href": "slides/advanced-01-introduction.html#data-splitting",
    "title": "1 - Introduction",
    "section": "Data splitting ",
    "text": "Data splitting \nOne of our first tasks is to split our data into (at a minimum) a training set and a testing set. The rsample package has numerous functions for this, prefixed by initial_. \nLetâ€™s create a 3:1 split of the simulated data and use a stratified random sample (by class):\n\nset.seed(429)\nsim_split &lt;- initial_split(class_data, prop = 0.75, strata = class)\nsim_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;1499/501/2000&gt;\n\nsim_train &lt;- training(sim_split)\nsim_test  &lt;- testing(sim_split)\n\n\nAML4TD, TMwR"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#data-splitting-1",
    "href": "slides/advanced-01-introduction.html#data-splitting-1",
    "title": "1 - Introduction",
    "section": "Data splitting  ",
    "text": "Data splitting  \n\n\n\nsim_train |&gt; \n  ggplot(aes(class)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nsim_test |&gt; \n  ggplot(aes(class)) + \n  geom_bar()"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#resampling",
    "href": "slides/advanced-01-introduction.html#resampling",
    "title": "1 - Introduction",
    "section": "Resampling",
    "text": "Resampling\n\n\nWeâ€™ll want to get accurate estimates of model performance.\nLetâ€™s use a resampling method to make multiple versions of our data (using 10-fold cross-validation).\n\nFor large amounts of data, a validation set is also a good alternative.\n\n\n\n\n\n\n\n\nAML4TD, TMwR"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#resampling-1",
    "href": "slides/advanced-01-introduction.html#resampling-1",
    "title": "1 - Introduction",
    "section": "Resampling ",
    "text": "Resampling \n\nset.seed(523)\nsim_rs &lt;- vfold_cv(sim_train, v = 10, strata = class)\nsim_rs\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [1348/151]&gt; Fold01\n#&gt;  2 &lt;split [1349/150]&gt; Fold02\n#&gt;  3 &lt;split [1349/150]&gt; Fold03\n#&gt;  4 &lt;split [1349/150]&gt; Fold04\n#&gt;  5 &lt;split [1349/150]&gt; Fold05\n#&gt;  6 &lt;split [1349/150]&gt; Fold06\n#&gt;  7 &lt;split [1349/150]&gt; Fold07\n#&gt;  8 &lt;split [1349/150]&gt; Fold08\n#&gt;  9 &lt;split [1350/149]&gt; Fold09\n#&gt; 10 &lt;split [1350/149]&gt; Fold10"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#resampled-data-sets",
    "href": "slides/advanced-01-introduction.html#resampled-data-sets",
    "title": "1 - Introduction",
    "section": "Resampled data sets ",
    "text": "Resampled data sets \n\nmodel_data_1 &lt;- sim_rs |&gt; get_rsplit(1) |&gt; analysis()\nmodel_data_1 |&gt; count(class)\n#&gt; # A tibble: 2 Ã— 2\n#&gt;   class        n\n#&gt;   &lt;fct&gt;    &lt;int&gt;\n#&gt; 1 event      151\n#&gt; 2 no_event  1197\n\nperf_data_1 &lt;- sim_rs |&gt; get_rsplit(1) |&gt; assessment()\nperf_data_1 |&gt; count(class)\n#&gt; # A tibble: 2 Ã— 2\n#&gt;   class        n\n#&gt;   &lt;fct&gt;    &lt;int&gt;\n#&gt; 1 event       17\n#&gt; 2 no_event   134"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#models-via-parsnip",
    "href": "slides/advanced-01-introduction.html#models-via-parsnip",
    "title": "1 - Introduction",
    "section": "Models via parsnip ",
    "text": "Models via parsnip \nLetâ€™s fit a simple decision tree to the data:\n\n# Specify what you want\ntree_spec &lt;- decision_tree(mode = \"classification\")\n\n# Then train:\ntree_fit &lt;- tree_spec |&gt; fit(class ~ ., data = model_data_1)  \ntree_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; n= 1348 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 1348 151 no_event (0.11201780 0.88798220)  \n#&gt;    2) predictor_27&lt; -2.268929 76  11 event (0.85526316 0.14473684) *\n#&gt;    3) predictor_27&gt;=-2.268929 1272  86 no_event (0.06761006 0.93238994)  \n#&gt;      6) predictor_29&gt;=2.144976 89  25 event (0.71910112 0.28089888)  \n#&gt;       12) predictor_29&gt;=2.599392 45   3 event (0.93333333 0.06666667) *\n#&gt;       13) predictor_29&lt; 2.599392 44  22 event (0.50000000 0.50000000)  \n#&gt;         26) predictor_18&gt;=0.2344964 32  11 event (0.65625000 0.34375000)  \n#&gt;           52) predictor_03&lt; -0.2257182 16   1 event (0.93750000 0.06250000) *\n#&gt;           53) predictor_03&gt;=-0.2257182 16   6 no_event (0.37500000 0.62500000) *\n#&gt;         27) predictor_18&lt; 0.2344964 12   1 no_event (0.08333333 0.91666667) *\n#&gt;      7) predictor_29&lt; 2.144976 1183  22 no_event (0.01859679 0.98140321) *"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#predicting",
    "href": "slides/advanced-01-introduction.html#predicting",
    "title": "1 - Introduction",
    "section": "Predictingâ€¦ ",
    "text": "Predictingâ€¦ \n\npredict(tree_fit, new_data = head(perf_data_1, 4))\n#&gt; # A tibble: 4 Ã— 1\n#&gt;   .pred_class\n#&gt;   &lt;fct&gt;      \n#&gt; 1 event      \n#&gt; 2 event      \n#&gt; 3 event      \n#&gt; 4 no_event\n\npredict(tree_fit, new_data = head(perf_data_1, 4), type = \"prob\")\n#&gt; # A tibble: 4 Ã— 2\n#&gt;   .pred_event .pred_no_event\n#&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1      0.855          0.145 \n#&gt; 2      0.933          0.0667\n#&gt; 3      0.855          0.145 \n#&gt; 4      0.0833         0.917"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#augmenting",
    "href": "slides/advanced-01-introduction.html#augmenting",
    "title": "1 - Introduction",
    "section": "Augmentingâ€¦  ",
    "text": "Augmentingâ€¦  \n\ntree_pred &lt;- augment(tree_fit, new_data = perf_data_1)\ntree_pred |&gt; slice(1:5)\n#&gt; # A tibble: 5 Ã— 34\n#&gt;   .pred_class .pred_event .pred_no_event class predictor_01 predictor_02\n#&gt;   &lt;fct&gt;             &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 event            0.855          0.145  event       -1.64      -1.74   \n#&gt; 2 event            0.933          0.0667 event       -0.846      0.477  \n#&gt; 3 event            0.855          0.145  event        0.551      0.00571\n#&gt; 4 no_event         0.0833         0.917  event       -1.46      -0.335  \n#&gt; 5 event            0.855          0.145  event       -0.194     -0.276  \n#&gt; # â„¹ 28 more variables: predictor_03 &lt;dbl&gt;, predictor_04 &lt;dbl&gt;,\n#&gt; #   predictor_05 &lt;dbl&gt;, predictor_06 &lt;dbl&gt;, predictor_07 &lt;dbl&gt;,\n#&gt; #   predictor_08 &lt;dbl&gt;, predictor_09 &lt;dbl&gt;, predictor_10 &lt;dbl&gt;,\n#&gt; #   predictor_11 &lt;dbl&gt;, predictor_12 &lt;dbl&gt;, predictor_13 &lt;dbl&gt;,\n#&gt; #   predictor_14 &lt;dbl&gt;, predictor_15 &lt;dbl&gt;, predictor_16 &lt;dbl&gt;,\n#&gt; #   predictor_17 &lt;dbl&gt;, predictor_18 &lt;dbl&gt;, predictor_19 &lt;dbl&gt;,\n#&gt; #   predictor_20 &lt;dbl&gt;, predictor_21 &lt;dbl&gt;, predictor_22 &lt;dbl&gt;, â€¦"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#performance-metrics",
    "href": "slides/advanced-01-introduction.html#performance-metrics",
    "title": "1 - Introduction",
    "section": "Performance metrics ",
    "text": "Performance metrics \nThere are many yardstick metrics* for class predictions and probability estimates.\nLetâ€™s make a collection of metrics and then evaluate our model.\n\ncls_metrics &lt;- metric_set(brier_class, roc_auc, sensitivity, specificity)\ntree_pred |&gt; cls_metrics(truth = class, estimate = .pred_class, .pred_event)\n#&gt; # A tibble: 4 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary        0.647 \n#&gt; 2 specificity binary        0.985 \n#&gt; 3 brier_class binary        0.0429\n#&gt; 4 roc_auc     binary        0.899\n\n\n* â€¦ and metrics for regression and others.\n\n\nAML4TD"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#recipes-and-workflows",
    "href": "slides/advanced-01-introduction.html#recipes-and-workflows",
    "title": "1 - Introduction",
    "section": "Recipes and workflows  ",
    "text": "Recipes and workflows  \nRecipes are preprocessors that perform sequential operations on the predictors.\n\n\nFor example, to center and scale our predictors:\n\nrec &lt;- \n  recipe(class ~ ., data = sim_train) |&gt; \n  step_normalize(all_numeric_predictors())\n\n\n\n\nA model, a recipe, and other objects can be added to a workflow to have a single object for the whole modeling sequence:\n\ntree_wflow &lt;- workflow(rec, tree_spec)"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#your-turn-1",
    "href": "slides/advanced-01-introduction.html#your-turn-1",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\nFit a different type of decision tree, this time:\n\nUsing the C5.0 engine\nChange the minimum number of samples required for splitting to 10.\n\n\nDid performance change much?\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/advanced-01-introduction.html#our-versions",
    "href": "slides/advanced-01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\nR version 4.5.1 (2025-06-13), Quarto (1.7.32)\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nalmanac\n1.0.0\n\n\nbetacal\n0.1.0\n\n\nbonsai\n0.4.0\n\n\nbroom\n1.0.9\n\n\nbrulee\n0.5.0\n\n\nC50\n0.2.0\n\n\nCubist\n0.5.0\n\n\nCVST\n0.2-3\n\n\ndesirability2\n0.2.0\n\n\ndials\n1.4.2\n\n\ndimRed\n0.2.7\n\n\ndplyr\n1.1.4\n\n\nDRR\n0.0.4\n\n\nearth\n5.3.4\n\n\nembed\n1.2.0\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nextrasteps\n0.3.0\n\n\nfinetune\n1.2.1\n\n\nforested\n0.2.0\n\n\nFormula\n1.2-5\n\n\nggplot2\n3.5.2\n\n\nigraph\n2.1.4\n\n\nimportant\n0.2.0\n\n\nirlba\n2.3.5.1\n\n\nkernlab\n0.9-33\n\n\nkknn\n1.4.1\n\n\nlattice\n0.22-7\n\n\nlightgbm\n4.6.0\n\n\nlme4\n1.1-37\n\n\nmirai\n2.5.0\n\n\nmodeldata\n1.5.1\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nparallelly\n1.45.1\n\n\nparsnip\n1.3.3\n\n\nplotmo\n3.6.4\n\n\nplotrix\n3.8-4\n\n\nplumber\n1.3.0\n\n\nprobably\n1.1.1\n\n\npurrr\n1.1.0\n\n\nRANN\n2.6.2\n\n\nrecipes\n1.3.1\n\n\nrpart\n4.1.24\n\n\nrsample\n1.3.1\n\n\nRSpectra\n0.16-2\n\n\nrules\n1.0.2\n\n\nscales\n1.4.0\n\n\nsplines2\n0.5.4\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nstacks\n1.1.1.9001\n\n\ntailor\n0.1.0.9000\n\n\ntext2vec\n0.6.4\n\n\ntextrecipes\n1.1.0\n\n\ntidymodels\n1.4.1\n\n\ntidyr\n1.3.1\n\n\ntune\n2.0.0\n\n\nuwot\n0.2.3\n\n\nvetiver\n0.2.5\n\n\nworkflows\n1.3.0\n\n\nworkflowsets\n1.1.1\n\n\nyardstick\n1.3.2"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#looking-at-the-predictors",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#looking-at-the-predictors",
    "title": "Extras - Recipes",
    "section": "Looking at the predictors",
    "text": "Looking at the predictors\n\nforested_train\n#&gt; # A tibble: 8,749 Ã— 19\n#&gt;    forested  year elevation eastness roughness tree_no_tree dew_temp precip_annual temp_annual_mean temp_annual_min temp_annual_max temp_january_min vapor_min vapor_max canopy_cover   lon   lat\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 Yes       1997        66       82        10 Tree            12.2           1315             18.4            1.88            24.9            11.8        121      1888           66 -84.9  32.4\n#&gt;  2 No        1997       284      -99        58 Tree            10.3           1236             16.1           -0.26            22.3             9.92        68      1586           80 -85.0  34.1\n#&gt;  3 Yes       2022       130       86        15 Tree            11.8           1194             17.6            1.3             24.1            11.2         61      1753           96 -83.0  33.1\n#&gt;  4 Yes       2021       202      -55         3 Tree            10.7           1235             16.6            0.05            23.0            10.2         72      1682           65 -83.0  33.9\n#&gt;  5 Yes       1995        75      -89         1 Tree            13.8           1256             19.2            3.63            25.5            12.9         57      1796           88 -83.4  31.3\n#&gt;  6 No        1995       110      -53         5 Tree            12.4           1236             18.6            2.53            24.8            12.4        102      1835           51 -83.9  32.2\n#&gt;  7 Yes       2022       111       73        12 Tree            11.5           1168             17.4            1               24.0            10.9         67      1772           84 -82.2  33.6\n#&gt;  8 Yes       1997       230       96        14 Tree             9.98          1373             15.4           -1.35            21.8             9.03        46      1552           68 -85.3  34.8\n#&gt;  9 Yes       2002       160      -88        13 Tree            11.1           1219             16.9            0.07            23.6            10.2         53      1731           95 -83.2  33.6\n#&gt; 10 Yes       2020        39        9         6 Tree            13.9           1237             19.2            3.25            25.6            12.8         58      1812           86 -82.0  31.7\n#&gt; # â„¹ 8,739 more rows\n#&gt; # â„¹ 2 more variables: land_type &lt;fct&gt;, county &lt;fct&gt;"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#working-with-other-models",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#working-with-other-models",
    "title": "Extras - Recipes",
    "section": "Working with other models",
    "text": "Working with other models\nSome models canâ€™t handle non-numeric data\n\nLinear Regression\nK Nearest Neighbors\n\n\n\nSome models struggle if numeric predictors arenâ€™t scaled\n\nK Nearest Neighbors\nAnything using gradient descent"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#types-of-needed-preprocessing",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#types-of-needed-preprocessing",
    "title": "Extras - Recipes",
    "section": "Types of needed preprocessing",
    "text": "Types of needed preprocessing\n\nDo qualitative predictors require a numeric encoding?\nShould columns with a single unique value be removed?\nDoes the model struggle with missing data?\nDoes the model struggle with correlated predictors?\nShould predictors be centered and scaled?\nIs it helpful to transform predictors to be more symmetric?\n\n\nhttps://www.tmwr.org/pre-proc-table.html"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#two-types-of-preprocessing",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#two-types-of-preprocessing",
    "title": "Extras - Recipes",
    "section": "Two types of preprocessing",
    "text": "Two types of preprocessing"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#two-types-of-preprocessing-1",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#two-types-of-preprocessing-1",
    "title": "Extras - Recipes",
    "section": "Two types of preprocessing",
    "text": "Two types of preprocessing"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#general-definitions",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#general-definitions",
    "title": "Extras - Recipes",
    "section": "General definitions",
    "text": "General definitions\n\nData preprocessing is what you do to make your model successful.\nFeature engineering is what you do to the original predictors to make the model do the least work to perform great."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#working-with-dates",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#working-with-dates",
    "title": "Extras - Recipes",
    "section": "Working with dates",
    "text": "Working with dates\nDatetime variables are automatically converted to an integer if given as a raw predictor. To avoid this, it can be re-encoded as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nLeap year\nIndicators for holidays"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#two-types-of-transformations",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#two-types-of-transformations",
    "title": "Extras - Recipes",
    "section": "Two types of transformations",
    "text": "Two types of transformations\n\n\n\nStatic\n\nSquare root, log, inverse\nDummies for known levels\nDate time extractions\n\n\nTrained\n\nCentering & scaling\nImputation\nPCA\nAnything for unknown factor levels\n\n\n\nTrained methods need to calculate sufficient information to be applied again."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#the-recipes-package",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#the-recipes-package",
    "title": "Extras - Recipes",
    "section": "The recipes package",
    "text": "The recipes package\n\n\nModular + extensible\nWorks well with pipes ,|&gt; and %&gt;%\nDeferred evaluation\nIsolates test data from training data\nCan do things formulas canâ€™t"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#how-to-write-a-recipe",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#how-to-write-a-recipe",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) |&gt;\nÂ Â step_dummy(all_nominal_predictors()) |&gt;\nÂ Â step_zv(all_predictors()) |&gt;\nÂ Â step_log(canopy_cover, offset = 0.5) |&gt;\nÂ Â step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#how-to-write-a-recipe-1",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#how-to-write-a-recipe-1",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) |&gt;\nÂ Â step_dummy(all_nominal_predictors()) |&gt;\nÂ Â step_zv(all_predictors()) |&gt;\nÂ Â step_log(canopy_cover, offset = 0.5) |&gt;\nÂ Â step_normalize(all_numeric_predictors())\n\n\nStart by calling recipe() to denote the data source and variables used."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#how-to-write-a-recipe-2",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#how-to-write-a-recipe-2",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) |&gt;\nÂ Â step_dummy(all_nominal_predictors()) |&gt;\nÂ Â step_zv(all_predictors()) |&gt;\nÂ Â step_log(canopy_cover, offset = 0.5) |&gt;\nÂ Â step_normalize(all_numeric_predictors())\n\n\nSpecify what actions to take by adding step_*()s."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#how-to-write-a-recipe-3",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#how-to-write-a-recipe-3",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) |&gt;\nÂ Â step_dummy(all_nominal_predictors()) |&gt;\nÂ Â step_zv(all_predictors()) |&gt;\nÂ Â step_log(canopy_cover, offset = 0.5) |&gt; Â Â step_normalize(all_numeric_predictors())\n\n\nUse {tidyselect} and recipes-specific selectors to denote affected variables."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#using-a-recipe",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#using-a-recipe",
    "title": "Extras - Recipes",
    "section": "Using a recipe",
    "text": "Using a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) |&gt;\nÂ Â step_dummy(all_nominal_predictors()) |&gt;\nÂ Â step_zv(all_predictors()) |&gt;\nÂ Â step_log(canopy_cover, offset = 0.5) |&gt; Â Â step_normalize(all_numeric_predictors())\n\n\nSave the recipe we like so that we can use it in various places, e.g., with different models."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#using-a-recipe-with-workflows",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#using-a-recipe-with-workflows",
    "title": "Extras - Recipes",
    "section": "Using a recipe with workflows",
    "text": "Using a recipe with workflows\nRecipes are typically combined with a model in a workflow() object:\n\n\nforested_wflow &lt;- workflow() |&gt;\nÂ Â add_recipe(forested_rec) |&gt;\nÂ Â add_model(linear_reg())"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#recipes-are-estimated",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#recipes-are-estimated",
    "title": "Extras - Recipes",
    "section": "Recipes are estimated",
    "text": "Recipes are estimated\nEvery preprocessing step in a recipe that involved calculations uses the training set. For example:\n\nLevels of a factor\nDetermination of zero-variance\nNormalization\nFeature extraction\n\nOnce a recipe is added to a workflow, this occurs when fit() is called."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#debugging-a-recipe",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#debugging-a-recipe",
    "title": "Extras - Recipes",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g.Â forested_rec) can be estimated manually with a function called prep(). It is analogous to fit(). See TMwR section 16.4.\n\n\n\n\nAnother function, bake(), is analogous to predict(), and gives you the processed data back."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#your-turn",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#your-turn",
    "title": "Extras - Recipes",
    "section": "Your turn",
    "text": "Your turn\n\n\nTake the recipe and prep() then bake() it to see what the resulting data set looks like.\nTry removing steps to see how the result changes.\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#printing-a-recipe",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#printing-a-recipe",
    "title": "Extras - Recipes",
    "section": "Printing a recipe",
    "text": "Printing a recipe\n\nforested_rec\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 18\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Dummy variables from: all_nominal_predictors()\n#&gt; â€¢ Zero variance filter on: all_predictors()\n#&gt; â€¢ Log transformation on: canopy_cover\n#&gt; â€¢ Centering and scaling for: all_numeric_predictors()"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#prepping-a-recipe",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#prepping-a-recipe",
    "title": "Extras - Recipes",
    "section": "Prepping a recipe",
    "text": "Prepping a recipe\n\nprep(forested_rec)\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 18\n#&gt; \n#&gt; â”€â”€ Training information\n#&gt; Training data contained 8749 data points and no incomplete rows.\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Dummy variables from: tree_no_tree, land_type, county | Trained\n#&gt; â€¢ Zero variance filter removed: &lt;none&gt; | Trained\n#&gt; â€¢ Log transformation on: canopy_cover | Trained\n#&gt; â€¢ Centering and scaling for: year elevation, ... | Trained"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#baking-a-recipe",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#baking-a-recipe",
    "title": "Extras - Recipes",
    "section": "Baking a recipe",
    "text": "Baking a recipe\n\nprep(forested_rec) |&gt;\n  bake(new_data = forested_train)\n#&gt; # A tibble: 8,749 Ã— 177\n#&gt;      year elevation eastness roughness dew_temp precip_annual temp_annual_mean temp_annual_min temp_annual_max temp_january_min vapor_min vapor_max canopy_cover     lon     lat forested\n#&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n#&gt;  1 -1.15    -0.521    1.13      0.0463  -0.232          0.178            0.172          -0.107           0.338         -0.00909    3.10     1.01          0.407  -1.44   -0.0184 Yes     \n#&gt;  2 -1.15     1.10    -1.50      3.91    -1.39          -0.568           -1.31           -1.33           -1.28          -1.28       0.0408  -0.941         0.652  -1.51    1.36   No      \n#&gt;  3  0.892   -0.0440   1.19      0.449   -0.422         -0.965           -0.305          -0.438          -0.170         -0.438     -0.363    0.140         0.884   0.331   0.556  Yes     \n#&gt;  4  0.810    0.493   -0.858    -0.518   -1.11          -0.577           -1.01           -1.15           -0.878         -1.10       0.272   -0.320         0.388   0.331   1.26   Yes     \n#&gt;  5 -1.31    -0.454   -1.35     -0.679    0.748         -0.379            0.701           0.893           0.683          0.702     -0.594    0.418         0.773  -0.0643 -0.992  Yes     \n#&gt;  6 -1.31    -0.193   -0.829    -0.357   -0.0546        -0.568            0.331           0.264           0.281          0.366      2.00     0.670         0.0802 -0.468  -0.237  No      \n#&gt;  7  0.892   -0.186    1.00      0.207   -0.631         -1.21            -0.444          -0.609          -0.264         -0.619     -0.0169   0.262         0.714   1.01    0.929  Yes     \n#&gt;  8 -1.15     0.702    1.34      0.369   -1.56           0.726           -1.77           -1.95           -1.60          -1.87      -1.23    -1.16          0.445  -1.77    2.03   Yes     \n#&gt;  9 -0.740    0.180   -1.34      0.288   -0.858         -0.729           -0.801          -1.14           -0.515         -1.08      -0.825   -0.00273       0.870   0.122   0.988  Yes     \n#&gt; 10  0.729   -0.722    0.0719   -0.276    0.816         -0.559            0.708           0.676           0.745          0.635     -0.536    0.521         0.744   1.22   -0.653  Yes     \n#&gt; # â„¹ 8,739 more rows\n#&gt; # â„¹ 161 more variables: tree_no_tree_No.tree &lt;dbl&gt;, land_type_Non.tree.vegetation &lt;dbl&gt;, land_type_Tree &lt;dbl&gt;, county_Atkinson &lt;dbl&gt;, county_Bacon &lt;dbl&gt;, county_Baker &lt;dbl&gt;, county_Baldwin &lt;dbl&gt;,\n#&gt; #   county_Banks &lt;dbl&gt;, county_Barrow &lt;dbl&gt;, county_Bartow &lt;dbl&gt;, county_Ben.Hill &lt;dbl&gt;, county_Berrien &lt;dbl&gt;, county_Bibb &lt;dbl&gt;, county_Bleckley &lt;dbl&gt;, county_Brantley &lt;dbl&gt;, county_Brooks &lt;dbl&gt;,\n#&gt; #   county_Bryan &lt;dbl&gt;, county_Bulloch &lt;dbl&gt;, county_Burke &lt;dbl&gt;, county_Butts &lt;dbl&gt;, county_Calhoun &lt;dbl&gt;, county_Camden &lt;dbl&gt;, county_Candler &lt;dbl&gt;, county_Carroll &lt;dbl&gt;, county_Catoosa &lt;dbl&gt;,\n#&gt; #   county_Charlton &lt;dbl&gt;, county_Chatham &lt;dbl&gt;, county_Chattahoochee &lt;dbl&gt;, county_Chattooga &lt;dbl&gt;, county_Cherokee &lt;dbl&gt;, county_Clarke &lt;dbl&gt;, county_Clay &lt;dbl&gt;, county_Clayton &lt;dbl&gt;,\n#&gt; #   county_Clinch &lt;dbl&gt;, county_Cobb &lt;dbl&gt;, county_Coffee &lt;dbl&gt;, county_Colquitt &lt;dbl&gt;, county_Columbia &lt;dbl&gt;, county_Cook &lt;dbl&gt;, county_Coweta &lt;dbl&gt;, county_Crawford &lt;dbl&gt;, county_Crisp &lt;dbl&gt;,\n#&gt; #   county_Dade &lt;dbl&gt;, county_Dawson &lt;dbl&gt;, county_Decatur &lt;dbl&gt;, county_DeKalb &lt;dbl&gt;, county_Dodge &lt;dbl&gt;, county_Dooly &lt;dbl&gt;, county_Dougherty &lt;dbl&gt;, county_Douglas &lt;dbl&gt;, county_Early &lt;dbl&gt;, â€¦"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#tidying-a-recipe",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#tidying-a-recipe",
    "title": "Extras - Recipes",
    "section": "Tidying a recipe",
    "text": "Tidying a recipe\nOnce a recipe as been estimated, there are various bits of information saved in it.\n\nThe tidy() function can be used to get specific results from the recipe."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#your-turn-1",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#your-turn-1",
    "title": "Extras - Recipes",
    "section": "Your turn",
    "text": "Your turn\n\nTake a prepped recipe and use the tidy() function on it.\nUse the number argument to inspect different steps.\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#tidying-a-recipe-1",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#tidying-a-recipe-1",
    "title": "Extras - Recipes",
    "section": "Tidying a recipe",
    "text": "Tidying a recipe\n\nprep(forested_rec) |&gt;\n  tidy()\n#&gt; # A tibble: 4 Ã— 6\n#&gt;   number operation type      trained skip  id             \n#&gt;    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n#&gt; 1      1 step      dummy     TRUE    FALSE dummy_hIEnQ    \n#&gt; 2      2 step      zv        TRUE    FALSE zv_ZrQBx       \n#&gt; 3      3 step      log       TRUE    FALSE log_6es7X      \n#&gt; 4      4 step      normalize TRUE    FALSE normalize_XsIxb"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#tidying-a-recipe-2",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#tidying-a-recipe-2",
    "title": "Extras - Recipes",
    "section": "Tidying a recipe",
    "text": "Tidying a recipe\n\nprep(forested_rec) |&gt;\n  tidy(number = 1)\n#&gt; # A tibble: 161 Ã— 3\n#&gt;    terms        columns             id         \n#&gt;    &lt;chr&gt;        &lt;chr&gt;               &lt;chr&gt;      \n#&gt;  1 tree_no_tree No tree             dummy_hIEnQ\n#&gt;  2 land_type    Non-tree vegetation dummy_hIEnQ\n#&gt;  3 land_type    Tree                dummy_hIEnQ\n#&gt;  4 county       Atkinson            dummy_hIEnQ\n#&gt;  5 county       Bacon               dummy_hIEnQ\n#&gt;  6 county       Baker               dummy_hIEnQ\n#&gt;  7 county       Baldwin             dummy_hIEnQ\n#&gt;  8 county       Banks               dummy_hIEnQ\n#&gt;  9 county       Barrow              dummy_hIEnQ\n#&gt; 10 county       Bartow              dummy_hIEnQ\n#&gt; # â„¹ 151 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#using-a-recipe-in-tidymodels",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#using-a-recipe-in-tidymodels",
    "title": "Extras - Recipes",
    "section": "Using a recipe in tidymodels",
    "text": "Using a recipe in tidymodels\nThe recommended way to use a recipe in tidymodels is to use it as part of a workflow().\n\nforested_wflow &lt;- workflow() |&gt;  \n  add_recipe(forested_rec) |&gt;  \n  add_model(linear_reg())\n\nWhen used in this way, you donâ€™t need to worry about prep() and bake() as it is handled for you."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-recipes.html#more-information",
    "href": "archive/2025-08-nyr/slides/intro-extra-recipes.html#more-information",
    "title": "Extras - Recipes",
    "section": "More information",
    "text": "More information\n\nhttps://recipes.tidymodels.org/\nhttps://www.tmwr.org/recipes.html"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#tuning-parameters",
    "href": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#tuning-parameters",
    "title": "5 - Tuning models",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#optimize-tuning-parameters",
    "href": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#optimize-tuning-parameters",
    "title": "5 - Tuning models",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#optimize-tuning-parameters-1",
    "href": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#optimize-tuning-parameters-1",
    "title": "5 - Tuning models",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search ğŸ’  which tests a pre-defined set of candidate values\nIterative search ğŸŒ€ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#specifying-tuning-parameters",
    "href": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#specifying-tuning-parameters",
    "title": "5 - Tuning models",
    "section": "Specifying tuning parameters",
    "text": "Specifying tuning parameters\nLetâ€™s take our previous random forest workflow and tag for tuning the minimum number of data points in each node:\n\nrf_spec &lt;- rand_forest(min_n = tune()) |&gt; \n  set_mode(\"classification\")\n\nrf_wflow &lt;- workflow(forested ~ ., rf_spec)\nrf_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   min_n = tune()\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#try-out-multiple-values",
    "href": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#try-out-multiple-values",
    "title": "5 - Tuning models",
    "section": "Try out multiple values",
    "text": "Try out multiple values\ntune_grid() works similar to fit_resamples() but covers multiple parameter values:\n\nset.seed(22)\nrf_res &lt;- tune_grid(\n  rf_wflow,\n  forested_folds,\n  grid = 5\n)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#compare-results",
    "href": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#compare-results",
    "title": "5 - Tuning models",
    "section": "Compare results",
    "text": "Compare results\nInspecting results and selecting the best-performing hyperparameter(s):\n\nshow_best(rf_res)\n#&gt; # A tibble: 5 Ã— 7\n#&gt;   min_n .metric .estimator  mean     n std_err .config             \n#&gt;   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1    40 roc_auc binary     0.762    10 0.0107  Preprocessor1_Model1\n#&gt; 2    31 roc_auc binary     0.761    10 0.0101  Preprocessor1_Model2\n#&gt; 3    22 roc_auc binary     0.760    10 0.0103  Preprocessor1_Model5\n#&gt; 4    12 roc_auc binary     0.758    10 0.0101  Preprocessor1_Model4\n#&gt; 5     3 roc_auc binary     0.755    10 0.00969 Preprocessor1_Model3\n\nbest_parameter &lt;- select_best(rf_res)\nbest_parameter\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   min_n .config             \n#&gt;   &lt;int&gt; &lt;chr&gt;               \n#&gt; 1    40 Preprocessor1_Model1\n\ncollect_metrics() and autoplot() are also available."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#the-final-fit",
    "href": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#the-final-fit",
    "title": "5 - Tuning models",
    "section": "The final fit",
    "text": "The final fit\n\nrf_wflow &lt;- finalize_workflow(rf_wflow, best_parameter)\n\nfinal_fit &lt;- last_fit(rf_wflow, forested_split) \n\ncollect_metrics(final_fit)\n#&gt; # A tibble: 3 Ã— 4\n#&gt;   .metric     .estimator .estimate .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary         0.764 Preprocessor1_Model1\n#&gt; 2 roc_auc     binary         0.764 Preprocessor1_Model1\n#&gt; 3 brier_class binary         0.161 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#your-turn",
    "href": "archive/2025-08-nyr/slides/intro-05-tuning-models.html#your-turn",
    "title": "5 - Tuning models",
    "section": "Your turn",
    "text": "Your turn\n\nModify your model workflow to tune one or more parameters.\nUse grid search to find the best parameter(s).\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#your-turn",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#your-turn",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nHow do you fit a linear model in R?\nHow many different ways can you think of?\n\n\n\nâˆ’+\n03:00\n\n\n\n\n\nlm for linear model\nglmnet for regularized regression\nkeras for regression using TensorFlow\nstan for Bayesian regression\nspark for large data sets\nbrulee for regression using torch"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-1",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-1",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg()\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\n\nModels have default engines"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-2",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-2",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-3",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-3",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg() |&gt;\n  set_engine(\"glmnet\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glmnet"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-4",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-4",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg() |&gt;\n  set_engine(\"stan\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: stan"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-5",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-5",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-6",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-6",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree()\n#&gt; Decision Tree Model Specification (unknown mode)\n#&gt; \n#&gt; Computational engine: rpart\n\n\nSome models have a default mode"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-7",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-7",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree() |&gt; \n  set_mode(\"classification\")\n#&gt; Decision Tree Model Specification (classification)\n#&gt; \n#&gt; Computational engine: rpart\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-8",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#to-specify-a-model-8",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#your-turn-1",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#your-turn-1",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_spec chunk in your .qmd.\nEdit this code to use a logistic regression model.\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/\n\n\nExtension/Challenge: Edit this code to use a different model. For example, try using a conditional inference tree as implemented in the partykit package by changing the engine - or try an entirely different model type!\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#models-well-be-using-today",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#models-well-be-using-today",
    "title": "3 - What makes a model?",
    "section": "Models weâ€™ll be using today",
    "text": "Models weâ€™ll be using today\n\nLogistic regression\nDecision trees"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#logistic-regression",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#logistic-regression",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#logistic-regression-1",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#logistic-regression-1",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#logistic-regression-2",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#logistic-regression-2",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogit of outcome probability modeled as linear combination of predictors:\n\n\\(log(\\frac{p}{1 - p}) = \\beta_0 + \\beta_1\\cdot \\text{A}\\)\n\nFind a sigmoid line that separates the two classes"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#decision-trees",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#decision-trees",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#decision-trees-1",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#decision-trees-1",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#decision-trees-2",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#decision-trees-2",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "title": "3 - What makes a model?",
    "section": "All models are wrong, but some are useful!",
    "text": "All models are wrong, but some are useful!\n\n\nLogistic regression\n\n\n\n\n\n\n\n\n\n\nDecision trees"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "title": "3 - What makes a model?",
    "section": "Workflows bind preprocessors and models",
    "text": "Workflows bind preprocessors and models\n\n\nExplain that PCA that is a preprocessor / dimensionality reduction, used to decorrelate data"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#what-is-wrong-with-this",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#what-is-wrong-with-this",
    "title": "3 - What makes a model?",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#why-a-workflow",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#why-a-workflow",
    "title": "3 - What makes a model?",
    "section": "Why a workflow()? ",
    "text": "Why a workflow()? \n\n\nWorkflows handle new data better than base R tools in terms of new factor levels\n\n\n\n\nYou can use other preprocessors besides formulas (more on feature engineering in Advanced tidymodels!)\n\n\n\n\nThey can help organize your work when working with multiple models\n\n\n\n\nMost importantly, a workflow captures the entire modeling process: fit() and predict() apply to the preprocessing steps in addition to the actual model fit\n\n\nTwo ways workflows handle levels better than base R:\n\nEnforces that new levels are not allowed at prediction time (this is an optional check that can be turned off)\nRestores missing levels that were present at fit time, but happen to be missing at prediction time (like, if your â€œnewâ€ data just doesnâ€™t have an instance of that level)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#a-model-workflow-1",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#a-model-workflow-1",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() |&gt; \n  set_mode(\"classification\")\n\ntree_spec |&gt; \n  fit(forested ~ ., data = forested_train) \n#&gt; parsnip model object\n#&gt; \n#&gt; n= 8749 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt; 1) root 8749 2427 Yes (0.7225969 0.2774031)  \n#&gt;   2) county=Appling,Atkinson,Bacon,Baldwin,Ben Hill,Brantley,Brooks,Bryan,Bulloch,Burke,Butts,Camden,Candler,Carroll,Charlton,Chattahoochee,Chattooga,Cherokee,Clinch,Coffee,Coweta,Crawford,Dade,Dawson,Dodge,Dougherty,Douglas,Echols,Effingham,Elbert,Emanuel,Evans,Fannin,Floyd,Gilmer,Glascock,Greene,Habersham,Hancock,Haralson,Harris,Heard,Jasper,Jeff Davis,Jefferson,Jenkins,Johnson,Jones,Lamar,Lanier,Laurens,Lee,Lincoln,Long,Lumpkin,Marion,McDuffie,Meriwether,Monroe,Montgomery,Morgan,Murray,Oconee,Oglethorpe,Paulding,Pickens,Pierce,Pike,Polk,Putnam,Quitman,Rabun,Randolph,Schley,Screven,Spalding,Stephens,Stewart,Talbot,Taliaferro,Tattnall,Taylor,Telfair,Terrell,Towns,Treutlen,Troup,Twiggs,Union,Upson,Walker,Ware,Warren,Washington,Wayne,Webster,Wheeler,White,Wilcox,Wilkes,Wilkinson 5598 1005 Yes (0.8204716 0.1795284) *\n#&gt;   3) county=Baker,Banks,Barrow,Bartow,Berrien,Bibb,Bleckley,Calhoun,Catoosa,Chatham,Clarke,Clay,Clayton,Cobb,Colquitt,Columbia,Cook,Crisp,Decatur,DeKalb,Dooly,Early,Fayette,Forsyth,Franklin,Fulton,Glynn,Gordon,Grady,Gwinnett,Hall,Hart,Henry,Houston,Irwin,Jackson,Liberty,Lowndes,Macon,Madison,McIntosh,Miller,Mitchell,Muscogee,Newton,Peach,Pulaski,Richmond,Rockdale,Seminole,Sumter,Thomas,Tift,Toombs,Turner,Walton,Whitfield,Worth 3151 1422 Yes (0.5487147 0.4512853)  \n#&gt;     6) canopy_cover&gt;=41.5 1773  603 Yes (0.6598985 0.3401015) *\n#&gt;     7) canopy_cover&lt; 41.5 1378  559 No (0.4056604 0.5943396) *"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#a-model-workflow-2",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#a-model-workflow-2",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() |&gt; \n  set_mode(\"classification\")\n\nworkflow() |&gt;\n  add_formula(forested ~ .) |&gt;\n  add_model(tree_spec) |&gt;\n  fit(data = forested_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 8749 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt; 1) root 8749 2427 Yes (0.7225969 0.2774031)  \n#&gt;   2) county=Appling,Atkinson,Bacon,Baldwin,Ben Hill,Brantley,Brooks,Bryan,Bulloch,Burke,Butts,Camden,Candler,Carroll,Charlton,Chattahoochee,Chattooga,Cherokee,Clinch,Coffee,Coweta,Crawford,Dade,Dawson,Dodge,Dougherty,Douglas,Echols,Effingham,Elbert,Emanuel,Evans,Fannin,Floyd,Gilmer,Glascock,Greene,Habersham,Hancock,Haralson,Harris,Heard,Jasper,Jeff Davis,Jefferson,Jenkins,Johnson,Jones,Lamar,Lanier,Laurens,Lee,Lincoln,Long,Lumpkin,Marion,McDuffie,Meriwether,Monroe,Montgomery,Morgan,Murray,Oconee,Oglethorpe,Paulding,Pickens,Pierce,Pike,Polk,Putnam,Quitman,Rabun,Randolph,Schley,Screven,Spalding,Stephens,Stewart,Talbot,Taliaferro,Tattnall,Taylor,Telfair,Terrell,Towns,Treutlen,Troup,Twiggs,Union,Upson,Walker,Ware,Warren,Washington,Wayne,Webster,Wheeler,White,Wilcox,Wilkes,Wilkinson 5598 1005 Yes (0.8204716 0.1795284) *\n#&gt;   3) county=Baker,Banks,Barrow,Bartow,Berrien,Bibb,Bleckley,Calhoun,Catoosa,Chatham,Clarke,Clay,Clayton,Cobb,Colquitt,Columbia,Cook,Crisp,Decatur,DeKalb,Dooly,Early,Fayette,Forsyth,Franklin,Fulton,Glynn,Gordon,Grady,Gwinnett,Hall,Hart,Henry,Houston,Irwin,Jackson,Liberty,Lowndes,Macon,Madison,McIntosh,Miller,Mitchell,Muscogee,Newton,Peach,Pulaski,Richmond,Rockdale,Seminole,Sumter,Thomas,Tift,Toombs,Turner,Walton,Whitfield,Worth 3151 1422 Yes (0.5487147 0.4512853)  \n#&gt;     6) canopy_cover&gt;=41.5 1773  603 Yes (0.6598985 0.3401015) *\n#&gt;     7) canopy_cover&lt; 41.5 1378  559 No (0.4056604 0.5943396) *"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#a-model-workflow-3",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#a-model-workflow-3",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() |&gt; \n  set_mode(\"classification\")\n\nworkflow(forested ~ ., tree_spec) |&gt; \n  fit(data = forested_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 8749 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt; 1) root 8749 2427 Yes (0.7225969 0.2774031)  \n#&gt;   2) county=Appling,Atkinson,Bacon,Baldwin,Ben Hill,Brantley,Brooks,Bryan,Bulloch,Burke,Butts,Camden,Candler,Carroll,Charlton,Chattahoochee,Chattooga,Cherokee,Clinch,Coffee,Coweta,Crawford,Dade,Dawson,Dodge,Dougherty,Douglas,Echols,Effingham,Elbert,Emanuel,Evans,Fannin,Floyd,Gilmer,Glascock,Greene,Habersham,Hancock,Haralson,Harris,Heard,Jasper,Jeff Davis,Jefferson,Jenkins,Johnson,Jones,Lamar,Lanier,Laurens,Lee,Lincoln,Long,Lumpkin,Marion,McDuffie,Meriwether,Monroe,Montgomery,Morgan,Murray,Oconee,Oglethorpe,Paulding,Pickens,Pierce,Pike,Polk,Putnam,Quitman,Rabun,Randolph,Schley,Screven,Spalding,Stephens,Stewart,Talbot,Taliaferro,Tattnall,Taylor,Telfair,Terrell,Towns,Treutlen,Troup,Twiggs,Union,Upson,Walker,Ware,Warren,Washington,Wayne,Webster,Wheeler,White,Wilcox,Wilkes,Wilkinson 5598 1005 Yes (0.8204716 0.1795284) *\n#&gt;   3) county=Baker,Banks,Barrow,Bartow,Berrien,Bibb,Bleckley,Calhoun,Catoosa,Chatham,Clarke,Clay,Clayton,Cobb,Colquitt,Columbia,Cook,Crisp,Decatur,DeKalb,Dooly,Early,Fayette,Forsyth,Franklin,Fulton,Glynn,Gordon,Grady,Gwinnett,Hall,Hart,Henry,Houston,Irwin,Jackson,Liberty,Lowndes,Macon,Madison,McIntosh,Miller,Mitchell,Muscogee,Newton,Peach,Pulaski,Richmond,Rockdale,Seminole,Sumter,Thomas,Tift,Toombs,Turner,Walton,Whitfield,Worth 3151 1422 Yes (0.5487147 0.4512853)  \n#&gt;     6) canopy_cover&gt;=41.5 1773  603 Yes (0.6598985 0.3401015) *\n#&gt;     7) canopy_cover&lt; 41.5 1378  559 No (0.4056604 0.5943396) *"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#your-turn-2",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#your-turn-2",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_wflow chunk in your .qmd.\nEdit this code to make a workflow with your own model of choice.\n\nExtension/Challenge: Other than formulas, what kinds of preprocessors are supported?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#predict-with-your-model",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#predict-with-your-model",
    "title": "3 - What makes a model?",
    "section": "Predict with your model  ",
    "text": "Predict with your model  \nHow do you use your new tree_fit model?\n\ntree_spec &lt;-\n  decision_tree() |&gt; \n  set_mode(\"classification\")\n\ntree_fit &lt;-\n  workflow(forested ~ ., tree_spec) |&gt; \n  fit(data = forested_train)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#your-turn-3",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#your-turn-3",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\npredict(tree_fit, new_data = forested_test)\nWhat do you notice about the structure of the result?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#your-turn-4",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#your-turn-4",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\naugment(tree_fit, new_data = forested_test)\nHow does the output compare to the output from predict()?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#understand-your-model",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#understand-your-model",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#understand-your-model-1",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#understand-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nlibrary(rpart.plot)\ntree_fit |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot(roundint = FALSE)\n\nYou can extract_*() several components of your fitted workflow.\n\nâš ï¸ Never predict() with any extracted components!\n\nroundint = FALSE is only to quiet a warning"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#understand-your-model-2",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#understand-your-model-2",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nYou can use your fitted workflow for model and/or prediction explanations:\n\n\n\noverall variable importance, such as with the vip package\n\n\n\n\nflexible model explainers, such as with the DALEXtra package\n\n\n\nLearn more at https://www.tmwr.org/explain.html"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#your-turn-5",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#your-turn-5",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\n\nExtract the model engine object from your fitted workflow and check it out.\n\n\n\nâˆ’+\n05:00\n\n\n\n\nAfterward, ask what kind of object people got from the extraction, and what they did with it (e.g.Â give it to summary(), plot(), broom::tidy() ). Live code along"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#the-whole-game---status-update",
    "href": "archive/2025-08-nyr/slides/intro-03-what-makes-a-model.html#the-whole-game---status-update",
    "title": "3 - What makes a model?",
    "section": "The whole game - status update",
    "text": "The whole game - status update\n\n\nStress that fitting a model on the entire training set was only for illustrating how to fit a model"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#who-are-you",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %&gt;% or base R |&gt; pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have some exposure to basic statistical concepts like linear models and residuals\nYou do not need intermediate or expert familiarity with modeling or ML"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#who-are-tidymodels",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#who-are-tidymodels",
    "title": "1 - Introduction",
    "section": "Who are tidymodels?",
    "text": "Who are tidymodels?\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\nMany thanks to Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill, and DesirÃ©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#section-2",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#section-2",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#section-3",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#section-3",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#plan-for-this-workshop",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Plan for this workshop",
    "text": "Plan for this workshop\n\nYour data budget\nWhat makes a model\nEvaluating models\nTuning models"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#section-4",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#section-4",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors ğŸ‘‹"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#what-is-machine-learning",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#what-is-machine-learning",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nhttps://xkcd.com/1838/"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#what-is-machine-learning-1",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#what-is-machine-learning-1",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#what-is-machine-learning-2024-edition",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#what-is-machine-learning-2024-edition",
    "title": "1 - Introduction",
    "section": "What is machine learning? (2024 edition)",
    "text": "What is machine learning? (2024 edition)\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/\n\n\nIn the early 2010s, â€œArtificial intelligenceâ€ (AI) was largely synonymous with what weâ€™ll refer to as â€œmachine learningâ€ in this workshop. In the late 2010s and early 2020s, AI usually referred to deep learning methods. Since the release of ChatGPT in late 2022, â€œAIâ€ has come to also encompass large language models / generative models."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#what-is-machine-learning-2",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#what-is-machine-learning-2",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#your-turn",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\n\n\nHow are statistics and machine learning related?\nHow are they similar? Different?\n\n\n\nâˆ’+\n03:00\n\n\n\n\nthe â€œtwo culturesâ€\nmodel first vs.Â data first\ninference vs.Â prediction"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#what-is-tidymodels",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#what-is-tidymodels",
    "title": "1 - Introduction",
    "section": "What is tidymodels? ",
    "text": "What is tidymodels? \n\nlibrary(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.3.0 â”€â”€\n#&gt; âœ” broom        1.0.9          âœ” rsample      1.3.1     \n#&gt; âœ” dials        1.4.1          âœ” tibble       3.3.0     \n#&gt; âœ” dplyr        1.1.4          âœ” tidyr        1.3.1     \n#&gt; âœ” infer        1.0.9          âœ” tune         1.3.0     \n#&gt; âœ” modeldata    1.5.1          âœ” workflows    1.2.0.9002\n#&gt; âœ” parsnip      1.3.2.9000     âœ” workflowsets 1.1.1     \n#&gt; âœ” purrr        1.1.0          âœ” yardstick    1.3.2     \n#&gt; âœ” recipes      1.3.1\n#&gt; â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\n#&gt; âœ– purrr::discard() masks scales::discard()\n#&gt; âœ– dplyr::filter()  masks stats::filter()\n#&gt; âœ– dplyr::lag()     masks stats::lag()\n#&gt; âœ– recipes::step()  masks stats::step()"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\nRoadmap for today\nMinimal version of predictive modeling process\nFeature engineering and tuning as iterative extensions"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-1",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-1",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-2",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-2",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\n\nStress that we are not fitting a model on the entire training set other than for illustrative purposes in deck 2."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-3",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-3",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-4",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-4",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-5",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-5",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-6",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-6",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-7",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#the-whole-game-7",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#lets-install-some-packages",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Letâ€™s install some packages",
    "text": "Letâ€™s install some packages\nIf you are using your own laptop instead of Posit Cloud:\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"bonsai\", \"Cubist\", \"doParallel\", \"earth\", \"embed\", \"finetune\", \n    \"lightgbm\", \"lme4\", \"pak\", \"parallelly\", \"plumber\", \"probably\", \n    \"ranger\", \"rpart\", \"rpart.plot\", \"rules\", \"splines2\", \"stacks\", \n    \"text2vec\", \"textrecipes\", \"tidymodels\", \"vetiver\")\n\ninstall.packages(pkgs)\n\npak::pak(\"simonpcouch/forested\")"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-01-introduction.html#our-versions",
    "href": "archive/2025-08-nyr/slides/intro-01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\nR version 4.5.0 (2025-04-11), Quarto (1.6.42)\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nbonsai\n0.4.0\n\n\nbroom\n1.0.9\n\n\nCubist\n0.5.0\n\n\ndials\n1.4.1\n\n\ndoParallel\n1.0.17\n\n\ndplyr\n1.1.4\n\n\nearth\n5.3.4\n\n\nembed\n1.1.5.9000\n\n\nfinetune\n1.2.1.9000\n\n\nforested\n0.2.0\n\n\nFormula\n1.2-5\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nggplot2\n3.5.2\n\n\nlattice\n0.22-7\n\n\nlightgbm\n4.6.0\n\n\nlme4\n1.1-37\n\n\nmodeldata\n1.5.1\n\n\nparallelly\n1.45.1\n\n\nparsnip\n1.3.2.9000\n\n\nplotmo\n3.6.4\n\n\nplotrix\n3.8-4\n\n\nplumber\n1.3.0\n\n\nprobably\n1.1.0\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\npurrr\n1.1.0\n\n\nranger\n0.17.0\n\n\nrecipes\n1.3.1\n\n\nrpart\n4.1.24\n\n\nrpart.plot\n3.1.3\n\n\nrsample\n1.3.1\n\n\nrules\n1.0.2\n\n\nscales\n1.4.0\n\n\nsplines2\n0.5.4\n\n\nstacks\n1.1.1.9000\n\n\ntext2vec\n0.6.4\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\ntextrecipes\n1.1.0.9000\n\n\ntibble\n3.3.0\n\n\ntidymodels\n1.3.0\n\n\ntidyr\n1.3.1\n\n\ntune\n1.3.0\n\n\nvetiver\n0.2.5\n\n\nworkflows\n1.2.0.9002\n\n\nworkflowsets\n1.1.1\n\n\nyardstick\n1.3.2"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#chicago-l-train-data",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#chicago-l-train-data",
    "title": "Case Study on Transportation",
    "section": "Chicago L-Train data",
    "text": "Chicago L-Train data\nSeveral years worth of pre-pandemic data were assembled to try to predict the daily number of people entering the Clark and Lake elevated (â€œLâ€) train station in Chicago.\nMore information:\n\nSeveral Chapters in Feature Engineering and Selection.\n\nStart with Section 4.1\nSee Section 1.3\n\nVideo: The Global Pandemic Ruined My Favorite Data Set"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#predictors",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#predictors",
    "title": "Case Study on Transportation",
    "section": "Predictors",
    "text": "Predictors\n\nthe 14-day lagged ridership at this and other stations (units: thousands of rides/day)\nweather data\nhome/away game schedules for Chicago teams\nthe date\n\nThe data are in modeldata. See ?Chicago."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#l-train-locations",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#l-train-locations",
    "title": "Case Study on Transportation",
    "section": "L Train Locations",
    "text": "L Train Locations"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#your-turn-explore-the-data",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#your-turn-explore-the-data",
    "title": "Case Study on Transportation",
    "section": "Your turn: Explore the Data",
    "text": "Your turn: Explore the Data\nTake a look at these data for a few minutes and see if you can find any interesting characteristics in the predictors or the outcome.\n\nlibrary(tidymodels)\nlibrary(rules)\ndata(\"Chicago\")\ndim(Chicago)\n#&gt; [1] 5698   50\nstations\n#&gt;  [1] \"Austin\"           \"Quincy_Wells\"     \"Belmont\"          \"Archer_35th\"     \n#&gt;  [5] \"Oak_Park\"         \"Western\"          \"Clark_Lake\"       \"Clinton\"         \n#&gt;  [9] \"Merchandise_Mart\" \"Irving_Park\"      \"Washington_Wells\" \"Harlem\"          \n#&gt; [13] \"Monroe\"           \"Polk\"             \"Ashland\"          \"Kedzie\"          \n#&gt; [17] \"Addison\"          \"Jefferson_Park\"   \"Montrose\"         \"California\"\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#splitting-with-chicago-data",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#splitting-with-chicago-data",
    "title": "Case Study on Transportation",
    "section": "Splitting with Chicago data ",
    "text": "Splitting with Chicago data \nLetâ€™s put the last two weeks of data into the test set. initial_time_split() can be used for this purpose:\n\ndata(Chicago)\n\nchi_split &lt;- initial_time_split(Chicago, prop = 1 - (14/nrow(Chicago)))\nchi_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;5684/14/5698&gt;\n\nchi_train &lt;- training(chi_split)\nchi_test  &lt;- testing(chi_split)\n\n## training\nnrow(chi_train)\n#&gt; [1] 5684\n \n## testing\nnrow(chi_test)\n#&gt; [1] 14"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#time-series-resampling",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#time-series-resampling",
    "title": "Case Study on Transportation",
    "section": "Time series resampling",
    "text": "Time series resampling\nOur Chicago data is over time. Regular cross-validation, which uses random sampling, may not be the best idea.\nWe can emulate our training/test split by making similar resamples.\n\nFold 1: Take the first X years of data as the analysis set, the next 2 weeks as the assessment set.\nFold 2: Take the first X years + 2 weeks of data as the analysis set, the next 2 weeks as the assessment set.\nand so on"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#rolling-forecast-origin-resampling",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#rolling-forecast-origin-resampling",
    "title": "Case Study on Transportation",
    "section": "Rolling forecast origin resampling",
    "text": "Rolling forecast origin resampling\n\n\nThis image shows overlapping assessment sets. We will use non-overlapping data but it could be done wither way."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#times-series-resampling",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#times-series-resampling",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train |&gt;\n  sliding_period(\n    index = \"date\",  \n\n\n\n\n  )\n\nUse the date column to find the date data."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#times-series-resampling-1",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#times-series-resampling-1",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train |&gt;\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n\n\n\n  )\n\nOur units will be weeks."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#times-series-resampling-2",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#times-series-resampling-2",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train |&gt;\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15  \n    \n    \n  )\n\nEvery analysis set has 15 years of data"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#times-series-resampling-3",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#times-series-resampling-3",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train |&gt;\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n\n  )\n\nEvery assessment set has 2 weeks of data"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#times-series-resampling-4",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#times-series-resampling-4",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train |&gt;\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n    step = 2 \n  )\n\nIncrement by 2 weeks so that there are no overlapping assessment sets.\n\nchi_rs$splits[[1]] |&gt; assessment() |&gt; pluck(\"date\") |&gt; range()\n#&gt; [1] \"2016-01-07\" \"2016-01-20\"\nchi_rs$splits[[2]] |&gt; assessment() |&gt; pluck(\"date\") |&gt; range()\n#&gt; [1] \"2016-01-21\" \"2016-02-03\""
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#our-resampling-object",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#our-resampling-object",
    "title": "Case Study on Transportation",
    "section": "Our resampling object ",
    "text": "Our resampling object \n\n\n\nchi_rs\n#&gt; # Sliding period resampling \n#&gt; # A tibble: 16 Ã— 2\n#&gt;    splits            id     \n#&gt;    &lt;list&gt;            &lt;chr&gt;  \n#&gt;  1 &lt;split [5463/14]&gt; Slice01\n#&gt;  2 &lt;split [5467/14]&gt; Slice02\n#&gt;  3 &lt;split [5467/14]&gt; Slice03\n#&gt;  4 &lt;split [5467/14]&gt; Slice04\n#&gt;  5 &lt;split [5467/14]&gt; Slice05\n#&gt;  6 &lt;split [5467/14]&gt; Slice06\n#&gt;  7 &lt;split [5467/14]&gt; Slice07\n#&gt;  8 &lt;split [5467/14]&gt; Slice08\n#&gt;  9 &lt;split [5467/14]&gt; Slice09\n#&gt; 10 &lt;split [5467/14]&gt; Slice10\n#&gt; 11 &lt;split [5467/14]&gt; Slice11\n#&gt; 12 &lt;split [5467/14]&gt; Slice12\n#&gt; 13 &lt;split [5467/14]&gt; Slice13\n#&gt; 14 &lt;split [5467/14]&gt; Slice14\n#&gt; 15 &lt;split [5467/14]&gt; Slice15\n#&gt; 16 &lt;split [5467/11]&gt; Slice16\n\n\n\n\nWe will fit 16 models on 16 slightly different analysis sets.\nEach will produce a separate performance metrics.\nWe will average the 16 metrics to get the resampling estimate of that statistic."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#feature-engineering-with-recipes",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#feature-engineering-with-recipes",
    "title": "Case Study on Transportation",
    "section": "Feature engineering with recipes ",
    "text": "Feature engineering with recipes \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train)\n\nBased on the formula, the function assigns columns to roles of â€œoutcomeâ€ or â€œpredictorâ€"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#a-recipe",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#a-recipe",
    "title": "Case Study on Transportation",
    "section": "A recipe",
    "text": "A recipe\n\nsummary(chi_rec)\n#&gt; # A tibble: 50 Ã— 4\n#&gt;    variable         type      role      source  \n#&gt;    &lt;chr&gt;            &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 Austin           &lt;chr [2]&gt; predictor original\n#&gt;  2 Quincy_Wells     &lt;chr [2]&gt; predictor original\n#&gt;  3 Belmont          &lt;chr [2]&gt; predictor original\n#&gt;  4 Archer_35th      &lt;chr [2]&gt; predictor original\n#&gt;  5 Oak_Park         &lt;chr [2]&gt; predictor original\n#&gt;  6 Western          &lt;chr [2]&gt; predictor original\n#&gt;  7 Clark_Lake       &lt;chr [2]&gt; predictor original\n#&gt;  8 Clinton          &lt;chr [2]&gt; predictor original\n#&gt;  9 Merchandise_Mart &lt;chr [2]&gt; predictor original\n#&gt; 10 Irving_Park      &lt;chr [2]&gt; predictor original\n#&gt; # â„¹ 40 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#a-recipe---work-with-dates",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#a-recipe---work-with-dates",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) |&gt; \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) \n\nThis creates three new columns in the data based on the date. Note that the day-of-the-week column is a factor."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#a-recipe---work-with-dates-1",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#a-recipe---work-with-dates-1",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) |&gt; \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) |&gt; \n  step_holiday(date) \n\nAdd indicators for major holidays. Specific holidays, especially those non-USA, can also be generated.\nAt this point, we donâ€™t need date anymore. Instead of deleting it (there is a step for that) we will change its role to be an identification variable.\n\nWe might want to change the role (instead of removing the column) because it will stay in the data set (even when resampled) and might be useful for diagnosing issues."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#a-recipe---work-with-dates-2",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#a-recipe---work-with-dates-2",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) |&gt; \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) |&gt; \n  step_holiday(date) |&gt; \n  update_role(date, new_role = \"id\") |&gt;\n  update_role_requirements(role = \"id\", bake = TRUE)\n\ndate is still in the data set but tidymodels knows not to treat it as an analysis column.\nupdate_role_requirements() is needed to make sure that this column is required when making new data points. The help page has a good discussion about the nuances."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#a-recipe---remove-constant-columns",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#a-recipe---remove-constant-columns",
    "title": "Case Study on Transportation",
    "section": "A recipe - remove constant columns ",
    "text": "A recipe - remove constant columns \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) |&gt; \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) |&gt; \n  step_holiday(date) |&gt; \n  update_role(date, new_role = \"id\") |&gt;\n  update_role_requirements(role = \"id\", bake = TRUE) |&gt; \n  step_zv(all_nominal_predictors())"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#a-recipe---handle-correlations",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#a-recipe---handle-correlations",
    "title": "Case Study on Transportation",
    "section": "A recipe - handle correlations ",
    "text": "A recipe - handle correlations \nThe station columns have a very high degree of correlation.\nWe might want to decorrelated them with principle component analysis to help the model fits go more easily.\nThe vector stations contains all station names and can be used to identify all the relevant columns.\n\nchi_pca_rec &lt;- \n  chi_rec |&gt; \n  step_normalize(all_of(!!stations)) |&gt; \n  step_pca(all_of(!!stations), num_comp = tune())\n\nWeâ€™ll tune the number of PCA components for (default) values of one to four."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#make-some-models",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#make-some-models",
    "title": "Case Study on Transportation",
    "section": "Make some models     ",
    "text": "Make some models     \nLetâ€™s try three models. The first one requires the rules package (loaded earlier).\n\ncb_spec &lt;- cubist_rules(committees = 25, neighbors = tune())\nmars_spec &lt;- mars(prod_degree = tune()) |&gt; set_mode(\"regression\")\nlm_spec &lt;- linear_reg()\n\nchi_set &lt;- \n  workflow_set(\n    list(pca = chi_pca_rec, basic = chi_rec), \n    list(cubist = cb_spec, mars = mars_spec, lm = lm_spec)\n  ) |&gt; \n  # Evaluate models using mean absolute errors\n  option_add(metrics = metric_set(mae))\n\n\nBriefly talk about Cubist being a (sort of) boosted rule-based model and MARS being a nonlinear regression model. Both incorporate feature selection nicely."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#process-them-on-the-resamples",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#process-them-on-the-resamples",
    "title": "Case Study on Transportation",
    "section": "Process them on the resamples",
    "text": "Process them on the resamples\n\n# Set up some objects for stacking ensembles (in a few slides)\ngrid_ctrl &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\nchi_res &lt;- \n  chi_set |&gt; \n  workflow_map(\n    resamples = chi_rs,\n    grid = 10,\n    control = grid_ctrl,\n    verbose = TRUE,\n    seed = 12\n  )"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#how-do-the-results-look",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#how-do-the-results-look",
    "title": "Case Study on Transportation",
    "section": "How do the results look?",
    "text": "How do the results look?\n\nrank_results(chi_res)\n#&gt; # A tibble: 34 Ã— 9\n#&gt;    wflow_id     .config              .metric  mean std_err     n preprocessor model   rank\n#&gt;    &lt;chr&gt;        &lt;chr&gt;                &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;  &lt;int&gt;\n#&gt;  1 pca_cubist   Preprocessor1_Model1 mae     0.804   0.101    16 recipe       cubisâ€¦     1\n#&gt;  2 basic_cubist Preprocessor1_Model1 mae     0.901   0.115    16 recipe       cubisâ€¦     2\n#&gt;  3 pca_cubist   Preprocessor2_Model2 mae     0.985   0.120    16 recipe       cubisâ€¦     3\n#&gt;  4 pca_cubist   Preprocessor4_Model2 mae     0.990   0.121    16 recipe       cubisâ€¦     4\n#&gt;  5 pca_cubist   Preprocessor1_Model3 mae     1.00    0.119    16 recipe       cubisâ€¦     5\n#&gt;  6 pca_cubist   Preprocessor4_Model1 mae     1.02    0.131    16 recipe       cubisâ€¦     6\n#&gt;  7 pca_cubist   Preprocessor3_Model3 mae     1.03    0.121    16 recipe       cubisâ€¦     7\n#&gt;  8 pca_cubist   Preprocessor3_Model2 mae     1.07    0.137    16 recipe       cubisâ€¦     8\n#&gt;  9 basic_cubist Preprocessor1_Model9 mae     1.07    0.115    16 recipe       cubisâ€¦     9\n#&gt; 10 basic_cubist Preprocessor1_Model8 mae     1.07    0.114    16 recipe       cubisâ€¦    10\n#&gt; # â„¹ 24 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#plot-the-results",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#plot-the-results",
    "title": "Case Study on Transportation",
    "section": "Plot the results  ",
    "text": "Plot the results  \n\nautoplot(chi_res)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#pull-out-specific-results",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#pull-out-specific-results",
    "title": "Case Study on Transportation",
    "section": "Pull out specific results  ",
    "text": "Pull out specific results  \nWe can also pull out the specific tuning results and look at them:\n\nchi_res |&gt; \n  extract_workflow_set_result(\"pca_cubist\") |&gt; \n  autoplot()"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#why-choose-just-one-final_fit",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#why-choose-just-one-final_fit",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit? \nModel stacks generate predictions that are informed by several models."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#why-choose-just-one-final_fit-1",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#why-choose-just-one-final_fit-1",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#why-choose-just-one-final_fit-2",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#why-choose-just-one-final_fit-2",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#why-choose-just-one-final_fit-3",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#why-choose-just-one-final_fit-3",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#why-choose-just-one-final_fit-4",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#why-choose-just-one-final_fit-4",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#why-choose-just-one-final_fit-5",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#why-choose-just-one-final_fit-5",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#building-a-model-stack",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#building-a-model-stack",
    "title": "Case Study on Transportation",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlibrary(stacks)\n\n\nDefine candidate members\nInitialize a data stack object\nAdd candidate ensemble members to the data stack\nEvaluate how to combine their predictions\nFit candidate ensemble members with non-zero stacking coefficients\nPredict on new data!"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#start-the-stack-and-add-members",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#start-the-stack-and-add-members",
    "title": "Case Study on Transportation",
    "section": "Start the stack and add members ",
    "text": "Start the stack and add members \nCollect all of the resampling results for all model configurations.\n\nchi_stack &lt;- \n  stacks() |&gt; \n  add_candidates(chi_res)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#estimate-weights-for-each-candidate",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#estimate-weights-for-each-candidate",
    "title": "Case Study on Transportation",
    "section": "Estimate weights for each candidate ",
    "text": "Estimate weights for each candidate \nWhich configurations should be retained? Uses a penalized linear model:\n\nset.seed(122)\nchi_stack_res &lt;- blend_predictions(chi_stack, penalty = 10^seq(-6, -1, length.out = 25))\n\nchi_stack_res\n#&gt; # A tibble: 5 Ã— 3\n#&gt;   member           type         weight\n#&gt;   &lt;chr&gt;            &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 pca_cubist_1_1   cubist_rules 0.354 \n#&gt; 2 basic_cubist_1_1 cubist_rules 0.263 \n#&gt; 3 pca_cubist_1_3   cubist_rules 0.203 \n#&gt; 4 basic_cubist_1_6 cubist_rules 0.155 \n#&gt; 5 pca_lm_2_1       linear_reg   0.0479"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#how-did-it-do",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#how-did-it-do",
    "title": "Case Study on Transportation",
    "section": "How did it do?  ",
    "text": "How did it do?  \nThe overall results of the penalized model:\n\nautoplot(chi_stack_res)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#what-does-it-use",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#what-does-it-use",
    "title": "Case Study on Transportation",
    "section": "What does it use?  ",
    "text": "What does it use?  \n\nautoplot(chi_stack_res, type = \"weights\")"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#fit-the-required-candidate-models",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#fit-the-required-candidate-models",
    "title": "Case Study on Transportation",
    "section": "Fit the required candidate models",
    "text": "Fit the required candidate models\nFor each model we retain in the stack, we need their model fit on the entire training set.\n\nchi_stack_res &lt;- fit_members(chi_stack_res)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#the-test-set-best-cubist-model",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#the-test-set-best-cubist-model",
    "title": "Case Study on Transportation",
    "section": "The test set: best Cubist model  ",
    "text": "The test set: best Cubist model  \nWe can pull out the results and the workflow to fit the single best cubist model.\n\nbest_cubist &lt;- \n  chi_res |&gt; \n  extract_workflow_set_result(\"pca_cubist\") |&gt; \n  select_best()\n\ncubist_res &lt;- \n  chi_res |&gt; \n  extract_workflow(\"pca_cubist\") |&gt; \n  finalize_workflow(best_cubist) |&gt; \n  last_fit(split = chi_split, metrics = metric_set(mae))"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#the-test-set-stack-ensemble",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#the-test-set-stack-ensemble",
    "title": "Case Study on Transportation",
    "section": "The test set: stack ensemble",
    "text": "The test set: stack ensemble\nWe donâ€™t have last_fit() for stacks (yet) so we manually make predictions.\n\nstack_pred &lt;- \n  predict(chi_stack_res, chi_test) |&gt; \n  bind_cols(chi_test)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#compare-the-results",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#compare-the-results",
    "title": "Case Study on Transportation",
    "section": "Compare the results  ",
    "text": "Compare the results  \nSingle best versus the stack:\n\ncollect_metrics(cubist_res)\n#&gt; # A tibble: 1 Ã— 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard       0.669 Preprocessor1_Model1\n\nstack_pred |&gt; mae(ridership, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       0.669"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-transit-case-study.html#plot-the-test-set",
    "href": "archive/2025-08-nyr/slides/extras-transit-case-study.html#plot-the-test-set",
    "title": "Case Study on Transportation",
    "section": "Plot the test set  ",
    "text": "Plot the test set  \n\n\nlibrary(probably)\ncubist_res |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(ridership, .pred)) + \n  geom_point(alpha = 1 / 2) + \n  geom_abline(lty = 2, col = \"green\") + \n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#section",
    "href": "archive/2025-08-nyr/slides/annotations.html#section",
    "title": "Annotations",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€\nThis page contains annotations for selected slides.\nThereâ€™s a lot that we want to tell you. We donâ€™t want people to have to frantically scribble down things that we say that are not on the slides.\nWeâ€™ve added sections to this document with longer explanations and links to other resources."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#data-splitting-and-spending",
    "href": "archive/2025-08-nyr/slides/annotations.html#data-splitting-and-spending",
    "title": "Annotations",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nMore about the initial data split can be found in Chapter 3 of Applied Machine Learning for Tabular Data (AML4TD).\nIn particular, a three-way split into training, validation, and testing set can be done via\n\nset.seed(123)\ninitial_validation_split(forested, prop = c(0.6, 0.2))\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;4264/1421/1422/7107&gt;"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#what-is-set.seed",
    "href": "archive/2025-08-nyr/slides/annotations.html#what-is-set.seed",
    "title": "Annotations",
    "section": "What is set.seed()?",
    "text": "What is set.seed()?\nWhat does set.seed() do?\nWeâ€™ll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random).\nThink of PRN as a box that takes a starting value (the â€œseedâ€) that produces random numbers using that starting value as an input into its process.\nIf we know a seed value, we can reproduce our â€œrandomâ€ numbers. To use a different set of random numbers, choose a different seed value.\nFor example:\n\nset.seed(1)\nrunif(3)\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\n# Get a new set of random numbers:\nset.seed(2)\nrunif(3)\n#&gt; [1] 0.1848823 0.7023740 0.5733263\n\n# We can reproduce the old ones with the same seed\nset.seed(1)\nrunif(3)\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\nIf we donâ€™t set the seed, R uses the clock time and the process ID to create a seed. This isnâ€™t reproducible.\nSince we want our code to be reproducible, we set the seeds before random numbers are used.\nIn theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same and we donâ€™t get reproducible results.\nThe value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to â€œspread the randomness aroundâ€. It is basically:\n\ncat(paste0(\"set.seed(\", sample.int(10000, 5), \")\", collapse = \"\\n\"))\n#&gt; set.seed(9725)\n#&gt; set.seed(8462)\n#&gt; set.seed(4050)\n#&gt; set.seed(8789)\n#&gt; set.seed(1301)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#what-is-wrong-with-this",
    "href": "archive/2025-08-nyr/slides/annotations.html#what-is-wrong-with-this",
    "title": "Annotations",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?\nIf we treat the preprocessing as a separate task, it raises the risk that we might accidentally overfit to the data at hand.\nFor example, someone might estimate something from the entire data set (such as the principle components) and treat that data as if it were known (and not estimated). Depending on the what was done with the data, consequences in doing that could be:\n\nYour performance metrics are slightly-to-moderately optimistic (e.g.Â you might think your accuracy is 85% when it is actually 75%)\nA consequential component of the analysis is not right and the model just doesnâ€™t work.\n\nThe big issue here is that you wonâ€™t be able to figure this out until you get a new piece of data, such as the test set.\nA really good example of this is in â€˜Selection bias in gene extraction on the basis of microarray gene-expression dataâ€™. The authors re-analyze a previous publication and show that the original researchers did not include feature selection in the workflow. Because of that, their performance statistics were extremely optimistic. In one case, they could do the original analysis on complete noise and still achieve zero errors.\nGenerally speaking, this problem is referred to as data leakage. Some other references:\n\nOverfitting to Predictors and External Validation\nAre We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning\nNavigating the pitfalls of applying machine learning in genomics\nA review of feature selection techniques in bioinformatics\nOn Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#brier-score",
    "href": "archive/2025-08-nyr/slides/annotations.html#brier-score",
    "title": "Annotations",
    "section": "Brier score",
    "text": "Brier score\nThe Brier score measures how close a model probability estimate is to its best possible value (i.e., zero or one).\nIn the best case, the model is perfect, and every prediction equals 0.0 or 1.0 (depending on the true class). In this case, the Brier score is zero.\nWhen the model is uninformative and there are two classes, the worst-case values range from 0.25 to about 0.50. Imagine that the model predicts the same noninformative prediction of 50% (basically â€œÂ¯\\(ãƒ„)/Â¯â€). In that case, every prediction is either \\((0.00 - 0.50)^2\\) or \\((1.00 - 0.50)^2\\). The average of those is 0.25.\nThere are many different ways a model can be bad though, and some of these will produce Brier scores between 0.25 and 0.50."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#dangers-of-overfitting",
    "href": "archive/2025-08-nyr/slides/annotations.html#dangers-of-overfitting",
    "title": "Annotations",
    "section": "Dangers of overfitting",
    "text": "Dangers of overfitting\nSee the â€œOverfittingâ€ chapter of AML4TD for more information."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#where-are-the-fitted-models",
    "href": "archive/2025-08-nyr/slides/annotations.html#where-are-the-fitted-models",
    "title": "Annotations",
    "section": "Where are the fitted models?",
    "text": "Where are the fitted models?\nThe primary purpose of resampling is to estimate model performance. The models are almost never needed again.\nAlso, if the data set is large, the model object may require a lot of memory to save so, by default, we donâ€™t keep them.\nFor more advanced use cases, you can extract and save them. See:\n\nhttps://www.tmwr.org/resampling.html#extract\nhttps://www.tidymodels.org/learn/models/coefficients/ (an example)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#resampling-strategy",
    "href": "archive/2025-08-nyr/slides/annotations.html#resampling-strategy",
    "title": "Annotations",
    "section": "Resampling Strategy",
    "text": "Resampling Strategy\nThese data have a time component, and while not a typical time series data set, we have the option to use a time series resampling method.\nAn example is shown in the extra slides â€œCase Study on Transportationâ€.\nConsider the agent data. Cross-validation may not group all of an agentâ€™s data into the analysis or assessment sets. In this case, our analysis data might have future data that is later than the agentâ€™s data in the assessment set."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#different-types-of-grids",
    "href": "archive/2025-08-nyr/slides/annotations.html#different-types-of-grids",
    "title": "Annotations",
    "section": "Different types of grids",
    "text": "Different types of grids\nMore on space-filling designs in Chapters 4 and 5 of Surrogates: Gaussian process modeling, design, and optimization for the applied sciences.\nIn the next version of tune (version 1.3.0) an improved set of space-filling designs will be the first choice if you ask for an automated grid."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#update-parameter-ranges",
    "href": "archive/2025-08-nyr/slides/annotations.html#update-parameter-ranges",
    "title": "Annotations",
    "section": "Update parameter ranges",
    "text": "Update parameter ranges\nIn about 90% of the cases, the dials function that you use to update the parameter range has the same name as the argument. For example, if you were to update the mtry parameter in a random forests model, the code would look like\n\nparameter_object |&gt; \n  update(mtry = mtry(c(1, 100)))\n\nThere are some cases where the parameter function, or its associated values, are different from the argument name.\nFor example, with step_spline_naturall(), we might want to tune the deg_free argument (for the degrees of freedom of a spline function. ). In this case, the argument name is deg_free but we update it with spline_degree().\ndeg_free represents the general concept of degrees of freedom and could be associated with many different things. For example, if we ever had an argument that was the number of degrees of freedom for a \\(t\\) distribution, we would call that argument deg_free.\nFor splines, we probably want a wider range for the degrees of freedom. We made a specialized function called spline_degree() to be used in these cases.\nHow can you tell when this happens? There is a helper function called tunable() and that gives information on how we make the default ranges for parameters. There is a column in these objects names call_info:\n\nlibrary(tidymodels)\nns_tunable &lt;- \n  recipe(mpg ~ ., data = mtcars) |&gt; \n  step_spline_natural(dis, deg_free = tune()) |&gt; \n  tunable()\n\nns_tunable\n#&gt; # A tibble: 1 Ã— 5\n#&gt;   name     call_info        source component           component_id        \n#&gt;   &lt;chr&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;chr&gt;               &lt;chr&gt;               \n#&gt; 1 deg_free &lt;named list [3]&gt; recipe step_spline_natural spline_natural_P1Tjg\nns_tunable$call_info\n#&gt; [[1]]\n#&gt; [[1]]$pkg\n#&gt; [1] \"dials\"\n#&gt; \n#&gt; [[1]]$fun\n#&gt; [1] \"spline_degree\"\n#&gt; \n#&gt; [[1]]$range\n#&gt; [1]  2 15"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#early-stopping-for-boosted-trees",
    "href": "archive/2025-08-nyr/slides/annotations.html#early-stopping-for-boosted-trees",
    "title": "Annotations",
    "section": "Early stopping for boosted trees",
    "text": "Early stopping for boosted trees\nWhen deciding on the number of boosting iterations, there are two main strategies:\n\nDirectly tune it (trees = tune())\nSet it to one value and tune the number of early stopping iterations (trees = 500, stop_iter = tune()).\n\nEarly stopping is when we monitor the performance of the model. If the model doesnâ€™t make any improvements for stop_iter iterations, training stops.\nHereâ€™s an example where, after eleven iterations, performance starts to get worse.\n\n\n\n\n\n\n\n\n\nThis is likely due to over-fitting so we stop the model at eleven boosting iterations.\nEarly stopping usually has good results and takes far less time.\nWe could an engine argument called validation here. Thatâ€™s not an argument to any function in the lightgbm package.\nbonsai has its own wrapper around (lightgbm::lgb.train()) called bonsai::train_lightgbm(). We use that here and it has a validation argument.\nHow would you know that? There are a few different ways:\n\nLook at the documentation in ?boost_tree and click on the lightgbm entry in the engine list.\nCheck out the pkgdown reference website https://parsnip.tidymodels.org/reference/index.html\nRun the translate() function on the parsnip specification object.\n\nThe first two options are best since they tell you a lot more about the particularities of each model engine (there are a lot for lightgbm)."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#gaussian-processes-and-optimization",
    "href": "archive/2025-08-nyr/slides/annotations.html#gaussian-processes-and-optimization",
    "title": "Annotations",
    "section": "Gaussian Processes and Optimization",
    "text": "Gaussian Processes and Optimization\nSome other references for GPâ€™s:\n\nChapter 5 of Surrogates: Gaussian process modeling, design, and optimization for the applied sciences\nBayesian Optimization, Chapter 3 (pdf)\nGaussian Processes for Machine Learning (pdf)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#acquisition-functions",
    "href": "archive/2025-08-nyr/slides/annotations.html#acquisition-functions",
    "title": "Annotations",
    "section": "Acquisition Functions",
    "text": "Acquisition Functions\nMore references:\n\nChapter 7 of Surrogates: Gaussian process modeling, design, and optimization for the applied sciences\nBayesian Optimization, Chapter 6 (pdf)\nGaussian Processes for Machine Learning"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#per-agent-statistics",
    "href": "archive/2025-08-nyr/slides/annotations.html#per-agent-statistics",
    "title": "Annotations",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics\nThe effect encoding method essentially takes the effect of a variable, like agent, and makes a data column for that effect. In our example, affect of the agent on the ADR is quantified by a model and then added as a data column to be used in the model.\nSuppose agent Max has a single reservation in the data and it had an ADR of â‚¬200. If we used a naive estimate for Maxâ€™s effect, the model is being told that Max should always produce an effect of â‚¬200. Thatâ€™s a very poor estimate since it is from a single data point.\nContrast this with seasoned agent Davis, who has taken 250 reservations with an average ADR of â‚¬100. Davisâ€™s mean is more predictive because it is estimated with better data (i.e., more total reservations). Partial pooling leverages the entire data set and can borrow strength from all of the agents. It is a common tool in Bayesian estimation and non-Bayesian mixed models. If a agentâ€™s data is of good quality, the partial pooling effect estimate is closer to the raw mean. Maxâ€™s data is not great and is â€œshrunkâ€ towards the center of the overall average. Since there is so little known about Maxâ€™s reservation history, this is a better effect estimate (until more data is available for him).\nThe Stan documentation has a pretty good vignette on this: https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html\nAlso, Bayes Rules! has a nice section on this: https://www.bayesrulesbook.com/chapter-15.html\nSince this example has a numeric outcome, partial pooling is very similar to the Jamesâ€“Stein estimator: https://en.wikipedia.org/wiki/Jamesâ€“Stein_estimator"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#agent-effects",
    "href": "archive/2025-08-nyr/slides/annotations.html#agent-effects",
    "title": "Annotations",
    "section": "Agent effects",
    "text": "Agent effects\nEffect encoding might result in a somewhat circular argument: the column is more likely to be important to the model since it is the output of a separate model. The risk here is that we might over-fit the effect to the data. For this reason, it is super important to make sure that we verify that we arenâ€™t overfitting by checking with resampling (or a validation set).\nPartial pooling somewhat lowers the risk of overfitting since it tends to correct for agents with small sample sizes. It canâ€™t correct for improper data usage or data leakage though."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/annotations.html#a-recipe---handle-correlations",
    "href": "archive/2025-08-nyr/slides/annotations.html#a-recipe---handle-correlations",
    "title": "Annotations",
    "section": "A recipe - handle correlations",
    "text": "A recipe - handle correlations\nIn this code chunk, Whatâ€™s the story with !!stations?\n\nchi_pca_rec &lt;- \n  chi_rec |&gt; \n  step_normalize(all_of(!!stations)) |&gt; \n  step_pca(all_of(!!stations), num_comp = tune())\n\nstations is a vector of names of 20 columns that we want to use in the steps. If the list were shorter, we could type them in (e.g., c(\"col1\", \"col2\") etc.).\nThe vector lives in our global workspace, and if we are in parallel, the worker processes might not have access to stations. The !! (frequently said as â€œbang bangâ€) inserts the actual contents of the vector into the all_of() calls so that it looks like you just typed it in.\nThis means that the parallel process workers have a copy of the data in their reach and the code will run without error."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#previously---setup",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#previously---setup",
    "title": "5 - Iterative Search",
    "section": "Previously - Setup",
    "text": "Previously - Setup\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates |&gt; \n  sample_n(5000) |&gt; \n  arrange(arrival_date) |&gt; \n  select(-arrival_date) |&gt;  \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#previously---data-usage",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#previously---data-usage",
    "title": "5 - Iterative Search",
    "section": "Previously - Data Usage",
    "text": "Previously - Data Usage\n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#our-boosting-model",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#our-boosting-model",
    "title": "5 - Iterative Search",
    "section": "Our Boosting Model",
    "text": "Our Boosting Model\nWe used feature hashing to generate a smaller set of indicator columns to deal with the large number of levels for the agent and country predictors.\n\nTree-based models (and a few others) donâ€™t require indicators for categorical predictors. They can split on these variables as-is.\n\nWeâ€™ll keep all categorical predictors as factors and focus on optimizing additional boosting parameters."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#our-boosting-model-1",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#our-boosting-model-1",
    "title": "5 - Iterative Search",
    "section": "Our Boosting Model",
    "text": "Our Boosting Model\n\nlgbm_spec &lt;- \n  boost_tree(trees = 1000, learn_rate = tune(), min_n = tune(), \n             tree_depth = tune(), loss_reduction = tune(), \n             stop_iter = tune()) |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow &lt;- workflow(avg_price_per_room ~ ., lgbm_spec)\n\nlgbm_param &lt;- \n  lgbm_wflow |&gt;\n    extract_parameter_set_dials() |&gt;\n    update(learn_rate = learn_rate(c(-5, -1)))"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#iterative-search",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#iterative-search",
    "title": "5 - Iterative Search",
    "section": "Iterative Search",
    "text": "Iterative Search\nInstead of pre-defining a grid of candidate points, we can model our current results to predict what the next candidate point should be.\n\nSuppose that we are only tuning the learning rate in our boosted tree.\n\nWe could do something like:\nmae_pred &lt;- lm(mae ~ learn_rate, data = resample_results)\nand use this to predict and rank new learning rate candidates."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#iterative-search-1",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#iterative-search-1",
    "title": "5 - Iterative Search",
    "section": "Iterative Search",
    "text": "Iterative Search\nA linear model probably isnâ€™t the best choice though (more in a minute).\nTo illustrate the process, we resampled a large grid of learning rate values for our data to show what the relationship is between MAE and learning rate.\nNow suppose that we used a grid of three points in the parameter range for learning rateâ€¦"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#a-large-grid",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#a-large-grid",
    "title": "5 - Iterative Search",
    "section": "A Large Grid",
    "text": "A Large Grid"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#a-three-point-grid",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#a-three-point-grid",
    "title": "5 - Iterative Search",
    "section": "A Three Point Grid",
    "text": "A Three Point Grid"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#gaussian-processes-and-optimization",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#gaussian-processes-and-optimization",
    "title": "5 - Iterative Search",
    "section": "Gaussian Processes and Optimization",
    "text": "Gaussian Processes and Optimization\nWe can make a â€œmeta-modelâ€ with a small set of historical performance results.\nGaussian Processes (GP) models are a good choice to model performance.\n\nIt is a Bayesian model so we are using Bayesian Optimization (BO).\nFor regression, we can assume that our data are multivariate normal.\nWe also define a covariance function for the variance relationship between data points. A common one is:\n\n\\[\\operatorname{cov}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\exp\\left(-\\frac{1}{2}|\\boldsymbol{x}_i - \\boldsymbol{x}_j|^2\\right) + \\sigma^2_{ij}\\]\n\nGPs are good because\n\nthey are flexible regression models (in the sense that splines are flexible).\nwe need to get mean and variance predictions (and they are Bayesian)\ntheir variability is based on spatial distances.\n\nSome people use random forests (with conformal variance estimates) or other methods but GPs are most popular."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#predicting-candidates",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#predicting-candidates",
    "title": "5 - Iterative Search",
    "section": "Predicting Candidates",
    "text": "Predicting Candidates\nThe GP model can take candidate tuning parameter combinations as inputs and make predictions for performance (e.g.Â MAE)\n\nThe mean performance\nThe variance of performance\n\nThe variance is mostly driven by spatial variability (the previous equation).\nThe predicted variance is zero at locations of actual data points and becomes very high when far away from any observed data."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#your-turn",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#your-turn",
    "title": "5 - Iterative Search",
    "section": "Your turn",
    "text": "Your turn\n\n\nYour GP makes predictions on two new candidate tuning parameters.\nWe want to minimize MAE.\nWhich should we choose?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#gp-fit-ribbon-is-mean---1sd",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#gp-fit-ribbon-is-mean---1sd",
    "title": "5 - Iterative Search",
    "section": "GP Fit (ribbon is mean +/- 1SD)",
    "text": "GP Fit (ribbon is mean +/- 1SD)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#choosing-new-candidates",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#choosing-new-candidates",
    "title": "5 - Iterative Search",
    "section": "Choosing New Candidates",
    "text": "Choosing New Candidates\nThis isnâ€™t a very good fit but we can still use it.\nHow can we use the outputs to choose the next point to measure?\n\nAcquisition functions take the predicted mean and variance and use them to balance:\n\nexploration: new candidates should explore new areas.\nexploitation: new candidates must stay near existing values.\n\nExploration focuses on the variance, exploitation is about the mean."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#acquisition-functions",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#acquisition-functions",
    "title": "5 - Iterative Search",
    "section": "Acquisition Functions",
    "text": "Acquisition Functions\nWeâ€™ll use an acquisition function to select a new candidate.\nThe most popular method appears to be expected improvement (EI) above the current best results.\n\nZero at existing data points.\nThe expected improvement is integrated over all possible improvement (â€œexpectedâ€ in the probability sense).\n\nWe would probably pick the point with the largest EI as the next point.\n(There are other functions beyond EI.)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#expected-improvement",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#expected-improvement",
    "title": "5 - Iterative Search",
    "section": "Expected Improvement",
    "text": "Expected Improvement"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#iteration",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#iteration",
    "title": "5 - Iterative Search",
    "section": "Iteration",
    "text": "Iteration\nOnce we pick the candidate point, we measure performance for it (e.g.Â resampling).\n\nAnother GP is fit, EI is recomputed, and so on.\n\nWe stop when we have completed the allowed number of iterations or if we donâ€™t see any improvement after a pre-set number of attempts."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#gp-fit-with-four-points",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#gp-fit-with-four-points",
    "title": "5 - Iterative Search",
    "section": "GP Fit with four points",
    "text": "GP Fit with four points"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#expected-improvement-1",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#expected-improvement-1",
    "title": "5 - Iterative Search",
    "section": "Expected Improvement",
    "text": "Expected Improvement"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#gp-evolution",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#gp-evolution",
    "title": "5 - Iterative Search",
    "section": "GP Evolution",
    "text": "GP Evolution"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#expected-improvement-evolution",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#expected-improvement-evolution",
    "title": "5 - Iterative Search",
    "section": "Expected Improvement Evolution",
    "text": "Expected Improvement Evolution"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#bo-in-tidymodels",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#bo-in-tidymodels",
    "title": "5 - Iterative Search",
    "section": "BO in tidymodels",
    "text": "BO in tidymodels\nWeâ€™ll use a function called tune_bayes() that has very similar syntax to tune_grid().\n\nIt has an additional initial argument for the initial set of performance estimates and parameter combinations for the GP model."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#initial-grid-points",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#initial-grid-points",
    "title": "5 - Iterative Search",
    "section": "Initial grid points",
    "text": "Initial grid points\ninitial can be the results of another tune_*() function or an integer (in which case tune_grid() is used under to hood to make such an initial set of results).\n\nWeâ€™ll run the optimization more than once, so letâ€™s make an initial grid of results to serve as the substrate for the BO.\nI suggest at least the number of tuning parameters plus two as the initial grid for BO."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#an-initial-grid",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#an-initial-grid",
    "title": "5 - Iterative Search",
    "section": "An Initial Grid",
    "text": "An Initial Grid\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\nset.seed(12)\ninit_res &lt;-\n  lgbm_wflow |&gt;\n  tune_grid(\n    resamples = hotel_rs,\n    grid = nrow(lgbm_param) + 2,\n    param_info = lgbm_param,\n    metrics = reg_metrics\n  )\n\nshow_best(init_res, metric = \"mae\") |&gt; select(-.metric, -.estimator)\n#&gt; # A tibble: 5 Ã— 9\n#&gt;   min_n tree_depth learn_rate loss_reduction stop_iter  mean     n std_err .config             \n#&gt;   &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;     &lt;int&gt; &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1    33         12   0.0215         8.25e- 9         5  10.1    10   0.182 Preprocessor1_Model6\n#&gt; 2    14         10   0.1            3.16e+ 1         8  10.2    10   0.192 Preprocessor1_Model3\n#&gt; 3     8          3   0.00464        1   e-10        11  14.4    10   0.305 Preprocessor1_Model2\n#&gt; 4    40          1   0.001          4.64e- 3        14  36.1    10   0.408 Preprocessor1_Model7\n#&gt; 5    27         15   0.000215       6.81e- 7        20  44.5    10   0.371 Preprocessor1_Model5"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#bo-using-tidymodels",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#bo-using-tidymodels",
    "title": "5 - Iterative Search",
    "section": "BO using tidymodels",
    "text": "BO using tidymodels\n\nctrl_bo &lt;- control_bayes(verbose_iter = TRUE) # &lt;- for demonstration\n\nset.seed(15)\nlgbm_bayes_res &lt;-\n  lgbm_wflow |&gt;\n  tune_bayes(\n    resamples = hotel_rs,\n    initial = init_res,     # &lt;- initial results\n    iter = 20,\n    param_info = lgbm_param,\n    control = ctrl_bo,\n    metrics = reg_metrics\n  )\n#&gt; Optimizing mae using the expected improvement\n#&gt; \n#&gt; â”€â”€ Iteration 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=10.05 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=7, tree_depth=13, learn_rate=0.0802, loss_reduction=1.13e-10, stop_iter=17\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â™¥ Newest results:    mae=9.599 (+/-0.135)\n#&gt; \n#&gt; â”€â”€ Iteration 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.599 (@iter 1)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=2, tree_depth=7, learn_rate=0.0255, loss_reduction=1.22e-07, stop_iter=14\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.895 (+/-0.183)\n#&gt; \n#&gt; â”€â”€ Iteration 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.599 (@iter 1)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=28, tree_depth=5, learn_rate=0.0507, loss_reduction=1.91e-09, stop_iter=16\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.975 (+/-0.186)\n#&gt; \n#&gt; â”€â”€ Iteration 4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.599 (@iter 1)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=3, tree_depth=11, learn_rate=0.0117, loss_reduction=0.00846, stop_iter=18\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=10.25 (+/-0.194)\n#&gt; \n#&gt; â”€â”€ Iteration 5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.599 (@iter 1)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=34, tree_depth=1, learn_rate=0.0695, loss_reduction=0.00028, stop_iter=10\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=15.57 (+/-0.254)\n#&gt; \n#&gt; â”€â”€ Iteration 6 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.599 (@iter 1)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=18, tree_depth=13, learn_rate=0.0229, loss_reduction=0.181, stop_iter=19\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.882 (+/-0.166)\n#&gt; \n#&gt; â”€â”€ Iteration 7 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.599 (@iter 1)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=39, tree_depth=11, learn_rate=0.0485, loss_reduction=0.000906, stop_iter=19\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.883 (+/-0.169)\n#&gt; \n#&gt; â”€â”€ Iteration 8 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.599 (@iter 1)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=27, tree_depth=11, learn_rate=0.0396, loss_reduction=1.54e-10, stop_iter=6\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.811 (+/-0.195)\n#&gt; \n#&gt; â”€â”€ Iteration 9 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.599 (@iter 1)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=39, tree_depth=5, learn_rate=0.0131, loss_reduction=6.45e-05, stop_iter=16\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=10.55 (+/-0.152)\n#&gt; \n#&gt; â”€â”€ Iteration 10 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.599 (@iter 1)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=15, tree_depth=15, learn_rate=0.00677, loss_reduction=5.01e-06, stop_iter=19\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=10.73 (+/-0.214)\n#&gt; \n#&gt; â”€â”€ Iteration 11 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.599 (@iter 1)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=15, tree_depth=15, learn_rate=0.0929, loss_reduction=2.08, stop_iter=18\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.82 (+/-0.141)\n#&gt; ! No improvement for 10 iterations; returning current results."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#best-results",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#best-results",
    "title": "5 - Iterative Search",
    "section": "Best results",
    "text": "Best results\n\nshow_best(lgbm_bayes_res, metric = \"mae\") |&gt; select(-.metric, -.estimator)\n#&gt; # A tibble: 5 Ã— 10\n#&gt;   min_n tree_depth learn_rate loss_reduction stop_iter  mean     n std_err .config .iter\n#&gt;   &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;     &lt;int&gt; &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n#&gt; 1     7         13     0.0802       1.13e-10        17  9.60    10   0.135 Iter1       1\n#&gt; 2    27         11     0.0396       1.54e-10         6  9.81    10   0.195 Iter8       8\n#&gt; 3    15         15     0.0929       2.08e+ 0        18  9.82    10   0.141 Iter11     11\n#&gt; 4    18         13     0.0229       1.81e- 1        19  9.88    10   0.166 Iter6       6\n#&gt; 5    39         11     0.0485       9.06e- 4        19  9.88    10   0.169 Iter7       7"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#plotting-bo-results",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#plotting-bo-results",
    "title": "5 - Iterative Search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(lgbm_bayes_res, metric = \"mae\")"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#plotting-bo-results-1",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#plotting-bo-results-1",
    "title": "5 - Iterative Search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"parameters\")"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#plotting-bo-results-2",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#plotting-bo-results-2",
    "title": "5 - Iterative Search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"performance\")"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#enhance",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#enhance",
    "title": "5 - Iterative Search",
    "section": "ENHANCE",
    "text": "ENHANCE\n\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"performance\") +\n  ylim(c(9, 14))"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#your-turn-1",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#your-turn-1",
    "title": "5 - Iterative Search",
    "section": "Your turn",
    "text": "Your turn\nLetâ€™s try a different acquisition function: conf_bound(kappa).\nWeâ€™ll use the objective argument to set it.\nChoose your own kappa value:\n\nLarger values will explore the space more.\nâ€œLargeâ€ values are usually less than one.\n\nBonus points: Before the optimization is done, press &lt;esc&gt; and see what happens.\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#notes",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#notes",
    "title": "5 - Iterative Search",
    "section": "Notes",
    "text": "Notes\n\nStopping tune_bayes() will return the current results.\nParallel processing can still be used to more efficiently measure each candidate point.\nThere are a lot of other iterative methods that you can use.\nThe finetune package also has functions for simulated annealing search."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#finalizing-the-model",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#finalizing-the-model",
    "title": "5 - Iterative Search",
    "section": "Finalizing the Model",
    "text": "Finalizing the Model\nLetâ€™s say that weâ€™ve tried a lot of different models and we like our lightgbm model the most.\nWhat do we do now?\n\nFinalize the workflow by choosing the values for the tuning parameters.\nFit the model on the entire training set.\nVerify performance using the test set.\nDocument and publish the model(?)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#locking-down-the-tuning-parameters",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#locking-down-the-tuning-parameters",
    "title": "5 - Iterative Search",
    "section": "Locking Down the Tuning Parameters",
    "text": "Locking Down the Tuning Parameters\nWe can take the results of the Bayesian optimization and accept the best results:\n\nbest_param &lt;- select_best(lgbm_bayes_res, metric = \"mae\")\nfinal_wflow &lt;- \n  lgbm_wflow |&gt; \n  finalize_workflow(best_param)\nfinal_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: boost_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; avg_price_per_room ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Boosted Tree Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt;   min_n = 7\n#&gt;   tree_depth = 13\n#&gt;   learn_rate = 0.0801616272924067\n#&gt;   loss_reduction = 1.13369097175613e-10\n#&gt;   stop_iter = 17\n#&gt; \n#&gt; Engine-Specific Arguments:\n#&gt;   num_threads = 1\n#&gt; \n#&gt; Computational engine: lightgbm"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#the-final-fit",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#the-final-fit",
    "title": "5 - Iterative Search",
    "section": "The Final Fit",
    "text": "The Final Fit\nWe can use individual functions:\nfinal_fit &lt;- final_wflow |&gt; fit(data = hotel_train)\n\n# then predict() or augment() \n# then compute metrics\n\nRemember that there is also a convenience function to do all of this:\n\nset.seed(3893)\nfinal_res &lt;- final_wflow |&gt; last_fit(hotel_split, metrics = reg_metrics)\nfinal_res\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits              id               .metrics         .notes           .predictions         .workflow \n#&gt;   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;           &lt;list&gt;               &lt;list&gt;    \n#&gt; 1 &lt;split [3749/1251]&gt; train/test split &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1,251 Ã— 4]&gt; &lt;workflow&gt;"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-05-iterative.html#test-set-results",
    "href": "archive/2025-08-nyr/slides/advanced-05-iterative.html#test-set-results",
    "title": "5 - Iterative Search",
    "section": "Test Set Results",
    "text": "Test Set Results\n\n\n\nfinal_res |&gt; \n  collect_predictions() |&gt; \n  cal_plot_regression(\n    truth = avg_price_per_room, \n    estimate = .pred)\n\nTest set performance:\n\nfinal_res |&gt; collect_metrics()\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard       9.68  Preprocessor1_Model1\n#&gt; 2 rsq     standard       0.946 Preprocessor1_Model1\n\n\n\n\n\n\n\n\n\n\n\n\nRecall that resampling predicted the MAE to be 9.599."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#previously---setup",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#previously---setup",
    "title": "3 - Tuning Hyperparameters",
    "section": "Previously - Setup ",
    "text": "Previously - Setup \n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates |&gt; \n  sample_n(5000) |&gt; \n  arrange(arrival_date) |&gt; \n  select(-arrival_date) |&gt; \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#previously---data-usage",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#previously---data-usage",
    "title": "3 - Tuning Hyperparameters",
    "section": "Previously - Data Usage ",
    "text": "Previously - Data Usage \n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#previously---feature-engineering",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#previously---feature-engineering",
    "title": "3 - Tuning Hyperparameters",
    "section": "Previously - Feature engineering  ",
    "text": "Previously - Feature engineering  \n\nlibrary(textrecipes)\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt;\n  step_YeoJohnson(lead_time) |&gt;\n  # Defaults to 32 signed indicator columns\n  step_dummy_hash(agent) |&gt;\n  step_dummy_hash(company) |&gt;\n  # Regular indicators for the others\n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors())"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#tuning-parameters",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#tuning-parameters",
    "title": "3 - Tuning Hyperparameters",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#optimize-tuning-parameters",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#optimize-tuning-parameters",
    "title": "3 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#tagging-parameters-for-tuning",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#tagging-parameters-for-tuning",
    "title": "3 - Tuning Hyperparameters",
    "section": "Tagging parameters for tuning ",
    "text": "Tagging parameters for tuning \nWith tidymodels, you can mark the parameters that you want to optimize with a value of tune().\n\nThe function itself just returnsâ€¦ itself:\n\ntune()\n#&gt; tune()\nstr(tune())\n#&gt;  language tune()\n\n# optionally add a label\ntune(\"I hope that the workshop is going well\")\n#&gt; tune(\"I hope that the workshop is going well\")\n\n\nFor exampleâ€¦"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#optimizing-the-hash-features",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#optimizing-the-hash-features",
    "title": "3 - Tuning Hyperparameters",
    "section": "Optimizing the hash features   ",
    "text": "Optimizing the hash features   \nOur new recipe is:\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt;\n  step_YeoJohnson(lead_time) |&gt;\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) |&gt;\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) |&gt;\n  step_zv(all_predictors())\n\n\nWe will be using a tree-based model in a minute.\n\nThe other categorical predictors are left as-is.\nThatâ€™s why there is no step_dummy()."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#boosted-trees",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#boosted-trees",
    "title": "3 - Tuning Hyperparameters",
    "section": "Boosted Trees",
    "text": "Boosted Trees\nThese are popular ensemble methods that build a sequence of tree models.\n\nEach tree uses the results of the previous tree to better predict samples, especially those that have been poorly predicted.\n\nEach tree in the ensemble is saved and new samples are predicted using a weighted average of the votes of each tree in the ensemble.\n\nWeâ€™ll focus on the popular lightgbm implementation."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "title": "3 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\nSome possible parameters:\n\nmtry: The number of predictors randomly sampled at each split (in \\([1, ncol(x)]\\) or \\((0, 1]\\)).\ntrees: The number of trees (\\([1, \\infty]\\), but usually up to thousands)\nmin_n: The number of samples needed to further split (\\([1, n]\\)).\nlearn_rate: The rate that each tree adapts from previous iterations (\\((0, \\infty]\\), usual maximum is 0.1).\nstop_iter: The number of iterations of boosting where no improvement was shown before stopping (\\([1, trees]\\))"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters-1",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\nTBH it is usually not difficult to optimize these models.\n\nOften, there are multiple candidate tuning parameter combinations that have very good results.\n\nTo demonstrate simple concepts, weâ€™ll look at optimizing the number of trees in the ensemble (between 1 and 100) and the learning rate (\\(10^{-5}\\) to \\(10^{-1}\\))."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters-2",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters-2",
    "title": "3 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters   ",
    "text": "Boosted Tree Tuning Parameters   \nWeâ€™ll need to load the bonsai package. This has the information needed to use lightgbm\n\nlibrary(bonsai)\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune()) |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow &lt;- workflow(hash_rec, lgbm_spec)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search ğŸ’  which tests a pre-defined set of candidate values\nIterative search ğŸŒ€ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#grid-search",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#grid-search",
    "title": "3 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nA small grid of points trying to minimize the error via learning rate:"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#grid-search-1",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#grid-search-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nIn reality we would probably sample the space more densely:"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#iterative-search",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#iterative-search",
    "title": "3 - Tuning Hyperparameters",
    "section": "Iterative Search",
    "text": "Iterative Search\nWe could start with a few points and search the space:"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#parameters",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#parameters",
    "title": "3 - Tuning Hyperparameters",
    "section": "Parameters",
    "text": "Parameters\n\nThe tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\nThe extract_parameter_set_dials() function extracts these tuning parameters and the info.\n\n\nGrids\n\nCreate your grid manually or automatically.\nThe grid_*() functions can make a grid.\n\n\n\nMost basic (but very effective) way to tune models"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#different-types-of-grids",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#different-types-of-grids",
    "title": "3 - Tuning Hyperparameters",
    "section": "Different types of grids ",
    "text": "Different types of grids \n\n\n\n\n\n\n\n\n\nSpace-filling designs (SFD) attempt to cover the parameter space without redundant candidates. We recommend these the most."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#create-a-grid",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#create-a-grid",
    "title": "3 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nlgbm_wflow |&gt; \n  extract_parameter_set_dials()\n\n# Individual functions: \ntrees()\nlearn_rate()\n\n\nA parameter set can be updated (e.g.Â to change the ranges)."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#create-a-grid-1",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#create-a-grid-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\ngrid &lt;- \n  lgbm_wflow |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_space_filling(size = 25)\n\ngrid\n#&gt; # A tibble: 25 Ã— 4\n#&gt;    trees learn_rate `agent hash` `company hash`\n#&gt;    &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1     1   7.50e- 6          574            574\n#&gt;  2    84   1.78e- 5         2048           2298\n#&gt;  3   167   5.62e-10         1824            912\n#&gt;  4   250   4.22e- 5         3250            512\n#&gt;  5   334   1.78e- 8          512           2896\n#&gt;  6   417   1.33e- 3          322           1625\n#&gt;  7   500   1   e- 1         1448           1149\n#&gt;  8   584   1   e- 7         1290            256\n#&gt;  9   667   2.37e-10          456            724\n#&gt; 10   750   1.78e- 2          645            322\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#your-turn",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#your-turn",
    "title": "3 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a grid for our tunable workflow.\nTry creating a regular grid.\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#create-a-regular-grid",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#create-a-regular-grid",
    "title": "3 - Tuning Hyperparameters",
    "section": "Create a regular grid  ",
    "text": "Create a regular grid  \n\ngrid &lt;- \n  lgbm_wflow |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_regular(levels = 4)\n\ngrid\n#&gt; # A tibble: 256 Ã— 4\n#&gt;    trees   learn_rate `agent hash` `company hash`\n#&gt;    &lt;int&gt;        &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1     1 0.0000000001          256            256\n#&gt;  2   667 0.0000000001          256            256\n#&gt;  3  1333 0.0000000001          256            256\n#&gt;  4  2000 0.0000000001          256            256\n#&gt;  5     1 0.0000001             256            256\n#&gt;  6   667 0.0000001             256            256\n#&gt;  7  1333 0.0000001             256            256\n#&gt;  8  2000 0.0000001             256            256\n#&gt;  9     1 0.0001                256            256\n#&gt; 10   667 0.0001                256            256\n#&gt; # â„¹ 246 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#your-turn-1",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#your-turn-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\n\nWhat advantage would a regular grid have?"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#update-parameter-ranges",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#update-parameter-ranges",
    "title": "3 - Tuning Hyperparameters",
    "section": "Update parameter ranges  ",
    "text": "Update parameter ranges  \n\nlgbm_param &lt;- \n  lgbm_wflow |&gt; \n  extract_parameter_set_dials() |&gt; \n  update(trees = trees(c(1L, 100L)),\n         learn_rate = learn_rate(c(-5, -1)))\n\ngrid &lt;- \n  lgbm_param |&gt; \n  grid_space_filling(size = 25)\n\ngrid\n#&gt; # A tibble: 25 Ã— 4\n#&gt;    trees learn_rate `agent hash` `company hash`\n#&gt;    &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1     1  0.00147            574            574\n#&gt;  2     5  0.00215           2048           2298\n#&gt;  3     9  0.0000215         1824            912\n#&gt;  4    13  0.00316           3250            512\n#&gt;  5    17  0.0001             512           2896\n#&gt;  6    21  0.0147             322           1625\n#&gt;  7    25  0.1               1448           1149\n#&gt;  8    29  0.000215          1290            256\n#&gt;  9    34  0.0000147          456            724\n#&gt; 10    38  0.0464             645            322\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#the-results",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#the-results",
    "title": "3 - Tuning Hyperparameters",
    "section": "The results  ",
    "text": "The results  \n\n\ngrid |&gt; \n  ggplot(aes(trees, learn_rate)) +\n  geom_point(size = 4) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\nNote that the learning rates are uniform on the log-10 scale and this shows 2 of 4 dimensions."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#choosing-tuning-parameters",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#choosing-tuning-parameters",
    "title": "3 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters     ",
    "text": "Choosing tuning parameters     \nLetâ€™s take our previous model and tune more parameters:\n\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune(),  min_n = tune()) |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow &lt;- workflow(hash_rec, lgbm_spec)\n\n# Update the feature hash ranges (log-2 units)\nlgbm_param &lt;-\n  lgbm_wflow |&gt;\n  extract_parameter_set_dials() |&gt;\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#grid-search-3",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#grid-search-3",
    "title": "3 - Tuning Hyperparameters",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nset.seed(9)\nctrl &lt;- control_grid(save_pred = TRUE)\n\nlgbm_res &lt;-\n  lgbm_wflow |&gt;\n  tune_grid(\n    resamples = hotel_rs,\n    grid = 25,\n    # The options below are not required by default\n    param_info = lgbm_param, \n    control = ctrl,\n    metrics = reg_metrics\n  )\n\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#grid-search-4",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#grid-search-4",
    "title": "3 - Tuning Hyperparameters",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nlgbm_res \n#&gt; # Tuning results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics          .notes           .predictions        \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;              \n#&gt;  1 &lt;split [3372/377]&gt; Fold01 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,425 Ã— 9]&gt;\n#&gt;  2 &lt;split [3373/376]&gt; Fold02 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  3 &lt;split [3373/376]&gt; Fold03 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  4 &lt;split [3373/376]&gt; Fold04 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  5 &lt;split [3373/376]&gt; Fold05 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  6 &lt;split [3374/375]&gt; Fold06 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,375 Ã— 9]&gt;\n#&gt;  7 &lt;split [3375/374]&gt; Fold07 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,350 Ã— 9]&gt;\n#&gt;  8 &lt;split [3376/373]&gt; Fold08 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,325 Ã— 9]&gt;\n#&gt;  9 &lt;split [3376/373]&gt; Fold09 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,325 Ã— 9]&gt;\n#&gt; 10 &lt;split [3376/373]&gt; Fold10 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,325 Ã— 9]&gt;"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#grid-results",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#grid-results",
    "title": "3 - Tuning Hyperparameters",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(lgbm_res)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#tuning-results",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#tuning-results",
    "title": "3 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(lgbm_res)\n#&gt; # A tibble: 50 Ã— 11\n#&gt;    trees min_n    learn_rate `agent hash` `company hash` .metric .estimator   mean     n std_err .config          \n#&gt;    &lt;int&gt; &lt;int&gt;         &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n#&gt;  1  1500    21 0.00000000316            8             60 mae     standard   53.2      10 0.427   pre01_mod19_post0\n#&gt;  2  1500    21 0.00000000316            8             60 rsq     standard    0.809    10 0.00661 pre01_mod19_post0\n#&gt;  3   917    16 0.0422                   9             39 mae     standard    9.85     10 0.150   pre02_mod12_post0\n#&gt;  4   917    16 0.0422                   9             39 rsq     standard    0.946    10 0.00362 pre02_mod12_post0\n#&gt;  5   584    13 0.0000001               10             10 mae     standard   53.2      10 0.427   pre03_mod08_post0\n#&gt;  6   584    13 0.0000001               10             10 rsq     standard    0.810    10 0.00745 pre03_mod08_post0\n#&gt;  7   167    19 0.00000133              12            143 mae     standard   53.2      10 0.426   pre04_mod03_post0\n#&gt;  8   167    19 0.00000133              12            143 rsq     standard    0.811    10 0.00698 pre04_mod03_post0\n#&gt;  9  1583    33 0.0000422               14             12 mae     standard   50.3      10 0.405   pre05_mod20_post0\n#&gt; 10  1583    33 0.0000422               14             12 rsq     standard    0.817    10 0.00768 pre05_mod20_post0\n#&gt; # â„¹ 40 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#tuning-results-1",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#tuning-results-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(lgbm_res, summarize = FALSE)\n#&gt; # A tibble: 500 Ã— 10\n#&gt;    id     trees min_n    learn_rate `agent hash` `company hash` .metric .estimator .estimate .config          \n#&gt;    &lt;chr&gt;  &lt;int&gt; &lt;int&gt;         &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            \n#&gt;  1 Fold01  1500    21 0.00000000316            8             60 mae     standard      51.8   pre01_mod19_post0\n#&gt;  2 Fold01  1500    21 0.00000000316            8             60 rsq     standard       0.805 pre01_mod19_post0\n#&gt;  3 Fold02  1500    21 0.00000000316            8             60 mae     standard      52.1   pre01_mod19_post0\n#&gt;  4 Fold02  1500    21 0.00000000316            8             60 rsq     standard       0.800 pre01_mod19_post0\n#&gt;  5 Fold03  1500    21 0.00000000316            8             60 mae     standard      52.2   pre01_mod19_post0\n#&gt;  6 Fold03  1500    21 0.00000000316            8             60 rsq     standard       0.783 pre01_mod19_post0\n#&gt;  7 Fold04  1500    21 0.00000000316            8             60 mae     standard      51.7   pre01_mod19_post0\n#&gt;  8 Fold04  1500    21 0.00000000316            8             60 rsq     standard       0.818 pre01_mod19_post0\n#&gt;  9 Fold05  1500    21 0.00000000316            8             60 mae     standard      55.2   pre01_mod19_post0\n#&gt; 10 Fold05  1500    21 0.00000000316            8             60 rsq     standard       0.845 pre01_mod19_post0\n#&gt; # â„¹ 490 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#choose-a-parameter-combination",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#choose-a-parameter-combination",
    "title": "3 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nshow_best(lgbm_res, metric = \"rsq\")\n#&gt; # A tibble: 5 Ã— 11\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config          \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n#&gt; 1  1250     9    0.1              107             80 rsq     standard   0.947    10 0.00395 pre19_mod16_post0\n#&gt; 2   917    16    0.0422             9             39 rsq     standard   0.946    10 0.00362 pre02_mod12_post0\n#&gt; 3  1666    30    0.00750          124             19 rsq     standard   0.943    10 0.00309 pre20_mod21_post0\n#&gt; 4  2000    24    0.00133           25            124 rsq     standard   0.919    10 0.00383 pre09_mod25_post0\n#&gt; 5   500    25    0.00316           52              8 rsq     standard   0.899    10 0.00435 pre14_mod07_post0"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \nCreate your own tibble for final parameters or use one of the tune::select_*() functions:\n\nlgbm_best &lt;- select_best(lgbm_res, metric = \"mae\")\nlgbm_best\n#&gt; # A tibble: 1 Ã— 6\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .config          \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;            \n#&gt; 1  1250     9        0.1          107             80 pre19_mod16_post0"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#checking-calibration",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#checking-calibration",
    "title": "3 - Tuning Hyperparameters",
    "section": "Checking Calibration  ",
    "text": "Checking Calibration  \n\n\nlibrary(probably)\nlgbm_res |&gt;\n  collect_predictions(\n    parameters = lgbm_best\n  ) |&gt;\n  cal_plot_regression(\n    truth = avg_price_per_room,\n    estimate = .pred\n  )"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#running-in-parallel",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#running-in-parallel",
    "title": "3 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\n\n\n\nGrid search, combined with resampling, requires fitting a lot of models!\nThese models donâ€™t depend on one another and can be run in parallel.\n\nWe can use a parallel backend to do this:\n\ncores &lt;- parallelly::availableCores(logical = FALSE)\n\nlibrary(future)\nplan(multisession, workers = cores)\n\n# Now call `tune_grid()`!"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#running-in-parallel-1",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#running-in-parallel-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\nFaceted on the expensiveness of preprocessing used."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#the-future-of-parallel-processing",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#the-future-of-parallel-processing",
    "title": "3 - Tuning Hyperparameters",
    "section": "The â€˜futureâ€™ of parallel processing",
    "text": "The â€˜futureâ€™ of parallel processing\nWe have relied on the foreach package for parallel processing.\nWe will start the transition to using the future package in the upcoming version of the tune package (version 1.3.0).\nThere will be a period of backward compatibility where you can still use foreach with future via the doFuture package. After that, the transition to future will occur.\nOverall, there will be minimal changes to your code."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#early-stopping-for-boosted-trees",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#early-stopping-for-boosted-trees",
    "title": "3 - Tuning Hyperparameters",
    "section": "Early stopping for boosted trees",
    "text": "Early stopping for boosted trees\nWe have directly optimized the number of trees as a tuning parameter.\nInstead we could\n\nSet the number of trees to a single large number.\nStop adding trees when performance gets worse.\n\nThis is known as â€œearly stoppingâ€ and there is a parameter for that: stop_iter.\nEarly stopping has a potential to decrease the tuning time."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#your-turn-2",
    "href": "archive/2025-08-nyr/slides/advanced-03-tuning-hyperparameters.html#your-turn-2",
    "title": "3 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\n\nSet trees = 2000 and tune the stop_iter parameter.\nNote that you will need to regenerate lgbm_param with your new workflow!\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#who-are-you",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %&gt;% or base R |&gt; pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have exposure to basic statistical concepts\nYou do not need intermediate or expert familiarity with modeling or ML\nYou have used some tidymodels packages\nYou have some experience with evaluating statistical models using resampling techniques"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#who-are-tidymodels",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#who-are-tidymodels",
    "title": "1 - Introduction",
    "section": "Who are tidymodels?",
    "text": "Who are tidymodels?\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\nMany thanks to Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill, and DesirÃ©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#section-2",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#section-2",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#section-3",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#section-3",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#tentative-plan-for-this-workshop",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#tentative-plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Tentative plan for this workshop",
    "text": "Tentative plan for this workshop\n\nFeature engineering with recipes\nModel optimization by tuning\n\nGrid search\nRacing\nIterative methods\n\nExtras (time permitting)\n\nEffect encodings\nA case study"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#section-4",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#section-4",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors ğŸ‘‹"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#lets-install-some-packages",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Letâ€™s install some packages",
    "text": "Letâ€™s install some packages\nIf you are using your own laptop instead of Posit Cloud:\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"bonsai\", \"Cubist\", \"doParallel\", \"earth\", \"embed\", \"finetune\", \n    \"lightgbm\", \"lme4\", \"pak\", \"parallelly\", \"plumber\", \"probably\", \n    \"ranger\", \"rpart\", \"rpart.plot\", \"rules\", \"splines2\", \"stacks\", \n    \"text2vec\", \"textrecipes\", \"tidymodels\", \"vetiver\")\n\ninstall.packages(pkgs)\n\npak::pak(\"simonpcouch/forested\")\n\nAlso, you should install the newest version of the dials package (version 1.3.0). To check this, you can run:\n\nrlang::check_installed(\"dials\", version = \"1.3.0\")"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#hotel-data",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#hotel-data",
    "title": "1 - Introduction",
    "section": "Hotel Data  ",
    "text": "Hotel Data  \nWeâ€™ll use data on hotels to predict the cost of a room.\nThe data are in the modeldata package. Weâ€™ll sample down the data and refactor some columns:\n\n\n\nlibrary(tidymodels)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates |&gt; \n  sample_n(5000) |&gt; \n  arrange(arrival_date) |&gt; \n  select(-arrival_date) |&gt; \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#hotel-date-columns",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#hotel-date-columns",
    "title": "1 - Introduction",
    "section": "Hotel date columns",
    "text": "Hotel date columns\n\nnames(hotel_rates)\n#&gt;  [1] \"avg_price_per_room\"             \"lead_time\"                     \n#&gt;  [3] \"stays_in_weekend_nights\"        \"stays_in_week_nights\"          \n#&gt;  [5] \"adults\"                         \"children\"                      \n#&gt;  [7] \"babies\"                         \"meal\"                          \n#&gt;  [9] \"country\"                        \"market_segment\"                \n#&gt; [11] \"distribution_channel\"           \"is_repeated_guest\"             \n#&gt; [13] \"previous_cancellations\"         \"previous_bookings_not_canceled\"\n#&gt; [15] \"reserved_room_type\"             \"assigned_room_type\"            \n#&gt; [17] \"booking_changes\"                \"agent\"                         \n#&gt; [19] \"company\"                        \"days_in_waiting_list\"          \n#&gt; [21] \"customer_type\"                  \"required_car_parking_spaces\"   \n#&gt; [23] \"total_of_special_requests\"      \"arrival_date_num\"              \n#&gt; [25] \"near_christmas\"                 \"near_new_years\"                \n#&gt; [27] \"historical_adr\""
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#data-splitting-strategy",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#data-splitting-strategy",
    "title": "1 - Introduction",
    "section": "Data splitting strategy",
    "text": "Data splitting strategy"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#data-spending",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#data-spending",
    "title": "1 - Introduction",
    "section": "Data Spending ",
    "text": "Data Spending \nLetâ€™s split the data into a training set (75%) and testing set (25%) using stratification:\n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#your-turn",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\nLetâ€™s take some time and investigate the training data. The outcome is avg_price_per_room.\nAre there any interesting characteristics of the data?\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-01-introduction.html#our-versions",
    "href": "archive/2025-08-nyr/slides/advanced-01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\nR version 4.5.0 (2025-04-11), Quarto (1.6.42)\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nbonsai\n0.4.0\n\n\nbroom\n1.0.9\n\n\nCubist\n0.5.0\n\n\ndials\n1.4.1\n\n\ndoParallel\n1.0.17\n\n\ndplyr\n1.1.4\n\n\nearth\n5.3.4\n\n\nembed\n1.1.5.9000\n\n\nfinetune\n1.2.1.9000\n\n\nforested\n0.2.0\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nFormula\n1.2-5\n\n\nggplot2\n3.5.2\n\n\nlattice\n0.22-7\n\n\nlightgbm\n4.6.0\n\n\nlme4\n1.1-37\n\n\nmodeldata\n1.5.1\n\n\nparallelly\n1.45.1\n\n\nparsnip\n1.3.2.9000\n\n\nplotmo\n3.6.4\n\n\nplotrix\n3.8-4\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nplumber\n1.3.0\n\n\nprobably\n1.1.0\n\n\npurrr\n1.1.0\n\n\nrecipes\n1.3.1\n\n\nrsample\n1.3.1\n\n\nrules\n1.0.2\n\n\nscales\n1.4.0\n\n\nsplines2\n0.5.4\n\n\nstacks\n1.1.1.9000\n\n\ntext2vec\n0.6.4\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\ntextrecipes\n1.1.0.9000\n\n\ntibble\n3.3.0\n\n\ntidymodels\n1.3.0\n\n\ntidyr\n1.3.1\n\n\ntune\n1.3.0\n\n\nvetiver\n0.2.5\n\n\nworkflows\n1.2.0.9002\n\n\nworkflowsets\n1.1.1\n\n\nyardstick\n1.3.2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels offered at posit::conf(2025). The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. This website hosts the materials for both the Introduction to Machine Learning in R with tidymodels and Getting More Out of Feature Engineering and Tuning for Machine Learning courses.\nIntroduction to Machine Learning in R with tidymodels will teach you core tidymodels packages and their uses: data splitting/resampling with rsample, model fitting with parsnip, measuring model performance with yardstick, and model optimization using the tune package. Time permitting, youâ€™ll be introduced to basic pre-processing with recipes. Youâ€™ll learn tidymodels syntax as well as the process of predictive modeling for tabular data.\nGetting More Out of Feature Engineering and Tuning for Machine Learning will teach you about model optimization using the tune and finetune packages, including racing and iterative methods. Youâ€™ll be able to do more sophisticated feature engineering with recipes. Time permitting, model ensembles via stacking will be introduced. This course is focused on the analysis of tabular data and does not include deep learning methods."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels offered at posit::conf(2025). The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. This website hosts the materials for both the Introduction to Machine Learning in R with tidymodels and Getting More Out of Feature Engineering and Tuning for Machine Learning courses.\nIntroduction to Machine Learning in R with tidymodels will teach you core tidymodels packages and their uses: data splitting/resampling with rsample, model fitting with parsnip, measuring model performance with yardstick, and model optimization using the tune package. Time permitting, youâ€™ll be introduced to basic pre-processing with recipes. Youâ€™ll learn tidymodels syntax as well as the process of predictive modeling for tabular data.\nGetting More Out of Feature Engineering and Tuning for Machine Learning will teach you about model optimization using the tune and finetune packages, including racing and iterative methods. Youâ€™ll be able to do more sophisticated feature engineering with recipes. Time permitting, model ensembles via stacking will be introduced. This course is focused on the analysis of tabular data and does not include deep learning methods."
  },
  {
    "objectID": "index.html#is-this-workshop-for-me",
    "href": "index.html#is-this-workshop-for-me",
    "title": "Machine learning with tidymodels",
    "section": "Is this workshop for me? ",
    "text": "Is this workshop for me? \nDepending on your background, one of Introduction to Machine Learning in R with tidymodels or Getting More Out of Feature Engineering and Tuning for Machine Learning might serve you better than the other.\n\nIntroduction to Machine Learning in R with tidymodels\nThis workshop is for you if you:\n\nare comfortable using tidyverse packages to read data into R, transform and reshape data, and make a variety of graphs, and\nhave had some exposure to basic statistical concepts such as linear models, residuals, etc.\n\nIntermediate or expert familiarity with modeling or machine learning is not required. Interested students who have intermediate or expert familiarity with modeling or machine learning may be interested in the Getting More Out of Feature Engineering and Tuning for Machine Learning workshop.\n\n\nGetting More Out of Feature Engineering and Tuning for Machine Learning\nThis workshop is for you if you:\n\nhave the prerequisite skills listed for the Introduction to Machine Learning in R with tidymodels workshops,\nhave used tidymodels packages like recipes, rsample, and parsnip, and\nhave some experience with evaluating statistical models using resampling techniques like v-fold cross-validation or the bootstrap.\n\nParticipants who are new to tidymodels or machine learning will benefit from taking the Introduction to Machine Learning in R with tidymodels workshop before joining this one. Participants who have completed the â€œIntroduction to Machine Learning in R with tidymodelsâ€ workshop previously will be well-prepared for this course."
  },
  {
    "objectID": "index.html#preparation",
    "href": "index.html#preparation",
    "title": "Machine learning with tidymodels",
    "section": "Preparation",
    "text": "Preparation\nThe process to set up your computer for either workshop will look the same. Please join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/\nA recent version of an IDE, either\n\nRStudio Desktop (RStudio Desktop Open Source License, at least v2024.04.0), available at https://posit.co/download/rstudio-desktop/ or\nPositron (Desktop version, at least 2025.08.0-130), available at https://positron.posit.co/download.html\n\nFor all of the slides, the following R packages can be installed from the R console:\n\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"almanac\", \"betacal\", \"bonsai\", \"brulee\", \"C50\", \"Cubist\", \"desirability2\", \n    \"dimRed\", \"embed\", \"extrasteps\", \"finetune\", \"forested\", \n    \"igraph\", \"important\", \"irlba\", \"kknn\", \"lightgbm\", \"lme4\", \"mirai\", \n    \"parallelly\", \"plumber\", \"probably\", \"ranger\", \"RANN\", \"rpart\", \n    \"rpart.plot\", \"RSpectra\", \"rules\", \"splines2\", \"stacks\", \"text2vec\", \n    \"textrecipes\", \"tidymodels\", \"uwot\", \"vetiver\")\n\ninstall.packages(pkgs)\n\nAlso, you should make sure that you have installed the newest version of a few packages. To check this, you can run the following code snippet. It will also prompt you to install the required versions if they are not already installed.\n\nrlang::check_installed(\"tidymodels\", version = \"1.4.1\")\nrlang::check_installed(\"embed\", version = \"1.2.0\")\n\nIf youâ€™re a Windows user and encounter an error message during installation noting a missing Rtools installation, install Rtools using the installer linked here."
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "Machine learning with tidymodels",
    "section": "Slides",
    "text": "Slides\nThese slides are designed to use with live teaching and are published for workshop participantsâ€™ convenience. They are not meant as standalone learning materials. For that, we recommend tidymodels.org and Tidy Modeling with R.\n\nIntroduction to Machine Learning in R with tidymodels\n\n01: Introduction\n02: Your data budget\n03: What makes a model?\n04: Evaluating models\n05: Tuning models\n06: Wrapping up\n\n\n\nGetting More Out of Feature Engineering and Tuning for Machine Learning\n\n01: Introduction\n02: Model optimization by tuning\n03: Grid search via racing\n04: Feature engineering: dummies and embeddings\n05: Feature engineering: splines, target encoding and dates\n06: Postprocessing\n07: Feature selection\n08: Wrapping up\n\n\n\nExtra content (time permitting)\n\nIntro: Using workflowsets\nIntro: Using recipes\nAdvanced: Transit case study (includes stacking)\nAdvanced: Iterative search\n\nThereâ€™s also a page for slide annotations; these are extra notes for selected slides."
  },
  {
    "objectID": "index.html#code",
    "href": "index.html#code",
    "title": "Machine learning with tidymodels",
    "section": "Code",
    "text": "Code\nQuarto files for working along are available on GitHub. (Donâ€™t worry if you havenâ€™t used Quarto before; it will feel familiar to R Markdown users.)"
  },
  {
    "objectID": "index.html#past-workshops",
    "href": "index.html#past-workshops",
    "title": "Machine learning with tidymodels",
    "section": "Past workshops",
    "text": "Past workshops\n\nEnglish\n\nAugust 2025 at New York Data Science & AI Conference\nAugust 2024 at posit::conf()\nSeptember 2023 at posit::conf()\nJuly 2023 at the New York R Conference\nAugust 2022 in Reykjavik\nJuly 2022 at rstudio::conf()\n\n\n\nSpanish\n\nMarch 2024 at conectaR"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Machine learning with tidymodels",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including the slides, is made with Quarto. Please submit an issue on the GitHub repo for this workshop if you find something that could be fixed or improved."
  },
  {
    "objectID": "index.html#reuse-and-licensing",
    "href": "index.html#reuse-and-licensing",
    "title": "Machine learning with tidymodels",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e.Â not an original creation and reused from another source), these educational materials are licensed under Creative Commons Attribution CC BY-SA 4.0."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#working-with-our-predictors",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#working-with-our-predictors",
    "title": "2 - Feature Engineering",
    "section": "Working with our predictors",
    "text": "Working with our predictors\nWe might want to modify our predictors columns for a few reasons:\n\nThe model requires them in a different format (e.g.Â dummy variables for linear regression).\nThe model needs certain data qualities (e.g.Â same units for K-NN).\nThe outcome is better predicted when one or more columns are transformed in some way (a.k.a â€œfeature engineeringâ€).\n\n\nThe first two reasons are fairly predictable (next page).\nThe last one depends on your modeling problem."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#what-is-feature-engineering",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#what-is-feature-engineering",
    "title": "2 - Feature Engineering",
    "section": "What is feature engineering?",
    "text": "What is feature engineering?\nThink of a feature as some representation of a predictor that will be used in a model.\n\nExample representations:\n\nInteractions\nPolynomial expansions/splines\nPrincipal component analysis (PCA) feature extraction\n\nThere are a lot of examples in Feature Engineering and Selection (FES) and Applied Machine Learning for Tabular Data (aml4td)."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#example-dates",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#example-dates",
    "title": "2 - Feature Engineering",
    "section": "Example: Dates",
    "text": "Example: Dates\nHow can we represent date columns for our model?\n\nWhen we use a date column in its native format, most models in R convert it to an integer.\n\n\nWe can re-engineer it as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nIndicators for holidays\n\n\nThe main point is that we try to maximize performance with different versions of the predictors.\nMention that, for the Chicago data, the day or the week features are usually the most important ones in the model."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#general-definitions",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#general-definitions",
    "title": "2 - Feature Engineering",
    "section": "General definitions ",
    "text": "General definitions \n\nData preprocessing steps allow your model to fit.\nFeature engineering steps help the model do the least work to predict the outcome as well as possible.\n\nThe recipes package can handle both!\n\nThese terms are often used interchangeably in the ML community but we want to distinguish them."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#previously---setup",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#previously---setup",
    "title": "2 - Feature Engineering",
    "section": "Previously - Setup ",
    "text": "Previously - Setup \n\n\n\nlibrary(tidymodels)\n\n# Add another package:\nlibrary(textrecipes)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates |&gt; \n  sample_n(5000) |&gt; \n  arrange(arrival_date) |&gt; \n  select(-arrival_date) |&gt; \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#previously---data-usage",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#previously---data-usage",
    "title": "2 - Feature Engineering",
    "section": "Previously - Data Usage ",
    "text": "Previously - Data Usage \n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\n\nWeâ€™ll go from here and create a set of resamples to use for model assessments."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#resampling-strategy",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#resampling-strategy",
    "title": "2 - Feature Engineering",
    "section": "Resampling Strategy",
    "text": "Resampling Strategy"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#resampling-strategy-1",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#resampling-strategy-1",
    "title": "2 - Feature Engineering",
    "section": "Resampling Strategy ",
    "text": "Resampling Strategy \nWeâ€™ll use simple 10-fold cross-validation (stratified sampling):\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)\nhotel_rs\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [3372/377]&gt; Fold01\n#&gt;  2 &lt;split [3373/376]&gt; Fold02\n#&gt;  3 &lt;split [3373/376]&gt; Fold03\n#&gt;  4 &lt;split [3373/376]&gt; Fold04\n#&gt;  5 &lt;split [3373/376]&gt; Fold05\n#&gt;  6 &lt;split [3374/375]&gt; Fold06\n#&gt;  7 &lt;split [3375/374]&gt; Fold07\n#&gt;  8 &lt;split [3376/373]&gt; Fold08\n#&gt;  9 &lt;split [3376/373]&gt; Fold09\n#&gt; 10 &lt;split [3376/373]&gt; Fold10"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#prepare-your-data-for-modeling",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#prepare-your-data-for-modeling",
    "title": "2 - Feature Engineering",
    "section": "Prepare your data for modeling ",
    "text": "Prepare your data for modeling \n\nThe recipes package is an extensible framework for pipeable sequences of preprocessing and feature engineering steps.\n\n\n\nStatistical parameters for the steps can be estimated from an initial data set and then applied to other data sets.\n\n\n\n\nThe resulting processed output can be used as inputs for statistical or machine learning models."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#a-first-recipe",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#a-first-recipe",
    "title": "2 - Feature Engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train)\n\n\n\nThe recipe() function assigns columns to roles of â€œoutcomeâ€ or â€œpredictorâ€ using the formula"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#a-first-recipe-1",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#a-first-recipe-1",
    "title": "2 - Feature Engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nsummary(hotel_rec)\n#&gt; # A tibble: 27 Ã— 4\n#&gt;    variable                type      role      source  \n#&gt;    &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 lead_time               &lt;chr [2]&gt; predictor original\n#&gt;  2 stays_in_weekend_nights &lt;chr [2]&gt; predictor original\n#&gt;  3 stays_in_week_nights    &lt;chr [2]&gt; predictor original\n#&gt;  4 adults                  &lt;chr [2]&gt; predictor original\n#&gt;  5 children                &lt;chr [2]&gt; predictor original\n#&gt;  6 babies                  &lt;chr [2]&gt; predictor original\n#&gt;  7 meal                    &lt;chr [3]&gt; predictor original\n#&gt;  8 country                 &lt;chr [3]&gt; predictor original\n#&gt;  9 market_segment          &lt;chr [3]&gt; predictor original\n#&gt; 10 distribution_channel    &lt;chr [3]&gt; predictor original\n#&gt; # â„¹ 17 more rows\n\nThe type column contains information on the variables"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#your-turn",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#your-turn",
    "title": "2 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\nWhat do you think are in the type vectors for the lead_time and country columns?\n\n\n\nâˆ’+\n02:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#create-indicator-variables",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#create-indicator-variables",
    "title": "2 - Feature Engineering",
    "section": "Create indicator variables ",
    "text": "Create indicator variables \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt; \n  step_dummy(all_nominal_predictors())\n\n\n\nFor any factor or character predictors, make binary indicators.\nThere are many recipe steps that can convert categorical predictors to numeric columns.\nstep_dummy() records the levels of the categorical predictors in the training set."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#filter-out-constant-columns",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#filter-out-constant-columns",
    "title": "2 - Feature Engineering",
    "section": "Filter out constant columns ",
    "text": "Filter out constant columns \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors())\n\n\nIn case there is a factor level that was never observed in the training data (resulting in a column of all 0s), we can delete any zero-variance predictors that have a single unique value.\n\nNote that the selector chooses all columns with a role of â€œpredictorâ€"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#normalization",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#normalization",
    "title": "2 - Feature Engineering",
    "section": "Normalization ",
    "text": "Normalization \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors())\n\n\n\nThis centers and scales the numeric predictors.\nThe recipe will use the training set to estimate the means and standard deviations of the data.\n\n\n\n\nAll data the recipe is applied to will be normalized using those statistics (there is no re-estimation)."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#reduce-correlation",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#reduce-correlation",
    "title": "2 - Feature Engineering",
    "section": "Reduce correlation ",
    "text": "Reduce correlation \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_corr(all_numeric_predictors(), threshold = 0.9)\n\n\nTo deal with highly correlated predictors, find the minimum set of predictor columns that make the pairwise correlations less than the threshold."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#other-possible-steps",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#other-possible-steps",
    "title": "2 - Feature Engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_pca(all_numeric_predictors())\n\n\nPCA feature extractionâ€¦"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#other-possible-steps-1",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#other-possible-steps-1",
    "title": "2 - Feature Engineering",
    "section": "Other possible steps  ",
    "text": "Other possible steps  \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  embed::step_umap(all_numeric_predictors(), outcome = vars(avg_price_per_room))\n\n\nA fancy machine learning supervised dimension reduction technique called UMAPâ€¦\n\nNote that this uses the outcome, and it is from an extension package"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#other-possible-steps-2",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#other-possible-steps-2",
    "title": "2 - Feature Engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_spline_natural(arrival_date_num, deg_free = 10)\n\n\nNonlinear transforms like natural splines, and so on!"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#your-turn-1",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#your-turn-1",
    "title": "2 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a recipe() for the hotel data to:\n\nuse a Yeo-Johnson (YJ) transformation on lead_time\nconvert factors to indicator variables\nremove zero-variance variables\nadd the spline technique shown above\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#minimal-recipe",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#minimal-recipe",
    "title": "2 - Feature Engineering",
    "section": "Minimal recipe ",
    "text": "Minimal recipe \n\nhotel_indicators &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt; \n  step_YeoJohnson(lead_time) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt; \n  step_spline_natural(arrival_date_num, deg_free = 10)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#measuring-performance",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#measuring-performance",
    "title": "2 - Feature Engineering",
    "section": "Measuring Performance ",
    "text": "Measuring Performance \nWeâ€™ll compute two measures: mean absolute error and the coefficient of determination (a.k.a \\(R^2\\)).\n\\[\\begin{align}\nMAE &= \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| \\notag \\\\\nR^2 &= cor(y_i, \\hat{y}_i)^2\n\\end{align}\\]\nThe focus will be on MAE for parameter optimization. Weâ€™ll use a metric set to compute these:\n\nreg_metrics &lt;- metric_set(mae, rsq)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#using-a-workflow",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#using-a-workflow",
    "title": "2 - Feature Engineering",
    "section": "Using a workflow    ",
    "text": "Using a workflow    \n\nset.seed(9)\n\nhotel_lm_wflow &lt;-\n  workflow() |&gt;\n  add_recipe(hotel_indicators) |&gt;\n  add_model(linear_reg())\n \nctrl &lt;- control_resamples(save_pred = TRUE)\nhotel_lm_res &lt;-\n  hotel_lm_wflow |&gt;\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\ncollect_metrics(hotel_lm_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   16.6      10 0.214   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.884    10 0.00339 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#your-turn-2",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#your-turn-2",
    "title": "2 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() to fit your workflow with a recipe.\nCollect the predictions from the results.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#holdout-predictions",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#holdout-predictions",
    "title": "2 - Feature Engineering",
    "section": "Holdout predictions    ",
    "text": "Holdout predictions    \n\n# Since we used `save_pred = TRUE`\nlm_cv_pred &lt;- collect_predictions(hotel_lm_res)\nlm_cv_pred |&gt; print(n = 7)\n#&gt; # A tibble: 3,749 Ã— 5\n#&gt;   .pred id      .row avg_price_per_room .config             \n#&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;              &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1  75.1 Fold01    20                 40 Preprocessor1_Model1\n#&gt; 2  49.3 Fold01    28                 54 Preprocessor1_Model1\n#&gt; 3  64.9 Fold01    45                 50 Preprocessor1_Model1\n#&gt; 4  52.8 Fold01    49                 42 Preprocessor1_Model1\n#&gt; 5  48.6 Fold01    61                 49 Preprocessor1_Model1\n#&gt; 6  29.8 Fold01    66                 40 Preprocessor1_Model1\n#&gt; 7  36.9 Fold01    88                 49 Preprocessor1_Model1\n#&gt; # â„¹ 3,742 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#calibration-plot",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#calibration-plot",
    "title": "2 - Feature Engineering",
    "section": "Calibration Plot ",
    "text": "Calibration Plot \n\nlibrary(probably)\n\ncal_plot_regression(hotel_lm_res)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#what-do-we-do-with-the-agent-and-company-data",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#what-do-we-do-with-the-agent-and-company-data",
    "title": "2 - Feature Engineering",
    "section": "What do we do with the agent and company data?",
    "text": "What do we do with the agent and company data?\nThere are 98 unique agent values and 100 unique companies in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables ğŸ˜³\nlump agents and companies that rarely occur into an â€œotherâ€ group\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the agent and company columns with the estimated effect of that predictor (in the extra materials)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#per-agent-statistics",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#per-agent-statistics",
    "title": "2 - Feature Engineering",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#collapsing-factor-levels",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#collapsing-factor-levels",
    "title": "2 - Feature Engineering",
    "section": "Collapsing factor levels ",
    "text": "Collapsing factor levels \nThere is a recipe step that will redefine factor levels based on their frequency in the training set:\n\nhotel_other_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt; \n  step_YeoJohnson(lead_time) |&gt;\n  step_other(agent, threshold = 0.001) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt; \n  step_spline_natural(arrival_date_num, deg_free = 10)\n\nUsing this code, 34 agents (out of 98) were collapsed into â€œotherâ€ based on the training set.\nWe could try to optimize the threshold for collapsing (see the next set of slides on model tuning)."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#does-othering-help",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#does-othering-help",
    "title": "2 - Feature Engineering",
    "section": "Does othering help?  ",
    "text": "Does othering help?  \n\nhotel_other_wflow &lt;-\n  hotel_lm_wflow |&gt;\n  update_recipe(hotel_other_rec)\n\nhotel_other_res &lt;-\n  hotel_other_wflow |&gt;\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\ncollect_metrics(hotel_other_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   16.7      10 0.213   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.884    10 0.00341 Preprocessor1_Model1\n\nAbout the same MAE and much faster to complete.\nNow letâ€™s look at a more sophisticated tool called effect feature hashing."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#feature-hashing",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#feature-hashing",
    "title": "2 - Feature Engineering",
    "section": "Feature Hashing",
    "text": "Feature Hashing\nBetween agent and company, simple dummy variables would create 198 new columns (that are mostly zeros).\nAnother option is to have a binary indicator that combines some levels of these variables.\nFeature hashing (for more see FES, SMLTAR, TMwR, and aml4td):\n\nuses the character values of the levels\nconverts them to integer hash values\nuses the integers to assign them to a specific indicator column."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#feature-hashing-1",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#feature-hashing-1",
    "title": "2 - Feature Engineering",
    "section": "Feature Hashing",
    "text": "Feature Hashing\nSuppose we want to use 32 indicator variables for agent.\nFor a agent with value â€œMax_Kuhnâ€, a hashing function converts it to an integer (say 210397726).\nTo assign it to one of the 32 columns, we would use modular arithmetic to assign it to a column:\n\n# For \"Max_Kuhn\" put a '1' in column: \n210397726 %% 32\n#&gt; [1] 30\n\nHash functions are meant to emulate randomness."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#feature-hashing-pros",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#feature-hashing-pros",
    "title": "2 - Feature Engineering",
    "section": "Feature Hashing Pros",
    "text": "Feature Hashing Pros\n\nThe procedure will automatically work on new values of the predictors.\nIt is fast.\nâ€œSignedâ€ hashes add a sign to help avoid aliasing."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#feature-hashing-cons",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#feature-hashing-cons",
    "title": "2 - Feature Engineering",
    "section": "Feature Hashing Cons",
    "text": "Feature Hashing Cons\n\nThere is no real logic behind which factor levels are combined.\nWe donâ€™t know how many columns to add (more in the next section).\nSome columns may have all zeros.\nIf a indicator column is important to the model, we canâ€™t easily determine why.\n\n\nThe signed hash make it slightly more possible to differentiate between confounded levels"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#feature-hashing-in-recipes",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#feature-hashing-in-recipes",
    "title": "2 - Feature Engineering",
    "section": "Feature Hashing in recipes   ",
    "text": "Feature Hashing in recipes   \nThe textrecipes package has a step that can be added to the recipe:\n\nlibrary(textrecipes)\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt;\n  step_YeoJohnson(lead_time) |&gt;\n  # Defaults to 32 signed indicator columns\n  step_dummy_hash(agent) |&gt;\n  step_dummy_hash(company) |&gt;\n  # Regular indicators for the others\n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_spline_natural(arrival_date_num, deg_free = 10)\n\nhotel_hash_wflow &lt;-\n  hotel_lm_wflow |&gt;\n  update_recipe(hash_rec)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#feature-hashing-in-recipes-1",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#feature-hashing-in-recipes-1",
    "title": "2 - Feature Engineering",
    "section": "Feature Hashing in recipes   ",
    "text": "Feature Hashing in recipes   \n\nhotel_hash_res &lt;-\n  hotel_hash_wflow |&gt;\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\ncollect_metrics(hotel_hash_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   16.7      10 0.239   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.884    10 0.00324 Preprocessor1_Model1\n\nAbout the same performance but now we can handle new values."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#debugging-a-recipe",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#debugging-a-recipe",
    "title": "2 - Feature Engineering",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g.Â hash_rec) can be estimated manually with a function called prep(). It is analogous to fit(). See TMwR section 16.4\n\n\n\n\nAnother function (bake()) is analogous to predict(), and gives you the processed data back.\n\n\n\n\nThe tidy() function can be used to get specific results from the recipe."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#example",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#example",
    "title": "2 - Feature Engineering",
    "section": "Example  ",
    "text": "Example  \n\nhash_rec_fit &lt;- prep(hash_rec)\n\n# Get the transformation coefficient\ntidy(hash_rec_fit, number = 1)\n\n# Get the processed data\nbake(hash_rec_fit, hotel_train |&gt; slice(1:3), contains(\"_agent_\"))"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#more-on-recipes",
    "href": "archive/2025-08-nyr/slides/advanced-02-feature-engineering.html#more-on-recipes",
    "title": "2 - Feature Engineering",
    "section": "More on recipes",
    "text": "More on recipes\n\nOnce fit() is called on a workflow, changing the model does not re-fit the recipe.\n\n\n\nA list of all known steps is at https://www.tidymodels.org/find/recipes/.\n\n\n\n\nSome steps can be skipped when using predict().\n\n\n\n\nThe order of the steps matters."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#previously---setup",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#previously---setup",
    "title": "4 - Grid Search via Racing",
    "section": "Previously - Setup ",
    "text": "Previously - Setup \n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates |&gt; \n  sample_n(5000) |&gt; \n  arrange(arrival_date) |&gt; \n  select(-arrival_date) |&gt; \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#previously---data-usage",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#previously---data-usage",
    "title": "4 - Grid Search via Racing",
    "section": "Previously - Data Usage ",
    "text": "Previously - Data Usage \n\nset.seed(4028)\nhotel_split &lt;-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#previously---boosting-model",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#previously---boosting-model",
    "title": "4 - Grid Search via Racing",
    "section": "Previously - Boosting Model    ",
    "text": "Previously - Boosting Model    \n\nhotel_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt;\n  step_YeoJohnson(lead_time) |&gt;\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) |&gt;\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) |&gt;\n  step_zv(all_predictors())\n\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune(), min_n = tune()) |&gt; \n  set_mode(\"regression\") |&gt; \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow &lt;- workflow(hotel_rec, lgbm_spec)\n\nlgbm_param &lt;-\n  lgbm_wflow |&gt;\n  extract_parameter_set_dials() |&gt;\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#first-a-shameless-promotion",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#first-a-shameless-promotion",
    "title": "4 - Grid Search via Racing",
    "section": "First, a shameless promotion",
    "text": "First, a shameless promotion"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#making-grid-search-more-efficient",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#making-grid-search-more-efficient",
    "title": "4 - Grid Search via Racing",
    "section": "Making Grid Search More Efficient",
    "text": "Making Grid Search More Efficient\nIn the last section, we evaluated 250 models (25 candidates times 10 resamples).\nWe can make this go faster using parallel processing.\nAlso, for some models, we can fit far fewer models than the number that are being evaluated.\n\nFor boosting, a model with X trees can often predict on candidates with less than X trees.\n\nBoth of these methods can lead to enormous speed-ups."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#model-racing",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#model-racing",
    "title": "4 - Grid Search via Racing",
    "section": "Model Racing",
    "text": "Model Racing\nRacing is an old tool that we can use to go even faster.\n\nEvaluate all of the candidate models but only for a few resamples.\nDetermine which candidates have a low probability of being selected.\nEliminate poor candidates.\nRepeat with next resample (until no more resamples remain)\n\nThis can result in fitting a small number of models."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#discarding-candidates",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#discarding-candidates",
    "title": "4 - Grid Search via Racing",
    "section": "Discarding Candidates",
    "text": "Discarding Candidates\nHow do we eliminate tuning parameter combinations?\nThere are a few methods to do so. Weâ€™ll use one based on analysis of variance (ANOVA).\nHoweverâ€¦ there is typically a large difference between resamples in the results."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#resampling-results-non-racing",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#resampling-results-non-racing",
    "title": "4 - Grid Search via Racing",
    "section": "Resampling Results (Non-Racing)",
    "text": "Resampling Results (Non-Racing)\n\n\nHere are some realistic (but simulated) examples of two candidate models.\nAn error estimate is measured for each of 10 resamples.\n\nThe lines connect resamples.\n\nThere is usually a significant resample-to-resample effect (rank corr: 0.83)."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#are-candidates-different",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#are-candidates-different",
    "title": "4 - Grid Search via Racing",
    "section": "Are Candidates Different?",
    "text": "Are Candidates Different?\nOne way to evaluate these models is to do a paired t-test\n\nor a t-test on their differences matched by resamples\n\nWith \\(n = 10\\) resamples, the confidence interval for the difference in RMSE is (0.99, 2.8), indicating that candidate number 2 has smaller error."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#evaluating-differences-in-candidates",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#evaluating-differences-in-candidates",
    "title": "4 - Grid Search via Racing",
    "section": "Evaluating Differences in Candidates",
    "text": "Evaluating Differences in Candidates\n\n\nWhat if we were to have compared the candidates while we seqeuntially evaluated each resample?\nğŸ‘‰\n\nOne candidate shows superiority when 4 resamples have been evaluated."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#interim-analysis-of-results",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#interim-analysis-of-results",
    "title": "4 - Grid Search via Racing",
    "section": "Interim Analysis of Results",
    "text": "Interim Analysis of Results\nOne version of racing uses a mixed model ANOVA to construct one-sided confidence intervals for each candidate versus the current best.\nAny candidates whose bound does not include zero are discarded. Here is an animation.\nThe resamples are analyzed in a random order (so set the seed).\n\nKuhn (2014) has examples and simulations to show that the method works.\nThe finetune package has functions tune_race_anova() and tune_race_win_loss()."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#racing",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#racing",
    "title": "4 - Grid Search via Racing",
    "section": "Racing     ",
    "text": "Racing     \n\n# Let's use a larger grid\nlgbm_grid &lt;- \n  lgbm_param |&gt; \n  grid_space_filling(size = 50)\n\nlibrary(finetune)\n\nset.seed(9)\nlgbm_race_res &lt;-\n  lgbm_wflow |&gt;\n  tune_race_anova(\n    resamples = hotel_rs,\n    grid = lgbm_grid, \n    metrics = reg_metrics\n  )\n\nThe syntax and helper functions are extremely similar to those shown for tune_grid()."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#racing-results",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#racing-results",
    "title": "4 - Grid Search via Racing",
    "section": "Racing Results ",
    "text": "Racing Results \n\nshow_best(lgbm_race_res, metric = \"mae\")\n#&gt; # A tibble: 2 Ã— 11\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1  1347     5     0.0655           66             26 mae     standard    9.64    10   0.173 Preprocessor34_Model1\n#&gt; 2   980     8     0.0429           17            135 mae     standard    9.76    10   0.164 Preprocessor25_Model1"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#racing-results-1",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#racing-results-1",
    "title": "4 - Grid Search via Racing",
    "section": "Racing Results ",
    "text": "Racing Results \n\n\nOnly 171 models were fit (out of 500).\nselect_best() never considers candidate models that did not get to the end of the race.\nThere is a helper function to see how candidate models were removed from consideration.\n\n\nplot_race(lgbm_race_res) + \n  scale_x_continuous(breaks = pretty_breaks())"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-04-racing.html#your-turn",
    "href": "archive/2025-08-nyr/slides/advanced-04-racing.html#your-turn",
    "title": "4 - Grid Search via Racing",
    "section": "Your turn",
    "text": "Your turn\n\nRun tune_race_anova() with a different seed.\nDid you get the same or similar results?\n\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-06-wrapping-up.html#your-turn",
    "href": "archive/2025-08-nyr/slides/advanced-06-wrapping-up.html#your-turn",
    "title": "6 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/advanced-06-wrapping-up.html#resources-to-keep-learning",
    "href": "archive/2025-08-nyr/slides/advanced-06-wrapping-up.html#resources-to-keep-learning",
    "title": "6 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://tidymodels.aml4td.org/\n\n\n\n\nhttps://smltar.com/\n\n\n\nFollow us on Mastodon and at the tidyverse blog for updates!"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-effect-encodings.html#previously---setup",
    "href": "archive/2025-08-nyr/slides/extras-effect-encodings.html#previously---setup",
    "title": "Extras - Effect Encodings",
    "section": "Previously - Setup",
    "text": "Previously - Setup\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates |&gt; \n  sample_n(5000) |&gt; \n  arrange(arrival_date) |&gt; \n  select(-arrival_date) |&gt; \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-effect-encodings.html#previously---data-usage",
    "href": "archive/2025-08-nyr/slides/extras-effect-encodings.html#previously---data-usage",
    "title": "Extras - Effect Encodings",
    "section": "Previously - Data Usage",
    "text": "Previously - Data Usage\n\nset.seed(4028)\nhotel_split &lt;-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-effect-encodings.html#what-do-we-do-with-the-agent-and-company-data",
    "href": "archive/2025-08-nyr/slides/extras-effect-encodings.html#what-do-we-do-with-the-agent-and-company-data",
    "title": "Extras - Effect Encodings",
    "section": "What do we do with the agent and company data?",
    "text": "What do we do with the agent and company data?\nThere are 98 unique agent values and 100 companies in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables ğŸ˜³\nlump agents and companies that rarely occur into an â€œotherâ€ group\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the agent and company columns with the estimated effect of that predictor"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-effect-encodings.html#per-agent-statistics",
    "href": "archive/2025-08-nyr/slides/extras-effect-encodings.html#per-agent-statistics",
    "title": "Extras - Effect Encodings",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-effect-encodings.html#what-is-an-effect-encoding",
    "href": "archive/2025-08-nyr/slides/extras-effect-encodings.html#what-is-an-effect-encoding",
    "title": "Extras - Effect Encodings",
    "section": "What is an effect encoding?",
    "text": "What is an effect encoding?\nWe replace the qualitativeâ€™s predictor data with their effect on the outcome.\n\n\nData before:\n\nbefore\n#&gt; # A tibble: 7 Ã— 3\n#&gt;   avg_price_per_room agent            .row\n#&gt;                &lt;dbl&gt; &lt;fct&gt;           &lt;int&gt;\n#&gt; 1               52.7 cynthia_worsley     1\n#&gt; 2               51.8 carlos_bryant       2\n#&gt; 3               53.8 lance_hitchcock     3\n#&gt; 4               51.8 lance_hitchcock     4\n#&gt; 5               46.8 cynthia_worsley     5\n#&gt; 6               54.7 charles_najera      6\n#&gt; 7               46.8 cynthia_worsley     7\n\n\nData after:\n\nafter\n#&gt; # A tibble: 7 Ã— 3\n#&gt;   avg_price_per_room agent  .row\n#&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1               52.7  88.5     1\n#&gt; 2               51.8  89.5     2\n#&gt; 3               53.8  79.8     3\n#&gt; 4               51.8  79.8     4\n#&gt; 5               46.8  88.5     5\n#&gt; 6               54.7 109.      6\n#&gt; 7               46.8  88.5     7\n\n\nThe agent column is replaced with an estimate of the ADR."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-effect-encodings.html#per-agent-statistics-again",
    "href": "archive/2025-08-nyr/slides/extras-effect-encodings.html#per-agent-statistics-again",
    "title": "Extras - Effect Encodings",
    "section": "Per-agent statistics again",
    "text": "Per-agent statistics again\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood statistical methods for estimating these means use partial pooling.\nPooling borrows strength across agents and shrinks extreme values towards the mean for agents with very few transations\nThe embed package has recipe steps for effect encodings.\n\n\n\nPartial pooling gives better estimates for agents with fewer reservations by shrinking the estimate to the overall ADR mean"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-effect-encodings.html#partial-pooling",
    "href": "archive/2025-08-nyr/slides/extras-effect-encodings.html#partial-pooling",
    "title": "Extras - Effect Encodings",
    "section": "Partial pooling",
    "text": "Partial pooling"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-effect-encodings.html#agent-effects",
    "href": "archive/2025-08-nyr/slides/extras-effect-encodings.html#agent-effects",
    "title": "Extras - Effect Encodings",
    "section": "Agent effects  ",
    "text": "Agent effects  \n\nlibrary(embed)\n\nhotel_effect_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) |&gt; \n  step_YeoJohnson(lead_time) |&gt;\n  step_lencode_mixed(agent, company, outcome = vars(avg_price_per_room)) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors())\n\n\nIt is very important to appropriately validate the effect encoding step to make sure that we are not overfitting."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-effect-encodings.html#effect-encoding-results",
    "href": "archive/2025-08-nyr/slides/extras-effect-encodings.html#effect-encoding-results",
    "title": "Extras - Effect Encodings",
    "section": "Effect encoding results    ",
    "text": "Effect encoding results    \n\nhotel_effect_wflow &lt;-\n  workflow() |&gt;\n  add_model(linear_reg()) |&gt; \n  update_recipe(hotel_effect_rec)\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\nhotel_effect_res &lt;-\n  hotel_effect_wflow |&gt;\n  fit_resamples(hotel_rs, metrics = reg_metrics)\n\ncollect_metrics(hotel_effect_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   17.8      10 0.189   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.870    10 0.00357 Preprocessor1_Model1\n\nSlightly worse but it can handle new agents (if they occur)."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-vetiver.html#deploying-a-model",
    "href": "archive/2025-08-nyr/slides/extras-vetiver.html#deploying-a-model",
    "title": "Extras - Model deployment",
    "section": "Deploying a model ",
    "text": "Deploying a model \nWe have a decision tree, tree_fit, to model whether or not a plot of land in Washington is forested or not.\nHow do we use our model in production?\n\nlibrary(vetiver)\nv &lt;- vetiver_model(tree_fit, \"forested\")\nv\n#&gt; \n#&gt; â”€â”€ forested â”€ &lt;bundled_workflow&gt; model for deployment \n#&gt; A rpart classification modeling workflow using 19 features\n\nLearn more at https://vetiver.rstudio.com"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-vetiver.html#deploy-your-model-1",
    "href": "archive/2025-08-nyr/slides/extras-vetiver.html#deploy-your-model-1",
    "title": "Extras - Model deployment",
    "section": "Deploy your model ",
    "text": "Deploy your model \nHow do we use our model in production?\n\nlibrary(plumber)\npr() |&gt;\n  vetiver_api(v)\n#&gt; # Plumber router with 4 endpoints, 4 filters, and 1 sub-router.\n#&gt; # Use `pr_run()` on this object to start the API.\n#&gt; â”œâ”€â”€[queryString]\n#&gt; â”œâ”€â”€[body]\n#&gt; â”œâ”€â”€[cookieParser]\n#&gt; â”œâ”€â”€[sharedSecret]\n#&gt; â”œâ”€â”€/logo\n#&gt; â”‚  â”‚ # Plumber static router serving from directory: /Users/emilhvitfeldt/Library/R/arm64/4.5/library/vetiver\n#&gt; â”œâ”€â”€/metadata (GET)\n#&gt; â”œâ”€â”€/ping (GET)\n#&gt; â”œâ”€â”€/predict (POST)\n#&gt; â””â”€â”€/prototype (GET)\n\nLearn more at https://vetiver.rstudio.com\n\nLive-code making a prediction"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/extras-vetiver.html#your-turn",
    "href": "archive/2025-08-nyr/slides/extras-vetiver.html#your-turn",
    "title": "Extras - Model deployment",
    "section": "Your turn",
    "text": "Your turn\n\nRun the vetiver chunk in your .qmd.\nCheck out the automated visual documentation.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-on-forests-in-georgia",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-on-forests-in-georgia",
    "title": "2 - Your data budget",
    "section": "Data on forests in Georgia",
    "text": "Data on forests in Georgia\n\n\n\nThe U.S. Forest Service maintains ML models to predict whether a plot of land is â€œforested.â€\nThis classification is important for all sorts of research, legislation, and land management purposes.\nPlots are typically remeasured every 10 years and this dataset contains the most recent measurement per plot.\nType ?forested_ga to learn more about this dataset, including references.\n\n\n\n\n\nCredit: https://www.svgrepo.com/svg/251793/forest-mountain"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-on-forests-in-georgia-1",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-on-forests-in-georgia-1",
    "title": "2 - Your data budget",
    "section": "Data on forests in Georgia",
    "text": "Data on forests in Georgia\n\n\n\nN = 10937 plots of land, one from each of 10937 6000-acre hexagons in Georgia.\nA nominal outcome, forested, with levels \"Yes\" and \"No\", measured â€œon-the-ground.â€\n18 remotely-sensed and easily-accessible predictors:\n\nnumeric variables based on weather and topography.\nnominal variables based on classifications from other governmental orgs.\n\n\n\n\n\n\nCredit: https://www.svgrepo.com/svg/67614/forest\n\n\n\nThose nominal variables are classifications similar to â€œforestedâ€ but from other agencies. e.g.Â land_type is from the European Space Agency, and is a remotely-sensed 3-class distribution based on predictions for how the land is used. (The data also includes a county variable, which is not itself from a model.)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#checklist-for-predictors",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#checklist-for-predictors",
    "title": "2 - Your data budget",
    "section": "Checklist for predictors",
    "text": "Checklist for predictors\n\nIs it ethical to use this variable? (Or even legal?)\nWill this variable be available at prediction time?\nDoes this variable contribute to explainability?\n\n\n\nre: ethics â€“ what issues might arise from releasing the true lat and lon? In reality, these lat and lon are slightly jittered to help ensure trust with landowners who allow surveyers to come take measurements."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-on-forests-in-georgia-2",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-on-forests-in-georgia-2",
    "title": "2 - Your data budget",
    "section": "Data on forests in Georgia",
    "text": "Data on forests in Georgia\n\nlibrary(tidymodels)\nlibrary(forested)\n\nforested_ga\n#&gt; # A tibble: 10,937 Ã— 19\n#&gt;    forested  year elevation eastness roughness tree_no_tree dew_temp\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt;\n#&gt;  1 Yes       2007        14        0         0 No tree          13.9\n#&gt;  2 Yes       2007        66      -53        10 Tree             13.8\n#&gt;  3 Yes       2006        59      -82         6 No tree          13.5\n#&gt;  4 Yes       2007       116      -78        20 Tree             12.3\n#&gt;  5 Yes       2006       283       63        13 Tree             10.0\n#&gt;  6 Yes       2007       250       63        14 Tree             10.8\n#&gt;  7 Yes       2007        58       31         1 No tree          13.8\n#&gt;  8 Yes       2023       140       56        11 Tree             12.2\n#&gt;  9 Yes       2024       118       72        17 Tree             12.2\n#&gt; 10 Yes       2024       217      -46        13 Tree             12.4\n#&gt; # â„¹ 10,927 more rows\n#&gt; # â„¹ 12 more variables: precip_annual &lt;dbl&gt;, temp_annual_mean &lt;dbl&gt;,\n#&gt; #   temp_annual_min &lt;dbl&gt;, temp_annual_max &lt;dbl&gt;, temp_january_min &lt;dbl&gt;,\n#&gt; #   vapor_min &lt;dbl&gt;, vapor_max &lt;dbl&gt;, canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;,\n#&gt; #   land_type &lt;fct&gt;, county &lt;fct&gt;"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-splitting-and-spending",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-splitting-and-spending",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nFor machine learning, we typically split data into training and test sets:\n\n\nThe training set is used to estimate model parameters.\nThe test set is used to find an independent assessment of model performance.\n\n\n\nDo not ğŸš« use the test set during training."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-splitting-and-spending-1",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-splitting-and-spending-1",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-splitting-and-spending-2",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-splitting-and-spending-2",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\n\nSpending too much data in training prevents us from computing a good assessment of predictive performance.\n\n\n\nSpending too much data in testing prevents us from computing a good estimate of model parameters."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#your-turn",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#your-turn",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nWhen is a good time to split your data?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#the-initial-split",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#the-initial-split",
    "title": "2 - Your data budget",
    "section": "The initial split ",
    "text": "The initial split \n\nset.seed(123)\nforested_split &lt;- initial_split(forested_ga)\nforested_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;8202/2735/10937&gt;\n\n\nHow much data in training vs testing? This function uses a good default, but this depends on your specific goal/data"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#what-is-set.seed",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#what-is-set.seed",
    "title": "2 - Your data budget",
    "section": "What is set.seed()?",
    "text": "What is set.seed()?\nTo create that split of the data, R generates â€œpseudo-randomâ€ numbers: while they are made to behave like random numbers, their generation is deterministic given a â€œseedâ€.\nThis allows us to reproduce results by setting that seed.\nWhich seed you pick doesnâ€™t matter, as long as you donâ€™t try a bunch of seeds and pick the one that gives you the best performance."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#accessing-the-data",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#accessing-the-data",
    "title": "2 - Your data budget",
    "section": "Accessing the data ",
    "text": "Accessing the data \n\nforested_train &lt;- training(forested_split)\nforested_test &lt;- testing(forested_split)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#the-training-set",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#the-training-set",
    "title": "2 - Your data budget",
    "section": "The training set",
    "text": "The training set\n\nforested_train\n#&gt; # A tibble: 8,202 Ã— 19\n#&gt;    forested  year elevation eastness roughness tree_no_tree dew_temp\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt;\n#&gt;  1 Yes       1997        66       82        10 Tree            12.2 \n#&gt;  2 No        1997       284      -99        58 Tree            10.3 \n#&gt;  3 Yes       2022       130       86        15 Tree            11.8 \n#&gt;  4 Yes       2021       202      -55         3 Tree            10.7 \n#&gt;  5 Yes       1995        75      -89         1 Tree            13.8 \n#&gt;  6 No        1995       110      -53         5 Tree            12.4 \n#&gt;  7 Yes       2022       111       73        12 Tree            11.5 \n#&gt;  8 Yes       1997       230       96        14 Tree             9.98\n#&gt;  9 Yes       2002       160      -88        13 Tree            11.1 \n#&gt; 10 Yes       2020        39        9         6 Tree            13.9 \n#&gt; # â„¹ 8,192 more rows\n#&gt; # â„¹ 12 more variables: precip_annual &lt;dbl&gt;, temp_annual_mean &lt;dbl&gt;,\n#&gt; #   temp_annual_min &lt;dbl&gt;, temp_annual_max &lt;dbl&gt;, temp_january_min &lt;dbl&gt;,\n#&gt; #   vapor_min &lt;dbl&gt;, vapor_max &lt;dbl&gt;, canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;,\n#&gt; #   land_type &lt;fct&gt;, county &lt;fct&gt;"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#the-test-set",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#the-test-set",
    "title": "2 - Your data budget",
    "section": "The test set ",
    "text": "The test set \nğŸ™ˆ\n\nThere are 2735 rows and 19 columns in the test set."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#your-turn-1",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#your-turn-1",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nSplit your data so 20% is held out for the test set.\nTry out different values in set.seed() to see how the results change.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-splitting-and-spending-3",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#data-splitting-and-spending-3",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nforested_split &lt;- initial_split(forested_ga, prop = 0.8)\nforested_train &lt;- training(forested_split)\nforested_test &lt;- testing(forested_split)\n\nnrow(forested_train)\n#&gt; [1] 8749\nnrow(forested_test)\n#&gt; [1] 2188"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#your-turn-2",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#your-turn-2",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nExplore the forested_train data on your own!\n\nWhatâ€™s the distribution of the outcome, forested?\nWhatâ€™s the distribution of numeric variables like precip_annual?\nHow does the distribution of forested differ across the categorical variables?\n\n\n\n\nâˆ’+\n08:00\n\n\n\n\nMake a plot or summary and then share with neighbor"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#section-1",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#section-1",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train |&gt; \n  ggplot(aes(x = forested)) +\n  geom_bar()"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#section-2",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#section-2",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train |&gt; \n  ggplot(aes(x = forested, fill = tree_no_tree)) +\n  geom_bar()"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#section-3",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#section-3",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train |&gt; \n  ggplot(aes(x = precip_annual, fill = forested, group = forested)) +\n  geom_histogram(position = \"identity\", alpha = .7)"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#section-4",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#section-4",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train |&gt; \n  ggplot(aes(x = precip_annual, fill = forested, group = forested)) +\n  geom_histogram(position = \"fill\")"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#section-5",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#section-5",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train |&gt; \n  ggplot(aes(x = lon, y = lat, col = forested)) +\n  geom_point()"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-02-data-budget.html#the-whole-game---status-update",
    "href": "archive/2025-08-nyr/slides/intro-02-data-budget.html#the-whole-game---status-update",
    "title": "2 - Your data budget",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#looking-at-predictions",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#looking-at-predictions",
    "title": "4 - Evaluating models",
    "section": "Looking at predictions",
    "text": "Looking at predictions\n\naugment(forested_fit, new_data = forested_train)\n#&gt; # A tibble: 8,749 Ã— 22\n#&gt;    .pred_class .pred_Yes .pred_No forested  year elevation eastness roughness\n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 Yes             0.931   0.0690 Yes       1997        66       82        10\n#&gt;  2 Yes             0.983   0.0172 No        1997       284      -99        58\n#&gt;  3 Yes             0.960   0.0401 Yes       2022       130       86        15\n#&gt;  4 Yes             0.870   0.130  Yes       2021       202      -55         3\n#&gt;  5 Yes             0.823   0.177  Yes       1995        75      -89         1\n#&gt;  6 Yes             0.758   0.242  No        1995       110      -53         5\n#&gt;  7 Yes             0.823   0.177  Yes       2022       111       73        12\n#&gt;  8 No              0.467   0.533  Yes       1997       230       96        14\n#&gt;  9 Yes             0.983   0.0172 Yes       2002       160      -88        13\n#&gt; 10 Yes             0.871   0.129  Yes       2020        39        9         6\n#&gt; # â„¹ 8,739 more rows\n#&gt; # â„¹ 14 more variables: tree_no_tree &lt;fct&gt;, dew_temp &lt;dbl&gt;, precip_annual &lt;dbl&gt;,\n#&gt; #   temp_annual_mean &lt;dbl&gt;, temp_annual_min &lt;dbl&gt;, temp_annual_max &lt;dbl&gt;,\n#&gt; #   temp_january_min &lt;dbl&gt;, vapor_min &lt;dbl&gt;, vapor_max &lt;dbl&gt;,\n#&gt; #   canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, land_type &lt;fct&gt;, county &lt;fct&gt;"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#confusion-matrix",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#confusion-matrix",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#confusion-matrix-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#confusion-matrix-1",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix \n\naugment(forested_fit, new_data = forested_train) |&gt;\n  conf_mat(truth = forested, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction  Yes   No\n#&gt;        Yes 5884  869\n#&gt;        No   438 1558"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#confusion-matrix-2",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#confusion-matrix-2",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix \n\naugment(forested_fit, new_data = forested_train) |&gt;\n  conf_mat(truth = forested, estimate = .pred_class) |&gt;\n  autoplot(type = \"heatmap\")"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#metrics-for-model-performance",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#metrics-for-model-performance",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  accuracy(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.851"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-accuracy",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-accuracy",
    "title": "4 - Evaluating models",
    "section": "Dangers of accuracy ",
    "text": "Dangers of accuracy \nWe need to be careful of using accuracy() since it can give â€œgoodâ€ performance by only predicting one way with imbalanced data:\n\naugment(forested_fit, new_data = forested_train) %&gt;%\n  mutate(.pred_class = factor(\"Yes\", levels = c(\"Yes\", \"No\"))) %&gt;%\n  accuracy(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.723"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#metrics-for-model-performance-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#metrics-for-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  sensitivity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.931"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#metrics-for-model-performance-2",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#metrics-for-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  sensitivity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.931\n\n\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  specificity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 specificity binary         0.642"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#metrics-for-model-performance-3",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#metrics-for-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \nWe can use metric_set() to combine multiple calculations into one\n\nforested_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  forested_metrics(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy    binary         0.851\n#&gt; 2 specificity binary         0.642\n#&gt; 3 sensitivity binary         0.931"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#metrics-for-model-performance-4",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#metrics-for-model-performance-4",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \nMetrics and metric sets work with grouped data frames!\n\naugment(forested_fit, new_data = forested_train) |&gt;\n  group_by(tree_no_tree) |&gt;\n  accuracy(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   tree_no_tree .metric  .estimator .estimate\n#&gt;   &lt;fct&gt;        &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 Tree         accuracy binary         0.875\n#&gt; 2 No tree      accuracy binary         0.807"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#your-turn",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#your-turn",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nApply the forested_metrics metric set to augment()\noutput grouped by tree_no_tree.\nDo any metrics differ substantially between groups?\n\n\n\nâˆ’+\n05:00\n\n\n\n\nThe specificity for \"Tree\" is a good bit lower than it is for \"No tree\".\nSpecificity is the proportion of negatives that are correctly identified as negatives. â€œNegativeâ€ is the non-event level of the outcome, i.e.Â â€œnon-forested.â€ So, when this index classifies the plot as having a tree, the model does not do well at correctly identifying the plot as non-forested when it is indeed non-forested."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#two-class-data",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#two-class-data",
    "title": "4 - Evaluating models",
    "section": "Two class data",
    "text": "Two class data\nThese metrics assume that we know the threshold for converting â€œsoftâ€ probability predictions into â€œhardâ€ class predictions.\n\nIs a 50% threshold good?\nWhat happens if we say that we need to be 80% sure to declare an event?\n\nsensitivity â¬‡ï¸, specificity â¬†ï¸\n\n\n\nWhat happens for a 20% threshold?\n\nsensitivity â¬†ï¸, specificity â¬‡ï¸"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#varying-the-threshold",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#varying-the-threshold",
    "title": "4 - Evaluating models",
    "section": "Varying the threshold",
    "text": "Varying the threshold"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#roc-curves",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#roc-curves",
    "title": "4 - Evaluating models",
    "section": "ROC curves",
    "text": "ROC curves\n\n\nFor an ROC (receiver operator characteristic) curve, we plot\n\nthe false positive rate (1 - specificity) on the x-axis\nthe true positive rate (sensitivity) on the y-axis\n\nwith sensitivity and specificity calculated at all possible thresholds.\n\n\n\n\n\n\n\n\n\n\n\n\nROC curves are insensitive to class imbalance."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#roc-curves-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#roc-curves-1",
    "title": "4 - Evaluating models",
    "section": "ROC curves",
    "text": "ROC curves\n\n\nWe can use the area under the ROC curve as a classification metric:\n\nROC AUC = 1 ğŸ’¯\nROC AUC = 1/2 ğŸ˜¢"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#roc-curves-2",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#roc-curves-2",
    "title": "4 - Evaluating models",
    "section": "ROC curves ",
    "text": "ROC curves \n\n# Assumes _first_ factor level is event; there are options to change that\naugment(forested_fit, new_data = forested_train) |&gt; \n  roc_curve(truth = forested, .pred_Yes) |&gt;\n  slice(1, 20, 50)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .threshold specificity sensitivity\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1   -Inf           0           1    \n#&gt; 2      0.143       0.267       0.990\n#&gt; 3      0.385       0.571       0.951\n\naugment(forested_fit, new_data = forested_train) |&gt; \n  roc_auc(truth = forested, .pred_Yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.881"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#roc-curve-plot",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#roc-curve-plot",
    "title": "4 - Evaluating models",
    "section": "ROC curve plot ",
    "text": "ROC curve plot \n\n\naugment(forested_fit, \n        new_data = forested_train) |&gt; \n  roc_curve(truth = forested, \n            .pred_Yes) |&gt;\n  autoplot()"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#your-turn-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#your-turn-1",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCompute and plot an ROC curve for your current model.\nWhat data are being used for this ROC curve plot?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#brier-score",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#brier-score",
    "title": "4 - Evaluating models",
    "section": "Brier score",
    "text": "Brier score\nWhat if we donâ€™t turn predicted probabilities into class predictions?\n\nThe Brier score is analogous to the mean squared error in regression models:\n\\[\nBrier_{class} = \\frac{1}{N}\\sum_{i=1}^N\\sum_{k=1}^C (y_{ik} - \\hat{p}_{ik})^2\n\\]"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#brier-score-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#brier-score-1",
    "title": "4 - Evaluating models",
    "section": "Brier score",
    "text": "Brier score\n\naugment(forested_fit, new_data = forested_train) |&gt; \n  brier_class(truth = forested, .pred_Yes) \n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary         0.113\n\n\nSmaller values are better, for binary classification the â€œbad model thresholdâ€ is about 0.25."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#separation-vs-calibration",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#separation-vs-calibration",
    "title": "4 - Evaluating models",
    "section": "Separation vs calibration",
    "text": "Separation vs calibration\n\n\nThe ROC captures separation.\n\n\n\n\n\n\n\n\n\n\nThe Brier score captures calibration.\n\n\n\n\n\n\n\n\n\n\n\n\nGood separation: the densities donâ€™t overlap.\nGood calibration: the calibration line follows the diagonal.\n\nCalibration plot: We bin observations according to predicted probability. In the bin for 20%-30% predicted prob, we should see an event rate of ~25% if the model is well-calibrated."
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-1",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting",
    "text": "Dangers of overfitting"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-2",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-2",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸",
    "text": "Dangers of overfitting âš ï¸"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-3",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-3",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\nforested_fit |&gt;\n  augment(forested_train)\n#&gt; # A tibble: 8,749 Ã— 22\n#&gt;    .pred_class .pred_Yes .pred_No forested  year elevation eastness roughness\n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 Yes             0.931   0.0690 Yes       1997        66       82        10\n#&gt;  2 Yes             0.983   0.0172 No        1997       284      -99        58\n#&gt;  3 Yes             0.960   0.0401 Yes       2022       130       86        15\n#&gt;  4 Yes             0.870   0.130  Yes       2021       202      -55         3\n#&gt;  5 Yes             0.823   0.177  Yes       1995        75      -89         1\n#&gt;  6 Yes             0.758   0.242  No        1995       110      -53         5\n#&gt;  7 Yes             0.823   0.177  Yes       2022       111       73        12\n#&gt;  8 No              0.467   0.533  Yes       1997       230       96        14\n#&gt;  9 Yes             0.983   0.0172 Yes       2002       160      -88        13\n#&gt; 10 Yes             0.871   0.129  Yes       2020        39        9         6\n#&gt; # â„¹ 8,739 more rows\n#&gt; # â„¹ 14 more variables: tree_no_tree &lt;fct&gt;, dew_temp &lt;dbl&gt;, precip_annual &lt;dbl&gt;,\n#&gt; #   temp_annual_mean &lt;dbl&gt;, temp_annual_min &lt;dbl&gt;, temp_annual_max &lt;dbl&gt;,\n#&gt; #   temp_january_min &lt;dbl&gt;, vapor_min &lt;dbl&gt;, vapor_max &lt;dbl&gt;,\n#&gt; #   canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, land_type &lt;fct&gt;, county &lt;fct&gt;\n\nWe call this â€œresubstitutionâ€ or â€œrepredicting the training setâ€"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-4",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-4",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\nforested_fit |&gt;\n  augment(forested_train) |&gt;\n  accuracy(forested, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.851\n\nWe call this a â€œresubstitution estimateâ€"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-5",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-5",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\nforested_fit |&gt;\n  augment(forested_train) |&gt;\n  accuracy(forested, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.851"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-6",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-6",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\nforested_fit |&gt;\n  augment(forested_train) |&gt;\n  accuracy(forested, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.851\n\n\n\nforested_fit |&gt;\n  augment(forested_test) |&gt;\n  accuracy(forested, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.711\n\n\n\nâš ï¸ Remember that weâ€™re demonstrating overfitting\n\n\nâš ï¸ Donâ€™t use the test set until the end of your modeling analysis"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#your-turn-2",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#your-turn-2",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse augment() and a metric function to compute a classification metric like brier_class().\nCompute the metrics for both training and testing data to demonstrate overfitting!\nNotice the evidence of overfitting! âš ï¸\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-7",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#dangers-of-overfitting-7",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\nforested_fit |&gt;\n  augment(forested_train) |&gt;\n  brier_class(forested, .pred_Yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary         0.113\n\n\n\nforested_fit |&gt;\n  augment(forested_test) |&gt;\n  brier_class(forested, .pred_Yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary         0.208\n\n\n\nWhat if we want to compare more models?\n\n\nAnd/or more model configurations?\n\n\nAnd we want to understand if these are important differences?"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#cross-validation",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#cross-validation",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#cross-validation-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#cross-validation-1",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#your-turn-3",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#your-turn-3",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nIf we use 10 folds, what percent of the training data\n\nends up in analysis\nends up in assessment\n\nfor each fold?\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#cross-validation-2",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#cross-validation-2",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(forested_train) # v = 10 is default\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [7874/875]&gt; Fold01\n#&gt;  2 &lt;split [7874/875]&gt; Fold02\n#&gt;  3 &lt;split [7874/875]&gt; Fold03\n#&gt;  4 &lt;split [7874/875]&gt; Fold04\n#&gt;  5 &lt;split [7874/875]&gt; Fold05\n#&gt;  6 &lt;split [7874/875]&gt; Fold06\n#&gt;  7 &lt;split [7874/875]&gt; Fold07\n#&gt;  8 &lt;split [7874/875]&gt; Fold08\n#&gt;  9 &lt;split [7874/875]&gt; Fold09\n#&gt; 10 &lt;split [7875/874]&gt; Fold10"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#cross-validation-3",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#cross-validation-3",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWhat is in this?\n\nforested_folds &lt;- vfold_cv(forested_train)\nforested_folds$splits[1:3]\n#&gt; [[1]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;7874/875/8749&gt;\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;7874/875/8749&gt;\n#&gt; \n#&gt; [[3]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;7874/875/8749&gt;\n\n\nTalk about a list column, storing non-atomic types in dataframe"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#cross-validation-4",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#cross-validation-4",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(forested_train, v = 5)\n#&gt; #  5-fold cross-validation \n#&gt; # A tibble: 5 Ã— 2\n#&gt;   splits              id   \n#&gt;   &lt;list&gt;              &lt;chr&gt;\n#&gt; 1 &lt;split [6999/1750]&gt; Fold1\n#&gt; 2 &lt;split [6999/1750]&gt; Fold2\n#&gt; 3 &lt;split [6999/1750]&gt; Fold3\n#&gt; 4 &lt;split [6999/1750]&gt; Fold4\n#&gt; 5 &lt;split [7000/1749]&gt; Fold5"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#cross-validation-5",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#cross-validation-5",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWeâ€™ll use this setup:\n\nset.seed(123)\nforested_folds &lt;- vfold_cv(forested_train, v = 10)\nforested_folds\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [7874/875]&gt; Fold01\n#&gt;  2 &lt;split [7874/875]&gt; Fold02\n#&gt;  3 &lt;split [7874/875]&gt; Fold03\n#&gt;  4 &lt;split [7874/875]&gt; Fold04\n#&gt;  5 &lt;split [7874/875]&gt; Fold05\n#&gt;  6 &lt;split [7874/875]&gt; Fold06\n#&gt;  7 &lt;split [7874/875]&gt; Fold07\n#&gt;  8 &lt;split [7874/875]&gt; Fold08\n#&gt;  9 &lt;split [7874/875]&gt; Fold09\n#&gt; 10 &lt;split [7875/874]&gt; Fold10\n\n\nSet the seed when creating resamples"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#fit-our-model-to-the-resamples",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#fit-our-model-to-the-resamples",
    "title": "4 - Evaluating models",
    "section": "Fit our model to the resamples",
    "text": "Fit our model to the resamples\n\nforested_res &lt;- fit_resamples(forested_wflow, forested_folds)\nforested_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 4\n#&gt;    splits             id     .metrics         .notes          \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n#&gt;  1 &lt;split [7874/875]&gt; Fold01 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  2 &lt;split [7874/875]&gt; Fold02 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  3 &lt;split [7874/875]&gt; Fold03 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  4 &lt;split [7874/875]&gt; Fold04 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  5 &lt;split [7874/875]&gt; Fold05 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  6 &lt;split [7874/875]&gt; Fold06 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  7 &lt;split [7874/875]&gt; Fold07 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  8 &lt;split [7874/875]&gt; Fold08 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  9 &lt;split [7874/875]&gt; Fold09 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt; 10 &lt;split [7875/874]&gt; Fold10 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#evaluating-model-performance",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#evaluating-model-performance",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nforested_res |&gt;\n  collect_metrics()\n#&gt; # A tibble: 3 Ã— 6\n#&gt;   .metric     .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.704    10 0.00653 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.214    10 0.00349 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.692    10 0.00496 Preprocessor1_Model1\n\n\ncollect_metrics() is one of a suite of collect_*() functions that can be used to work with columns of tuning results. Most columns in a tuning result prefixed with . have a corresponding collect_*() function with options for common summaries.\n\n\nWe can reliably measure performance using only the training data ğŸ‰"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#comparing-metrics",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#comparing-metrics",
    "title": "4 - Evaluating models",
    "section": "Comparing metrics ",
    "text": "Comparing metrics \nHow do the metrics from resampling compare to the metrics from training and testing?\n\n\n\nforested_res |&gt;\n  collect_metrics() |&gt; \n  select(.metric, mean, n)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric      mean     n\n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 accuracy    0.704    10\n#&gt; 2 brier_class 0.214    10\n#&gt; 3 roc_auc     0.692    10\n\n\nThe ROC AUC previously was\n\n0.88 for the training set\n0.7 for test set\n\n\n\nRemember that:\nâš ï¸ the training set gives you overly optimistic metrics\nâš ï¸ the test set is precious"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#evaluating-model-performance-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#evaluating-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nctrl_forested &lt;- control_resamples(save_pred = TRUE)\nforested_res &lt;- fit_resamples(forested_wflow, forested_folds, control = ctrl_forested)\n\nforested_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [7874/875]&gt; Fold01 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [7874/875]&gt; Fold02 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [7874/875]&gt; Fold03 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [7874/875]&gt; Fold04 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [7874/875]&gt; Fold05 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [7874/875]&gt; Fold06 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [7874/875]&gt; Fold07 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [7874/875]&gt; Fold08 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [7874/875]&gt; Fold09 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [7875/874]&gt; Fold10 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#evaluating-model-performance-2",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#evaluating-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nforested_preds &lt;- collect_predictions(forested_res)\nforested_preds\n#&gt; # A tibble: 8,749 Ã— 7\n#&gt;    .pred_class .pred_Yes .pred_No id      .row forested .config             \n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt; &lt;fct&gt;    &lt;chr&gt;               \n#&gt;  1 Yes             0.953   0.0473 Fold01     2 No       Preprocessor1_Model1\n#&gt;  2 Yes             0.6     0.4    Fold01     6 No       Preprocessor1_Model1\n#&gt;  3 Yes             0.848   0.152  Fold01     7 Yes      Preprocessor1_Model1\n#&gt;  4 Yes             0.941   0.0588 Fold01    36 Yes      Preprocessor1_Model1\n#&gt;  5 Yes             0.895   0.105  Fold01    38 No       Preprocessor1_Model1\n#&gt;  6 Yes             1       0      Fold01    58 Yes      Preprocessor1_Model1\n#&gt;  7 No              0.187   0.812  Fold01    69 No       Preprocessor1_Model1\n#&gt;  8 Yes             0.905   0.0952 Fold01    71 Yes      Preprocessor1_Model1\n#&gt;  9 Yes             0.907   0.0930 Fold01    74 Yes      Preprocessor1_Model1\n#&gt; 10 Yes             0.904   0.0962 Fold01    80 Yes      Preprocessor1_Model1\n#&gt; # â„¹ 8,739 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#evaluating-model-performance-3",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#evaluating-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nforested_preds |&gt; \n  group_by(id) |&gt;\n  forested_metrics(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 30 Ã— 4\n#&gt;    id     .metric  .estimator .estimate\n#&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt;  1 Fold01 accuracy binary         0.729\n#&gt;  2 Fold02 accuracy binary         0.685\n#&gt;  3 Fold03 accuracy binary         0.687\n#&gt;  4 Fold04 accuracy binary         0.711\n#&gt;  5 Fold05 accuracy binary         0.736\n#&gt;  6 Fold06 accuracy binary         0.670\n#&gt;  7 Fold07 accuracy binary         0.699\n#&gt;  8 Fold08 accuracy binary         0.710\n#&gt;  9 Fold09 accuracy binary         0.699\n#&gt; 10 Fold10 accuracy binary         0.719\n#&gt; # â„¹ 20 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#where-are-the-fitted-models",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#where-are-the-fitted-models",
    "title": "4 - Evaluating models",
    "section": "Where are the fitted models? ",
    "text": "Where are the fitted models? \n\nforested_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [7874/875]&gt; Fold01 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [7874/875]&gt; Fold02 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [7874/875]&gt; Fold03 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [7874/875]&gt; Fold04 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [7874/875]&gt; Fold05 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [7874/875]&gt; Fold06 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [7874/875]&gt; Fold07 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [7874/875]&gt; Fold08 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [7874/875]&gt; Fold09 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [7875/874]&gt; Fold10 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;\n\n\nğŸ—‘ï¸"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#bootstrapping",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#bootstrapping",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#bootstrapping-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#bootstrapping-1",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(3214)\nbootstraps(forested_train)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 25 Ã— 2\n#&gt;    splits              id         \n#&gt;    &lt;list&gt;              &lt;chr&gt;      \n#&gt;  1 &lt;split [8749/3218]&gt; Bootstrap01\n#&gt;  2 &lt;split [8749/3264]&gt; Bootstrap02\n#&gt;  3 &lt;split [8749/3220]&gt; Bootstrap03\n#&gt;  4 &lt;split [8749/3208]&gt; Bootstrap04\n#&gt;  5 &lt;split [8749/3230]&gt; Bootstrap05\n#&gt;  6 &lt;split [8749/3197]&gt; Bootstrap06\n#&gt;  7 &lt;split [8749/3193]&gt; Bootstrap07\n#&gt;  8 &lt;split [8749/3226]&gt; Bootstrap08\n#&gt;  9 &lt;split [8749/3243]&gt; Bootstrap09\n#&gt; 10 &lt;split [8749/3233]&gt; Bootstrap10\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#the-whole-game---status-update",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#the-whole-game---status-update",
    "title": "4 - Evaluating models",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#your-turn-4",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#your-turn-4",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCreate:\n\nMonte Carlo Cross-Validation sets\nvalidation set\n\n(use the reference guide to find the functions)\nDonâ€™t forget to set a seed when you resample!\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#monte-carlo-cross-validation",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#monte-carlo-cross-validation",
    "title": "4 - Evaluating models",
    "section": "Monte Carlo Cross-Validation ",
    "text": "Monte Carlo Cross-Validation \n\nset.seed(322)\nmc_cv(forested_train, times = 10)\n#&gt; # Monte Carlo cross-validation (0.75/0.25) with 10 resamples \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits              id        \n#&gt;    &lt;list&gt;              &lt;chr&gt;     \n#&gt;  1 &lt;split [6561/2188]&gt; Resample01\n#&gt;  2 &lt;split [6561/2188]&gt; Resample02\n#&gt;  3 &lt;split [6561/2188]&gt; Resample03\n#&gt;  4 &lt;split [6561/2188]&gt; Resample04\n#&gt;  5 &lt;split [6561/2188]&gt; Resample05\n#&gt;  6 &lt;split [6561/2188]&gt; Resample06\n#&gt;  7 &lt;split [6561/2188]&gt; Resample07\n#&gt;  8 &lt;split [6561/2188]&gt; Resample08\n#&gt;  9 &lt;split [6561/2188]&gt; Resample09\n#&gt; 10 &lt;split [6561/2188]&gt; Resample10"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#validation-set",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#validation-set",
    "title": "4 - Evaluating models",
    "section": "Validation set ",
    "text": "Validation set \n\nset.seed(853)\nforested_val_split &lt;- initial_validation_split(forested)\nvalidation_set(forested_val_split)\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   splits              id        \n#&gt;   &lt;list&gt;              &lt;chr&gt;     \n#&gt; 1 &lt;split [4264/1421]&gt; validation\n\n\nA validation set is just another type of resample"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#random-forest-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#random-forest-1",
    "title": "4 - Evaluating models",
    "section": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ",
    "text": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\n\nEnsemble many decision tree models\nAll the trees vote! ğŸ—³ï¸\nBootstrap aggregating + random predictor sampling\n\n\n\nOften works well without tuning hyperparameters (more on this later!), as long as there are enough trees"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#create-a-random-forest-model",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#create-a-random-forest-model",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_spec &lt;- rand_forest(trees = 1000, mode = \"classification\")\nrf_spec\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#create-a-random-forest-model-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#create-a-random-forest-model-1",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_wflow &lt;- workflow(forested ~ ., rf_spec)\nrf_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#your-turn-5",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#your-turn-5",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() and rf_wflow to:\n\nkeep predictions\ncompute metrics\n\n\n\n\nâˆ’+\n08:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#evaluating-model-performance-4",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#evaluating-model-performance-4",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nctrl_forested &lt;- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\n\nset.seed(2)\nrf_res &lt;- fit_resamples(rf_wflow, forested_folds, control = ctrl_forested)\ncollect_metrics(rf_res)\n#&gt; # A tibble: 3 Ã— 6\n#&gt;   .metric     .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.752    10 0.00469 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.167    10 0.00317 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.757    10 0.0100  Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#the-whole-game---status-update-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#the-whole-game---status-update-1",
    "title": "4 - Evaluating models",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#the-final-fit",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#the-final-fit",
    "title": "4 - Evaluating models",
    "section": "The final fit ",
    "text": "The final fit \nSuppose that we are happy with our random forest model.\nLetâ€™s fit the model on the training set and verify our performance using the test set.\n\nWeâ€™ve shown you fit() and predict() (+ augment()) but there is a shortcut:\n\n# forested_split has train + test info\nfinal_fit &lt;- last_fit(rf_wflow, forested_split) \n\nfinal_fit\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits              id               .metrics .notes   .predictions .workflow \n#&gt;   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n#&gt; 1 &lt;split [8749/2188]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#what-is-in-final_fit",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#what-is-in-final_fit",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_metrics(final_fit)\n#&gt; # A tibble: 3 Ã— 4\n#&gt;   .metric     .estimator .estimate .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary         0.761 Preprocessor1_Model1\n#&gt; 2 roc_auc     binary         0.761 Preprocessor1_Model1\n#&gt; 3 brier_class binary         0.162 Preprocessor1_Model1\n\n\nThese are metrics computed with the test set"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#what-is-in-final_fit-1",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#what-is-in-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_predictions(final_fit)\n#&gt; # A tibble: 2,188 Ã— 7\n#&gt;    .pred_class .pred_Yes .pred_No id                .row forested .config       \n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;fct&gt;    &lt;chr&gt;         \n#&gt;  1 Yes             0.964   0.0363 train/test split     4 Yes      Preprocessor1â€¦\n#&gt;  2 Yes             0.793   0.207  train/test split     8 Yes      Preprocessor1â€¦\n#&gt;  3 Yes             0.815   0.185  train/test split    10 Yes      Preprocessor1â€¦\n#&gt;  4 Yes             0.852   0.148  train/test split    19 Yes      Preprocessor1â€¦\n#&gt;  5 Yes             0.838   0.162  train/test split    23 Yes      Preprocessor1â€¦\n#&gt;  6 Yes             0.552   0.448  train/test split    28 No       Preprocessor1â€¦\n#&gt;  7 Yes             0.831   0.169  train/test split    34 Yes      Preprocessor1â€¦\n#&gt;  8 Yes             0.614   0.386  train/test split    35 No       Preprocessor1â€¦\n#&gt;  9 Yes             0.975   0.0251 train/test split    38 Yes      Preprocessor1â€¦\n#&gt; 10 Yes             0.944   0.0561 train/test split    40 Yes      Preprocessor1â€¦\n#&gt; # â„¹ 2,178 more rows"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#what-is-in-final_fit-2",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#what-is-in-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\nextract_workflow(final_fit)\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  1000 \n#&gt; Sample size:                      8749 \n#&gt; Number of independent variables:  18 \n#&gt; Mtry:                             4 \n#&gt; Target node size:                 10 \n#&gt; Variable importance mode:         none \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.1669661\n\n\nUse this for prediction on new data, like for deploying"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#the-whole-game",
    "href": "archive/2025-08-nyr/slides/intro-04-evaluating-models.html#the-whole-game",
    "title": "4 - Evaluating models",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-06-wrapping-up.html#your-turn",
    "href": "archive/2025-08-nyr/slides/intro-06-wrapping-up.html#your-turn",
    "title": "6 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-06-wrapping-up.html#resources-to-keep-learning",
    "href": "archive/2025-08-nyr/slides/intro-06-wrapping-up.html#resources-to-keep-learning",
    "title": "6 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://tidymodels.aml4td.org/\n\n\n\n\nhttps://smltar.com/\n\n\n\nFollow us on Mastodon and at the tidyverse blog for updates!"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-workflowsets.html#how-can-we-compare-multiple-model-workflows-at-once",
    "href": "archive/2025-08-nyr/slides/intro-extra-workflowsets.html#how-can-we-compare-multiple-model-workflows-at-once",
    "title": "Extras - workflowsets",
    "section": "How can we compare multiple model workflows at once?",
    "text": "How can we compare multiple model workflows at once?"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-workflowsets.html#evaluate-a-workflow-set",
    "href": "archive/2025-08-nyr/slides/intro-extra-workflowsets.html#evaluate-a-workflow-set",
    "title": "Extras - workflowsets",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nwf_set &lt;- workflow_set(list(forested ~ .), list(tree_spec, rf_spec))\nwf_set\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result    \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-workflowsets.html#evaluate-a-workflow-set-1",
    "href": "archive/2025-08-nyr/slides/intro-extra-workflowsets.html#evaluate-a-workflow-set-1",
    "title": "Extras - workflowsets",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nwf_set_fit &lt;- wf_set |&gt;\n  workflow_map(\"fit_resamples\", resamples = forested_folds)\nwf_set_fit\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result   \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-workflowsets.html#evaluate-a-workflow-set-2",
    "href": "archive/2025-08-nyr/slides/intro-extra-workflowsets.html#evaluate-a-workflow-set-2",
    "title": "Extras - workflowsets",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nwf_set_fit |&gt;\n  rank_results()\n#&gt; # A tibble: 6 Ã— 9\n#&gt;   wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n#&gt;   &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n#&gt; 1 formula_rand_forâ€¦ Preproâ€¦ accuraâ€¦ 0.753 0.00435    10 formula      randâ€¦     1\n#&gt; 2 formula_rand_forâ€¦ Preproâ€¦ brier_â€¦ 0.167 0.00316    10 formula      randâ€¦     1\n#&gt; 3 formula_rand_forâ€¦ Preproâ€¦ roc_auc 0.757 0.00992    10 formula      randâ€¦     1\n#&gt; 4 formula_decisionâ€¦ Preproâ€¦ accuraâ€¦ 0.704 0.00653    10 formula      deciâ€¦     2\n#&gt; 5 formula_decisionâ€¦ Preproâ€¦ brier_â€¦ 0.214 0.00349    10 formula      deciâ€¦     2\n#&gt; 6 formula_decisionâ€¦ Preproâ€¦ roc_auc 0.692 0.00496    10 formula      deciâ€¦     2\n\nThe first metric of the metric set is used for ranking. Use rank_metric to change that.\n\nLots more available with workflow sets, like collect_metrics(), autoplot() methods, and more!"
  },
  {
    "objectID": "archive/2025-08-nyr/slides/intro-extra-workflowsets.html#your-turn",
    "href": "archive/2025-08-nyr/slides/intro-extra-workflowsets.html#your-turn",
    "title": "Extras - workflowsets",
    "section": "Your turn",
    "text": "Your turn\n\nWhen do you think a workflow set would be useful?\nDiscuss with your neighbors!\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#startup",
    "href": "slides/advanced-02-model-tuning.html#startup",
    "title": "2 - Model optimization by tuning",
    "section": "Startup! ",
    "text": "Startup! \n\nlibrary(tidymodels)\nlibrary(probably)\nlibrary(desirability2)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\n\n# check torch:\nif (torch::torch_is_installed()) {\n  library(torch)\n}\n\n# Load our example data for this section\n\"https://raw.githubusercontent.com/tidymodels/\" |&gt; \n  paste0(\"workshops/main/slides/class_data.RData\") |&gt; \n  url() |&gt; \n  load()"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#where-we-are",
    "href": "slides/advanced-02-model-tuning.html#where-we-are",
    "title": "2 - Model optimization by tuning",
    "section": "Where we are ",
    "text": "Where we are \n\nset.seed(429)\nsim_split &lt;- initial_split(class_data, prop = 0.75, strata = class)\nsim_train &lt;- training(sim_split)\nsim_test  &lt;- testing(sim_split)\n\nset.seed(523)\nsim_rs &lt;- vfold_cv(sim_train, v = 10, strata = class)"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#the-brulee-package",
    "href": "slides/advanced-02-model-tuning.html#the-brulee-package",
    "title": "2 - Model optimization by tuning",
    "section": "The brulee package ",
    "text": "The brulee package \nSeveral packages exist for this, but weâ€™ll use the brulee package, which relies on the torch deep learning framework.\n\nLetâ€™s load the package and look at the documentation for the function that we will use:\n\n\n# This might trigger a torch install: \nlibrary(brulee)\n# ?brulee_mlp\n\n(HTML docs are nice too)"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#notable-arguments-part-1",
    "href": "slides/advanced-02-model-tuning.html#notable-arguments-part-1",
    "title": "2 - Model optimization by tuning",
    "section": "Notable arguments Part 1 ",
    "text": "Notable arguments Part 1 \nModel Structure:\n\nhidden_units: the primary way to specify model complexity.\nactivation: the name of the nonlinear function used to connect the predictors to the hidden layer.\n\n\nLoss Function:\n\npenalty: amount of regularization used to prevent overfitting.\nmixture: the proportion of L1 and L2 penalties.\nvalidation: proportion of data to leave out to assess early stopping."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#notable-arguments-part-2",
    "href": "slides/advanced-02-model-tuning.html#notable-arguments-part-2",
    "title": "2 - Model optimization by tuning",
    "section": "Notable arguments Part 2 ",
    "text": "Notable arguments Part 2 \nOptimization:\n\noptimizer: the type of gradient-based optimization.\nepochs: how many passes through the entire data set (i.e., iterations).\nstop_iter: number of bad iterations before stopping.\nlearn_rate: how fast does gradient descent move?\nrate_schedule: should the learning rate change over epochs?\nbatch_size: for stochastic gradient descent.\n\n\nThatâ€™s a lot ğŸ˜©"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#cost-sensitive-learning",
    "href": "slides/advanced-02-model-tuning.html#cost-sensitive-learning",
    "title": "2 - Model optimization by tuning",
    "section": "Cost-sensitive learning ",
    "text": "Cost-sensitive learning \nOne other option: class_weights: amount to upweight the minority class (event) when computing the objective function (cross-entropy).\n\nWe have a moderate class imbalance, and weâ€™ll use this argument to deal with it.\n\nThis will push the minority class probability estimates to be more accurate/calibrated. Overall the model will be less effective; this assumes the minority class is the class of interest."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#a-single-model",
    "href": "slides/advanced-02-model-tuning.html#a-single-model",
    "title": "2 - Model optimization by tuning",
    "section": "A single model    ",
    "text": "A single model    \n\nnnet_ex_spec &lt;- \n  mlp(hidden_units = 20, penalty = 0.01, learn_rate = 0.005, epochs = 100) |&gt; \n  set_engine(\"brulee\", class_weights = 3, stop_iter = 10) |&gt; \n  set_mode(\"classification\")\n\nrec &lt;- \n  recipe(class ~ ., data = sim_train) |&gt; \n  step_normalize(all_numeric_predictors())\n\nnnet_ex_wflow &lt;- workflow(rec, nnet_ex_spec)\n\n# Fit on the first fold's 90% analysis set\nset.seed(147)\nnnet_ex_fit &lt;- fit(nnet_ex_wflow, data = analysis(sim_rs$splits[[1]]))"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#did-it-converge",
    "href": "slides/advanced-02-model-tuning.html#did-it-converge",
    "title": "2 - Model optimization by tuning",
    "section": "Did it converge?  ",
    "text": "Did it converge?  \n\n\n\nnnet_ex_fit |&gt;\n  # pull out the brulee fit:\n  extract_fit_engine() |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nThe y-axis statistics are computed on the held-out predictions at each iteration.\nThe vertical green line shows that early stopping occurred."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#did-it-work",
    "href": "slides/advanced-02-model-tuning.html#did-it-work",
    "title": "2 - Model optimization by tuning",
    "section": "Did it work?    ",
    "text": "Did it work?    \n\nassessment_data &lt;- assessment(sim_rs$splits[[1]])\ncls_mtr &lt;- metric_set(brier_class, roc_auc, sensitivity, specificity)\nholdout_pred &lt;- augment(nnet_ex_fit, assessment_data) \n\n# Performance metrics\nholdout_pred |&gt; cls_mtr(class, estimate = .pred_class, .pred_event)\n#&gt; # A tibble: 4 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary        0.765 \n#&gt; 2 specificity binary        0.948 \n#&gt; 3 brier_class binary        0.0500\n#&gt; 4 roc_auc     binary        0.958\n\nKind of?\nIf sensitivity is important, then the model is moderately successful."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#calibration",
    "href": "slides/advanced-02-model-tuning.html#calibration",
    "title": "2 - Model optimization by tuning",
    "section": "Calibration",
    "text": "Calibration\nCalibration is a property of individual predictions. A well-calibrated probability occurs in the wild at the same rate as the estimate.\nThe Brier score is the closest we can come to estimating it:\n\\[\nBrier = \\frac{1}{NC}\\sum_{i=1}^N\\sum_{k=1}^C (y_{ik} - \\hat{p}_{ik})^2\n\\]\nZero is best and, for two classes, values above 0.25 are bad.\n\nAML4TD"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#calibration-plots",
    "href": "slides/advanced-02-model-tuning.html#calibration-plots",
    "title": "2 - Model optimization by tuning",
    "section": "Calibration Plots",
    "text": "Calibration Plots\n\n\n\nholdout_pred |&gt;\n  cal_plot_windowed(\n    truth = class,\n    estimate = .pred_event,\n    window_size = 0.2,\n    step_size = 0.025\n  )\n\n\nğŸ˜±\nThis is partly due to the small sample size.\n\n\n\n\n\n\n\n\n\n\n\n\nAML4TD"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#tuning-parameters",
    "href": "slides/advanced-02-model-tuning.html#tuning-parameters",
    "title": "2 - Model optimization by tuning",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#optimize-tuning-parameters",
    "href": "slides/advanced-02-model-tuning.html#optimize-tuning-parameters",
    "title": "2 - Model optimization by tuning",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#tagging-parameters-for-tuning",
    "href": "slides/advanced-02-model-tuning.html#tagging-parameters-for-tuning",
    "title": "2 - Model optimization by tuning",
    "section": "Tagging parameters for tuning ",
    "text": "Tagging parameters for tuning \nWith tidymodels, you can mark the parameters that you want to optimize with a value of tune().\n\nThe function itself just returnsâ€¦ itself:\n\ntune()\n#&gt; tune()\nstr(tune())\n#&gt;  language tune()\n\n# optionally add a label\ntune(\"I hope that the workshop is going well\")\n#&gt; tune(\"I hope that the workshop is going well\")\n\n\nFor exampleâ€¦"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#optimizing-the-neural-network",
    "href": "slides/advanced-02-model-tuning.html#optimizing-the-neural-network",
    "title": "2 - Model optimization by tuning",
    "section": "Optimizing the neural network  ",
    "text": "Optimizing the neural network  \n\nnnet_spec &lt;- \n  mlp(hidden_units = tune(), penalty = tune(), learn_rate = tune(), \n      epochs = 100, activation = tune()) |&gt; \n  set_engine(\"brulee\", class_weights = tune(), stop_iter = 10) |&gt; \n  set_mode(\"classification\")\n\nnnet_wflow &lt;- workflow(rec, nnet_spec)\nnnet_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Recipe\n#&gt; Model: mlp()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; â€¢ step_normalize()\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Single Layer Neural Network Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   hidden_units = tune()\n#&gt;   penalty = tune()\n#&gt;   epochs = 100\n#&gt;   activation = tune()\n#&gt;   learn_rate = tune()\n#&gt; \n#&gt; Engine-Specific Arguments:\n#&gt;   class_weights = tune()\n#&gt;   stop_iter = 10\n#&gt; \n#&gt; Computational engine: brulee"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#optimize-tuning-parameters-1",
    "href": "slides/advanced-02-model-tuning.html#optimize-tuning-parameters-1",
    "title": "2 - Model optimization by tuning",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe two main strategies for optimization are:\n\nGrid search, which tests a pre-defined set of candidate values.\nIterative search, which suggests/estimates new values of candidate parameters to evaluate.\n\n\nWe wonâ€™t be discussing iterative search methods in the regular notes. But you can learn more in AML4TD or in the extra slides for this subject."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#grid-search",
    "href": "slides/advanced-02-model-tuning.html#grid-search",
    "title": "2 - Model optimization by tuning",
    "section": "Grid search",
    "text": "Grid search\nA small grid of points trying to minimize the error via learning rate:"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#grid-search-1",
    "href": "slides/advanced-02-model-tuning.html#grid-search-1",
    "title": "2 - Model optimization by tuning",
    "section": "Grid search",
    "text": "Grid search\nIn reality, we would probably sample the space more densely:"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#iterative-search",
    "href": "slides/advanced-02-model-tuning.html#iterative-search",
    "title": "2 - Model optimization by tuning",
    "section": "Iterative Search",
    "text": "Iterative Search\nWe could start with a few points and search the space:"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#parameters",
    "href": "slides/advanced-02-model-tuning.html#parameters",
    "title": "2 - Model optimization by tuning",
    "section": "Parameters",
    "text": "Parameters\n\nThe tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc.).\nThe extract_parameter_set_dials() function extracts these tuning parameters and the info.\n\n\nnnet_param &lt;- \n  nnet_wflow |&gt; \n  extract_parameter_set_dials() \nnnet_param  \n#&gt; Collection of 5 parameters for tuning\n#&gt; \n#&gt;     identifier          type    object\n#&gt;   hidden_units  hidden_units nparam[+]\n#&gt;        penalty       penalty nparam[+]\n#&gt;     activation    activation dparam[+]\n#&gt;     learn_rate    learn_rate nparam[+]\n#&gt;  class_weights class_weights nparam[+]\n#&gt;"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#different-types-of-grids",
    "href": "slides/advanced-02-model-tuning.html#different-types-of-grids",
    "title": "2 - Model optimization by tuning",
    "section": "Different types of grids ",
    "text": "Different types of grids \n\n\n\n\n\n\n\n\n\nSpace-filling designs (SFD) attempt to cover the parameter space without redundant candidates. We recommend these the most, and they are the default."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#extract-and-update-parameters",
    "href": "slides/advanced-02-model-tuning.html#extract-and-update-parameters",
    "title": "2 - Model optimization by tuning",
    "section": "Extract and update parameters  ",
    "text": "Extract and update parameters  \n\nnnet_param &lt;- \n  nnet_param |&gt; \n  update(class_weights = class_weights(c(1, 50)))\n  \n  nnet_param  \n#&gt; Collection of 5 parameters for tuning\n#&gt; \n#&gt;     identifier          type    object\n#&gt;   hidden_units  hidden_units nparam[+]\n#&gt;        penalty       penalty nparam[+]\n#&gt;     activation    activation dparam[+]\n#&gt;     learn_rate    learn_rate nparam[+]\n#&gt;  class_weights class_weights nparam[+]\n#&gt;"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#grid-search-3",
    "href": "slides/advanced-02-model-tuning.html#grid-search-3",
    "title": "2 - Model optimization by tuning",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nnnet_res \n#&gt; # Tuning results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics           .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [1348/151]&gt; Fold01 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [3 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [1349/150]&gt; Fold02 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [1 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [1349/150]&gt; Fold03 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [3 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [1349/150]&gt; Fold04 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [4 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [1349/150]&gt; Fold05 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [2 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [1349/150]&gt; Fold06 &lt;tibble [96 Ã— 9]&gt;  &lt;tibble [5 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [1349/150]&gt; Fold07 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [1349/150]&gt; Fold08 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [3 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [1350/149]&gt; Fold09 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [1 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [1350/149]&gt; Fold10 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [3 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt; \n#&gt; There were issues with some computations:\n#&gt; \n#&gt;   - Error(s) x1: 'best_epoch' should be an integer\n#&gt;   - Warning(s) x1: Loss is NaN at epoch 1. Training is stopped.\n#&gt;   - Warning(s) x1: Loss is NaN at epoch 10. Training is stopped.\n#&gt;   - Warning(s) x1: Loss is NaN at epoch 11. Training is stopped.\n#&gt;   - Warning(s) x1: Loss is NaN at epoch 12. Training is stopped.\n#&gt;   - Warning(s) x2: Loss is NaN at epoch 2. Training is stopped.\n#&gt;   - Warning(s) x1: Loss is NaN at epoch 4. Training is stopped.\n#&gt;   - Warning(s) x3: Loss is NaN at epoch 5. Training is stopped.\n#&gt;   - Warning(s) x2: Loss is NaN at epoch 6. Training is stopped.\n#&gt;   - Warning(s) x6: Loss is NaN at epoch 7. Training is stopped.\n#&gt;   - Warning(s) x5: Loss is NaN at epoch 8. Training is stopped.\n#&gt;   - Warning(s) x1: Loss is NaN at epoch 9. Training is stopped.\n#&gt; \n#&gt; Run `show_notes(.Last.tune.result)` for more information."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#running-in-parallel",
    "href": "slides/advanced-02-model-tuning.html#running-in-parallel",
    "title": "2 - Model optimization by tuning",
    "section": "Running in parallel",
    "text": "Running in parallel\n\nGrid search, combined with resampling, requires fitting a lot of models!\nThese models donâ€™t depend on one another and can be run in parallel.\n\nWe can use the future or mirai packages to do this:\n\ncores &lt;- parallelly::availableCores(logical = FALSE)\n\n\n\n\n\nlibrary(future)\nplan(multisession, workers = cores)\n\n# Now call `tune_grid()`!\n\n\n\nlibrary(mirai)\ndaemons(cores)\n\n# Now call `tune_grid()`!"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#distributing-tasks",
    "href": "slides/advanced-02-model-tuning.html#distributing-tasks",
    "title": "2 - Model optimization by tuning",
    "section": "Distributing tasks",
    "text": "Distributing tasks\nWhen only tuning the model:"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#running-in-parallel-1",
    "href": "slides/advanced-02-model-tuning.html#running-in-parallel-1",
    "title": "2 - Model optimization by tuning",
    "section": "Running in parallel",
    "text": "Running in parallel\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\nFaceted on the expensiveness of preprocessing used."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#running-in-parallel-2",
    "href": "slides/advanced-02-model-tuning.html#running-in-parallel-2",
    "title": "2 - Model optimization by tuning",
    "section": "Running in parallel",
    "text": "Running in parallel\nWeâ€™ll use mirai as our parallel backend for our notes.\n\nUsing 10 cores with mirai, time to execute the previous tune_grid() call was reduced from 256s to 45s, a speed-up of 5.7-fold.\n\nThe next slide deck (on racing) includes more examples of speed-ups for a different model."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#grid-search-4",
    "href": "slides/advanced-02-model-tuning.html#grid-search-4",
    "title": "2 - Model optimization by tuning",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nnnet_res \n#&gt; # Tuning results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics           .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [1348/151]&gt; Fold01 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [3 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [1349/150]&gt; Fold02 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [1 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [1349/150]&gt; Fold03 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [3 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [1349/150]&gt; Fold04 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [4 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [1349/150]&gt; Fold05 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [2 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [1349/150]&gt; Fold06 &lt;tibble [96 Ã— 9]&gt;  &lt;tibble [5 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [1349/150]&gt; Fold07 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [1349/150]&gt; Fold08 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [3 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [1350/149]&gt; Fold09 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [1 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [1350/149]&gt; Fold10 &lt;tibble [100 Ã— 9]&gt; &lt;tibble [3 Ã— 4]&gt; &lt;tibble&gt;    \n#&gt; \n#&gt; There were issues with some computations:\n#&gt; \n#&gt;   - Error(s) x1: 'best_epoch' should be an integer\n#&gt;   - Warning(s) x1: Loss is NaN at epoch 1. Training is stopped.\n#&gt;   - Warning(s) x1: Loss is NaN at epoch 10. Training is stopped.\n#&gt;   - Warning(s) x1: Loss is NaN at epoch 11. Training is stopped.\n#&gt;   - Warning(s) x1: Loss is NaN at epoch 12. Training is stopped.\n#&gt;   - Warning(s) x2: Loss is NaN at epoch 2. Training is stopped.\n#&gt;   - Warning(s) x1: Loss is NaN at epoch 4. Training is stopped.\n#&gt;   - Warning(s) x3: Loss is NaN at epoch 5. Training is stopped.\n#&gt;   - Warning(s) x2: Loss is NaN at epoch 6. Training is stopped.\n#&gt;   - Warning(s) x6: Loss is NaN at epoch 7. Training is stopped.\n#&gt;   - Warning(s) x5: Loss is NaN at epoch 8. Training is stopped.\n#&gt;   - Warning(s) x1: Loss is NaN at epoch 9. Training is stopped.\n#&gt; \n#&gt; Run `show_notes(.Last.tune.result)` for more information."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#grid-results",
    "href": "slides/advanced-02-model-tuning.html#grid-results",
    "title": "2 - Model optimization by tuning",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(nnet_res)"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#brier-results",
    "href": "slides/advanced-02-model-tuning.html#brier-results",
    "title": "2 - Model optimization by tuning",
    "section": "Brier results ",
    "text": "Brier results \n\nautoplot(nnet_res, metric = \"brier_class\") + \n  facet_grid(. ~ name, scale = \"free_x\") + \n  lims(y = c(0.04, 0.22)) # no tanh"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#roc-curve-results",
    "href": "slides/advanced-02-model-tuning.html#roc-curve-results",
    "title": "2 - Model optimization by tuning",
    "section": "ROC curve results ",
    "text": "ROC curve results \n\nautoplot(nnet_res, metric = \"roc_auc\") + \n  facet_grid(. ~ name, scale = \"free_x\") + \n  lims(y = c(0.83, 0.97)) # no tanh"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#sensitivityspecificity-results",
    "href": "slides/advanced-02-model-tuning.html#sensitivityspecificity-results",
    "title": "2 - Model optimization by tuning",
    "section": "Sensitivity/Specificity results ",
    "text": "Sensitivity/Specificity results \n\nautoplot(nnet_res, metric = c(\"sensitivity\", \"specificity\"))"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#grid-results-1",
    "href": "slides/advanced-02-model-tuning.html#grid-results-1",
    "title": "2 - Model optimization by tuning",
    "section": "Grid results",
    "text": "Grid results\n\ntanh activation ğŸ‘ğŸ‘\nAs class weight â¬†ï¸:\n\nsensitivity â¬†ï¸\nspecificity â¬‡ï¸â¬‡ï¸\nBrier: â¬†ï¸\nROC AUC: ğŸ¤·"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#tuning-results",
    "href": "slides/advanced-02-model-tuning.html#tuning-results",
    "title": "2 - Model optimization by tuning",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(nnet_res) |&gt; \n  relocate(.metric, mean) \n#&gt; # A tibble: 100 Ã— 11\n#&gt;    .metric      mean hidden_units    penalty activation learn_rate class_weights\n#&gt;    &lt;chr&gt;       &lt;dbl&gt;        &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n#&gt;  1 brier_class 0.215            2 0.00000147 elu           0.0962           17.3\n#&gt;  2 roc_auc     0.835            2 0.00000147 elu           0.0962           17.3\n#&gt;  3 sensitivity 0.905            2 0.00000147 elu           0.0962           17.3\n#&gt;  4 specificity 0.644            2 0.00000147 elu           0.0962           17.3\n#&gt;  5 brier_class 0.211            4 0.000464   tanhshrink    0.0736           31.6\n#&gt;  6 roc_auc     0.901            4 0.000464   tanhshrink    0.0736           31.6\n#&gt;  7 sensitivity 0.935            4 0.000464   tanhshrink    0.0736           31.6\n#&gt;  8 specificity 0.641            4 0.000464   tanhshrink    0.0736           31.6\n#&gt;  9 brier_class 0.167            6 0.00000383 relu          0.00224          41.8\n#&gt; 10 roc_auc     0.943            6 0.00000383 relu          0.00224          41.8\n#&gt; # â„¹ 90 more rows\n#&gt; # â„¹ 4 more variables: .estimator &lt;chr&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt;"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#tuning-results-1",
    "href": "slides/advanced-02-model-tuning.html#tuning-results-1",
    "title": "2 - Model optimization by tuning",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(nnet_res, summarize = FALSE) |&gt; \n  relocate(.metric, .estimate)\n#&gt; # A tibble: 996 Ã— 10\n#&gt;    .metric     .estimate id     hidden_units    penalty activation learn_rate\n#&gt;    &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;         &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n#&gt;  1 sensitivity    0.824  Fold01            2 0.00000147 elu            0.0962\n#&gt;  2 specificity    0.910  Fold01            2 0.00000147 elu            0.0962\n#&gt;  3 brier_class    0.0659 Fold01            2 0.00000147 elu            0.0962\n#&gt;  4 roc_auc        0.967  Fold01            2 0.00000147 elu            0.0962\n#&gt;  5 sensitivity    1      Fold02            2 0.00000147 elu            0.0962\n#&gt;  6 specificity    0.474  Fold02            2 0.00000147 elu            0.0962\n#&gt;  7 brier_class    0.305  Fold02            2 0.00000147 elu            0.0962\n#&gt;  8 roc_auc        0.797  Fold02            2 0.00000147 elu            0.0962\n#&gt;  9 sensitivity    0.941  Fold03            2 0.00000147 elu            0.0962\n#&gt; 10 specificity    0.910  Fold03            2 0.00000147 elu            0.0962\n#&gt; # â„¹ 986 more rows\n#&gt; # â„¹ 3 more variables: class_weights &lt;dbl&gt;, .estimator &lt;chr&gt;, .config &lt;chr&gt;"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#choose-a-parameter-combination",
    "href": "slides/advanced-02-model-tuning.html#choose-a-parameter-combination",
    "title": "2 - Model optimization by tuning",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nshow_best(nnet_res, metric = \"brier_class\") |&gt; \n  relocate(.metric, mean) \n#&gt; # A tibble: 5 Ã— 11\n#&gt;   .metric       mean hidden_units    penalty activation learn_rate class_weights\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;        &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 brier_class 0.0463           36    2.61e-5 elu           0.0562           3.04\n#&gt; 2 brier_class 0.0628           16    8.25e-8 log_sigmoâ€¦    0.00171          5.08\n#&gt; 3 brier_class 0.0646           40    2.15e-2 tanh          0.00293          7.12\n#&gt; 4 brier_class 0.0673           48    1.78e-9 relu          0.0112          11.2 \n#&gt; 5 brier_class 0.0694           26    4.64e-9 relu          0.482            9.17\n#&gt; # â„¹ 4 more variables: .estimator &lt;chr&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt;"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#choose-a-parameter-combination-1",
    "href": "slides/advanced-02-model-tuning.html#choose-a-parameter-combination-1",
    "title": "2 - Model optimization by tuning",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \nCreate your own tibble for final parameters or use one of the tune::select_*() functions:\n\nnnet_best &lt;- select_best(nnet_res, metric = \"brier_class\")\nnnet_best\n#&gt; # A tibble: 1 Ã— 6\n#&gt;   hidden_units   penalty activation learn_rate class_weights .config         \n#&gt;          &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;           \n#&gt; 1           36 0.0000261 elu            0.0562          3.04 pre0_mod18_post0\n\ncollect_metrics(nnet_res) |&gt; \n  inner_join(nnet_best) |&gt; \n  select(.metric, mean)\n#&gt; # A tibble: 4 Ã— 2\n#&gt;   .metric       mean\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;\n#&gt; 1 brier_class 0.0463\n#&gt; 2 roc_auc     0.967 \n#&gt; 3 sensitivity 0.799 \n#&gt; 4 specificity 0.958"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#checking-approximate-calibration",
    "href": "slides/advanced-02-model-tuning.html#checking-approximate-calibration",
    "title": "2 - Model optimization by tuning",
    "section": "Checking (Approximate) Calibration  ",
    "text": "Checking (Approximate) Calibration  \n\n\nnnet_holdout_pred &lt;-\n  nnet_res |&gt; \n  collect_predictions(\n    parameters = nnet_best\n  )\n\nnnet_holdout_pred |&gt;\n  cal_plot_windowed(\n    truth = class,\n    estimate = .pred_event,\n    window_size = 0.2,\n    step_size = 0.025,\n  )\n\n\n\n\n\n\n\n\n\n\nThis plot pools the out-of-sample predictions. Our Brier estimate is the mean of 10 data sets.\nThe curve is approximate for that reason."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#multiple-goals",
    "href": "slides/advanced-02-model-tuning.html#multiple-goals",
    "title": "2 - Model optimization by tuning",
    "section": "Multiple goals",
    "text": "Multiple goals\nOptimizing the Brier score optimizes for accuracy and calibration of the class probabilities.\n\nThatâ€™s counter-productive to finding the rare class â€œeventâ€ events.\n\nCan we find a way to optimize multiple metrics at once?\n\nThere are a few ways, and weâ€™ll focus on desirability functions."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#desirability-functions",
    "href": "slides/advanced-02-model-tuning.html#desirability-functions",
    "title": "2 - Model optimization by tuning",
    "section": "Desirability functions",
    "text": "Desirability functions\nWe create simple functions to translate our variable to [0, 1]; 1.0 is most desirable.\n\nWe can combine them with a geometric mean."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#multimetric-optimization",
    "href": "slides/advanced-02-model-tuning.html#multimetric-optimization",
    "title": "2 - Model optimization by tuning",
    "section": "Multimetric optimization ",
    "text": "Multimetric optimization \n\nshow_best_desirability(\n  nnet_res, \n  maximize(sensitivity),\n  minimize(brier_class),\n  constrain(specificity, low = 0.8, high = 1.0)\n) |&gt; \n  relocate(class_weights, sensitivity, specificity, brier_class, .d_overall) \n#&gt; # A tibble: 5 Ã— 14\n#&gt;   class_weights sensitivity specificity brier_class .d_overall hidden_units\n#&gt;           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;        &lt;int&gt;\n#&gt; 1         29.6        0.947       0.859      0.0948      0.940           38\n#&gt; 2         50          0.958       0.841      0.111       0.937           30\n#&gt; 3          7.12       0.899       0.914      0.0646      0.930           40\n#&gt; 4         11.2        0.894       0.914      0.0673      0.925           48\n#&gt; 5         25.5        0.923       0.833      0.111       0.917           28\n#&gt; # â„¹ 8 more variables: penalty &lt;dbl&gt;, activation &lt;chr&gt;, learn_rate &lt;dbl&gt;,\n#&gt; #   .config &lt;chr&gt;, roc_auc &lt;dbl&gt;, .d_max_sensitivity &lt;dbl&gt;,\n#&gt; #   .d_min_brier_class &lt;dbl&gt;, .d_box_specificity &lt;dbl&gt;"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#however",
    "href": "slides/advanced-02-model-tuning.html#however",
    "title": "2 - Model optimization by tuning",
    "section": "Howeverâ€¦  ",
    "text": "Howeverâ€¦  \n\n\nmore_sens &lt;-\n  select_best_desirability(\n    nnet_res,\n    maximize(sensitivity),\n    minimize(brier_class),\n    constrain(specificity, low = 0.8, high = 1.0)\n  )\n\nnnet_res |&gt;\n  collect_predictions(\n    parameters = more_sens\n  ) |&gt;\n  cal_plot_windowed(\n    truth = class,\n    estimate = .pred_event,\n    window_size = 0.2,\n    step_size = 0.025,\n  )\n\n\n\n\n\n\n\n\n\n\nThe curve hugging the bottom of the plot means that we are drastically overestimating the probability of the â€œeventâ€ class relative to how often it occurs â€œin the wild.â€\nThatâ€™s the point of cost-sensitive learning, but itâ€™s bad for calibration."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#conflicting-goals",
    "href": "slides/advanced-02-model-tuning.html#conflicting-goals",
    "title": "2 - Model optimization by tuning",
    "section": "Conflicting goals",
    "text": "Conflicting goals\nThe problem here is that we are biasing the probability estimates so that we can predict more data to be the rare â€œeventâ€ class using a default probability cutoff of 1/2.\n\nThat is compromising the overall model fit; our probabilities are not accurate.\n\nIf we donâ€™t use the class probability estimates, this is fine.\n\n\nIn the postprocessing slides, weâ€™ll examine an alternative approach that involves different kinds of tradeoffs."
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#fitting-a-workflow",
    "href": "slides/advanced-02-model-tuning.html#fitting-a-workflow",
    "title": "2 - Model optimization by tuning",
    "section": "Fitting a workflow  ",
    "text": "Fitting a workflow  \nLetâ€™s say that we want to train the model on the â€œbestâ€ parameter estimates.\nWe can use a tibble of tuning parameters and splice them into the workflow in place of tune():\n\n\n\nset.seed(398)\nnnet_sens_fit &lt;-\n  nnet_wflow |&gt;\n  finalize_workflow(more_sens) |&gt;\n  fit(sim_train)\n\n\n\nnnet_sens_fit |&gt; extract_fit_engine()\n#&gt; Multilayer perceptron\n#&gt; \n#&gt; elu activation,\n#&gt; 38 hidden units,\n#&gt; 1,256 model parameters\n#&gt; 1,499 samples, 30 features, 2 classes \n#&gt; class weights event=29.58333, no_event= 1.00000 \n#&gt; weight decay: 1e-05 \n#&gt; dropout proportion: 0 \n#&gt; batch size: 1350 \n#&gt; learn rate: 0.001 \n#&gt; validation loss after 68 epochs: 0.145"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#ordering-off-of-the-menu",
    "href": "slides/advanced-02-model-tuning.html#ordering-off-of-the-menu",
    "title": "2 - Model optimization by tuning",
    "section": "Ordering off of the menu  ",
    "text": "Ordering off of the menu  \nIf we want to choose from the numerically best for an existing metric, there is a simpler function:\n\n\n\nset.seed(398)\nmlp_brier_fit &lt;-\n  nnet_res |&gt;\n  fit_best(metric = \"brier_class\")\n\n\n\nmlp_brier_fit |&gt; extract_fit_engine()\n#&gt; Multilayer perceptron\n#&gt; \n#&gt; elu activation,\n#&gt; 36 hidden units,\n#&gt; 1,190 model parameters\n#&gt; 1,499 samples, 30 features, 2 classes \n#&gt; class weights event=3.041667, no_event=1.000000 \n#&gt; weight decay: 2.610157e-05 \n#&gt; dropout proportion: 0 \n#&gt; batch size: 1350 \n#&gt; learn rate: 0.05623413 \n#&gt; validation loss after 6 epochs: 0.411"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#extracting-results",
    "href": "slides/advanced-02-model-tuning.html#extracting-results",
    "title": "2 - Model optimization by tuning",
    "section": "Extracting Results     ",
    "text": "Extracting Results     \nIf we want to know about the resampled workflow, we can write a function that can return information from tune_grid().\n\nFor example, this one can save the optimization process results:\n\n\n\nextract_iter_hist &lt;- function(wflow) {\n  require(tidymodels)\n  require(tibble)\n  wflow |&gt;\n    extract_fit_engine() |&gt;\n    pluck(\"loss\") |&gt;\n    as_tibble_col(\"loss\") |&gt;\n    mutate(epoch = row_number() - 1)\n}\n\n\n\nmlp_brier_fit |&gt; \n  extract_iter_hist() |&gt; \n  slice(1:5)\n#&gt; # A tibble: 5 Ã— 2\n#&gt;    loss epoch\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 0.517     0\n#&gt; 2 0.501     1\n#&gt; 3 0.483     2\n#&gt; 4 0.461     3\n#&gt; 5 0.437     4"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#your-turn",
    "href": "slides/advanced-02-model-tuning.html#your-turn",
    "title": "2 - Model optimization by tuning",
    "section": "Your turn",
    "text": "Your turn\n\nRead the docs for control_grid(), specifically the extract option.\nCreate a control object that extracts the iteration history.\nRe-run tune_grid() with the same grid (or fewer grid points) and use the control option with the extraction function.\nAfterward use collect_extracts() to get the results.\nPlot the iteration history for one or more grid points, coloring by the resample id.\n\nParallel processing will make this go more quickly.\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#whats-next",
    "href": "slides/advanced-02-model-tuning.html#whats-next",
    "title": "2 - Model optimization by tuning",
    "section": "Whatâ€™s next ",
    "text": "Whatâ€™s next \nLetâ€™s look at racing: an old variation of grid search that adaptively processes the grid.\n\nIf we are\n\ninitially screening many models/preprocessors/postprocessors and/or\nhave a large grid\n\nThis can lead to remarkable speed-ups."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#getting-set-up",
    "href": "slides/advanced-04-feature-engineering-part-one.html#getting-set-up",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Getting set up   ",
    "text": "Getting set up   \n\nlibrary(tidymodels)\nlibrary(embed)\nlibrary(extrasteps)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\n\n# Load our example data for this section\n\"https://github.com/tidymodels/workshops/raw/refs/heads/2025-GMOFETML/slides/leaf_data.RData\" |&gt; \n  url() |&gt; \n  load()"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#leaf-data-set",
    "href": "slides/advanced-04-feature-engineering-part-one.html#leaf-data-set",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Leaf data set",
    "text": "Leaf data set\nSlightly modified version of modeldata::leaf_id_flavia.\n\nglimpse(leaf_data)\n#&gt; Rows: 1,907\n#&gt; Columns: 55\n#&gt; $ species                    &lt;fct&gt; chinese_redbud, chinese_redbud, chinese_redâ€¦\n#&gt; $ apex                       &lt;fct&gt; none, none, none, none, none, none, none, nâ€¦\n#&gt; $ base                       &lt;fct&gt; none, none, none, none, none, none, none, nâ€¦\n#&gt; $ shape                      &lt;fct&gt; heart_shape, heart_shape, heart_shape, hearâ€¦\n#&gt; $ edge                       &lt;chr&gt; \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smâ€¦\n#&gt; $ outlying_polar             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n#&gt; $ skewed_polar               &lt;dbl&gt; 0.4610066, 0.4747877, 0.5356883, 0.4771338,â€¦\n#&gt; $ clumpy_polar               &lt;dbl&gt; 0.006479784, 0.011100482, 0.017317486, 0.01â€¦\n#&gt; $ sparse_polar               &lt;dbl&gt; 0.01449941, 0.01451566, 0.02953578, 0.01425â€¦\n#&gt; $ striated_polar             &lt;dbl&gt; 0.9788360, 0.9797980, 0.6758621, 0.9896373,â€¦\n#&gt; $ convex_polar               &lt;dbl&gt; 0.000899987, 0.000152532, 0.035230741, 0.00â€¦\n#&gt; $ skinny_polar               &lt;dbl&gt; 0.1177954, 0.5440493, 0.7764908, 0.7067394,â€¦\n#&gt; $ stringy_polar              &lt;dbl&gt; 1.0000000, 1.0000000, 0.8544140, 1.0000000,â€¦\n#&gt; $ monotonic_polar            &lt;dbl&gt; 0.026807610, 0.005554220, 0.068481538, 0.09â€¦\n#&gt; $ outlying_contour           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n#&gt; $ skewed_contour             &lt;dbl&gt; 0.4954035, 0.5321818, 0.5451171, 0.4675013,â€¦\n#&gt; $ clumpy_contour             &lt;dbl&gt; 0.007898114, 0.010846498, 0.020271493, 0.01â€¦\n#&gt; $ sparse_contour             &lt;dbl&gt; 0.01449941, 0.01436114, 0.02903878, 0.01451â€¦\n#&gt; $ striated_contour           &lt;dbl&gt; 0.9744898, 0.9811321, 0.6766917, 0.9784946,â€¦\n#&gt; $ convex_contour             &lt;dbl&gt; 0.000479470, 0.000000000, 0.035908221, 0.00â€¦\n#&gt; $ skinny_contour             &lt;dbl&gt; 0.1810093, 1.0000000, 0.7806988, 0.1348225,â€¦\n#&gt; $ stringy_contour            &lt;dbl&gt; 1.0000000, 1.0000000, 0.8864787, 1.0000000,â€¦\n#&gt; $ monotonic_contour          &lt;dbl&gt; 0.000816260, 0.000008740, 0.001253195, 0.32â€¦\n#&gt; $ num_max_points             &lt;dbl&gt; 1, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1â€¦\n#&gt; $ num_min_points             &lt;dbl&gt; 2, 4, 7, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 8, 2â€¦\n#&gt; $ diameter                   &lt;dbl&gt; 1255.830, 1227.826, 1113.228, 1219.185, 118â€¦\n#&gt; $ area                       &lt;dbl&gt; 936626.5, 917300.5, 855376.5, 901303.0, 885â€¦\n#&gt; $ perimeter                  &lt;dbl&gt; 3830.534, 3781.445, 3695.646, 3678.409, 383â€¦\n#&gt; $ physiological_length       &lt;dbl&gt; 1253, 1218, 1098, 1205, 1153, 1179, 1188, 1â€¦\n#&gt; $ physiological_width        &lt;dbl&gt; 1088, 1091, 1083, 1053, 1078, 1073, 1054, 1â€¦\n#&gt; $ aspect_ratio               &lt;dbl&gt; 0.8683160, 0.8957307, 0.9863388, 0.8738589,â€¦\n#&gt; $ rectangularity             &lt;dbl&gt; 0.6870470, 0.6903027, 0.7193273, 0.7103222,â€¦\n#&gt; $ circularity                &lt;dbl&gt; 0.8021540, 0.8061315, 0.7870211, 0.8370680,â€¦\n#&gt; $ compactness                &lt;dbl&gt; 15.66578, 15.58849, 15.96701, 15.01237, 16.â€¦\n#&gt; $ narrow_factor              &lt;dbl&gt; 1.154255, 1.125413, 1.027912, 1.157821, 1.1â€¦\n#&gt; $ perimeter_ratio_diameter   &lt;dbl&gt; 3.050202, 3.079790, 3.319756, 3.017104, 3.2â€¦\n#&gt; $ perimeter_ratio_length     &lt;dbl&gt; 3.520711, 3.466036, 3.412416, 3.493266, 3.5â€¦\n#&gt; $ perimeter_ratio_lw         &lt;dbl&gt; 1.636281, 1.637698, 1.694473, 1.629056, 1.7â€¦\n#&gt; $ num_convex_points          &lt;dbl&gt; 125, 128, 114, 138, 125, 125, 121, 112, 115â€¦\n#&gt; $ perimeter_convexity        &lt;dbl&gt; 0.9404967, 0.9393550, 0.9232549, 0.9502414,â€¦\n#&gt; $ area_convexity             &lt;dbl&gt; 0.02359372, 0.03175513, 0.03932713, 0.01740â€¦\n#&gt; $ area_ratio_convexity       &lt;dbl&gt; 0.9769501, 0.9692222, 0.9621610, 0.9828892,â€¦\n#&gt; $ equivalent_diameter        &lt;dbl&gt; 1092.0393, 1080.7142, 1043.5992, 1071.2491,â€¦\n#&gt; $ eccentricity               &lt;dbl&gt; 0.44400376, 0.26866788, 0.31952502, 0.44811â€¦\n#&gt; $ contrast                   &lt;dbl&gt; 38.69163, 32.73154, 23.20849, 30.14546, 23.â€¦\n#&gt; $ correlation_texture        &lt;dbl&gt; 0.9959609, 0.9967740, 0.9975354, 0.9968981,â€¦\n#&gt; $ inverse_difference_moments &lt;dbl&gt; 0.5951133, 0.6022763, 0.6363508, 0.6135506,â€¦\n#&gt; $ entropy                    &lt;dbl&gt; 6.291552, 6.273267, 5.764995, 6.203471, 5.9â€¦\n#&gt; $ mean_red_val               &lt;dbl&gt; 38.02429, 34.55425, 30.92509, 33.44151, 33.â€¦\n#&gt; $ mean_green_val             &lt;dbl&gt; 72.38449, 69.89448, 69.22250, 71.32743, 70.â€¦\n#&gt; $ mean_blue_val              &lt;dbl&gt; 42.78902, 42.93044, 40.80089, 40.73282, 42.â€¦\n#&gt; $ std_red_val                &lt;dbl&gt; 41.66486, 40.48649, 38.55110, 39.09408, 40.â€¦\n#&gt; $ std_green_val              &lt;dbl&gt; 74.26665, 73.05275, 76.77596, 75.85529, 75.â€¦\n#&gt; $ std_blue_val               &lt;dbl&gt; 46.36232, 48.04195, 48.09177, 46.57170, 48.â€¦\n#&gt; $ correlation                &lt;dbl&gt; -0.027629688, -0.003103965, -0.036341630, 0â€¦"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#leaf-data-set-1",
    "href": "slides/advanced-04-feature-engineering-part-one.html#leaf-data-set-1",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Leaf data set",
    "text": "Leaf data set\n\n\nLakshika, J. P., & Talagala, T. S. (2021). Computer-aided interpretable features for leaf image classification. arXiv preprint arXiv:2106.08077."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#leaf-data-set-2",
    "href": "slides/advanced-04-feature-engineering-part-one.html#leaf-data-set-2",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Leaf data set",
    "text": "Leaf data set\n\n\nLakshika, J. P., & Talagala, T. S. (2021). Computer-aided interpretable features for leaf image classification. arXiv preprint arXiv:2106.08077."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#leaf-data-set-3",
    "href": "slides/advanced-04-feature-engineering-part-one.html#leaf-data-set-3",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Leaf data set",
    "text": "Leaf data set\n\n\nLakshika, J. P., & Talagala, T. S. (2021). Computer-aided interpretable features for leaf image classification. arXiv preprint arXiv:2106.08077."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#your-turn",
    "href": "slides/advanced-04-feature-engineering-part-one.html#your-turn",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Your turn",
    "text": "Your turn\n\nLoad and explore the leaf data\nMake time to look at the edge column and think about how one would encode it into numerics\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#leaf-edges",
    "href": "slides/advanced-04-feature-engineering-part-one.html#leaf-edges",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Leaf edges",
    "text": "Leaf edges\n\n\n\nLow number of unique levels\nSome overlap\n\n\n\nleaf_data |&gt;\n  count(edge)\n#&gt; # A tibble: 7 Ã— 2\n#&gt;   edge                 n\n#&gt;   &lt;chr&gt;            &lt;int&gt;\n#&gt; 1 \"\"                  62\n#&gt; 2 \"denate\"            65\n#&gt; 3 \"lobed\"            109\n#&gt; 4 \"lobed, smooth\"     52\n#&gt; 5 \"lobed, toothed\"   109\n#&gt; 6 \"smooth\"          1204\n#&gt; 7 \"toothed\"          306"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#leaf-edges-dummies",
    "href": "slides/advanced-04-feature-engineering-part-one.html#leaf-edges-dummies",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Leaf edges dummies",
    "text": "Leaf edges dummies\n(technically one-hot encoding)\n\n\n\n\n\n\nX\ndenate\nlobed\nlobed..smooth\nlobed..toothed\nsmooth\ntoothed\n\n\n\n\n0\n0\n0\n0\n1\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n1\n\n\n0\n0\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n0\n1\n0\n\n\n0\n0\n0\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n0\n0\n0"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#dummy-variables",
    "href": "slides/advanced-04-feature-engineering-part-one.html#dummy-variables",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\nPros\n\nCommonly used\nEasy interpretation\nWill rarely lead to a decrease in performance\n\n\nCons\n\nCan create many columns\nNeeds clean levels\nNot likely the most efficient representation"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#overlapping",
    "href": "slides/advanced-04-feature-engineering-part-one.html#overlapping",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Overlapping",
    "text": "Overlapping\n\n\nSome of the labels\n\nlobed\nsmooth\ntoothed\nlobed, smooth\nlobed, toothed\n\n\nIf you want to find a â€œtoothedâ€ leaf, which level do you pick?\n\ntoothed\nlobed, toothed\ntoothed and lobed, toothed\n\n\nWe can let lobed, toothed be counted for lobed and toothed"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies-1",
    "href": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies-1",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Advanced Dummies",
    "text": "Advanced Dummies\nThis is basically a poor manâ€™s Natural Language Processing.\n\n\ntokenization -&gt; counting\n\n\nWe think it is a frequent enough case that it is considered its own method.\nWe have 2 variants: â€œextractionâ€ and â€œmulti choiceâ€"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---extraction",
    "href": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---extraction",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Advanced Dummies - Extraction",
    "text": "Advanced Dummies - Extraction\nWorks on a singular column, using a regular expression to extract the items we want to count.\nDone using regular expressions, either by specifying sep to split the string by, or by using pattern to extract the items.\nAllows for 0 to many items in each string.\nImplemented as step_dummy_extract().\n\nFEAZ"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---extraction-1",
    "href": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---extraction-1",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Advanced Dummies - Extraction",
    "text": "Advanced Dummies - Extraction\n\n\nInput\n\n\"\"\n\"denate\"\n\"lobed\"\n\"lobed, smooth\"\n\"lobed, toothed\"\n\"smooth\"\n\"toothed\"\n\n\nUsing\nsep = \", \"\n\nResult\n\n\n\n\n\n\ndenate\nlobed\nsmooth\ntoothed\n\n\n\n\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n\n\n0\n1\n0\n0\n\n\n0\n1\n1\n0\n\n\n0\n1\n0\n1\n\n\n0\n0\n1\n0\n\n\n0\n0\n0\n1"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---extraction-2",
    "href": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---extraction-2",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Advanced Dummies - Extraction",
    "text": "Advanced Dummies - Extraction\n\n\nInput\n\n\"Etching on paper\"\n\"Oil paint on canvas\"\n\"Acrylic paint on paper\"\n\"Oil paint and wax on canvas\"\n\"Oil paint, ink on canvas\"\n\n\nUsing\nsep = \", \"\n\nResult\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcrylic.paint.on.paper\nEtching.on.paper\nink.on.canvas\nOil.paint\nOil.paint.and.wax.on.canvas\nOil.paint.on.canvas\n\n\n\n\n0\n1\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n1\n\n\n1\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n1\n0\n\n\n0\n0\n1\n1\n0\n0"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---extraction-3",
    "href": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---extraction-3",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Advanced Dummies - Extraction",
    "text": "Advanced Dummies - Extraction\n\n\nInput\n\n\"Etching on paper\"\n\"Oil paint on canvas\"\n\"Acrylic paint on paper\"\n\"Oil paint and wax on canvas\"\n\"Oil paint, ink on canvas\"\n\n\nUsing\nsep = \"(, )|( and )|( on )\"\n\nResult\n\n\n\n\n\nAcrylic.paint\ncanvas\nEtching\nink\nOil.paint\npaper\nwax\n\n\n\n\n0\n0\n1\n0\n0\n1\n0\n\n\n0\n1\n0\n0\n1\n0\n0\n\n\n1\n0\n0\n0\n0\n1\n0\n\n\n0\n1\n0\n0\n1\n0\n1\n\n\n0\n1\n0\n1\n1\n0\n0"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---extraction-4",
    "href": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---extraction-4",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Advanced Dummies - Extraction",
    "text": "Advanced Dummies - Extraction\n\n\nInput\n\n\"['red', 'blue']\"\n\"['red', 'blue', 'white']\"\n\"['blue', 'blue', 'blue']\"\n\n\nUsing\npattern = \"(?&lt;=')[^',]+(?=')\"\n\nResult\n\n\n\n\n\nblue\nred\nwhite\n\n\n\n\n1\n1\n0\n\n\n1\n1\n1\n\n\n3\n0\n0"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---multi-choice",
    "href": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---multi-choice",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Advanced Dummies - Multi Choice",
    "text": "Advanced Dummies - Multi Choice\nWorks on multiple columns, counting items across columns.\nIt can be seen as joining multiple applications of dummy variables together.\nAllows for 0 to many items.\nImplemented as step_dummy_multi_choice().\n\nFEAZ"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---multi-choice-1",
    "href": "slides/advanced-04-feature-engineering-part-one.html#advanced-dummies---multi-choice-1",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Advanced Dummies - Multi Choice",
    "text": "Advanced Dummies - Multi Choice\n\n\nInput\n\n\n\n\n\nlang_1\nlang_2\nlang_3\n\n\n\n\nEnglish\nNA\nItalian\n\n\nSpanish\nFrench\nFrench\n\n\nNA\nNA\nNA\n\n\nEnglish\nNA\nNA\n\n\n\n\n\n\nResult\n\n\n\n\n\nEnglish\nFrench\nItalian\nSpanish\n\n\n\n\n1\n0\n1\n0\n\n\n0\n1\n0\n1\n\n\n0\n0\n0\n0\n\n\n1\n0\n0\n0"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#othering",
    "href": "slides/advanced-04-feature-engineering-part-one.html#othering",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Othering",
    "text": "Othering\nBoth step_dummy_extract() and step_dummy_multi_choice() contain a threshold argument.\nThis is used to combine infrequent levels together.\nWe have a step step_other() that does this for nominal variables, but it doesnâ€™t work in these cases, as it has to happen after the extraction/combination."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#othering-1",
    "href": "slides/advanced-04-feature-engineering-part-one.html#othering-1",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Othering",
    "text": "Othering\nthreshold = 0 produces 1217 columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX1.monitor\nX1.person\nX1.projection\nX10.light.boxes\nX10.tranformers\nX100.digital.prints\nX100.works\nX11.photographs\nX11.works\nX12.ceramic.tea.mugs\nX12.drawings\nX13.canvases\nX133.slides\nX14.monitors\nX14.photographs\nX14.small.cubes\nX14.works\nX15.engraved.plaques\nX15.hand.coloured.photographs\nX15.photographs\nX15.rooms\nX15.steel.kitchen.ustensils\nX15.unglazed.ceramic.forms\nX151.text.panels\nX16.channels\nX16.mm\nX16.paintings\nX16.sheets\nX160.slides\nX16mm\nX18.drawings\nX18.photographs\nX19.wooden.pieces.of.furniture\nX2.aluminium.panels\nX2.aluminium.tables\nX2.back.projections\nX2.banners\nX2.canvases\nX2.car.batteries\nX2.copper.panels\nX2.digital.prints\nX2.electric.cables\nX2.flat.screens\nX2.fluorescent.tubes\nX2.hooks\nX2.horses\nX2.light.bulbs\nX2.lithographs\nX2.maps\nX2.marker.pens\nX2.metal.scaffold.towers\nX2.monitors\nX2.paintings\nX2.papers\nX2.people\nX2.photographs\nX2.porcelain.light.sockets\nX2.projections\nX2.ropes\nX2.screenprints\nX2.sections.of.pre.cast.concrete.pipe\nX2.sheets\nX2.slippers\nX2.stainless.steel.discs\nX2.trains\nX2.transparencies\nX2.videos\nX2.wall.pieces\nX2.works\nX20.cotton.mattresses\nX20.flat.screens.or.1.projection\nX20.photographs\nX20.wooden.beds\nX2000.gerberas\nX201.photographs\nX21.aluminium.bricks\nX21.channels\nX21.works\nX22.photographs\nX220.works\nX23.monitors\nX24.light.bulbs\nX24.works\nX25.photographs\nX26.works\nX27.photographs\nX28.etchings\nX3.balls\nX3.books\nX3.canvases\nX3.flat.screens\nX3.Kentia.palms.in.3.terracotta.pots\nX3.lamps\nX3.light.bulbs\nX3.monitors\nX3.people\nX3.photographs\nX3.printers\nX3.prints\nX3.projections\nX3.screenprints\nX3.skateboards\nX3.transformers\nX3.wall.mounted.LCD.monitors\nX3.works\nX30.light.emitting.diodes.units\nX30.works\nX31.photographs\nX32.digital.prints\nX32.flat.screens\nX32.papers\nX320.slides\nX34.photographs\nX34.wooden.sculptures\nX35.mm\nX35.works\nX350.digital.prints\nX355.photographs\nX35mm\nX36.tin.dogs\nX39.cardboard.boxes\nX39.metronomes\nX4.aluminium.pixel.boxes.with.DMX.control.box\nX4.Belisha.beacons\nX4.canvases\nX4.channels\nX4.corrugated.slabs.of.concrete\nX4.digital.prints\nX4.digital.prints.with.acrylic.paint\nX4.fibreboard.panels\nX4.flat.screens\nX4.gascookers\nX4.hardboard.walls\nX4.light.bulbs\nX4.mannequins\nX4.monitors\nX4.photocopies\nX4.photographs\nX4.projections\nX4.steel.capsules\nX40.light.emitting.diode.units\nX40.photographs\nX40.works\nX42.electric.lamps\nX42.photographs\nX46.photographs\nX49.Perspex.boxes\nX5.digital.prints\nX5.drawings\nX5.flat.screens\nX5.knives\nX5.monitors\nX5.painted.plaster.busts\nX5.painted.wooden.shelves\nX5.photographs\nX5.projections\nX5.tables\nX5.works\nX50.canvases\nX50.cardboard.boxes\nX50.slides\nX53.photographs\nX54.digital.prints\nX6.digital.prints\nX6.fabrics\nX6.lithographs\nX6.monitors\nX6.Perspex.panels\nX6.photographs\nX6.projections\nX6.works\nX60.digital.prints\nX60.works\nX62.florescent.lights\nX64.tin.soldiers\nX7.channels\nX7.drawings\nX7.light.emitting.diode.columns\nX7.panels\nX7.projections\nX7.stools\nX7.works\nX71.vinyl.records\nX76.works\nX8.channels\nX8.digital.prints\nX8.headphones\nX8.letterpress.prints\nX8.mm\nX8.monitors\nX8.projections\nX8.synthetic.fabric.flags\nX8.woodblock\nX8.works\nX80.slides\nX800.digital.prints\nX84.sleeves\nX9.doors\nX9.LED.lights\nX9.photographs\nX9.works\nX90.photographs\nacetate\nacrylic\nacrylic.fibre\nacrylic.glass\nacrylic.pai\nacrylic.paint\nAcrylic.paint\nacrylic.resin\nacrylic.sheet\nAcrylic.sheet\nacrylic.sheets\nAcrylic.tubes\nadhesive\nadhesive.tape\naerials\nAgfa.Isolette.camera\nAlabaster\nalarm.clocks\nAlkyd.paint\nAltered.Eames.plywood.leg.splints\naluminium\nAluminium\naluminium.buckles\naluminium.disc\naluminium.lightbox\nAluminium.machinery.part\naluminium.paint\naluminium.panel\naluminuim\nAluminuim\naluminum.panel\namplifier\nand.sound\nand.sound..mono\nand.sound..mono.\nand.sound..stereo.\nanimal.bones\nAnimal.intestines\naquatint\nAquatint\naquatint.with.hand.colouring\nartificial.foliage\nartificial.moss\nartificial.sand\nartificial.wig\nartist.s.hair\nash\nAsh\nashtray\naudio\nAudio\naudio.system\naudiotrack\nback.projection\nbadge\nbadges.with.printed.papers\nballasts\nbalsa.wood\nBamboo.cane\nBamboo.poles\nbark\nbaseball.cap\nbath.bombs\nbatteri\nbauxite.rocks\nbeads\nBeanbag\nbeech\nbeer.can\nbells\nbicycle.belt\nbindis\nbins\nblack\nblackboard\nBlind.embossed.print\nboard\nBoard\nBoat\nbone\nbones\nbook\nBook\nbook.cover\nbooklets\nbooks\nboots\nbowl\nbox\nbrass\nBrass\nbrass.pins\nbrass.plate\nbreeze.blocks\nbrick\nBriefcase\nbronze\nBronze\nBronze.bells\nbronze.powder\nBronze.with.silk.scarf\nBronze..with.Jamie.Sargeant\nBronze..with.Nicholas.Sloan\nbrooms\nbuckets\nbuckram\nbuoy\nburlaq.sack\nburnt.wood\nButterflies\nbuttons\ncable\ncables\ncalf\ncalico\ncandle\ncanvas\nCanvas\nCanvas.lining.with.ingrained.dust\ncanvas.paper\nCanvas.tacking.edges\ncarbon.fibre\ncarborundum\ncarborundum.mezzotint\ncard\nCard\ncardboard\nCardboard\ncardboard.architectural.model\ncardboard.base\ncardboard.box\nCardboard.box\ncardboard.boxes\ncardboard.coffin\ncarpet\nCarpet\ncarpets\nCarrara.marble\nCast.iron\ncastor.wheels\nCedar.wood\nceiling\nCellophane\ncellulose\ncellulose.lacquer\ncellulose.print\ncement\ncement.blocks\ncentral.processing.unit\nceramic\nCeramic\nceramic.tiles\nchain\nchair\nchair.parts\nchairs\nchalk\nChalk\ncharcoal\nCharcoal\nchine.collÃ©\nchipboard\nChipboard\nchrome\nchrome.steel.balls\nChromogenic.print\ncibachrome\ncibachrome.print\nCibachrome.print\ncigarette.ash\ncigarette.boxes\ncigarette.butts\ncigarette.sheets\ncigarettes\nclay\nClay\nclothes\ncloves\ncoal\ncoat\nCoca.cola.bottle\ncoconut.oil\ncoffee\ncoin\ncollage\ncollagraph\ncolour\ncolour.and\ncolour.ans.sound..stereo.\ncoloured.graphite\ncoloured.light.bulbs\ncoloured.pencil\ncoloured.pencils\ncolumns\nCommercial.paint\ncompressor\ncompressors\ncomputer\nConcrete\nContÃ©\nContractor.s.shed\nCooked.couscous\ncooker\ncopper\ncopper.sulphate\ncopperplate\ncopperplated\ncoral\ncord\nCork.panels\ncorrection.fluid\nCorrection.fluid\ncotton\nCotton\ncotton.costume\ncotton.thread\ncotton.wool\ncow\nCowhide\ncrayon\nCrayon\ncrude.oil\ncrystalline.particles\ncue.stand\ncues\ncurtains\nCut.paper\nDelabole.slate\ndesks\ndetritus\ndigging.tools\ndigital.image\ndigital.print\nDigital.print\nDigital.print.with.acrylic.paint\ndigital.prints\nDigital.prints.with.acrylic.paint\ndolls\ndoor\ndrawings\nDresden.D1.projector\ndress.shirt\ndry.pigment.paint\ndrypoint\nDrypoint\nduck.tape\ndye\nearth\nEarth\nearthenware\nEarthenware\nelectric.cable\nelectric.wire\nelectrical.a\nelectrical.cable\nelectrical.components\nelectrical.pump\nelectronic.circuit\nelephant.dung\nemulsion\nEmulsion\nenamel\nEnamel\nenamel.button\nenamel.paint\nEnamel.paint\nencre.de.Chine\nEngine\nengraving\nEngraving\nenvelopes\nepoxy\netching\nEtching\nEtching.with.hand.colouring\nexpanding.foam\neyebrow.pencil\neyeshadow\nfabric\nFabric\nfabric.patches\nfaced.particleboard\nfan.heater\nFeathers\nFed.2.type.camera\nfelt\nFelt\nfelt.pen\nFerric.oxide\nfibre.clay\nfibreboard\nFibreboard\nFibreboard.cabinet\nfibreboard.panels\nfibreboard.plinth\nfibreboard.with.clay\nfibreglass\nFibreglass\nfilm\nFilm\nFilm.16.mm\nflat.screen\nflat.screen.or.projector\nflax\nFlies\nFlint\nfloodlight\nflowers\nfluorescent.light\nfluorescent.lights\nfluorescent.site.jacket\nfluorescent.tubes\nflyers\nfoam\nfoam.core\nfoam.rubber\nfoil\nfoil.tape\nfood\nformaldehyde.solution\nformica\nFormica\nfound.posters\nframed.\nFuton.mattress\nGallery.lighting\ngalvanised.steel\ngalvanized.steel\ngel\ngelatin.silver.print\ngelatin.silver.printon.paper\ngelatine.silver.print.with.dye\nglas\nglass\nGlass\nglass.a\nglass.beads\nglass.box\nGlass.chandelier\nglass.vitrine.containing.beauty.and\nGlass.10.hardbacked.books\nglitter\nglove\nglue\ngobo\ngold.leaf\ngold.metallic.paint\nGold.paint\ngouache\nGouache\nGranite\ngraphite\nGraphite\ngrill\nGrpahite\nguitar.strings\ngun\nGunpowder\nhair\nhair.over.metal\nHand.coloured.photograph\nhand.colouring\nHand.cut.book\nhandcolouring\nhardboard\nhardboard.plinth\nhardboard.table\nhardwood\nhat\nheadphones\nHeart.to.Hear\nHeavy.goods.vehicle\nhelium.balloon\nhemp.cord\nhessian\nHessian\nhigh.definition\nhigh.definition_2\nhook\nhorn\nhosepipes\nHousehold.emulsion.paint\nhousehold.gloss.paint\nhousehold.paint\nHousehold.paint\nhuman.hair\nhydraulic.rams\nidentity.card\ninfrared.sensors\ningrained.dust\nink\nInk\ninteractive\nIris.print\niron\niron.powder\njacket\nJacket\njute\nkapok\nkey.\nKilkenny.limestone\nkitchen.utensils\nkite\nknife\nl\nlab.coat\nlacquer\nLacquered.wood\nladder\nlamb\nLambda.print\nlamps\nLamps\nlatex\nlead\nLead\nleather\nLeather\nleather.briefcase\nletterpress\nLetterpress\nlice\nlight\nlight.box\nlight.boxes\nlight.bulb\nlight.bulbs\nlight.control.unit\nlightbox\nLightboxes\nlighter\nlighting.system\nlights\nlinen\nlinocut\nLinocut\nLinocut.print\nlinoleum\nlithograph\nLithograph\nlithograph.with.plasticine\nLithograph.woodcut\nlithographs\nmagnetic.rods\nmagnets\nMahogany\nmango.seeds\nmannequin\nMannequin\nmap\nMap\nmap.pins\nmarble\nMarble\nmarble.base\nmarker.pen\nMarker.pen\nMDF\nMDF.backboard\nme\nmelamine\nmelanine.fibreboard\nMelinex\nmetal\nMetal\nMetal.bicycle\nmetal.bottle.caps\nmetal.bucket\nmetal.chain\nmetal.clamp\nmetal.clothes.rail\nMetal.cot\nmetal.detritus\nmetal.film.spool\nmetal.foil\nMetal.frame\nMetal.frames\nmetal.hook\nmetal.pipe\nmetal.sheet\nmetal.string\nMetal.table\nMetal.tin.cans\nmetal.wire\nmetalcut.prints\nmetallic.powder\nmezzotint\nMezzotint\nmica.flakes\nmicrophone\nmin.DV.cam.tape\nmini.disc.player\nmirror\nMirror\nmirrors\nMiss.Piggy.Bag..and.contents.\nMixed.media\nmobile.pho\nmodelling.putty\nmonitor\nmonitor.or.flat.screen\nmonitor.or.projection\nmono\nMonoprint\nmonotype\nMonotype\nmorse.code.unit\nmotor\nmotorised.base\nmoun\nMountain.bicycle\nMountain.peak.in.container\nmountaineering.equipment\nmounted\nmounted.onto.aluminium\nMud\nmultiple.projections\nmuslin\nMylar.screen\nnails\nnatural.fibres\nneon.gas\nneon.lights\nNeon.lights\nneon.paint\nnewspapers\nnotebook\nnylon\nnylon.straps\nnylon.strings\nNylon.tights\nnyloprints\noak\nOak\noak.table\noak.twig\noffice\noil\noil.paint\nOil.paint\noil.pastel\noil.stick\nOil.stick\noil.tint\non.47.panels\non.aluminium\non.aluminium.panel\non.board\non.cardboard\non.paper\non.paper.between.glass\non.paper.mounted\non.paper.mounted.onto.acrylic.glass\non.paper.mounted.onto.aluminium\non.paper.mounted.onto.aluminium.panel\non.paper.mounted.onto.aluminuim\non.paper.mounted.onto.board\non.paper.mounted.onto.panel\non.paper.mounted.onto.paper\non.paper.mounted.onto.Perspex\non.paper.mounted.onto.plastic\non.paper.with.chalk\non.paper.with.dry.transfert.print.mounted.onto.paper\non.paper.with.paint\non.papers\non.papers.with.paint\non.plastic\non.sticker.paper\non.vinyl.mounted.onto.aluminium\nopen.cell.foam\noptical.gelatin.silver.fibre.print\nor.video\nOrgan\nOstrich.egg\nother.m\nother.materia\nother.materials\nothers.materials\noxidised.brass\np\npaint\nPaint\nPaint.brush\npainted.aluminium\nPainted.aluminium\npainted.boards\nPainted.fibreboard\npainted.glass\npainted.MDF\nPainted.steel\npainted.wall\npainted.wall.text\npainted.wood\nPainted.wood\nPainted.wooden.building.with.asphalt.shingle.roof\npalm.fronds\nPalm.tree\npanel\npaper\nPaper\npaper.mounted\npaper.mounted.onto.aluminium\npaper.mounted.onto.aluminium.panel\npaper.mounted.onto.board\npaper.mounted.onto.foam.core\npaper.mounted.onto.muslin\npaper.mounted.onto.panel\npaper.mounted.onto.paper\npaper.pulp\npaper.tape\npaper.with.dry.transfer.print\npaper.with.dye\npaper.with.ink\npaper.with.oil.paint\npaper.with.watercolour\npaper..Verso..ink\npapers\nparaffin\nparaffin.lamp\npassport\npastel\nPastel\npatina\npeacock.feathers\npen\npendant.lamps\npeople\nPerformance\nperfume.flask.with.pouch\nperspex\nPerspex\nPewter\npharmaceutical.packaging\nphoto.etching\nPhoto.etching\nphotograph\nPhotograph\nphotograph.mounted\nPhotographic.contact.sheet\nphotographs\nPhotographs\nphotographs.\nPiano\npigment\nPigment\nPigment.transfer\npigments\nPine.plywood\npins\nplants\nplaster\nPlaster\nplaster.figures\nplaster.powder\nplasterboard\nplastic\nPlastic\nplastic.bag\nplastic.bags\nplastic.beads\nPlastic.beads\nplastic.box\nplastic.boxes\nplastic.cover\nPlastic.lids\nplastic.pearls\nplastic.pen.lids\nplastic.pillow\nplastic.shoe\nplastic.thread\nplastic.tubes\nplastic.watch.with.photograph\nplastic.watering.can\nplastic.watering.can..spray.bot\nplasticine\nPlasticine\nplatinium.print\nPlexiglas\nplexiglass\nplinth\nplug.boards\nplywood\nPlywood\nplywood.board\npo\nPolychromed.aluminium\nPolyester\npolyester.foam\npolyester.resin\nPolyester.resin\npolyester.textile\npolyfibre\npolymer\npolystyrene\npolystyrene.foam\npolythene\npolyurethane\npolyurethane.foam\nPolyurethane.resin\nPolyurethane.rubber\npolyvinyl.acetate.paint\nPolyvinyl.acetate.paint\nporcelain\nPorcelain\nporcelain.model\nporcelaine\nportable.keyboard.keys\nPortorino.marble\npostcard\nPostcard\npostcards\nposters\nPotato.print\nPowder.coated.aluminium\npowder.coated.steel\nPowder.coated.steel\npowder.paint\npowder.coated.steel_2\nPowder.coated.steel_2\nprint\nPrint\nprinted.map\nprinted.p\nprinted.paper\nPrinted.paper\nprinted.papers\nPrinted.papers\nprojection\nprojection.or.7.monitors\nprojection.or.monitor\npumps\nPVC\nradio\nradio.transmitter\nramin\nrecipes\nRecord.deck\nrecord.player\nrecorded.voice\nreel.to.reel.tape.deck\nrelief\nRelief\nRelief.print\nresin\nResin\nresin.base\nresin.block\nribbon\nRibbon\nRiver.mud\nRoman.vessel\nrope\nropes\nrosary.beads\nrubber\nRubber\nrubber.coated.steel\nRubber.inner.tubes\nrubber.padding\nsafety.pins\nsalt\nsand\nsatin\nsatin.ribbon\nsawdust\nSax.oil.paint\nscanachrome.print\nScanachrome.print\nscissors\nscreenprint\nScreenprint\nScreenprint.with.acrylic.varnish\nScreenprints\nscrews\nsea.shells\nseat\nsection.of.concrete.wall\nsellotape\nSellotape\nSequins\nsewing.machine\nsheep.excrement\nshellac\nshellac.resin\nshells\nShells\nshield\nshirt\nshoeboxes\nshoelace\nshoes\nShop.mannequin\nshown.as\nshown.as.video\nshredder\nsilicon.hose\nsilicon.tubing\nsilicone\nsilicone.adhesive\nsilicone.rubber\nSilicone.rubber\nsilk.tie\nsilkcreen.print\nsilkscreen\nSilkscreen\nSilkscreen.print\nsilver\nsilver.foil\nsilver.gelatin.print\nsilver.solder\nSilver.teapot\nSilverpoint\nsisal.rope\nsisal.string\nSlate\nslide\nSlide\nslides\nsmall.speakers\nsmoke\nsmoke.machine\nso\nsoap\nSoap\nsocks\nSofa\nsoftware\nSoftware\nSondor.playback.machine\nsound\nsound..mono\nsound..mono.\nsound..stereo.\nsound..surround.\nsound.recording\nsound..wood\nspeakers\nspices\nspinning.top\nspray.enamel\nspray.paint\nSpray.paint\nSprayed.Q.Cell\nsreenprint\nStack.of.printed.paper\nstainless.steel\nStainless.steel\nstainless.steel.ashtray\nstainless.steel.screws\nstainless.steel.tea.urn\nstainless.steel.teapot\nstainless.steel.wire\nstand\nstaple.gun\nStarfish\nsteel\nSteel\nsteel.bars\nSteel.bedsprings\nsteel.bracket\nsteel.brake.wires\nsteel.cable\nSteel.locker\nSteel.plate\nsteel.screws\nsteel.shelving\nsteel.wire\nstencil\nstereo\nstereo.\nstickers\nstockings\nstone.base\nstone..with.Jamie.Sargeant\nstool\nstoryboards\nstraw\nstrin\nstring\nstyrene\nstyrofoam\nsugar.paper\nsuitcase\nSuper.16.mm\nSuper.8\nSuper.8.mm\nsweet.wrappers\nswing.seat\nsynthetic.fibre\nsynthetic.fibres\ntable\nTable\ntables\ntape\ntar\ntarpaulin\ntaxidermy\ntea\ntelephone\nTempera\ntent\ntextile\nTextile\ntextiles\nTextiles\ntheatre.light\nthread\nthreads\ntickets\ntights\nTimber\ntissue\ntoilet\ntooth.b\ntorchlight\ntoy.eyeballs\ntracing.paper\ntracking.system\ntram.window\ntransfer.lettering\ntransparency\nTransparency\ntransparent.paper\ntree.branches\ntripod\ntrousers\ntwigs\ntwo.pack.car.lacquer\ntype\ntypescript\ntypewritten.ink\nUHF.Radio\numbrella\nunderpants\nvarnish\nvelvet\nvelvet.p\nVHS.tape\nvideo\nVideo\nvideo.camera\nviewfinder\nvinyl\nVinyl.dispersion\nvinyl.floor.covering\nvinyl.paint\nVinyl.record\nvinyl.seat\nVinyl.tape\nvinyl.text\nvinyl.wall.text\nVinyl.wall.text\nvinyl.wall.texts\nwalkman\nWalkman\nwall\nwall.clock\nWall.painting\nwall.text\nWallpaper\nWashers\nwater\nwatercolour\nWatercolour\nwax\nweb.search.program\nwebbing\nWestern.red.cedar\nwheels\nwhite\nwire\nwire.coat.hanger.\nwires\nwood\nWood\nWood.engraving\nwood.tray\nwood.varnish\nwood.veneer\nWoodblock\nwoodcut\nWoodcut\nWooden.abacus.beads\nwooden.base\nwooden.beams\nWooden.billiard.table\nWooden.birdhouse.with.metal.roof\nwooden.boxes\nWooden.cabinet\nwooden.ceiling.props\nwooden.chair\nWooden.chair\nWooden.construction\nWooden.desk\nwooden.dowels\nwooden.floor\nwooden.frames\nwooden.glass.negative.plate.carrier\nwooden.pallets\nwooden.panel\nwooden.planks\nwooden.platform\nwooden.steps\nwooden.table\nWooden.table\nwooden.trestle\nwool\nWool\nwork.overalls\nworks\nWorry.beads\nwraps\nzinc\nother\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#othering-2",
    "href": "slides/advanced-04-feature-engineering-part-one.html#othering-2",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Othering",
    "text": "Othering\nthreshold = 0.05 produces 11 columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naquatint\ncanvas\ncolour\nEtching\ngelatin.silver.print\nLithograph\non.paper\npaper\nPhotograph\nScreenprint\nother\n\n\n\n\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n3\n\n\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#your-turn-1",
    "href": "slides/advanced-04-feature-engineering-part-one.html#your-turn-1",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Your turn",
    "text": "Your turn\n\nFigure out whether step_dummy_extract() or step_dummy_multi_choice() is most appropriate on the edge variable in leaf_data and apply it\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#dimensionality-reduction-1",
    "href": "slides/advanced-04-feature-engineering-part-one.html#dimensionality-reduction-1",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nWhat is it?\nTechniques that remove or alter features in order to have fewer but more informative features.\nWhy do we do it?\n\nredundant information\nineffective representation\ncomputational speed"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#redundant-information",
    "href": "slides/advanced-04-feature-engineering-part-one.html#redundant-information",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Redundant Information",
    "text": "Redundant Information\nA feature with no information could be included.\n\nleaf_data |&gt;\n  count(outlying_contour)\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   outlying_contour     n\n#&gt;              &lt;dbl&gt; &lt;int&gt;\n#&gt; 1                0  1907"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#redundant-information-1",
    "href": "slides/advanced-04-feature-engineering-part-one.html#redundant-information-1",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Redundant Information",
    "text": "Redundant Information"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#ineffective-representation",
    "href": "slides/advanced-04-feature-engineering-part-one.html#ineffective-representation",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Ineffective Representation",
    "text": "Ineffective Representation\nWhile keeping our models in mind, we want to make sure the data is well-suited\n\nCorrelated data\n\nhard for some models\n\nlat/lon compared to distance/angle\n\nhard for most models"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#ineffective-representation-1",
    "href": "slides/advanced-04-feature-engineering-part-one.html#ineffective-representation-1",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Ineffective Representation",
    "text": "Ineffective Representation"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#ineffective-representation-2",
    "href": "slides/advanced-04-feature-engineering-part-one.html#ineffective-representation-2",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Ineffective Representation",
    "text": "Ineffective Representation"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#computational-speed",
    "href": "slides/advanced-04-feature-engineering-part-one.html#computational-speed",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Computational Speed",
    "text": "Computational Speed\nDepending on what method we are using and how the data is affected by it, we could see a large reduction in features. This, in turn, leads to a smaller model that is faster to train on.\nOnly exploration and trial and error can determine whether you should use dimensionality reduction techniques. Knowing which methods do what helps you determine what to try."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#dimensionality-reduction-method",
    "href": "slides/advanced-04-feature-engineering-part-one.html#dimensionality-reduction-method",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Dimensionality Reduction Method",
    "text": "Dimensionality Reduction Method\n\nZero Variance removal\nPCA\n\nTruncated PCA\nSparse PCA\n\nNNMF\nUMAP\nIsomap"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#restrictions",
    "href": "slides/advanced-04-feature-engineering-part-one.html#restrictions",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Restrictions",
    "text": "Restrictions\nAll the methods shown today will not be able to handle\n\nmissing data\nNon-numeric data"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#why-not-t-sne",
    "href": "slides/advanced-04-feature-engineering-part-one.html#why-not-t-sne",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Why not t-SNE?",
    "text": "Why not t-SNE?\nOne of the main requirements for a feature engineering method is that you can reapply the trained transformation done on the training data set to the testing data set.\nThis is not possible with t-SNE as it is an iterative method that shifts observations in the lower-dimensional space based on their distances to points in the higher-dimensional space.\nIt doesnâ€™t create a mapping that can be reused.\n\nVisualizing Data using t-SNE, Laurens van der Maaten and Geoffrey Hinton, Journal of Machine Learning Research, 2008."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#pca",
    "href": "slides/advanced-04-feature-engineering-part-one.html#pca",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "PCA",
    "text": "PCA\nPrincipal Compoment Analysis is a linear combination of the original data such that most of the variation is captured in the first variable, then the second, then the third, and so on.\n\nFEAZ, FES"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#pca-algorithm",
    "href": "slides/advanced-04-feature-engineering-part-one.html#pca-algorithm",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "PCA Algorithm",
    "text": "PCA Algorithm\nThe first principal component of a set of features \\(X_1, X_2, ..., X_p\\) is the normalized linear combination of the features.\n\\[\nZ_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + ... + \\phi_{p1}X_p\n\\]\nthat has the largest variance under the constraint that \\(\\sum_{j=1}^p  \\phi_{j1}^2 = 1\\).\nwe refer to \\(\\phi_{11}, ..., \\phi_{p1}\\) as the loadings of the first principal component.\nAnd think of them as the loading vector \\(\\phi_1\\)."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#pca-algorithm-1",
    "href": "slides/advanced-04-feature-engineering-part-one.html#pca-algorithm-1",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "PCA Algorithm",
    "text": "PCA Algorithm\nsince we have \\(z_{i1} = \\phi_{11} x_{i1} + \\phi_{21} x_{i2} + ... + \\phi_{p1}x_{ip}\\), then we can write\n\\[\n\\underset{\\phi_{11}, ..., \\phi_{p1}}{\\text{maximize}} \\left\\{ \\dfrac{1}{n} \\sum^n_{i=j} z_{i1} ^2 \\right\\} \\quad \\text{subject to} \\quad \\sum_{j=1}^p  \\phi_{j1}^2 = 1\n\\]\nWe are, in essence, maximizing the sample variance of the \\(n\\) values of \\(z_{i1}\\).\nWe refer to \\(z_{11}, ..., z_{n1}\\) as the scores of the first principal component."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#pca-algorithm-2",
    "href": "slides/advanced-04-feature-engineering-part-one.html#pca-algorithm-2",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "PCA Algorithm",
    "text": "PCA Algorithm\nLuckily, this can be solved using techniques from Linear Algebra, more specifically, it can be solved using an eigen decomposition.\nOne of the main strengths of PCA is that you donâ€™t need to use optimization to get the results without approximations."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#pca-algorithm-3",
    "href": "slides/advanced-04-feature-engineering-part-one.html#pca-algorithm-3",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "PCA Algorithm",
    "text": "PCA Algorithm\nOnce the first principal component is calculated, we can calculate the second principal component.\nWe find the second principal component \\(Z_2\\) as a linear combination of \\(X_1, ..., X_p\\) that has the maximal variance out of the linear combinations that are uncorrelated with \\(Z_1\\)\nthis is the same as saying that \\(\\phi_2\\) should be orthogonal to the direction \\(\\phi_1\\)"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#how-is-that-a-dimensionality-reduction-method",
    "href": "slides/advanced-04-feature-engineering-part-one.html#how-is-that-a-dimensionality-reduction-method",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "How is that a dimensionality reduction method?",
    "text": "How is that a dimensionality reduction method?\nBy itself, it isnâ€™t, as it rotates all the features in the feature space.\nIt becomes a dimensionality reduction method if we only calculate some of the principal components.\nThis is typically done by retaining a specific number of components or as a threshold on the variance explained."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#different-percent-variance-plots",
    "href": "slides/advanced-04-feature-engineering-part-one.html#different-percent-variance-plots",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "4 different percent variance plots",
    "text": "4 different percent variance plots"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#applying-pca-with-recipes",
    "href": "slides/advanced-04-feature-engineering-part-one.html#applying-pca-with-recipes",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Applying PCA with recipes ",
    "text": "Applying PCA with recipes \nEither use the num_comp argument.\n\nrec &lt;- recipe(mpg ~ ., data = mtcars) |&gt;\n  step_normalize(all_predictors()) |&gt;\n  step_pca(all_numeric_predictors(), num_comp = 5)\n\n\nor using the threshold argument\n\nrec &lt;- recipe(mpg ~ ., data = mtcars) |&gt;\n  step_normalize(all_predictors()) |&gt;\n  step_pca(all_numeric_predictors(), threshold = 0.8)"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#your-turn-2",
    "href": "slides/advanced-04-feature-engineering-part-one.html#your-turn-2",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Your turn",
    "text": "Your turn\n\nApply PCA using step_pca() to leaf_data data set.\nExperiment with different values of num_comp and or threshold.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#pca-pros-and-cons",
    "href": "slides/advanced-04-feature-engineering-part-one.html#pca-pros-and-cons",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "PCA Pros and Cons",
    "text": "PCA Pros and Cons\n\n\nPros\n\nFast\nReliable\nExact results (up to sign changes)\n\n\nCons\n\nComputational time is linear in the number of columns\nCan be quite hard to interpret"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#truncated-pca",
    "href": "slides/advanced-04-feature-engineering-part-one.html#truncated-pca",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Truncated PCA",
    "text": "Truncated PCA\n\nComputational time is linear in the number of columns\n\nBy default, step_pca() calculates all the loading vectors. And then subset them down to what you need.\nInstead, we can use a different implementation that only calculates what you need. This is what we call truncated PCA.\n\nFEAZ"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#truncated-pca-with-recipes",
    "href": "slides/advanced-04-feature-engineering-part-one.html#truncated-pca-with-recipes",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Truncated PCA with recipes  ",
    "text": "Truncated PCA with recipes  \nCan only be done using num_comp\n\nlibrary(embed)\n\nrec &lt;- recipe(mpg ~ ., data = mtcars) |&gt;\n  step_normalize(all_predictors()) |&gt;\n  step_pca_truncated(all_numeric_predictors(), num_comp = 5)"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#sparse-pca",
    "href": "slides/advanced-04-feature-engineering-part-one.html#sparse-pca",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Sparse PCA",
    "text": "Sparse PCA\n\nCan be quite hard to interpret\n\nEvery component is a linear combination of all predictors.\n\\[\n\\begin{alignat}{4}\nPC1 &= cyl \\cdot -0.021 &&+ disp \\cdot -0.85 &&+ hp \\cdot -0.52\\\\\nPC2 &= cyl \\cdot 0.013 &&+ disp \\cdot -0.52 &&+ hp \\cdot 0.85\\\\\nPC3 &= cyl \\cdot -0.12 &&+ disp \\cdot 0.016 &&+ hp \\cdot 0.081\\\\\nPC4 &= cyl \\cdot -0.22 &&+ disp \\cdot -0.0061 &&+ hp \\cdot 0.033\\\\\nPC5 &= cyl \\cdot 0.73 &&+ disp \\cdot -0.014 &&+ hp \\cdot 0.0016\n\\end{alignat}\n\\]\nIf we could force some of the loadings to be 0, it would reduce things a lot.\n\nFEAZ"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#sparse-pca-with-recipes",
    "href": "slides/advanced-04-feature-engineering-part-one.html#sparse-pca-with-recipes",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Sparse PCA with recipes  ",
    "text": "Sparse PCA with recipes  \nCan only be done using num_comp.\nThe predictor_prop argument is used to determine how many zeroes in the loadings.\n\nlibrary(embed)\n\nrec &lt;- recipe(mpg ~ ., data = mtcars) |&gt;\n  step_normalize(all_predictors()) |&gt;\n  step_pca_sparse(all_numeric_predictors(), num_comp = 5, predictor_prop = 0.8)"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#sparse-pca-with-recipes-1",
    "href": "slides/advanced-04-feature-engineering-part-one.html#sparse-pca-with-recipes-1",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Sparse PCA with recipes  ",
    "text": "Sparse PCA with recipes  \npredictor_prop = 0.8\n\nrecipe(mpg ~ ., data = mtcars) |&gt;\n  step_normalize(all_predictors()) |&gt;\n  step_pca_sparse(all_numeric_predictors(), num_comp = 4, predictor_prop = 0.8) |&gt;\n  prep() |&gt;\n  tidy(number = 2) |&gt;\n  pivot_wider(names_from = component, values_from = value) |&gt;\n  select(-id)\n#&gt; # A tibble: 10 Ã— 5\n#&gt;    terms      PC1    PC2     PC3      PC4\n#&gt;    &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 cyl   -0.503    0      0.0755  0      \n#&gt;  2 disp  -0.504    0      0       0.227  \n#&gt;  3 hp    -0.377   -0.228 -0.0985  0      \n#&gt;  4 drat   0.267   -0.262  0       0.911  \n#&gt;  5 wt    -0.420    0.105 -0.383   0.236  \n#&gt;  6 qsec   0        0.473 -0.425   0.0966 \n#&gt;  7 vs     0.313    0.203 -0.429  -0.0774 \n#&gt;  8 am     0.0687  -0.440  0.191  -0.00802\n#&gt;  9 gear   0       -0.482 -0.237  -0.208  \n#&gt; 10 carb  -0.00616 -0.422 -0.617  -0.0654"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#sparse-pca-with-recipes-2",
    "href": "slides/advanced-04-feature-engineering-part-one.html#sparse-pca-with-recipes-2",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Sparse PCA with recipes  ",
    "text": "Sparse PCA with recipes  \npredictor_prop = 0.2\n\nrecipe(mpg ~ ., data = mtcars) |&gt;\n  step_normalize(all_predictors()) |&gt;\n  step_pca_sparse(all_numeric_predictors(), num_comp = 4, predictor_prop = 0.2) |&gt;\n  prep() |&gt;\n  tidy(number = 2) |&gt;\n  pivot_wider(names_from = component, values_from = value) |&gt;\n  select(-id)\n#&gt; # A tibble: 10 Ã— 5\n#&gt;    terms   PC1    PC2    PC3   PC4\n#&gt;    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 cyl   0.712  0      0     0    \n#&gt;  2 disp  0.702  0      0     0    \n#&gt;  3 hp    0      0      0     0    \n#&gt;  4 drat  0      0      0     0.978\n#&gt;  5 wt    0      0     -0.278 0    \n#&gt;  6 qsec  0      0.666  0     0    \n#&gt;  7 vs    0      0      0     0    \n#&gt;  8 am    0      0      0     0.211\n#&gt;  9 gear  0     -0.746  0     0    \n#&gt; 10 carb  0      0     -0.961 0"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#nnmf",
    "href": "slides/advanced-04-feature-engineering-part-one.html#nnmf",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "NNMF",
    "text": "NNMF\nNon-Negative Matrix Factorization is conceptually similar to PCA, but it has different objectives.\nPCA aims to generate uncorrelated components that maximize the variances. One component at a time.\nNNMF, on the other hand, simultaneously optimizes all the components under the constraint that all the loadings are non-negative. While the data is also non-negative.\n\nFEAZ FES"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#nnmf-restriction",
    "href": "slides/advanced-04-feature-engineering-part-one.html#nnmf-restriction",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "NNMF restriction",
    "text": "NNMF restriction\nThe data has to be non-negative, i.e., 0 or higher.\nThis might feel like a pretty big restriction. And that is not wrong. But a lot of data sets end up being naturally non-negative.\nOr could at least be turned into non-negative ones with transformations. As long as you donâ€™t scale them below 0.\nIt makes for much easier interpretations as the loadings donâ€™t cancel each other out like they do for PCA."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#nnmf-pros-and-cons",
    "href": "slides/advanced-04-feature-engineering-part-one.html#nnmf-pros-and-cons",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "NNMF Pros and Cons",
    "text": "NNMF Pros and Cons\n\n\n\nMore interpretable results\nPulls out better structures\n\n\n\nData must be non-negative\nComputationally expensive\nTraining depends on the seed"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#nnmf-with-recipes",
    "href": "slides/advanced-04-feature-engineering-part-one.html#nnmf-with-recipes",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "NNMF with recipes  ",
    "text": "NNMF with recipes  \n\nset.seed(1234)\n\nrecipe(mpg ~ ., data = mtcars) |&gt;\n  step_nnmf_sparse(all_numeric_predictors(), num_comp = 2) |&gt;\n  prep() |&gt;\n  tidy(number = 1) |&gt;\n  pivot_wider(names_from = component, values_from = value) |&gt;\n  select(-id)\n#&gt; # A tibble: 10 Ã— 3\n#&gt;    terms    NNMF1   NNMF2\n#&gt;    &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 am    0        0.00631\n#&gt;  2 carb  0.00428  0.0249 \n#&gt;  3 cyl   0.0134   0.0177 \n#&gt;  4 disp  0.639    0      \n#&gt;  5 drat  0.00545  0.0188 \n#&gt;  6 gear  0.00509  0.0240 \n#&gt;  7 hp    0.294    0.840  \n#&gt;  8 qsec  0.0316   0.0606 \n#&gt;  9 vs    0.000293 0.00247\n#&gt; 10 wt    0.00742  0.00519"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#umap",
    "href": "slides/advanced-04-feature-engineering-part-one.html#umap",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "UMAP",
    "text": "UMAP\nUniform Manifold Approximation and Projection is another method that takes high-dimensional data and transforms it into a lower-dimensional space.\nRuns relatively fast and is popular in visualizations."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#umap-algorithm",
    "href": "slides/advanced-04-feature-engineering-part-one.html#umap-algorithm",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "UMAP Algorithm",
    "text": "UMAP Algorithm\nRough algorithm\n\nUse spectral embedding to embed points in a low-dimensional space\nCalculate similarity scores between points based on the original data set\nRandomly samples a pair of points based on their similarity scores\nFlips a coin to decide which of the pair of points to give to the other one\nRandomly picks a non-neighbor point to move away from\nMoves the selected point towards its neighbor and away from its non-neighbor\nRepeat 3-6"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#umap-parameters",
    "href": "slides/advanced-04-feature-engineering-part-one.html#umap-parameters",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "UMAP parameters",
    "text": "UMAP parameters\n\n\n\nn_neighbors\n\n\nDetermines how many points are considered neighbors. A point counts as its own neighbor. Lower values lead to a local view.\n\n\n\n\nmin_dist\n\n\nDetermines how close points are allowed to be to each other in the low-dimensional space.\n\n\n\n\nmetric\n\n\nHow distances are calculated in the input data: euclidean, manhattan, jaccard, etc."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#umap-hesitancy",
    "href": "slides/advanced-04-feature-engineering-part-one.html#umap-hesitancy",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "UMAP hesitancy",
    "text": "UMAP hesitancy\nDue to the flexibility of how this method works, it is almost always possible to generate graphs that appear to have insights in them."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#which-was-created-with-random-data",
    "href": "slides/advanced-04-feature-engineering-part-one.html#which-was-created-with-random-data",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Which was created with random data?",
    "text": "Which was created with random data?"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#all-of-them-different-n_neighbors",
    "href": "slides/advanced-04-feature-engineering-part-one.html#all-of-them-different-n_neighbors",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "All of them! Different n_neighbors",
    "text": "All of them! Different n_neighbors"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#umap-with-recipes",
    "href": "slides/advanced-04-feature-engineering-part-one.html#umap-with-recipes",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "umap with recipes  ",
    "text": "umap with recipes  \n\nlibrary(embed)\nset.seed(1234)\n\nrecipe(mpg ~ ., data = mtcars) |&gt;\n  step_umap(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(NULL)\n#&gt; # A tibble: 32 Ã— 3\n#&gt;      mpg  UMAP1 UMAP2\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  21    1.50   2.26\n#&gt;  2  21    1.26   2.02\n#&gt;  3  22.8  2.17   4.19\n#&gt;  4  21.4 -1.38  -1.67\n#&gt;  5  18.7 -2.67  -2.66\n#&gt;  6  18.1 -0.759 -1.57\n#&gt;  7  14.3 -3.19  -3.37\n#&gt;  8  24.4  1.31   3.91\n#&gt;  9  22.8  2.06   2.46\n#&gt; 10  19.2  0.876  2.31\n#&gt; # â„¹ 22 more rows"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#isomap",
    "href": "slides/advanced-04-feature-engineering-part-one.html#isomap",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Isomap",
    "text": "Isomap\nIsometric mapping is a non-linear dimensionality reduction method.\nThis is another method that uses distances between points to produce graphs of neighboring points. Where this method is different than other methods is that it uses geodesic distances as opposed to straight-line distances.\nThe geodesic distance is the sum of edge weights along the shortest path between two points.\nThe eigenvectors of the deodesic distance metric are then used to represent the new coordinates."
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#isomap-algorithm",
    "href": "slides/advanced-04-feature-engineering-part-one.html#isomap-algorithm",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Isomap Algorithm",
    "text": "Isomap Algorithm\nA very high-level description of the Isomap algorithm is given below.\n\nFind the neighbors for each point\nConstruct the neighborhood graph, using Euclidean distance as edge length\nCalculate the shortest path between each pair of points\nUse Multidimensional scaling to compute a lower-dimensional embedding"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#isomap-pros-and-cons",
    "href": "slides/advanced-04-feature-engineering-part-one.html#isomap-pros-and-cons",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "Isomap Pros and Cons",
    "text": "Isomap Pros and Cons\n\n\nPros\n\nCaptures non-linear effects\nCaptures long-range structure, not just local structure\nNo parameters to set other than neighbors\n\n\nCons\n\nComputationally expensive\nAssumes a single connected manifold"
  },
  {
    "objectID": "slides/advanced-04-feature-engineering-part-one.html#isomap-with-recipes",
    "href": "slides/advanced-04-feature-engineering-part-one.html#isomap-with-recipes",
    "title": "4 - Feature engineering: dummies and embeddings",
    "section": "isomap with recipes ",
    "text": "isomap with recipes \n\nlibrary(embed)\nset.seed(1234)\n\nrecipe(mpg ~ ., data = mtcars) |&gt;\n  step_isomap(all_numeric_predictors(), neighbors = 10) |&gt;\n  prep() |&gt;\n  bake(NULL)\n#&gt; # A tibble: 32 Ã— 6\n#&gt;      mpg Isomap1 Isomap2 Isomap3 Isomap4 Isomap5\n#&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1  21    -98.5     35.6    35.3   137.    105. \n#&gt;  2  21    -98.5     35.6    35.1   137.    104. \n#&gt;  3  22.8 -161.      48.1    20.4   160.    129. \n#&gt;  4  21.4    8.55    14.6    37.2    86.8   125. \n#&gt;  5  18.7  136.      28.5    35.7   -30.8   102. \n#&gt;  6  18.1  -27.7     21.7    38.3   104.    116. \n#&gt;  7  14.3  207.     -16.5    34.6   -55.5   158. \n#&gt;  8  24.4 -147.      44.2    23.7   182.     88.4\n#&gt;  9  22.8 -125.      41.1    18.1   151.    118. \n#&gt; 10  19.2  -91.6     33.7    48.6   135.    101. \n#&gt; # â„¹ 22 more rows"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#startup",
    "href": "slides/advanced-06-postprocessing.html#startup",
    "title": "6 - Postprocessing",
    "section": "Startup!   ",
    "text": "Startup!   \n\nlibrary(tidymodels)\nlibrary(desirability2)\nlibrary(probably)\nlibrary(mirai)\n\n# check torch:\nif (torch::torch_is_installed()) {\n  library(torch)\n}\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\ndaemons(parallel::detectCores())"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#more-startup",
    "href": "slides/advanced-06-postprocessing.html#more-startup",
    "title": "6 - Postprocessing",
    "section": "More startup! ",
    "text": "More startup! \n\n# Load our example data for this section\n\"https://raw.githubusercontent.com/tidymodels/\" |&gt; \n  paste0(\"workshops/main/slides/class_data.RData\") |&gt; \n  url() |&gt; \n  load()\n\nset.seed(429)\nsim_split &lt;- initial_split(class_data, prop = 0.75, strata = class)\nsim_train &lt;- training(sim_split)\nsim_test  &lt;- testing(sim_split)\n\nset.seed(523)\nsim_rs &lt;- vfold_cv(sim_train, v = 10, strata = class)"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#our-neural-network-model",
    "href": "slides/advanced-06-postprocessing.html#our-neural-network-model",
    "title": "6 - Postprocessing",
    "section": "Our neural network model    ",
    "text": "Our neural network model    \n\nrec &lt;- \n  recipe(class ~ ., data = sim_train) |&gt; \n  step_normalize(all_numeric_predictors())\n\nnnet_spec &lt;- \n  mlp(hidden_units = tune(), penalty = tune(), learn_rate = tune(), \n      epochs = 100, activation = tune()) |&gt; \n  # Remove the class_weights argument\n  set_engine(\"brulee\", stop_iter = 10) |&gt; \n  set_mode(\"classification\")\n  \nnnet_wflow &lt;- workflow(rec, nnet_spec)\n\nnnet_param &lt;- \n  nnet_wflow |&gt; \n  extract_parameter_set_dials()"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#adjusting-model-predictions",
    "href": "slides/advanced-06-postprocessing.html#adjusting-model-predictions",
    "title": "6 - Postprocessing",
    "section": "Adjusting model predictions",
    "text": "Adjusting model predictions\nHow can we modify our predictions? Some examples:\n\nFixing calibration issues*.\nLimit the range of predictions.\nAlternative cutoffs for binary data.\nDeclining to predict.\n\n\n* Requires further estimation (and data).\n\nLetâ€™s first consider the easiest case: alternative cutoffs."
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#alternative-thresholds",
    "href": "slides/advanced-06-postprocessing.html#alternative-thresholds",
    "title": "6 - Postprocessing",
    "section": "Alternative thresholds",
    "text": "Alternative thresholds\nInstead of up-weighting the samples in the minority (via class_weights), we can try to fit the best model and then define what it means to be an â€œevent.â€\n\nInstead of using a 50% threshold, we might lower the level of evidence needed to call a prediction an event.\n\nHow do we tune the threshold?"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#tailors",
    "href": "slides/advanced-06-postprocessing.html#tailors",
    "title": "6 - Postprocessing",
    "section": "Tailors ",
    "text": "Tailors \nThe tailor package is similar to recipes but specifies how to adjust predictions.\nA simple example:\n\nthrsh_tlr &lt;-\n  tailor() |&gt;\n  adjust_probability_threshold(threshold = 1 / 3)\n\nthrsh_tlr\n\n\nLike a recipe, this initial call doesnâ€™t do anything but declare intent.\nUnlike a recipe, it does not need the data (i.e., predictions) at this point.\n\nRelevant prediction columns are selected when fit() is used (next slide)."
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#manual-use-of-a-tailor",
    "href": "slides/advanced-06-postprocessing.html#manual-use-of-a-tailor",
    "title": "6 - Postprocessing",
    "section": "Manual use of a tailor ",
    "text": "Manual use of a tailor \nThere is a fit() method that requires data and the names of the prediction columns:\n\nthree_rows &lt;- \n  tribble(\n     ~ class, ~ .pred_class, ~.pred_event, ~.pred_nonevent,\n     \"event\",       \"event\",          0.6,             0.4,\n     \"event\",    \"nonevent\",          0.4,             0.6, \n  \"nonevent\",    \"nonevent\",          0.1,             0.9  \n  ) |&gt; \n  mutate(across(where(is.character), factor))\n\nthrsh_fit &lt;-\n  thrsh_tlr |&gt;\n  fit(\n    three_rows,\n    outcome = class,\n    estimate = .pred_class,\n    .pred_event:.pred_nonevent  # No argument name and order matches factor levels\n  )"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#manual-use-of-a-tailor-1",
    "href": "slides/advanced-06-postprocessing.html#manual-use-of-a-tailor-1",
    "title": "6 - Postprocessing",
    "section": "Manual use of a tailor ",
    "text": "Manual use of a tailor \npredict() applies the adjustments:\n\nthrsh_fit\n\npredict(thrsh_fit, three_rows)\n#&gt; # A tibble: 3 Ã— 4\n#&gt;   class    .pred_class .pred_event .pred_nonevent\n#&gt;   &lt;fct&gt;    &lt;fct&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1 event    event               0.6            0.4\n#&gt; 2 event    event               0.4            0.6\n#&gt; 3 nonevent nonevent            0.1            0.9"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#tailors-within-workflows",
    "href": "slides/advanced-06-postprocessing.html#tailors-within-workflows",
    "title": "6 - Postprocessing",
    "section": "tailors within workflows  ",
    "text": "tailors within workflows  \nIn practice, we would add the tailor to a workflow to make it easier to use:\n\n\nnnet_wflow &lt;- workflow(rec, nnet_spec, thrsh_tlr)\n\n\n\nWe donâ€™t have to set the names of the outcome or prediction columns (yet).\nfit() and predict() happen automatically."
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#current-adjustments",
    "href": "slides/advanced-06-postprocessing.html#current-adjustments",
    "title": "6 - Postprocessing",
    "section": "Current adjustments",
    "text": "Current adjustments\n\nadjust_equivocal_zone(): decline to predict.\nadjust_numeric_calibration(): try to readjust predictions to be consistent with numeric predictions.\nadjust_numeric_range(): restrict the range of predictions.\nadjust_predictions_custom(): similar to dplyr::mutate().\nadjust_probability_calibration(): try to readjust predictions to be consistent with probability predictions.\nadjust_probability_threshold(): custom rule for hard-class predictions from probabilities."
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#some-notes",
    "href": "slides/advanced-06-postprocessing.html#some-notes",
    "title": "6 - Postprocessing",
    "section": "Some notes",
    "text": "Some notes\n\nAdjustment order matters; tailor will error early if the ordering rules are violated.\nAdjustments that change class probabilities also affect hard class predictions.\nAdjustments happen before performance estimation.\n\nUndoing something like a log transformation is a bad idea here.\n\nWe have more calibration methods in mind."
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#your-turn",
    "href": "slides/advanced-06-postprocessing.html#your-turn",
    "title": "6 - Postprocessing",
    "section": "Your turn",
    "text": "Your turn\n\nDiscuss with those around you what the â€œordering rulesâ€ could be.\n\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#more-notes",
    "href": "slides/advanced-06-postprocessing.html#more-notes",
    "title": "6 - Postprocessing",
    "section": "More Notes",
    "text": "More Notes\n\nWhen estimation is required, the data considerations become more complex.\nMost arguments can be tuned.\nFor grid search, we use a conditional execution algorithm that avoids redundant retraining of the preprocessor or model."
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#tuning-the-probability-threshold",
    "href": "slides/advanced-06-postprocessing.html#tuning-the-probability-threshold",
    "title": "6 - Postprocessing",
    "section": "Tuning the probability threshold    ",
    "text": "Tuning the probability threshold    \n\nthrsh_tlr &lt;-\n  tailor() |&gt;\n  adjust_probability_threshold(threshold = tune())\n\nnnet_thrsh_wflow &lt;- workflow(rec, nnet_spec, thrsh_tlr)\n  \nnnet_thrsh_param &lt;- \n  nnet_thrsh_wflow |&gt; \n  extract_parameter_set_dials() |&gt; \n  update(threshold = threshold(c(0.001, 0.5)))"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#tuning-the-probability-threshold-1",
    "href": "slides/advanced-06-postprocessing.html#tuning-the-probability-threshold-1",
    "title": "6 - Postprocessing",
    "section": "Tuning the probability threshold  ",
    "text": "Tuning the probability threshold  \nNearly the same code as before:\n\n\nctrl &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE)\ncls_mtr &lt;- metric_set(brier_class, roc_auc, sensitivity, specificity)\n\nset.seed(12)\nnnet_thrsh_res &lt;-\n  nnet_thrsh_wflow |&gt;\n  tune_grid(\n    resamples = sim_rs,\n    grid = 25,\n    param_info = nnet_thrsh_param, \n    control = ctrl,\n    metrics = cls_mtr\n  )"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#grid-results",
    "href": "slides/advanced-06-postprocessing.html#grid-results",
    "title": "6 - Postprocessing",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(nnet_thrsh_res)"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#grid-results-1",
    "href": "slides/advanced-06-postprocessing.html#grid-results-1",
    "title": "6 - Postprocessing",
    "section": "Grid results",
    "text": "Grid results\n\ntanh activation is doing much better.\nthreshold should not (and does not) affect the Brier or ROC metrics.\nWe can achieve low Brier scores.\nWe could run another grid with values &lt;2% for a better threshold estimate."
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#multimetric-optimization",
    "href": "slides/advanced-06-postprocessing.html#multimetric-optimization",
    "title": "6 - Postprocessing",
    "section": "Multimetric optimization ",
    "text": "Multimetric optimization \n\nnnet_thrsh_res |&gt;\n  show_best_desirability(\n    maximize(sensitivity),\n    minimize(brier_class),\n    constrain(specificity, low = 0.8, high = 1.0)\n  ) |&gt;\n  relocate(threshold, sensitivity, specificity, brier_class, .d_overall)\n#&gt; # A tibble: 5 Ã— 14\n#&gt;   threshold sensitivity specificity brier_class .d_overall hidden_units  penalty\n#&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;        &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1     0.250       0.853       0.947      0.0425      0.939           28 2.61e-10\n#&gt; 2     0.292       0.822       0.953      0.0405      0.939           38 1   e- 5\n#&gt; 3     0.105       0.882       0.897      0.0470      0.924           48 1.78e- 9\n#&gt; 4     0.209       0.839       0.929      0.0468      0.910            8 1   e-10\n#&gt; 5     0.126       0.852       0.903      0.0526      0.880           42 3.16e- 3\n#&gt; # â„¹ 7 more variables: activation &lt;chr&gt;, learn_rate &lt;dbl&gt;, .config &lt;chr&gt;,\n#&gt; #   roc_auc &lt;dbl&gt;, .d_max_sensitivity &lt;dbl&gt;, .d_min_brier_class &lt;dbl&gt;,\n#&gt; #   .d_box_specificity &lt;dbl&gt;"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#calibration",
    "href": "slides/advanced-06-postprocessing.html#calibration",
    "title": "6 - Postprocessing",
    "section": "Calibration ",
    "text": "Calibration \n\n\nmore_sens &lt;-\n  nnet_thrsh_res |&gt;\n  select_best_desirability(\n    maximize(sensitivity),\n    minimize(brier_class),\n    constrain(specificity, low = 0.8, high = 1.0)\n  )\n\nnnet_thrsh_res |&gt;\n  collect_predictions(\n    parameters = more_sens\n  ) |&gt;\n  cal_plot_windowed(\n    truth = class,\n    estimate = .pred_event,\n    window_size = 0.2,\n    step_size = 0.025,\n  )"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#thoughts-about-these-results",
    "href": "slides/advanced-06-postprocessing.html#thoughts-about-these-results",
    "title": "6 - Postprocessing",
    "section": "Thoughts about these results",
    "text": "Thoughts about these results\nThe calibration issue in the previous plot shows that some very likely non-events will have underestimated probabilities.\n\nThat may not matter if we are very focused on events.\nThresholding does not affect calibration.\nWe might be able to:\n\nFurther tune the neural network to solve the issue and/or\nAdd a calibration postprocessor"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#thoughts-about-the-approach",
    "href": "slides/advanced-06-postprocessing.html#thoughts-about-the-approach",
    "title": "6 - Postprocessing",
    "section": "Thoughts about the approach",
    "text": "Thoughts about the approach\nLetâ€™s say that we pick a threshold of 2%. Our explanation to the user/stakeholder would be\n\nâ€œAs long as the model is at least 2% sure it is an event, we will call it an eventâ€.\n\nIt may be challenging to convince someone that this is the best option.\n\nThat said, this is probably a better approach than cost-sensitive learning."
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#data-to-train-the-adjustments",
    "href": "slides/advanced-06-postprocessing.html#data-to-train-the-adjustments",
    "title": "6 - Postprocessing",
    "section": "Data to train the adjustments",
    "text": "Data to train the adjustments\nIf an adjustment requires data, where do we get it from?\n\nFitting a calibration model to the training set re-predictions would be bad.\nAlso, we donâ€™t want to touch the validation or test sets.\n\n\n\nWe need another data set."
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#data-sources",
    "href": "slides/advanced-06-postprocessing.html#data-sources",
    "title": "6 - Postprocessing",
    "section": "Data sources",
    "text": "Data sources\nTwo possibilities:\n\nShave some data off the training set to create a calibration set.\n\nDuring resampling, we can do the same to the analysis set.\nThe â€œshavingâ€ process emulates the original sampling method.\nThere are fewer data points for training the preprocessor and primary model.\n\nUse a static calibration set outside our training/validation/testing splits.\n\nCurrently, we have implemented the first method."
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#example-3-fold-cv",
    "href": "slides/advanced-06-postprocessing.html#example-3-fold-cv",
    "title": "6 - Postprocessing",
    "section": "Example: 3-fold CV",
    "text": "Example: 3-fold CV"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#example-3-fold-cv-internal-split",
    "href": "slides/advanced-06-postprocessing.html#example-3-fold-cv-internal-split",
    "title": "6 - Postprocessing",
    "section": "Example: 3-fold CV internal split",
    "text": "Example: 3-fold CV internal split"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#breakdown-for-the-class-imbalance-data",
    "href": "slides/advanced-06-postprocessing.html#breakdown-for-the-class-imbalance-data",
    "title": "6 - Postprocessing",
    "section": "Breakdown for the class imbalance data",
    "text": "Breakdown for the class imbalance data\n\n\n\n\n\n\nData\nStrategy\nevent\nno_event\n\n\n\n\nOriginal\nAll\n225\n1775\n\n\nTraining\nNo Calibration\n168\n1331\n\n\nAnalysis\nNo Calibration\n151\n1197\n\n\nTraining\nCalibration\n126\n998\n\n\nAnalysis\nCalibration\n135\n1077\n\n\nCalibration\nCalibration\n16\n120"
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#calibration-1",
    "href": "slides/advanced-06-postprocessing.html#calibration-1",
    "title": "6 - Postprocessing",
    "section": "Calibration",
    "text": "Calibration\nCalibration models, in essence, try to predict the true class using the model predictions. Symbolically:\n\nFor regression models: outcome ~ .pred\nFor binary classifiers: class ~ .pred_class_1\nFor multiclass: class ~ .pred_class_1 + .pred_class_2 + ...\n\nEach calibration method works slightly differently.\nFor example, in regression, a (generalized) linear model is fit, and the residuals are added to new predictions."
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#calibration-expectations",
    "href": "slides/advanced-06-postprocessing.html#calibration-expectations",
    "title": "6 - Postprocessing",
    "section": "Calibration expectations",
    "text": "Calibration expectations\nKeep expectations low. For these methods to work:\n\nThe systematic issue will need to be large, or at least not subtle.\nA large calibration set is needed to work effectively.\n\nThe example in ALM4TD is an illustrative example and details.\nIn many cases, trying a different model or tuning parameters would be better.\n\nYou can tune the calibration method, one of which is no calibration."
  },
  {
    "objectID": "slides/advanced-06-postprocessing.html#your-turn-1",
    "href": "slides/advanced-06-postprocessing.html#your-turn-1",
    "title": "6 - Postprocessing",
    "section": "Your turn",
    "text": "Your turn\n\nBased on previous results, choose and fix a specific activation type (i.e., no tune()).\nAdd a calibrator to your tailor with a method = tune() value.\nRun another grid search\n\nDoes it help with this data set?\n\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "slides/advanced-08-wrapping-up.html#your-turn",
    "href": "slides/advanced-08-wrapping-up.html#your-turn",
    "title": "8 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/advanced-08-wrapping-up.html#resources-to-keep-learning",
    "href": "slides/advanced-08-wrapping-up.html#resources-to-keep-learning",
    "title": "8 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://tidymodels.aml4td.org/\n\n\n\n\nhttps://smltar.com/\n\n\n\n\nhttps://feaz-book.com/\n\n\n\nFollow us on Bluesky, Mastodon and at the tidyverse blog for updates!"
  },
  {
    "objectID": "slides/extras-iterative-search.html#startup",
    "href": "slides/extras-iterative-search.html#startup",
    "title": "Extras - Iterative search",
    "section": "Startup!  ",
    "text": "Startup!  \n\nlibrary(tidymodels)\nlibrary(important)\nlibrary(probably)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\nmirai::daemons(parallel::detectCores())"
  },
  {
    "objectID": "slides/extras-iterative-search.html#more-startup",
    "href": "slides/extras-iterative-search.html#more-startup",
    "title": "Extras - Iterative search",
    "section": "More startup! ",
    "text": "More startup! \n\n# Load our example data for this section\n\"https://raw.githubusercontent.com/tidymodels/\" |&gt; \n  paste0(\"workshops/main/slides/class_data.RData\") |&gt; \n  url() |&gt; \n  load()\n\nset.seed(429)\nsim_split &lt;- initial_split(class_data, prop = 0.75, strata = class)\nsim_train &lt;- training(sim_split)\nsim_test  &lt;- testing(sim_split)\n\nset.seed(523)\nsim_rs &lt;- vfold_cv(sim_train, v = 10, strata = class)"
  },
  {
    "objectID": "slides/extras-iterative-search.html#neural-network",
    "href": "slides/extras-iterative-search.html#neural-network",
    "title": "Extras - Iterative search",
    "section": "Neural network  ",
    "text": "Neural network  \nFrom the seventh advanced slide deck:\n\nnnet_spec &lt;-\n  mlp(hidden_units = tune(), penalty = tune(), learn_rate = tune(),\n      epochs = 100, activation = tune()\n  ) |&gt;\n  set_engine(\"brulee\", stop_iter = 10) |&gt;\n  set_mode(\"classification\")\n\nrec &lt;- \n  recipe(class ~ ., data = sim_train) |&gt; \n  step_normalize(all_numeric_predictors())\n  \nthrsh_tlr &lt;-\n  tailor() |&gt;\n  adjust_probability_threshold(threshold = tune()) \n  \nnnet_wflow &lt;- workflow(rec, nnet_spec, thrsh_tlr)\n\nnnet_param &lt;-\n  nnet_wflow |&gt; \n  extract_parameter_set_dials() |&gt; \n  update(threshold = threshold(c(0.0001, 0.1)))\n  \ncls_mtr &lt;- metric_set(brier_class, roc_auc, sensitivity, specificity)"
  },
  {
    "objectID": "slides/extras-iterative-search.html#iterative-search",
    "href": "slides/extras-iterative-search.html#iterative-search",
    "title": "Extras - Iterative search",
    "section": "Iterative Search",
    "text": "Iterative Search\nInstead of pre-defining a grid of candidate points, we can model our current results to predict what the next candidate point should be.\n\nSuppose that we are only tuning the learning rate in our neural network.\n\nWe could do something like:\nbrier_model &lt;- lm(brier ~ learn_rate, data = resample_results)\nand use this to predict and rank new learning rate candidates."
  },
  {
    "objectID": "slides/extras-iterative-search.html#iterative-search-1",
    "href": "slides/extras-iterative-search.html#iterative-search-1",
    "title": "Extras - Iterative search",
    "section": "Iterative Search",
    "text": "Iterative Search\nA linear model probably isnâ€™t the best choice though (more in a minute).\nTo illustrate the process, we resampled a large grid of learning rate values for our data to show what the relationship is between error and learning rate.\nNow suppose that we used a grid of three points in the parameter range for learning rateâ€¦"
  },
  {
    "objectID": "slides/extras-iterative-search.html#a-large-grid",
    "href": "slides/extras-iterative-search.html#a-large-grid",
    "title": "Extras - Iterative search",
    "section": "A Large Grid",
    "text": "A Large Grid"
  },
  {
    "objectID": "slides/extras-iterative-search.html#a-three-point-grid",
    "href": "slides/extras-iterative-search.html#a-three-point-grid",
    "title": "Extras - Iterative search",
    "section": "A Three Point Grid",
    "text": "A Three Point Grid"
  },
  {
    "objectID": "slides/extras-iterative-search.html#gaussian-processes-and-optimization",
    "href": "slides/extras-iterative-search.html#gaussian-processes-and-optimization",
    "title": "Extras - Iterative search",
    "section": "Gaussian Processes and Optimization",
    "text": "Gaussian Processes and Optimization\nWe can make a â€œmeta-modelâ€ with a small set of historical performance results.\nGaussian Processes (GP) models are a good choice to model performance.\n\nIt is a Bayesian model so we are using Bayesian Optimization (BO).\nFor regression, we can assume that our data are multivariate normal.\nWe also define a covariance function for the variance relationship between data points. A common one is:\n\n\\[\\operatorname{cov}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\exp\\left(-\\frac{1}{2}|\\boldsymbol{x}_i - \\boldsymbol{x}_j|^2\\right) + \\sigma^2_{ij}\\]\n\nGPs are good because\n\nthey are flexible regression models (in the sense that splines are flexible).\nwe need to get mean and variance predictions (and they are Bayesian)\ntheir variability is based on spatial distances.\n\nSome people use random forests (with conformal variance estimates) or other methods but GPs are most popular."
  },
  {
    "objectID": "slides/extras-iterative-search.html#predicting-candidates",
    "href": "slides/extras-iterative-search.html#predicting-candidates",
    "title": "Extras - Iterative search",
    "section": "Predicting Candidates",
    "text": "Predicting Candidates\nThe GP model can take candidate tuning parameter combinations as inputs and make predictions for performance (e.g.Â Brier, ROC AUC, RMSE, etc.)\n\nThe mean performance\nThe variance of performance\n\nThe variance is mostly driven by spatial variability (the previous equation).\nThe predicted variance is zero at locations of actual data points and becomes very high when far away from any observed data."
  },
  {
    "objectID": "slides/extras-iterative-search.html#your-turn",
    "href": "slides/extras-iterative-search.html#your-turn",
    "title": "Extras - Iterative search",
    "section": "Your turn",
    "text": "Your turn\n\n\nYour GP makes predictions on two new candidate tuning parameters.\nWe want to minimize error.\nWhich should we choose?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/extras-iterative-search.html#gp-fit-ribbon-is-mean---1sd",
    "href": "slides/extras-iterative-search.html#gp-fit-ribbon-is-mean---1sd",
    "title": "Extras - Iterative search",
    "section": "GP Fit (ribbon is mean +/- 1SD)",
    "text": "GP Fit (ribbon is mean +/- 1SD)"
  },
  {
    "objectID": "slides/extras-iterative-search.html#choosing-new-candidates",
    "href": "slides/extras-iterative-search.html#choosing-new-candidates",
    "title": "Extras - Iterative search",
    "section": "Choosing New Candidates",
    "text": "Choosing New Candidates\nThis isnâ€™t a very good fit but we can still use it.\nHow can we use the outputs to choose the next point to measure?\n\nAcquisition functions take the predicted mean and variance and use them to balance:\n\nexploration: new candidates should explore new areas.\nexploitation: new candidates must stay near existing values.\n\nExploration focuses on the variance, exploitation is about the mean."
  },
  {
    "objectID": "slides/extras-iterative-search.html#acquisition-functions",
    "href": "slides/extras-iterative-search.html#acquisition-functions",
    "title": "Extras - Iterative search",
    "section": "Acquisition Functions",
    "text": "Acquisition Functions\nWeâ€™ll use an acquisition function to select a new candidate.\nThe most popular method appears to be expected improvement (EI) above the current best results.\n\nZero at existing data points.\nThe expected improvement is integrated over all possible improvement (â€œexpectedâ€ in the probability sense).\n\nWe would probably pick the point with the largest EI as the next point.\n(There are other functions beyond EI.)"
  },
  {
    "objectID": "slides/extras-iterative-search.html#expected-improvement",
    "href": "slides/extras-iterative-search.html#expected-improvement",
    "title": "Extras - Iterative search",
    "section": "Expected Improvement",
    "text": "Expected Improvement"
  },
  {
    "objectID": "slides/extras-iterative-search.html#iteration",
    "href": "slides/extras-iterative-search.html#iteration",
    "title": "Extras - Iterative search",
    "section": "Iteration",
    "text": "Iteration\nOnce we pick the candidate point, we measure performance for it (e.g.Â resampling).\n\nAnother GP is fit, EI is recomputed, and so on.\n\nWe stop when we have completed the allowed number of iterations or if we donâ€™t see any improvement after a pre-set number of attempts."
  },
  {
    "objectID": "slides/extras-iterative-search.html#gp-fit-with-four-points",
    "href": "slides/extras-iterative-search.html#gp-fit-with-four-points",
    "title": "Extras - Iterative search",
    "section": "GP Fit with four points",
    "text": "GP Fit with four points"
  },
  {
    "objectID": "slides/extras-iterative-search.html#expected-improvement-1",
    "href": "slides/extras-iterative-search.html#expected-improvement-1",
    "title": "Extras - Iterative search",
    "section": "Expected Improvement",
    "text": "Expected Improvement\n\n\nAML4TD, TMwR"
  },
  {
    "objectID": "slides/extras-iterative-search.html#bo-in-tidymodels",
    "href": "slides/extras-iterative-search.html#bo-in-tidymodels",
    "title": "Extras - Iterative search",
    "section": "BO in tidymodels",
    "text": "BO in tidymodels\nWeâ€™ll use a function called tune_bayes() that has very similar syntax to tune_grid().\n\nIt has an additional initial argument for the initial set of performance estimates and parameter combinations for the GP model."
  },
  {
    "objectID": "slides/extras-iterative-search.html#initial-grid-points",
    "href": "slides/extras-iterative-search.html#initial-grid-points",
    "title": "Extras - Iterative search",
    "section": "Initial grid points",
    "text": "Initial grid points\ninitial can be the results of another tune_*() function or an integer (in which case tune_grid() is used under to hood to make such an initial set of results).\n\nWeâ€™ll run the optimization more than once, so letâ€™s make an initial grid of results to serve as the substrate for the BO.\nI suggest at least the number of tuning parameters plus two as the initial grid for BO."
  },
  {
    "objectID": "slides/extras-iterative-search.html#qualitative-parameters",
    "href": "slides/extras-iterative-search.html#qualitative-parameters",
    "title": "Extras - Iterative search",
    "section": "Qualitative parameters",
    "text": "Qualitative parameters\n\nWhat about non-numeric tuning parameters such as activation?\nCurrently, tidymodels converts these to dummy indicators and uses those in the GP. This is not unusual but also not great.\n\nOur initial grid should include more points; one for each level of the qualitative parameter.\nIn our case, the activation function is preset to use 5 possible values.\n\nAn upcoming version of tune will use a different R package to fit the GP that uses factor or Gower kernels. This will avoid making indicators and require fewer initial points."
  },
  {
    "objectID": "slides/extras-iterative-search.html#an-initial-grid",
    "href": "slides/extras-iterative-search.html#an-initial-grid",
    "title": "Extras - Iterative search",
    "section": "An Initial Grid",
    "text": "An Initial Grid\n\nset.seed(12)\ninit_res &lt;-\n  nnet_wflow |&gt;\n  tune_grid(\n    resamples = sim_rs,\n    grid = nrow(nnet_param) + 6, # for activation values + 1 extra\n    param_info = nnet_param,\n    metrics = cls_mtr\n  )\n\nshow_best(init_res, metric = \"brier_class\", n = 3) |&gt; select(-.metric, -.estimator)\n#&gt; # A tibble: 3 Ã— 9\n#&gt;   hidden_units      penalty activation learn_rate threshold   mean     n std_err\n#&gt;          &lt;int&gt;        &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1           30 0.001        log_sigmoâ€¦     0.331     0.0101 0.0401    10 0.00236\n#&gt; 2           40 0.0000000001 elu            0.0132    0.0600 0.0428    10 0.00199\n#&gt; 3            6 0.0001       elu            0.0251    0.0800 0.0452    10 0.00241\n#&gt; # â„¹ 1 more variable: .config &lt;chr&gt;"
  },
  {
    "objectID": "slides/extras-iterative-search.html#bo-using-tidymodels",
    "href": "slides/extras-iterative-search.html#bo-using-tidymodels",
    "title": "Extras - Iterative search",
    "section": "BO using tidymodels",
    "text": "BO using tidymodels\n\nctrl_bo &lt;- control_bayes(verbose_iter = TRUE, no_improve = Inf) \n\nset.seed(125)\nnnet_bayes_res &lt;-\n  nnet_wflow |&gt;\n  tune_bayes(\n    resamples = sim_rs,\n    initial = init_res,     # &lt;- initial results\n    iter = 25,\n    control = ctrl_bo,\n    param_info = nnet_param,\n    metrics = cls_mtr\n  )\n#&gt; Optimizing brier_class using the expected improvement\n#&gt; \n#&gt; â”€â”€ Iteration 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=11, penalty=1.13e-10, activation=log_sigmoid, learn_rate=0.36,\n#&gt;   threshold=0.0678\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.05066 (+/-0.00347)\n#&gt; \n#&gt; â”€â”€ Iteration 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; â†’ A | warning: did not converge in 10 iterations\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=8, penalty=0.22, activation=tanh, learn_rate=0.0143,\n#&gt;   threshold=0.0983\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.09955 (+/-0.000591)\n#&gt; \n#&gt; â”€â”€ Iteration 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=15, penalty=3.26e-06, activation=log_sigmoid, learn_rate=0.62,\n#&gt;   threshold=0.0729\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.05927 (+/-0.00613)\n#&gt; \n#&gt; â”€â”€ Iteration 4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=46, penalty=0.000214, activation=log_sigmoid,\n#&gt;   learn_rate=0.537, threshold=0.0526\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.04447 (+/-0.00401)\n#&gt; \n#&gt; â”€â”€ Iteration 5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=46, penalty=0.287, activation=log_sigmoid, learn_rate=0.63,\n#&gt;   threshold=0.0109\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.09951 (+/-0.000609)\n#&gt; \n#&gt; â”€â”€ Iteration 6 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=9, penalty=0.00057, activation=elu, learn_rate=0.0486,\n#&gt;   threshold=0.0268\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.04341 (+/-0.00331)\n#&gt; \n#&gt; â”€â”€ Iteration 7 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=4, penalty=0.000623, activation=tanh, learn_rate=0.394,\n#&gt;   threshold=0.0324\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.06138 (+/-0.00713)\n#&gt; \n#&gt; â”€â”€ Iteration 8 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=21, penalty=1.15e-10, activation=tanh, learn_rate=0.00174,\n#&gt;   threshold=0.0478\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.04668 (+/-0.00289)\n#&gt; \n#&gt; â”€â”€ Iteration 9 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=14, penalty=0.00108, activation=relu, learn_rate=0.514,\n#&gt;   threshold=0.0514\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.05046 (+/-0.00298)\n#&gt; \n#&gt; â”€â”€ Iteration 10 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=36, penalty=0.00052, activation=elu, learn_rate=0.0278,\n#&gt;   threshold=0.0983\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.04751 (+/-0.00525)\n#&gt; \n#&gt; â”€â”€ Iteration 11 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=34, penalty=0.000502, activation=tanhshrink, learn_rate=0.211,\n#&gt;   threshold=0.0169\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.0615 (+/-0.00519)\n#&gt; \n#&gt; â”€â”€ Iteration 12 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=4, penalty=1.12e-10, activation=tanhshrink,\n#&gt;   learn_rate=0.00411, threshold=0.0306\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.08108 (+/-0.00614)\n#&gt; \n#&gt; â”€â”€ Iteration 13 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=46, penalty=1.24e-10, activation=tanh, learn_rate=0.0381,\n#&gt;   threshold=0.0258\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.05036 (+/-0.00374)\n#&gt; \n#&gt; â”€â”€ Iteration 14 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=23, penalty=1.05e-09, activation=log_sigmoid,\n#&gt;   learn_rate=0.00102, threshold=0.0621\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.09518 (+/-0.00543)\n#&gt; \n#&gt; â”€â”€ Iteration 15 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=20, penalty=1.02e-10, activation=relu, learn_rate=0.00114,\n#&gt;   threshold=0.0793\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.05048 (+/-0.00376)\n#&gt; \n#&gt; â”€â”€ Iteration 16 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=21, penalty=2.79e-08, activation=tanh, learn_rate=0.152,\n#&gt;   threshold=0.0377\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.05213 (+/-0.00364)\n#&gt; \n#&gt; â”€â”€ Iteration 17 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=11, penalty=9.6e-09, activation=relu, learn_rate=0.00578,\n#&gt;   threshold=0.00296\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.04632 (+/-0.00282)\n#&gt; \n#&gt; â”€â”€ Iteration 18 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=10, penalty=4.01e-07, activation=tanh, learn_rate=0.00101,\n#&gt;   threshold=0.0157\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.04711 (+/-0.00288)\n#&gt; \n#&gt; â”€â”€ Iteration 19 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=24, penalty=2.16e-07, activation=relu, learn_rate=0.00914,\n#&gt;   threshold=0.0819\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.05218 (+/-0.00422)\n#&gt; \n#&gt; â”€â”€ Iteration 20 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=6, penalty=0.000116, activation=relu, learn_rate=0.0022,\n#&gt;   threshold=0.00909\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.05317 (+/-0.00339)\n#&gt; \n#&gt; â”€â”€ Iteration 21 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=35, penalty=0.000918, activation=log_sigmoid,\n#&gt;   learn_rate=0.00863, threshold=0.00272\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.0496 (+/-0.00581)\n#&gt; \n#&gt; â”€â”€ Iteration 22 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=29, penalty=2.58e-08, activation=log_sigmoid,\n#&gt;   learn_rate=0.00116, threshold=0.0455\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.09068 (+/-0.00624)\n#&gt; \n#&gt; â”€â”€ Iteration 23 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=32, penalty=0.000724, activation=log_sigmoid,\n#&gt;   learn_rate=0.577, threshold=0.00293\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.04983 (+/-0.00503)\n#&gt; \n#&gt; â”€â”€ Iteration 24 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=29, penalty=0.000244, activation=log_sigmoid,\n#&gt;   learn_rate=0.0793, threshold=0.0939\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.0412 (+/-0.00289)\n#&gt; \n#&gt; â”€â”€ Iteration 25 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      brier_class=0.04006 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i hidden_units=38, penalty=0.000286, activation=log_sigmoid, learn_rate=0.24,\n#&gt;   threshold=0.0495\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    brier_class=0.04345 (+/-0.00354)"
  },
  {
    "objectID": "slides/extras-iterative-search.html#best-results",
    "href": "slides/extras-iterative-search.html#best-results",
    "title": "Extras - Iterative search",
    "section": "Best results",
    "text": "Best results\n\nshow_best(nnet_bayes_res, metric = \"brier_class\") |&gt; select(-.metric, -.estimator)\n#&gt; # A tibble: 5 Ã— 10\n#&gt;   hidden_units      penalty activation learn_rate threshold   mean     n std_err\n#&gt;          &lt;int&gt;        &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1           30 0.001        log_sigmoâ€¦     0.331     0.0101 0.0401    10 0.00236\n#&gt; 2           29 0.000244     log_sigmoâ€¦     0.0793    0.0939 0.0412    10 0.00289\n#&gt; 3           40 0.0000000001 elu            0.0132    0.0600 0.0428    10 0.00199\n#&gt; 4            9 0.000570     elu            0.0486    0.0268 0.0434    10 0.00331\n#&gt; 5           38 0.000286     log_sigmoâ€¦     0.240     0.0495 0.0435    10 0.00354\n#&gt; # â„¹ 2 more variables: .config &lt;chr&gt;, .iter &lt;int&gt;"
  },
  {
    "objectID": "slides/extras-iterative-search.html#plotting-bo-results",
    "href": "slides/extras-iterative-search.html#plotting-bo-results",
    "title": "Extras - Iterative search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(nnet_bayes_res, metric = \"brier_class\")"
  },
  {
    "objectID": "slides/extras-iterative-search.html#plotting-bo-results-1",
    "href": "slides/extras-iterative-search.html#plotting-bo-results-1",
    "title": "Extras - Iterative search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(nnet_bayes_res, metric = \"brier_class\", type = \"parameters\")"
  },
  {
    "objectID": "slides/extras-iterative-search.html#plotting-bo-results-2",
    "href": "slides/extras-iterative-search.html#plotting-bo-results-2",
    "title": "Extras - Iterative search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(nnet_bayes_res, metric = \"brier_class\", type = \"performance\")"
  },
  {
    "objectID": "slides/extras-iterative-search.html#your-turn-1",
    "href": "slides/extras-iterative-search.html#your-turn-1",
    "title": "Extras - Iterative search",
    "section": "Your turn",
    "text": "Your turn\nLetâ€™s try a different acquisition function: conf_bound(kappa).\nWeâ€™ll use the objective argument to set it.\nChoose your own kappa value:\n\nLarger values will explore the space more.\nâ€œLargeâ€ values are usually less than one.\n\nBonus points: Before the optimization is done, press &lt;esc&gt; and see what happens.\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "slides/extras-iterative-search.html#notes",
    "href": "slides/extras-iterative-search.html#notes",
    "title": "Extras - Iterative search",
    "section": "Notes",
    "text": "Notes\n\nStopping tune_bayes() will return the current results.\nParallel processing can still be used to more efficiently measure each candidate point.\nThere are a lot of other iterative methods that you can use.\nThe finetune package also has functions for simulated annealing search."
  },
  {
    "objectID": "slides/intro-01-introduction.html#venue-information",
    "href": "slides/intro-01-introduction.html#venue-information",
    "title": "1 - Introduction",
    "section": "Venue information",
    "text": "Venue information\n\nThere are gender neutral bathrooms located on floor LL2, next to Chicago A\nA meditation/prayer room is located on floor LL2 in Chicago A\nA lactation room is located on floor LL2 in Chicago B"
  },
  {
    "objectID": "slides/intro-01-introduction.html#workshop-policies",
    "href": "slides/intro-01-introduction.html#workshop-policies",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease review the posit::conf code of conduct, which applies to all workshops: https://posit.co/code-of-conduct\nCoC site has info on how to report a problem (in person, email, phone)\nPlease do not photograph people wearing red lanyards"
  },
  {
    "objectID": "slides/intro-01-introduction.html#who-are-you",
    "href": "slides/intro-01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %&gt;% or base R |&gt; pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have some exposure to basic statistical concepts like linear models and residuals\nYou do not need intermediate or expert familiarity with modeling or ML"
  },
  {
    "objectID": "slides/intro-01-introduction.html#who-are-tidymodels",
    "href": "slides/intro-01-introduction.html#who-are-tidymodels",
    "title": "1 - Introduction",
    "section": "Who are tidymodels?",
    "text": "Who are tidymodels?\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\n+ our TA today, Kristin Bott!\n\n\nMany thanks to Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill, and DesirÃ©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "slides/intro-01-introduction.html#section-2",
    "href": "slides/intro-01-introduction.html#section-2",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors ğŸ‘‹"
  },
  {
    "objectID": "slides/intro-01-introduction.html#getting-the-materials",
    "href": "slides/intro-01-introduction.html#getting-the-materials",
    "title": "1 - Introduction",
    "section": "Getting the materials",
    "text": "Getting the materials\n\nIf you are using Posit Cloud:\n Log in to Posit Cloud (free): TODO-ADD-LATER\n\nIf you are working locally:\n# local download\nusethis::use_course(\"tidymodels/workshops\", destdir = \"some_path\")\n\n# or fork via\nusethis::create_from_github(\"tidymodels/workshops\", fork = TRUE)"
  },
  {
    "objectID": "slides/intro-01-introduction.html#asking-for-help",
    "href": "slides/intro-01-introduction.html#asking-for-help",
    "title": "1 - Introduction",
    "section": "Asking for help",
    "text": "Asking for help\n\nğŸŸª â€œIâ€™m stuck and need help!â€\n\n\nğŸŸ© â€œI finished the exerciseâ€"
  },
  {
    "objectID": "slides/intro-01-introduction.html#discord",
    "href": "slides/intro-01-introduction.html#discord",
    "title": "1 - Introduction",
    "section": "Discord ",
    "text": "Discord \n\npos.it/conf-event-portal (login)\nClick on â€œJoin Discord, the virtual networking platform!â€\nBrowse Channels -&gt; #workshop-tidymodels"
  },
  {
    "objectID": "slides/intro-01-introduction.html#section-3",
    "href": "slides/intro-01-introduction.html#section-3",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€"
  },
  {
    "objectID": "slides/intro-01-introduction.html#section-4",
    "href": "slides/intro-01-introduction.html#section-4",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "slides/intro-01-introduction.html#plan-for-this-workshop",
    "href": "slides/intro-01-introduction.html#plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Plan for this workshop",
    "text": "Plan for this workshop\n\nYour data budget\nWhat makes a model\nEvaluating models\nTuning models"
  },
  {
    "objectID": "slides/intro-01-introduction.html#what-is-machine-learning",
    "href": "slides/intro-01-introduction.html#what-is-machine-learning",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nhttps://xkcd.com/1838/"
  },
  {
    "objectID": "slides/intro-01-introduction.html#what-is-machine-learning-1",
    "href": "slides/intro-01-introduction.html#what-is-machine-learning-1",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "slides/intro-01-introduction.html#what-is-machine-learning-2025-edition",
    "href": "slides/intro-01-introduction.html#what-is-machine-learning-2025-edition",
    "title": "1 - Introduction",
    "section": "What is machine learning? (2025 edition)",
    "text": "What is machine learning? (2025 edition)\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/\n\n\nIn the early 2010s, â€œArtificial intelligenceâ€ (AI) was largely synonymous with what weâ€™ll refer to as â€œmachine learningâ€ in this workshop. In the late 2010s and early 2020s, AI usually referred to deep learning methods. Since the release of ChatGPT in late 2022, â€œAIâ€ has come to also encompass large language models / generative models."
  },
  {
    "objectID": "slides/intro-01-introduction.html#what-is-machine-learning-2",
    "href": "slides/intro-01-introduction.html#what-is-machine-learning-2",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "slides/intro-01-introduction.html#your-turn",
    "href": "slides/intro-01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\n\n\nHow are statistics and machine learning related?\nHow are they similar? Different?\n\n\n\nâˆ’+\n03:00\n\n\n\n\nthe â€œtwo culturesâ€\nmodel first vs.Â data first\ninference vs.Â prediction"
  },
  {
    "objectID": "slides/intro-01-introduction.html#what-is-tidymodels",
    "href": "slides/intro-01-introduction.html#what-is-tidymodels",
    "title": "1 - Introduction",
    "section": "What is tidymodels? ",
    "text": "What is tidymodels? \n\nlibrary(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.4.1 â”€â”€\n#&gt; âœ” broom        1.0.9          âœ” rsample      1.3.1     \n#&gt; âœ” dials        1.4.2          âœ” tailor       0.1.0.9000\n#&gt; âœ” dplyr        1.1.4          âœ” tidyr        1.3.1     \n#&gt; âœ” infer        1.0.9          âœ” tune         2.0.0     \n#&gt; âœ” modeldata    1.5.1          âœ” workflows    1.3.0     \n#&gt; âœ” parsnip      1.3.3          âœ” workflowsets 1.1.1     \n#&gt; âœ” purrr        1.1.0          âœ” yardstick    1.3.2     \n#&gt; âœ” recipes      1.3.1\n#&gt; â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\n#&gt; âœ– purrr::discard() masks scales::discard()\n#&gt; âœ– dplyr::filter()  masks stats::filter()\n#&gt; âœ– dplyr::lag()     masks stats::lag()\n#&gt; âœ– recipes::step()  masks stats::step()"
  },
  {
    "objectID": "slides/intro-01-introduction.html#the-whole-game",
    "href": "slides/intro-01-introduction.html#the-whole-game",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\nRoadmap for today\nMinimal version of predictive modeling process\nFeature engineering and tuning as iterative extensions"
  },
  {
    "objectID": "slides/intro-01-introduction.html#the-whole-game-1",
    "href": "slides/intro-01-introduction.html#the-whole-game-1",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "slides/intro-01-introduction.html#the-whole-game-2",
    "href": "slides/intro-01-introduction.html#the-whole-game-2",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\n\nStress that we are not fitting a model on the entire training set other than for illustrative purposes in deck 2."
  },
  {
    "objectID": "slides/intro-01-introduction.html#the-whole-game-3",
    "href": "slides/intro-01-introduction.html#the-whole-game-3",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "slides/intro-01-introduction.html#the-whole-game-4",
    "href": "slides/intro-01-introduction.html#the-whole-game-4",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "slides/intro-01-introduction.html#the-whole-game-5",
    "href": "slides/intro-01-introduction.html#the-whole-game-5",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "slides/intro-01-introduction.html#the-whole-game-6",
    "href": "slides/intro-01-introduction.html#the-whole-game-6",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "slides/intro-01-introduction.html#the-whole-game-7",
    "href": "slides/intro-01-introduction.html#the-whole-game-7",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "slides/intro-01-introduction.html#lets-install-some-packages",
    "href": "slides/intro-01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Letâ€™s install some packages",
    "text": "Letâ€™s install some packages\nIf you are using your own laptop instead of Posit Cloud:\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"bonsai\", \"Cubist\", \"doParallel\", \"earth\", \"embed\", \"finetune\", \n    \"lightgbm\", \"lme4\", \"parallelly\", \"plumber\", \"probably\", \n    \"ranger\", \"rpart\", \"rpart.plot\", \"rules\", \"splines2\", \"stacks\", \n    \"text2vec\", \"textrecipes\", \"tidymodels\", \"vetiver\")\n\ninstall.packages(pkgs)"
  },
  {
    "objectID": "slides/intro-01-introduction.html#our-versions",
    "href": "slides/intro-01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\nR version 4.5.1 (2025-06-13), Quarto (1.7.32)\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nbonsai\n0.4.0\n\n\nbroom\n1.0.9\n\n\nCubist\n0.5.0\n\n\ndials\n1.4.2\n\n\ndoParallel\n1.0.17\n\n\ndplyr\n1.1.4\n\n\nearth\n5.3.4\n\n\nembed\n1.2.0\n\n\nfinetune\n1.2.1\n\n\nforested\n0.2.0\n\n\nFormula\n1.2-5\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nggplot2\n3.5.2\n\n\nlattice\n0.22-7\n\n\nlightgbm\n4.6.0\n\n\nlme4\n1.1-37\n\n\nmodeldata\n1.5.1\n\n\nparallelly\n1.45.1\n\n\nparsnip\n1.3.3\n\n\nplotmo\n3.6.4\n\n\nplotrix\n3.8-4\n\n\nplumber\n1.3.0\n\n\nprobably\n1.1.1\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\npurrr\n1.1.0\n\n\nranger\n0.17.0\n\n\nrecipes\n1.3.1\n\n\nrpart\n4.1.24\n\n\nrpart.plot\n3.1.3\n\n\nrsample\n1.3.1\n\n\nrules\n1.0.2\n\n\nscales\n1.4.0\n\n\nsplines2\n0.5.4\n\n\nstacks\n1.1.1.9001\n\n\ntailor\n0.1.0.9000\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\ntext2vec\n0.6.4\n\n\ntextrecipes\n1.1.0\n\n\ntidymodels\n1.4.1\n\n\ntidyr\n1.3.1\n\n\ntune\n2.0.0\n\n\nvetiver\n0.2.5\n\n\nworkflows\n1.3.0\n\n\nworkflowsets\n1.1.1\n\n\nyardstick\n1.3.2"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#your-turn",
    "href": "slides/intro-03-what-makes-a-model.html#your-turn",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nHow do you fit a linear model in R?\nHow many different ways can you think of?\n\n\n\nâˆ’+\n03:00\n\n\n\n\n\nlm for linear model\nglmnet for regularized regression\nkeras for regression using TensorFlow\nstan for Bayesian regression\nspark for large data sets\nbrulee for regression using torch"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#to-specify-a-model",
    "href": "slides/intro-03-what-makes-a-model.html#to-specify-a-model",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nChoose a model type\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-1",
    "href": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-1",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg()\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\n\nModels have default engines"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-2",
    "href": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-2",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model type\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-3",
    "href": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-3",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg() |&gt;\n  set_engine(\"glmnet\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glmnet"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-4",
    "href": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-4",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlogistic_reg() |&gt;\n  set_engine(\"stan\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: stan"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-5",
    "href": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-5",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model type\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-6",
    "href": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-6",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree()\n#&gt; Decision Tree Model Specification (unknown mode)\n#&gt; \n#&gt; Computational engine: rpart\n\n\nSome models have a default mode"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-7",
    "href": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-7",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree() |&gt; \n  set_mode(\"classification\")\n#&gt; Decision Tree Model Specification (classification)\n#&gt; \n#&gt; Computational engine: rpart\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-8",
    "href": "slides/intro-03-what-makes-a-model.html#to-specify-a-model-8",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model type\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#your-turn-1",
    "href": "slides/intro-03-what-makes-a-model.html#your-turn-1",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_spec chunk in your .qmd.\nEdit this code to use a logistic regression model.\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/\n\n\nExtension/Challenge: Edit this code to use a different model. For example, try using a conditional inference tree as implemented in the partykit package by changing the engine - or try an entirely different model type!\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#models-well-be-using-today",
    "href": "slides/intro-03-what-makes-a-model.html#models-well-be-using-today",
    "title": "3 - What makes a model?",
    "section": "Models weâ€™ll be using today",
    "text": "Models weâ€™ll be using today\n\nLogistic regression\nDecision trees"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#logistic-regression",
    "href": "slides/intro-03-what-makes-a-model.html#logistic-regression",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#logistic-regression-1",
    "href": "slides/intro-03-what-makes-a-model.html#logistic-regression-1",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#logistic-regression-2",
    "href": "slides/intro-03-what-makes-a-model.html#logistic-regression-2",
    "title": "3 - What makes a model?",
    "section": "Logistic regression",
    "text": "Logistic regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogit of outcome probability modeled as linear combination of predictors:\n\n\\(log(\\frac{p}{1 - p}) = \\beta_0 + \\beta_1\\cdot \\text{A}\\)\n\nFind a sigmoid line that separates the two classes"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#decision-trees",
    "href": "slides/intro-03-what-makes-a-model.html#decision-trees",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#decision-trees-1",
    "href": "slides/intro-03-what-makes-a-model.html#decision-trees-1",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#decision-trees-2",
    "href": "slides/intro-03-what-makes-a-model.html#decision-trees-2",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "href": "slides/intro-03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "title": "3 - What makes a model?",
    "section": "All models are wrong, but some are useful!",
    "text": "All models are wrong, but some are useful!\n\n\nLogistic regression\n\n\n\n\n\n\n\n\n\n\nDecision trees"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "href": "slides/intro-03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "title": "3 - What makes a model?",
    "section": "Workflows bind preprocessors and models",
    "text": "Workflows bind preprocessors and models\n\n\nExplain that PCA that is a preprocessor / dimensionality reduction, used to decorrelate data"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#what-is-wrong-with-this",
    "href": "slides/intro-03-what-makes-a-model.html#what-is-wrong-with-this",
    "title": "3 - What makes a model?",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?\n \n\n\nThe source of data leakage here is preprocessing on the whole source data set rather than on the whole training set. So, the risk is â€œthe values from the test set affect the mean and standard deviations used in the preprocessing.â€ That discussion shouldnâ€™t mention metrics, as they havenâ€™t been introduced yet.\nWhile centering + scaling are easy to explain the general idea with, itâ€™s not necessarily the biggest danger in practice. The more elaborate the estimation in your preprocessing, the higher the risk of overfitting coming back to bite you. An example of a more elaborate preprocessing estimation is principal components."
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#why-a-workflow",
    "href": "slides/intro-03-what-makes-a-model.html#why-a-workflow",
    "title": "3 - What makes a model?",
    "section": "Why a workflow()? ",
    "text": "Why a workflow()? \n\n\nWorkflows handle new data better than base R tools in terms of new factor levels\n\n\n\n\nYou can use other preprocessors besides formulas (more on this in Getting More Out of Feature Engineering and Tuning for Machine Learning)\n\n\n\n\nThey can help organize your work when working with multiple models\n\n\n\n\nMost importantly, a workflow captures the entire modeling process: fit() and predict() apply to the preprocessing steps in addition to the actual model fit\n\n\nTwo ways workflows handle levels better than base R:\n\nEnforces that new levels are not allowed at prediction time (this is an optional check that can be turned off)\nRestores missing levels that were present at fit time, but happen to be missing at prediction time (like, if your â€œnewâ€ data just doesnâ€™t have an instance of that level)"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#a-model-workflow-1",
    "href": "slides/intro-03-what-makes-a-model.html#a-model-workflow-1",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() |&gt; \n  set_mode(\"classification\")\n\ntree_spec |&gt; \n  fit(forested ~ ., data = forested_train) \n#&gt; parsnip model object\n#&gt; \n#&gt; n= 8749 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt; 1) root 8749 2427 Yes (0.7225969 0.2774031)  \n#&gt;   2) county=Appling,Atkinson,Bacon,Baldwin,Ben Hill,Brantley,Brooks,Bryan,Bulloch,Burke,Butts,Camden,Candler,Carroll,Charlton,Chattahoochee,Chattooga,Cherokee,Clinch,Coffee,Coweta,Crawford,Dade,Dawson,Dodge,Dougherty,Douglas,Echols,Effingham,Elbert,Emanuel,Evans,Fannin,Floyd,Gilmer,Glascock,Greene,Habersham,Hancock,Haralson,Harris,Heard,Jasper,Jeff Davis,Jefferson,Jenkins,Johnson,Jones,Lamar,Lanier,Laurens,Lee,Lincoln,Long,Lumpkin,Marion,McDuffie,Meriwether,Monroe,Montgomery,Morgan,Murray,Oconee,Oglethorpe,Paulding,Pickens,Pierce,Pike,Polk,Putnam,Quitman,Rabun,Randolph,Schley,Screven,Spalding,Stephens,Stewart,Talbot,Taliaferro,Tattnall,Taylor,Telfair,Terrell,Towns,Treutlen,Troup,Twiggs,Union,Upson,Walker,Ware,Warren,Washington,Wayne,Webster,Wheeler,White,Wilcox,Wilkes,Wilkinson 5598 1005 Yes (0.8204716 0.1795284) *\n#&gt;   3) county=Baker,Banks,Barrow,Bartow,Berrien,Bibb,Bleckley,Calhoun,Catoosa,Chatham,Clarke,Clay,Clayton,Cobb,Colquitt,Columbia,Cook,Crisp,Decatur,DeKalb,Dooly,Early,Fayette,Forsyth,Franklin,Fulton,Glynn,Gordon,Grady,Gwinnett,Hall,Hart,Henry,Houston,Irwin,Jackson,Liberty,Lowndes,Macon,Madison,McIntosh,Miller,Mitchell,Muscogee,Newton,Peach,Pulaski,Richmond,Rockdale,Seminole,Sumter,Thomas,Tift,Toombs,Turner,Walton,Whitfield,Worth 3151 1422 Yes (0.5487147 0.4512853)  \n#&gt;     6) canopy_cover&gt;=41.5 1773  603 Yes (0.6598985 0.3401015) *\n#&gt;     7) canopy_cover&lt; 41.5 1378  559 No (0.4056604 0.5943396) *"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#a-model-workflow-2",
    "href": "slides/intro-03-what-makes-a-model.html#a-model-workflow-2",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() |&gt; \n  set_mode(\"classification\")\n\nworkflow() |&gt;\n  add_formula(forested ~ .) |&gt;\n  add_model(tree_spec) |&gt;\n  fit(data = forested_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 8749 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt; 1) root 8749 2427 Yes (0.7225969 0.2774031)  \n#&gt;   2) county=Appling,Atkinson,Bacon,Baldwin,Ben Hill,Brantley,Brooks,Bryan,Bulloch,Burke,Butts,Camden,Candler,Carroll,Charlton,Chattahoochee,Chattooga,Cherokee,Clinch,Coffee,Coweta,Crawford,Dade,Dawson,Dodge,Dougherty,Douglas,Echols,Effingham,Elbert,Emanuel,Evans,Fannin,Floyd,Gilmer,Glascock,Greene,Habersham,Hancock,Haralson,Harris,Heard,Jasper,Jeff Davis,Jefferson,Jenkins,Johnson,Jones,Lamar,Lanier,Laurens,Lee,Lincoln,Long,Lumpkin,Marion,McDuffie,Meriwether,Monroe,Montgomery,Morgan,Murray,Oconee,Oglethorpe,Paulding,Pickens,Pierce,Pike,Polk,Putnam,Quitman,Rabun,Randolph,Schley,Screven,Spalding,Stephens,Stewart,Talbot,Taliaferro,Tattnall,Taylor,Telfair,Terrell,Towns,Treutlen,Troup,Twiggs,Union,Upson,Walker,Ware,Warren,Washington,Wayne,Webster,Wheeler,White,Wilcox,Wilkes,Wilkinson 5598 1005 Yes (0.8204716 0.1795284) *\n#&gt;   3) county=Baker,Banks,Barrow,Bartow,Berrien,Bibb,Bleckley,Calhoun,Catoosa,Chatham,Clarke,Clay,Clayton,Cobb,Colquitt,Columbia,Cook,Crisp,Decatur,DeKalb,Dooly,Early,Fayette,Forsyth,Franklin,Fulton,Glynn,Gordon,Grady,Gwinnett,Hall,Hart,Henry,Houston,Irwin,Jackson,Liberty,Lowndes,Macon,Madison,McIntosh,Miller,Mitchell,Muscogee,Newton,Peach,Pulaski,Richmond,Rockdale,Seminole,Sumter,Thomas,Tift,Toombs,Turner,Walton,Whitfield,Worth 3151 1422 Yes (0.5487147 0.4512853)  \n#&gt;     6) canopy_cover&gt;=41.5 1773  603 Yes (0.6598985 0.3401015) *\n#&gt;     7) canopy_cover&lt; 41.5 1378  559 No (0.4056604 0.5943396) *"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#a-model-workflow-3",
    "href": "slides/intro-03-what-makes-a-model.html#a-model-workflow-3",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() |&gt; \n  set_mode(\"classification\")\n\nworkflow(forested ~ ., tree_spec) |&gt; \n  fit(data = forested_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 8749 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt; 1) root 8749 2427 Yes (0.7225969 0.2774031)  \n#&gt;   2) county=Appling,Atkinson,Bacon,Baldwin,Ben Hill,Brantley,Brooks,Bryan,Bulloch,Burke,Butts,Camden,Candler,Carroll,Charlton,Chattahoochee,Chattooga,Cherokee,Clinch,Coffee,Coweta,Crawford,Dade,Dawson,Dodge,Dougherty,Douglas,Echols,Effingham,Elbert,Emanuel,Evans,Fannin,Floyd,Gilmer,Glascock,Greene,Habersham,Hancock,Haralson,Harris,Heard,Jasper,Jeff Davis,Jefferson,Jenkins,Johnson,Jones,Lamar,Lanier,Laurens,Lee,Lincoln,Long,Lumpkin,Marion,McDuffie,Meriwether,Monroe,Montgomery,Morgan,Murray,Oconee,Oglethorpe,Paulding,Pickens,Pierce,Pike,Polk,Putnam,Quitman,Rabun,Randolph,Schley,Screven,Spalding,Stephens,Stewart,Talbot,Taliaferro,Tattnall,Taylor,Telfair,Terrell,Towns,Treutlen,Troup,Twiggs,Union,Upson,Walker,Ware,Warren,Washington,Wayne,Webster,Wheeler,White,Wilcox,Wilkes,Wilkinson 5598 1005 Yes (0.8204716 0.1795284) *\n#&gt;   3) county=Baker,Banks,Barrow,Bartow,Berrien,Bibb,Bleckley,Calhoun,Catoosa,Chatham,Clarke,Clay,Clayton,Cobb,Colquitt,Columbia,Cook,Crisp,Decatur,DeKalb,Dooly,Early,Fayette,Forsyth,Franklin,Fulton,Glynn,Gordon,Grady,Gwinnett,Hall,Hart,Henry,Houston,Irwin,Jackson,Liberty,Lowndes,Macon,Madison,McIntosh,Miller,Mitchell,Muscogee,Newton,Peach,Pulaski,Richmond,Rockdale,Seminole,Sumter,Thomas,Tift,Toombs,Turner,Walton,Whitfield,Worth 3151 1422 Yes (0.5487147 0.4512853)  \n#&gt;     6) canopy_cover&gt;=41.5 1773  603 Yes (0.6598985 0.3401015) *\n#&gt;     7) canopy_cover&lt; 41.5 1378  559 No (0.4056604 0.5943396) *"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#your-turn-2",
    "href": "slides/intro-03-what-makes-a-model.html#your-turn-2",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_wflow chunk in your intro-03-classwork.qmd.\n\ntree_spec &lt;- decision_tree() |&gt; \n  set_mode(\"classification\")\n\ntree_wflow &lt;- workflow() |&gt;\n  add_formula(forested ~ .) |&gt;\n  add_model(tree_spec)\n\ntree_wflow\n\nEdit this code to make a workflow with your own model of choice.\n\nExtension/Challenge: Other than formulas, what kinds of preprocessors are supported?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#predict-with-your-model",
    "href": "slides/intro-03-what-makes-a-model.html#predict-with-your-model",
    "title": "3 - What makes a model?",
    "section": "Predict with your model  ",
    "text": "Predict with your model  \nHow do you use your new tree_fit model?\n\ntree_spec &lt;-\n  decision_tree() |&gt; \n  set_mode(\"classification\")\n\ntree_fit &lt;-\n  workflow(forested ~ ., tree_spec) |&gt; \n  fit(data = forested_train)"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#your-turn-3",
    "href": "slides/intro-03-what-makes-a-model.html#your-turn-3",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\npredict(tree_fit, new_data = forested_test)\nWhat do you notice about the structure of the result?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#your-turn-4",
    "href": "slides/intro-03-what-makes-a-model.html#your-turn-4",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\naugment(tree_fit, new_data = forested_test)\nHow does the output compare to the output from predict()?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#understand-your-model",
    "href": "slides/intro-03-what-makes-a-model.html#understand-your-model",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#understand-your-model-1",
    "href": "slides/intro-03-what-makes-a-model.html#understand-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nlibrary(rpart.plot)\ntree_fit |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot()\n\nYou can extract_*() several components of your fitted workflow.\n\nâš ï¸ Never predict() with any extracted components!\n\nroundint = FALSE is only to quiet a warning"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#understand-your-model-2",
    "href": "slides/intro-03-what-makes-a-model.html#understand-your-model-2",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nYou can use your fitted workflow for model and/or prediction explanations:\n\n\n\noverall variable importance, such as with the vip package\n\n\n\n\nflexible model explainers, such as with the DALEXtra package\n\n\n\nLearn more at https://www.tmwr.org/explain.html"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#your-turn-5",
    "href": "slides/intro-03-what-makes-a-model.html#your-turn-5",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\n\nExtract the model engine object from your fitted workflow and check it out.\n\n\n\nâˆ’+\n05:00\n\n\n\n\nAfterward, ask what kind of object people got from the extraction, and what they did with it (e.g.Â give it to summary(), plot(), broom::tidy() ). Live code along"
  },
  {
    "objectID": "slides/intro-03-what-makes-a-model.html#the-whole-game---status-update",
    "href": "slides/intro-03-what-makes-a-model.html#the-whole-game---status-update",
    "title": "3 - What makes a model?",
    "section": "The whole game - status update",
    "text": "The whole game - status update\n\n\nStress that fitting a model on the entire training set was only for illustrating how to fit a model"
  },
  {
    "objectID": "slides/intro-05-tuning-models.html#tuning-parameters",
    "href": "slides/intro-05-tuning-models.html#tuning-parameters",
    "title": "5 - Tuning models",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "slides/intro-05-tuning-models.html#optimize-tuning-parameters",
    "href": "slides/intro-05-tuning-models.html#optimize-tuning-parameters",
    "title": "5 - Tuning models",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "slides/intro-05-tuning-models.html#optimize-tuning-parameters-1",
    "href": "slides/intro-05-tuning-models.html#optimize-tuning-parameters-1",
    "title": "5 - Tuning models",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search ğŸ’  which tests a pre-defined set of candidate values\nIterative search ğŸŒ€ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "slides/intro-05-tuning-models.html#specifying-tuning-parameters",
    "href": "slides/intro-05-tuning-models.html#specifying-tuning-parameters",
    "title": "5 - Tuning models",
    "section": "Specifying tuning parameters",
    "text": "Specifying tuning parameters\nLetâ€™s take our previous random forest workflow and tag for tuning the minimum number of data points in each node:\n\nrf_spec &lt;- rand_forest(min_n = tune()) |&gt; \n  set_mode(\"classification\")\n\nrf_wflow &lt;- workflow(forested ~ ., rf_spec)\nrf_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   min_n = tune()\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "slides/intro-05-tuning-models.html#try-out-multiple-values",
    "href": "slides/intro-05-tuning-models.html#try-out-multiple-values",
    "title": "5 - Tuning models",
    "section": "Try out multiple values",
    "text": "Try out multiple values\ntune_grid() works similar to fit_resamples() but covers multiple parameter values:\n\nset.seed(22)\nrf_res &lt;- tune_grid(\n  rf_wflow,\n  forested_folds,\n  grid = 5\n)"
  },
  {
    "objectID": "slides/intro-05-tuning-models.html#compare-results",
    "href": "slides/intro-05-tuning-models.html#compare-results",
    "title": "5 - Tuning models",
    "section": "Compare results",
    "text": "Compare results\nInspecting results and selecting the best-performing hyperparameter(s):\n\nshow_best(rf_res)\n#&gt; # A tibble: 5 Ã— 7\n#&gt;   min_n .metric .estimator  mean     n std_err .config        \n#&gt;   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1    40 roc_auc binary     0.762    10 0.0103  pre0_mod5_post0\n#&gt; 2    31 roc_auc binary     0.761    10 0.0100  pre0_mod4_post0\n#&gt; 3    22 roc_auc binary     0.760    10 0.00997 pre0_mod3_post0\n#&gt; 4    12 roc_auc binary     0.758    10 0.0102  pre0_mod2_post0\n#&gt; 5     3 roc_auc binary     0.755    10 0.00980 pre0_mod1_post0\n\nbest_parameter &lt;- select_best(rf_res)\nbest_parameter\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   min_n .config        \n#&gt;   &lt;int&gt; &lt;chr&gt;          \n#&gt; 1    40 pre0_mod5_post0\n\ncollect_metrics() and autoplot() are also available."
  },
  {
    "objectID": "slides/intro-05-tuning-models.html#the-final-fit",
    "href": "slides/intro-05-tuning-models.html#the-final-fit",
    "title": "5 - Tuning models",
    "section": "The final fit",
    "text": "The final fit\n\nrf_wflow &lt;- finalize_workflow(rf_wflow, best_parameter)\n\nfinal_fit &lt;- last_fit(rf_wflow, forested_split) \n\ncollect_metrics(final_fit)\n#&gt; # A tibble: 3 Ã— 4\n#&gt;   .metric     .estimator .estimate .config        \n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n#&gt; 1 accuracy    binary         0.764 pre0_mod0_post0\n#&gt; 2 roc_auc     binary         0.764 pre0_mod0_post0\n#&gt; 3 brier_class binary         0.161 pre0_mod0_post0"
  },
  {
    "objectID": "slides/intro-05-tuning-models.html#your-turn",
    "href": "slides/intro-05-tuning-models.html#your-turn",
    "title": "5 - Tuning models",
    "section": "Your turn",
    "text": "Your turn\n\nModify your model workflow to tune one or more parameters.\nUse grid search to find the best parameter(s).\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#looking-at-the-predictors",
    "href": "slides/intro-extra-recipes.html#looking-at-the-predictors",
    "title": "Extras - Recipes",
    "section": "Looking at the predictors",
    "text": "Looking at the predictors\n\nforested_train\n#&gt; # A tibble: 8,749 Ã— 19\n#&gt;    forested  year elevation eastness roughness tree_no_tree dew_temp precip_annual temp_annual_mean temp_annual_min temp_annual_max temp_january_min vapor_min vapor_max canopy_cover   lon   lat\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 Yes       1997        66       82        10 Tree            12.2           1315             18.4            1.88            24.9            11.8        121      1888           66 -84.9  32.4\n#&gt;  2 No        1997       284      -99        58 Tree            10.3           1236             16.1           -0.26            22.3             9.92        68      1586           80 -85.0  34.1\n#&gt;  3 Yes       2022       130       86        15 Tree            11.8           1194             17.6            1.3             24.1            11.2         61      1753           96 -83.0  33.1\n#&gt;  4 Yes       2021       202      -55         3 Tree            10.7           1235             16.6            0.05            23.0            10.2         72      1682           65 -83.0  33.9\n#&gt;  5 Yes       1995        75      -89         1 Tree            13.8           1256             19.2            3.63            25.5            12.9         57      1796           88 -83.4  31.3\n#&gt;  6 No        1995       110      -53         5 Tree            12.4           1236             18.6            2.53            24.8            12.4        102      1835           51 -83.9  32.2\n#&gt;  7 Yes       2022       111       73        12 Tree            11.5           1168             17.4            1               24.0            10.9         67      1772           84 -82.2  33.6\n#&gt;  8 Yes       1997       230       96        14 Tree             9.98          1373             15.4           -1.35            21.8             9.03        46      1552           68 -85.3  34.8\n#&gt;  9 Yes       2002       160      -88        13 Tree            11.1           1219             16.9            0.07            23.6            10.2         53      1731           95 -83.2  33.6\n#&gt; 10 Yes       2020        39        9         6 Tree            13.9           1237             19.2            3.25            25.6            12.8         58      1812           86 -82.0  31.7\n#&gt; # â„¹ 8,739 more rows\n#&gt; # â„¹ 2 more variables: land_type &lt;fct&gt;, county &lt;fct&gt;"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#working-with-other-models",
    "href": "slides/intro-extra-recipes.html#working-with-other-models",
    "title": "Extras - Recipes",
    "section": "Working with other models",
    "text": "Working with other models\nSome models canâ€™t handle non-numeric data\n\nLinear Regression\nK Nearest Neighbors\n\n\n\nSome models struggle if numeric predictors arenâ€™t scaled\n\nK Nearest Neighbors\nAnything using gradient descent"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#types-of-needed-preprocessing",
    "href": "slides/intro-extra-recipes.html#types-of-needed-preprocessing",
    "title": "Extras - Recipes",
    "section": "Types of needed preprocessing",
    "text": "Types of needed preprocessing\n\nDo qualitative predictors require a numeric encoding?\nShould columns with a single unique value be removed?\nDoes the model struggle with missing data?\nDoes the model struggle with correlated predictors?\nShould predictors be centered and scaled?\nIs it helpful to transform predictors to be more symmetric?\n\n\nhttps://www.tmwr.org/pre-proc-table.html"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#two-types-of-preprocessing",
    "href": "slides/intro-extra-recipes.html#two-types-of-preprocessing",
    "title": "Extras - Recipes",
    "section": "Two types of preprocessing",
    "text": "Two types of preprocessing"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#two-types-of-preprocessing-1",
    "href": "slides/intro-extra-recipes.html#two-types-of-preprocessing-1",
    "title": "Extras - Recipes",
    "section": "Two types of preprocessing",
    "text": "Two types of preprocessing"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#general-definitions",
    "href": "slides/intro-extra-recipes.html#general-definitions",
    "title": "Extras - Recipes",
    "section": "General definitions",
    "text": "General definitions\n\nData preprocessing is what you do to make your model successful.\nFeature engineering is what you do to the original predictors to make the model do the least work to perform great."
  },
  {
    "objectID": "slides/intro-extra-recipes.html#working-with-dates",
    "href": "slides/intro-extra-recipes.html#working-with-dates",
    "title": "Extras - Recipes",
    "section": "Working with dates",
    "text": "Working with dates\nDatetime variables are automatically converted to an integer if given as a raw predictor. To avoid this, it can be re-encoded as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nLeap year\nIndicators for holidays"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#two-types-of-transformations",
    "href": "slides/intro-extra-recipes.html#two-types-of-transformations",
    "title": "Extras - Recipes",
    "section": "Two types of transformations",
    "text": "Two types of transformations\n\n\n\nStatic\n\nSquare root, log, inverse\nDummies for known levels\nDate time extractions\n\n\nTrained\n\nCentering & scaling\nImputation\nPCA\nAnything for unknown factor levels\n\n\n\nTrained methods need to calculate sufficient information to be applied again."
  },
  {
    "objectID": "slides/intro-extra-recipes.html#the-recipes-package",
    "href": "slides/intro-extra-recipes.html#the-recipes-package",
    "title": "Extras - Recipes",
    "section": "The recipes package",
    "text": "The recipes package\n\n\nModular + extensible\nWorks well with pipes ,|&gt; and %&gt;%\nDeferred evaluation\nIsolates test data from training data\nCan do things formulas canâ€™t"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#how-to-write-a-recipe",
    "href": "slides/intro-extra-recipes.html#how-to-write-a-recipe",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) |&gt;\nÂ Â step_dummy(all_nominal_predictors()) |&gt;\nÂ Â step_zv(all_predictors()) |&gt;\nÂ Â step_log(canopy_cover, offset = 0.5) |&gt;\nÂ Â step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#how-to-write-a-recipe-1",
    "href": "slides/intro-extra-recipes.html#how-to-write-a-recipe-1",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) |&gt;\nÂ Â step_dummy(all_nominal_predictors()) |&gt;\nÂ Â step_zv(all_predictors()) |&gt;\nÂ Â step_log(canopy_cover, offset = 0.5) |&gt;\nÂ Â step_normalize(all_numeric_predictors())\n\n\nStart by calling recipe() to denote the data source and variables used."
  },
  {
    "objectID": "slides/intro-extra-recipes.html#how-to-write-a-recipe-2",
    "href": "slides/intro-extra-recipes.html#how-to-write-a-recipe-2",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) |&gt;\nÂ Â step_dummy(all_nominal_predictors()) |&gt;\nÂ Â step_zv(all_predictors()) |&gt;\nÂ Â step_log(canopy_cover, offset = 0.5) |&gt;\nÂ Â step_normalize(all_numeric_predictors())\n\n\nSpecify what actions to take by adding step_*()s."
  },
  {
    "objectID": "slides/intro-extra-recipes.html#how-to-write-a-recipe-3",
    "href": "slides/intro-extra-recipes.html#how-to-write-a-recipe-3",
    "title": "Extras - Recipes",
    "section": "How to write a recipe",
    "text": "How to write a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) |&gt;\nÂ Â step_dummy(all_nominal_predictors()) |&gt;\nÂ Â step_zv(all_predictors()) |&gt;\nÂ Â step_log(canopy_cover, offset = 0.5) |&gt; Â Â step_normalize(all_numeric_predictors())\n\n\nUse {tidyselect} and recipes-specific selectors to denote affected variables."
  },
  {
    "objectID": "slides/intro-extra-recipes.html#using-a-recipe",
    "href": "slides/intro-extra-recipes.html#using-a-recipe",
    "title": "Extras - Recipes",
    "section": "Using a recipe",
    "text": "Using a recipe\n\nforested_rec &lt;- recipe(forested ~ ., data = forested_train) |&gt;\nÂ Â step_dummy(all_nominal_predictors()) |&gt;\nÂ Â step_zv(all_predictors()) |&gt;\nÂ Â step_log(canopy_cover, offset = 0.5) |&gt; Â Â step_normalize(all_numeric_predictors())\n\n\nSave the recipe we like so that we can use it in various places, e.g., with different models."
  },
  {
    "objectID": "slides/intro-extra-recipes.html#using-a-recipe-with-workflows",
    "href": "slides/intro-extra-recipes.html#using-a-recipe-with-workflows",
    "title": "Extras - Recipes",
    "section": "Using a recipe with workflows",
    "text": "Using a recipe with workflows\nRecipes are typically combined with a model in a workflow() object:\n\n\nforested_wflow &lt;- workflow() |&gt;\nÂ Â add_recipe(forested_rec) |&gt;\nÂ Â add_model(linear_reg())"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#recipes-are-estimated",
    "href": "slides/intro-extra-recipes.html#recipes-are-estimated",
    "title": "Extras - Recipes",
    "section": "Recipes are estimated",
    "text": "Recipes are estimated\nEvery preprocessing step in a recipe that involved calculations uses the training set. For example:\n\nLevels of a factor\nDetermination of zero-variance\nNormalization\nFeature extraction\n\nOnce a recipe is added to a workflow, this occurs when fit() is called."
  },
  {
    "objectID": "slides/intro-extra-recipes.html#debugging-a-recipe",
    "href": "slides/intro-extra-recipes.html#debugging-a-recipe",
    "title": "Extras - Recipes",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g.Â forested_rec) can be estimated manually with a function called prep(). It is analogous to fit(). See TMwR section 16.4.\n\n\n\n\nAnother function, bake(), is analogous to predict(), and gives you the processed data back."
  },
  {
    "objectID": "slides/intro-extra-recipes.html#your-turn",
    "href": "slides/intro-extra-recipes.html#your-turn",
    "title": "Extras - Recipes",
    "section": "Your turn",
    "text": "Your turn\n\n\nTake the recipe and prep() then bake() it to see what the resulting data set looks like.\nTry removing steps to see how the result changes.\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#printing-a-recipe",
    "href": "slides/intro-extra-recipes.html#printing-a-recipe",
    "title": "Extras - Recipes",
    "section": "Printing a recipe",
    "text": "Printing a recipe\n\nforested_rec\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 18\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Dummy variables from: all_nominal_predictors()\n#&gt; â€¢ Zero variance filter on: all_predictors()\n#&gt; â€¢ Log transformation on: canopy_cover\n#&gt; â€¢ Centering and scaling for: all_numeric_predictors()"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#prepping-a-recipe",
    "href": "slides/intro-extra-recipes.html#prepping-a-recipe",
    "title": "Extras - Recipes",
    "section": "Prepping a recipe",
    "text": "Prepping a recipe\n\nprep(forested_rec)\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:    1\n#&gt; predictor: 18\n#&gt; \n#&gt; â”€â”€ Training information\n#&gt; Training data contained 8749 data points and no incomplete rows.\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Dummy variables from: tree_no_tree, land_type, county | Trained\n#&gt; â€¢ Zero variance filter removed: &lt;none&gt; | Trained\n#&gt; â€¢ Log transformation on: canopy_cover | Trained\n#&gt; â€¢ Centering and scaling for: year elevation, ... | Trained"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#baking-a-recipe",
    "href": "slides/intro-extra-recipes.html#baking-a-recipe",
    "title": "Extras - Recipes",
    "section": "Baking a recipe",
    "text": "Baking a recipe\n\nprep(forested_rec) |&gt;\n  bake(new_data = forested_train)\n#&gt; # A tibble: 8,749 Ã— 177\n#&gt;      year elevation eastness roughness dew_temp precip_annual temp_annual_mean temp_annual_min temp_annual_max temp_january_min vapor_min vapor_max canopy_cover     lon     lat forested\n#&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n#&gt;  1 -1.15    -0.521    1.13      0.0463  -0.232          0.178            0.172          -0.107           0.338         -0.00909    3.10     1.01          0.407  -1.44   -0.0184 Yes     \n#&gt;  2 -1.15     1.10    -1.50      3.91    -1.39          -0.568           -1.31           -1.33           -1.28          -1.28       0.0408  -0.941         0.652  -1.51    1.36   No      \n#&gt;  3  0.892   -0.0440   1.19      0.449   -0.422         -0.965           -0.305          -0.438          -0.170         -0.438     -0.363    0.140         0.884   0.331   0.556  Yes     \n#&gt;  4  0.810    0.493   -0.858    -0.518   -1.11          -0.577           -1.01           -1.15           -0.878         -1.10       0.272   -0.320         0.388   0.331   1.26   Yes     \n#&gt;  5 -1.31    -0.454   -1.35     -0.679    0.748         -0.379            0.701           0.893           0.683          0.702     -0.594    0.418         0.773  -0.0643 -0.992  Yes     \n#&gt;  6 -1.31    -0.193   -0.829    -0.357   -0.0546        -0.568            0.331           0.264           0.281          0.366      2.00     0.670         0.0802 -0.468  -0.237  No      \n#&gt;  7  0.892   -0.186    1.00      0.207   -0.631         -1.21            -0.444          -0.609          -0.264         -0.619     -0.0169   0.262         0.714   1.01    0.929  Yes     \n#&gt;  8 -1.15     0.702    1.34      0.369   -1.56           0.726           -1.77           -1.95           -1.60          -1.87      -1.23    -1.16          0.445  -1.77    2.03   Yes     \n#&gt;  9 -0.740    0.180   -1.34      0.288   -0.858         -0.729           -0.801          -1.14           -0.515         -1.08      -0.825   -0.00273       0.870   0.122   0.988  Yes     \n#&gt; 10  0.729   -0.722    0.0719   -0.276    0.816         -0.559            0.708           0.676           0.745          0.635     -0.536    0.521         0.744   1.22   -0.653  Yes     \n#&gt; # â„¹ 8,739 more rows\n#&gt; # â„¹ 161 more variables: tree_no_tree_No.tree &lt;dbl&gt;, land_type_Non.tree.vegetation &lt;dbl&gt;, land_type_Tree &lt;dbl&gt;, county_Atkinson &lt;dbl&gt;, county_Bacon &lt;dbl&gt;, county_Baker &lt;dbl&gt;, county_Baldwin &lt;dbl&gt;,\n#&gt; #   county_Banks &lt;dbl&gt;, county_Barrow &lt;dbl&gt;, county_Bartow &lt;dbl&gt;, county_Ben.Hill &lt;dbl&gt;, county_Berrien &lt;dbl&gt;, county_Bibb &lt;dbl&gt;, county_Bleckley &lt;dbl&gt;, county_Brantley &lt;dbl&gt;, county_Brooks &lt;dbl&gt;,\n#&gt; #   county_Bryan &lt;dbl&gt;, county_Bulloch &lt;dbl&gt;, county_Burke &lt;dbl&gt;, county_Butts &lt;dbl&gt;, county_Calhoun &lt;dbl&gt;, county_Camden &lt;dbl&gt;, county_Candler &lt;dbl&gt;, county_Carroll &lt;dbl&gt;, county_Catoosa &lt;dbl&gt;,\n#&gt; #   county_Charlton &lt;dbl&gt;, county_Chatham &lt;dbl&gt;, county_Chattahoochee &lt;dbl&gt;, county_Chattooga &lt;dbl&gt;, county_Cherokee &lt;dbl&gt;, county_Clarke &lt;dbl&gt;, county_Clay &lt;dbl&gt;, county_Clayton &lt;dbl&gt;,\n#&gt; #   county_Clinch &lt;dbl&gt;, county_Cobb &lt;dbl&gt;, county_Coffee &lt;dbl&gt;, county_Colquitt &lt;dbl&gt;, county_Columbia &lt;dbl&gt;, county_Cook &lt;dbl&gt;, county_Coweta &lt;dbl&gt;, county_Crawford &lt;dbl&gt;, county_Crisp &lt;dbl&gt;,\n#&gt; #   county_Dade &lt;dbl&gt;, county_Dawson &lt;dbl&gt;, county_Decatur &lt;dbl&gt;, county_DeKalb &lt;dbl&gt;, county_Dodge &lt;dbl&gt;, county_Dooly &lt;dbl&gt;, county_Dougherty &lt;dbl&gt;, county_Douglas &lt;dbl&gt;, county_Early &lt;dbl&gt;, â€¦"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#tidying-a-recipe",
    "href": "slides/intro-extra-recipes.html#tidying-a-recipe",
    "title": "Extras - Recipes",
    "section": "Tidying a recipe",
    "text": "Tidying a recipe\nOnce a recipe as been estimated, there are various bits of information saved in it.\n\nThe tidy() function can be used to get specific results from the recipe."
  },
  {
    "objectID": "slides/intro-extra-recipes.html#your-turn-1",
    "href": "slides/intro-extra-recipes.html#your-turn-1",
    "title": "Extras - Recipes",
    "section": "Your turn",
    "text": "Your turn\n\nTake a prepped recipe and use the tidy() function on it.\nUse the number argument to inspect different steps.\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#tidying-a-recipe-1",
    "href": "slides/intro-extra-recipes.html#tidying-a-recipe-1",
    "title": "Extras - Recipes",
    "section": "Tidying a recipe",
    "text": "Tidying a recipe\n\nprep(forested_rec) |&gt;\n  tidy()\n#&gt; # A tibble: 4 Ã— 6\n#&gt;   number operation type      trained skip  id             \n#&gt;    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n#&gt; 1      1 step      dummy     TRUE    FALSE dummy_hIEnQ    \n#&gt; 2      2 step      zv        TRUE    FALSE zv_ZrQBx       \n#&gt; 3      3 step      log       TRUE    FALSE log_6es7X      \n#&gt; 4      4 step      normalize TRUE    FALSE normalize_XsIxb"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#tidying-a-recipe-2",
    "href": "slides/intro-extra-recipes.html#tidying-a-recipe-2",
    "title": "Extras - Recipes",
    "section": "Tidying a recipe",
    "text": "Tidying a recipe\n\nprep(forested_rec) |&gt;\n  tidy(number = 1)\n#&gt; # A tibble: 161 Ã— 3\n#&gt;    terms        columns             id         \n#&gt;    &lt;chr&gt;        &lt;chr&gt;               &lt;chr&gt;      \n#&gt;  1 tree_no_tree No tree             dummy_hIEnQ\n#&gt;  2 land_type    Non-tree vegetation dummy_hIEnQ\n#&gt;  3 land_type    Tree                dummy_hIEnQ\n#&gt;  4 county       Atkinson            dummy_hIEnQ\n#&gt;  5 county       Bacon               dummy_hIEnQ\n#&gt;  6 county       Baker               dummy_hIEnQ\n#&gt;  7 county       Baldwin             dummy_hIEnQ\n#&gt;  8 county       Banks               dummy_hIEnQ\n#&gt;  9 county       Barrow              dummy_hIEnQ\n#&gt; 10 county       Bartow              dummy_hIEnQ\n#&gt; # â„¹ 151 more rows"
  },
  {
    "objectID": "slides/intro-extra-recipes.html#using-a-recipe-in-tidymodels",
    "href": "slides/intro-extra-recipes.html#using-a-recipe-in-tidymodels",
    "title": "Extras - Recipes",
    "section": "Using a recipe in tidymodels",
    "text": "Using a recipe in tidymodels\nThe recommended way to use a recipe in tidymodels is to use it as part of a workflow().\n\nforested_wflow &lt;- workflow() |&gt;  \n  add_recipe(forested_rec) |&gt;  \n  add_model(linear_reg())\n\nWhen used in this way, you donâ€™t need to worry about prep() and bake() as it is handled for you."
  },
  {
    "objectID": "slides/intro-extra-recipes.html#more-information",
    "href": "slides/intro-extra-recipes.html#more-information",
    "title": "Extras - Recipes",
    "section": "More information",
    "text": "More information\n\nhttps://recipes.tidymodels.org/\nhttps://www.tmwr.org/recipes.html"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#workshop-policies",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#workshop-policies",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease do not photograph people wearing red lanyards\nThere are gender neutral bathrooms near National Harbor rooms\nA meditation room is located at National Harbor 9 (8am - 5pm, Mon - Thurs)\nA lactation room is located at Potomac Dressing Room (8am - 5pm, Mon - Thurs)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#workshop-policies-1",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#workshop-policies-1",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease review the rstudio::conf code of conduct, which applies to all workshops: https://www.rstudio.com/conference/2022/2022-conf-code-of-conduct/\nCoC site has info on how to report a problem (in person, email, phone)\nYou are required to wear a mask that fully covers your mouth and nose at all times in all public spaces"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#who-are-you",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %&gt;% or base R |&gt; pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have exposure to basic statistical concepts\nYou do not need intermediate or expert familiarity with modeling or ML"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#who-are-we",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#who-are-we",
    "title": "1 - Introduction",
    "section": "Who are we?",
    "text": "Who are we?\n\n\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\n\nJulia Silge\nDavid Robinson\nDavis Vaughan"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#who-are-we-1",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#who-are-we-1",
    "title": "1 - Introduction",
    "section": "Who are we?",
    "text": "Who are we?\n\n\n\nKelly Bodwin\nMichael Chow\nPritam Dalal\nMatt Dancho\nJon Harmon\n\n\n\nMike Mahoney\nEdgar Ruiz\nAsmae Toumi\nQiushi Yan\n\n\n\nMany thanks to Julie Jung, Alison Hill, and DesirÃ©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#asking-for-help",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#asking-for-help",
    "title": "1 - Introduction",
    "section": "Asking for help",
    "text": "Asking for help\n\nğŸŸª â€œIâ€™m stuck and need help!â€\n\n\nğŸŸ© â€œI finished the exerciseâ€"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#section-2",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#section-2",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#plan-for-this-workshop",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Plan for this workshop",
    "text": "Plan for this workshop\n\nToday:\n\nYour data budget\nWhat makes a model\nEvaluating models\n\nTomorrow:\n\nFeature engineering\nTuning hyperparameters\nWrapping up!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#section-3",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#section-3",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors ğŸ‘‹\n\n Log in to RStudio Cloud here (free):\nbit.ly/tidymodels-rstudioconf-2022"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-machine-learning",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-machine-learning",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nhttps://xkcd.com/1838/"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-machine-learning-1",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-machine-learning-1",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-machine-learning-2",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-machine-learning-2",
    "title": "1 - Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\n\n\nIllustration credit: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\n\n\nHow are statistics and machine learning related?\nHow are they similar? Different?\n\n\n\n03:00\n\n\n\n\nthe â€œtwo culturesâ€\nmodel first vs.Â data first\ninference vs.Â prediction"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-tidymodels",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#what-is-tidymodels",
    "title": "1 - Introduction",
    "section": "What is tidymodels? ",
    "text": "What is tidymodels? \n\nlibrary(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.0.0 â”€â”€\n#&gt; âœ” broom        1.0.0     âœ” rsample      1.0.0\n#&gt; âœ” dials        1.0.0     âœ” tibble       3.1.8\n#&gt; âœ” dplyr        1.0.9     âœ” tidyr        1.2.0\n#&gt; âœ” infer        1.0.2     âœ” tune         1.0.0\n#&gt; âœ” modeldata    1.0.0     âœ” workflows    1.0.0\n#&gt; âœ” parsnip      1.0.0     âœ” workflowsets 1.0.0\n#&gt; âœ” purrr        0.3.4     âœ” yardstick    1.0.0\n#&gt; âœ” recipes      1.0.1\n#&gt; â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\n#&gt; âœ– purrr::discard() masks scales::discard()\n#&gt; âœ– dplyr::filter()  masks stats::filter()\n#&gt; âœ– dplyr::lag()     masks stats::lag()\n#&gt; âœ– recipes::step()  masks stats::step()\n#&gt; â€¢ Learn how to get started at https://www.tidymodels.org/start/"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#the-whole-game",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#the-whole-game",
    "title": "1 - Introduction",
    "section": "The whole game",
    "text": "The whole game\n\nTomorrow we will walk through a case study in detail to illustrate feature engineering and model tuning.\nToday we will walk through the analysis at a higher level to show the model development process as a whole and give you an introduction to the data set.\nThe data are from the NHL where we want to predict whether a shot was on-goal or not! ğŸ’\nItâ€™s a good example to show how model development works."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#shots-on-goal",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#shots-on-goal",
    "title": "1 - Introduction",
    "section": "Shots on goal",
    "text": "Shots on goal"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#data-spending",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#data-spending",
    "title": "1 - Introduction",
    "section": "Data spending",
    "text": "Data spending"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#a-first-model",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#a-first-model",
    "title": "1 - Introduction",
    "section": "A first model",
    "text": "A first model"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#starting-point-logistic-regression",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#starting-point-logistic-regression",
    "title": "1 - Introduction",
    "section": "Starting point: logistic regression",
    "text": "Starting point: logistic regression\n\nWeâ€™ll start by using basic logistic regression to predict our binary outcome.\nOur first model will have 16 simple predictor columns.\nOne initial question: there are 640 players taking shots.\nFor logistic regression, do we convert these to binary indicators (a.k.a. â€œdummiesâ€)?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#basic-features-inc-dummy-variables",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#basic-features-inc-dummy-variables",
    "title": "1 - Introduction",
    "section": "Basic features (inc dummy variables)",
    "text": "Basic features (inc dummy variables)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#different-player-encoding",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#different-player-encoding",
    "title": "1 - Introduction",
    "section": "Different player encoding",
    "text": "Different player encoding"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#what-about-location",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#what-about-location",
    "title": "1 - Introduction",
    "section": "What about location",
    "text": "What about location\nThe previous models used the x/y coordinates.\nAre there better ways to represent shot location?\nHow can we make location more usable for the model?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#add-shot-angle",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#add-shot-angle",
    "title": "1 - Introduction",
    "section": "Add shot angle?",
    "text": "Add shot angle?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#add-shot-distance",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#add-shot-distance",
    "title": "1 - Introduction",
    "section": "Add shot distance?",
    "text": "Add shot distance?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#add-shot-behind-goal-line",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#add-shot-behind-goal-line",
    "title": "1 - Introduction",
    "section": "Add shot behind goal line?",
    "text": "Add shot behind goal line?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#nonlinear-terms-for-angle-and-distance",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#nonlinear-terms-for-angle-and-distance",
    "title": "1 - Introduction",
    "section": "Nonlinear terms for angle and distance",
    "text": "Nonlinear terms for angle and distance"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#try-another-model",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#try-another-model",
    "title": "1 - Introduction",
    "section": "Try another model",
    "text": "Try another model"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#switch-to-boosting-and-basic-features",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#switch-to-boosting-and-basic-features",
    "title": "1 - Introduction",
    "section": "Switch to boosting and basic features",
    "text": "Switch to boosting and basic features"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#boosting-with-location-features",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#boosting-with-location-features",
    "title": "1 - Introduction",
    "section": "Boosting with location features",
    "text": "Boosting with location features"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#choose-wisely",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#choose-wisely",
    "title": "1 - Introduction",
    "section": "Choose wiselyâ€¦",
    "text": "Choose wiselyâ€¦"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#finalize-and-verify",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#finalize-and-verify",
    "title": "1 - Introduction",
    "section": "Finalize and verify",
    "text": "Finalize and verify"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#and-so-on",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#and-so-on",
    "title": "1 - Introduction",
    "section": "â€¦ and so on",
    "text": "â€¦ and so on\nOnce we find an acceptable model and feature set, the process is to\n\nConfirm our results on the test set.\nDocument the data and model development process.\nDeploy, monitor, etc."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#lets-install-some-packages",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Letâ€™s install some packages",
    "text": "Letâ€™s install some packages\nIf you are using your own laptop instead of RStudio Cloud:\n\ninstall.packages(c(\"DALEXtra\", \"doParallel\", \"embed\", \"forcats\",\n                   \"lme4\", \"ranger\", \"remotes\", \"rpart\", \n                   \"rpart.plot\", \"stacks\", \"tidymodels\",\n                   \"vetiver\", \"xgboost\"))\n\nremotes::install_github(\"topepo/ongoal@v0.0.2\")\n\n\n\n Or log in to RStudio Cloud:\nbit.ly/tidymodels-rstudioconf-2022"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/01-introduction.html#our-versions",
    "href": "archive/2022-07-RStudio-conf/01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\nbroom (1.0.0, CRAN), DALEX (2.4.0, CRAN), DALEXtra (2.2.0, CRAN), dials (1.0.0, CRAN), doParallel (1.0.17, CRAN), dplyr (1.0.9, CRAN), embed (1.0.0, CRAN), ggplot2 (3.3.6, CRAN), modeldata (1.0.0, CRAN), ongoal (0.0.2, Github (topepo/ongoal@02cd6b233), parsnip (1.0.0, CRAN), purrr (0.3.4, CRAN), ranger (0.13.1, CRAN), recipes (1.0.1, local), rpart (4.1.16, CRAN), rpart.plot (3.1.1, CRAN), rsample (1.0.0, CRAN), scales (1.2.0, CRAN), stacks (0.2.3, CRAN), tibble (3.1.8, CRAN), tidymodels (1.0.0, CRAN), tidyr (1.2.0, CRAN), tune (1.0.0, CRAN), vetiver (0.1.5, CRAN), workflows (1.0.0, CRAN), workflowsets (1.0.0, CRAN), xgboost (1.6.0.1, CRAN), and yardstick (1.0.0, CRAN)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nHow do you fit a linear model in R?\nHow many different ways can you think of?\n\n\n\n03:00\n\n\n\n\n\nlm for linear model\nglm for generalized linear model (e.g.Â logistic regression)\nglmnet for regularized regression\nkeras for regression using TensorFlow\nstan for Bayesian regression\nspark for large data sets"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-1",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlinear_reg()\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\nModels have default engines"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-2",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-2",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-3",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-3",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlinear_reg() %&gt;%\n  set_engine(\"glmnet\")\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: glmnet"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-4",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-4",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\nlinear_reg() %&gt;%\n  set_engine(\"stan\")\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: stan"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-5",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-5",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-6",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-6",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree()\n#&gt; Decision Tree Model Specification (unknown)\n#&gt; \n#&gt; Computational engine: rpart\n\n\nSome models have a default mode"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-7",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-7",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\ndecision_tree() %&gt;% \n  set_mode(\"regression\")\n#&gt; Decision Tree Model Specification (regression)\n#&gt; \n#&gt; Computational engine: rpart\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-8",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#to-specify-a-model-8",
    "title": "3 - What makes a model?",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nChoose a model\nSpecify an engine\nSet the mode"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-1",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_spec chunk in your .qmd.\nEdit this code so it creates a different model.\n\n\n\n05:00\n\n\n\n\n\nAll available models are listed at https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#models-well-be-using-today",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#models-well-be-using-today",
    "title": "3 - What makes a model?",
    "section": "Models weâ€™ll be using today",
    "text": "Models weâ€™ll be using today\n\nLinear regression\nDecision trees"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#linear-regression",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#linear-regression",
    "title": "3 - What makes a model?",
    "section": "Linear regression",
    "text": "Linear regression"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#linear-regression-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#linear-regression-1",
    "title": "3 - What makes a model?",
    "section": "Linear regression",
    "text": "Linear regression"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#linear-regression-2",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#linear-regression-2",
    "title": "3 - What makes a model?",
    "section": "Linear regression",
    "text": "Linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome modeled as linear combination of predictors:\n\n\\(\\mbox{latency} = \\beta_0 + \\beta_1\\cdot\\mbox{age} + \\epsilon\\)\n\nFind a line that minimizes the mean squared error (MSE)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#decision-trees",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#decision-trees",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#decision-trees-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#decision-trees-1",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#decision-trees-2",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#decision-trees-2",
    "title": "3 - What makes a model?",
    "section": "Decision trees",
    "text": "Decision trees"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#all-models-are-wrong-but-some-are-useful",
    "title": "3 - What makes a model?",
    "section": "All models are wrong, but some are useful!",
    "text": "All models are wrong, but some are useful!\n\n\nLinear regression\n\n\n\n\n\n\n\n\n\n\nDecision trees"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#workflows-bind-preprocessors-and-models",
    "title": "3 - What makes a model?",
    "section": "Workflows bind preprocessors and models",
    "text": "Workflows bind preprocessors and models\n\n\nExplain that PCA that is a preprocessor / dimensionality reduction, used to decorrelate data"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#what-is-wrong-with-this",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#what-is-wrong-with-this",
    "title": "3 - What makes a model?",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#why-a-workflow",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#why-a-workflow",
    "title": "3 - What makes a model?",
    "section": "Why a workflow()? ",
    "text": "Why a workflow()? \n\n\nWorkflows handle new data better than base R tools in terms of new factor levels\n\n\n\n\nYou can use other preprocessors besides formulas (more on feature engineering tomorrow!)\n\n\n\n\nThey can help organize your work when working with multiple models\n\n\n\n\nMost importantly, a workflow captures the entire modeling process: fit() and predict() apply to the preprocessing steps in addition to the actual model fit\n\n\nTwo ways workflows handle levels better than base R:\n\nEnforces that new levels are not allowed at prediction time (this is an optional check that can be turned off)\nRestores missing levels that were present at fit time, but happen to be missing at prediction time (like, if your â€œnewâ€ data just doesnâ€™t have an instance of that level)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#a-model-workflow-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#a-model-workflow-1",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"regression\")\n\ntree_spec %&gt;% \n  fit(latency ~ ., data = frog_train) \n#&gt; parsnip model object\n#&gt; \n#&gt; n= 456 \n#&gt; \n#&gt; node), split, n, deviance, yval\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 456 2197966.00  92.90351  \n#&gt;    2) age&gt;=4.947975 256  252347.40  60.89844  \n#&gt;      4) treatment=control 131   91424.06  48.42748 *\n#&gt;      5) treatment=gentamicin 125  119197.90  73.96800 *\n#&gt;    3) age&lt; 4.947975 200 1347741.00 133.87000  \n#&gt;      6) treatment=control 140  986790.70 118.25710  \n#&gt;       12) reflex=mid,full 129  754363.70 111.56590 *\n#&gt;       13) reflex=low 11  158918.20 196.72730 *\n#&gt;      7) treatment=gentamicin 60  247194.60 170.30000  \n#&gt;       14) age&lt; 4.664439 30  102190.20 147.83330  \n#&gt;         28) age&gt;=4.566638 22   53953.86 129.77270 *\n#&gt;         29) age&lt; 4.566638 8   21326.00 197.50000 *\n#&gt;       15) age&gt;=4.664439 30  114719.40 192.76670 *"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#a-model-workflow-2",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#a-model-workflow-2",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"regression\")\n\nworkflow() %&gt;%\n  add_formula(latency ~ .) %&gt;%\n  add_model(tree_spec) %&gt;%\n  fit(data = frog_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; latency ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 456 \n#&gt; \n#&gt; node), split, n, deviance, yval\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 456 2197966.00  92.90351  \n#&gt;    2) age&gt;=4.947975 256  252347.40  60.89844  \n#&gt;      4) treatment=control 131   91424.06  48.42748 *\n#&gt;      5) treatment=gentamicin 125  119197.90  73.96800 *\n#&gt;    3) age&lt; 4.947975 200 1347741.00 133.87000  \n#&gt;      6) treatment=control 140  986790.70 118.25710  \n#&gt;       12) reflex=mid,full 129  754363.70 111.56590 *\n#&gt;       13) reflex=low 11  158918.20 196.72730 *\n#&gt;      7) treatment=gentamicin 60  247194.60 170.30000  \n#&gt;       14) age&lt; 4.664439 30  102190.20 147.83330  \n#&gt;         28) age&gt;=4.566638 22   53953.86 129.77270 *\n#&gt;         29) age&lt; 4.566638 8   21326.00 197.50000 *\n#&gt;       15) age&gt;=4.664439 30  114719.40 192.76670 *"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#a-model-workflow-3",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#a-model-workflow-3",
    "title": "3 - What makes a model?",
    "section": "A model workflow  ",
    "text": "A model workflow  \n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"regression\")\n\nworkflow(latency ~ ., tree_spec) %&gt;% \n  fit(data = frog_train) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; latency ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 456 \n#&gt; \n#&gt; node), split, n, deviance, yval\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 456 2197966.00  92.90351  \n#&gt;    2) age&gt;=4.947975 256  252347.40  60.89844  \n#&gt;      4) treatment=control 131   91424.06  48.42748 *\n#&gt;      5) treatment=gentamicin 125  119197.90  73.96800 *\n#&gt;    3) age&lt; 4.947975 200 1347741.00 133.87000  \n#&gt;      6) treatment=control 140  986790.70 118.25710  \n#&gt;       12) reflex=mid,full 129  754363.70 111.56590 *\n#&gt;       13) reflex=low 11  158918.20 196.72730 *\n#&gt;      7) treatment=gentamicin 60  247194.60 170.30000  \n#&gt;       14) age&lt; 4.664439 30  102190.20 147.83330  \n#&gt;         28) age&gt;=4.566638 22   53953.86 129.77270 *\n#&gt;         29) age&lt; 4.566638 8   21326.00 197.50000 *\n#&gt;       15) age&gt;=4.664439 30  114719.40 192.76670 *"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-2",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-2",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the tree_wflow chunk in your .qmd.\nEdit this code so it uses a linear model.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#predict-with-your-model",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#predict-with-your-model",
    "title": "3 - What makes a model?",
    "section": "Predict with your model  ",
    "text": "Predict with your model  \nHow do you use your new tree_fit model?\n\ntree_spec &lt;-\n  decision_tree() %&gt;% \n  set_mode(\"regression\")\n\ntree_fit &lt;-\n  workflow(latency ~ ., tree_spec) %&gt;% \n  fit(data = frog_train)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-3",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-3",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\npredict(tree_fit, new_data = frog_test)\nWhat do you get?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-4",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-4",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun:\naugment(tree_fit, new_data = frog_test)\nWhat do you get?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#understand-your-model",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#understand-your-model",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#understand-your-model-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#understand-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nlibrary(rpart.plot)\ntree_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot(roundint = FALSE)\n\nYou can extract_*() several components of your fitted workflow.\n\nroundint = FALSE is only to quiet a warning"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#understand-your-model-2",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#understand-your-model-2",
    "title": "3 - What makes a model?",
    "section": "Understand your model  ",
    "text": "Understand your model  \nHow do you understand your new tree_fit model?\n\nYou can use your fitted workflow for model and/or prediction explanations:\n\n\n\noverall variable importance, such as with the vip package\n\n\n\n\nflexible model explainers, such as with the DALEXtra package\n\n\n\nLearn more at https://www.tmwr.org/explain.html"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-5",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-5",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nExtract the model engine object from your fitted linear workflow.\nâš ï¸ Never predict() with any extracted components!\n\n\n\n05:00\n\n\n\n\nAfterward, ask what kind of object people got from the extraction, and what they did with it (e.g.Â give it to summary(), plot(), broom::tidy() ). Live code along"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#deploying-a-model",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#deploying-a-model",
    "title": "3 - What makes a model?",
    "section": "Deploying a model ",
    "text": "Deploying a model \nHow do you use your new tree_fit model in production?\n\nlibrary(vetiver)\nv &lt;- vetiver_model(tree_fit, \"frog_hatching\")\nv\n#&gt; \n#&gt; â”€â”€ frog_hatching â”€ &lt;butchered_workflow&gt; model for deployment \n#&gt; A rpart regression modeling workflow using 4 features\n\nLearn more at https://vetiver.rstudio.com"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#deploy-your-model-1",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#deploy-your-model-1",
    "title": "3 - What makes a model?",
    "section": "Deploy your model ",
    "text": "Deploy your model \nHow do you use your new model tree_fit in production?\n\nlibrary(plumber)\npr() %&gt;%\n  vetiver_api(v)\n#&gt; # Plumber router with 2 endpoints, 4 filters, and 1 sub-router.\n#&gt; # Use `pr_run()` on this object to start the API.\n#&gt; â”œâ”€â”€[queryString]\n#&gt; â”œâ”€â”€[body]\n#&gt; â”œâ”€â”€[cookieParser]\n#&gt; â”œâ”€â”€[sharedSecret]\n#&gt; â”œâ”€â”€/logo\n#&gt; â”‚  â”‚ # Plumber static router serving from directory: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/vetiver\n#&gt; â”œâ”€â”€/ping (GET)\n#&gt; â””â”€â”€/predict (POST)\n\nLearn more at https://vetiver.rstudio.com\n\nLive-code making a prediction"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-6",
    "href": "archive/2022-07-RStudio-conf/03-what-makes-a-model.html#your-turn-6",
    "title": "3 - What makes a model?",
    "section": "Your turn",
    "text": "Your turn\n\nRun the vetiver chunk in your .qmd.\nCheck out the automated visual documentation.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#working-with-our-predictors",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#working-with-our-predictors",
    "title": "5 - Feature engineering",
    "section": "Working with our predictors",
    "text": "Working with our predictors\nWe might want to modify our predictors columns for a few reasons:\n\nThe model requires them in a different format (e.g.Â dummy variables for lm()).\nThe model needs certain data qualities (e.g.Â same units for K-NN).\nThe outcome is better predicted when one or more columns are transformed in some way (a.k.a â€œfeature engineeringâ€).\n\n\nThe first two reasons are fairly predictable (next page).\nThe last one depends on your modeling problem."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#what-is-feature-engineering",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#what-is-feature-engineering",
    "title": "5 - Feature engineering",
    "section": "What is feature engineering?",
    "text": "What is feature engineering?\nThink of a feature as some representation of a predictor that will be used in a model.\n\nExample representations:\n\nInteractions\nPolynomial expansions/splines\nPCA feature extraction\n\nThere are a lot of examples in Feature Engineering and Selection."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#example-dates",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#example-dates",
    "title": "5 - Feature engineering",
    "section": "Example: Dates",
    "text": "Example: Dates\nHow can we represent date columns for our model?\n\nWhen a date column is used in its native format, it is usually converted by an R model to an integer.\n\n\nIt can be re-engineered as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nIndicators for holidays\n\n\nThe main point is that we try to maximize performance with different versions of the predictors.\nMention that, for the Chicago data, the day or the week features are usually the most important ones in the model."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#general-definitions",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#general-definitions",
    "title": "5 - Feature engineering",
    "section": "General definitions ",
    "text": "General definitions \n\nData preprocessing steps allow your model to fit.\nFeature engineering steps help the model do the least work to predict the outcome as well as possible.\n\nThe recipes package can handle both!\nIn a little bit, weâ€™ll see successful (and unsuccessful) feature engineering methods for our example data.\n\nThese terms are often used interchangeably in the ML community but we want to distinguish them."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#the-nhl-data",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#the-nhl-data",
    "title": "5 - Feature engineering",
    "section": "The NHL data ğŸ’",
    "text": "The NHL data ğŸ’\n\nFrom Pittsburgh Penguins games, 12,147 shots\nData from the 2015-2016 season\n\n\nLetâ€™s predict whether a shot is on-goal (a goal or blocked by goaltender) or not."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#case-study",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#case-study",
    "title": "5 - Feature engineering",
    "section": "Case study",
    "text": "Case study\n\nlibrary(tidymodels)\nlibrary(ongoal)\n\ntidymodels_prefer()\n\nglimpse(season_2015)\n#&gt; Rows: 12,147\n#&gt; Columns: 17\n#&gt; $ on_goal           &lt;fct&gt; yes, no, no, yes, no, no, yes, no, yes, no, no, no, â€¦\n#&gt; $ period            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦\n#&gt; $ period_type       &lt;fct&gt; regular, regular, regular, regular, regular, regularâ€¦\n#&gt; $ coord_x           &lt;dbl&gt; -53, 68, -42, -77, -67, 55, 77, 62, 59, 76, 44, 62, â€¦\n#&gt; $ coord_y           &lt;dbl&gt; -18, -12, -18, 9, -5, -12, 13, 14, -5, -6, 7, -2, -2â€¦\n#&gt; $ game_time         &lt;dbl&gt; 0.300000, 0.900000, 1.250000, 1.783333, 2.050000, 3.â€¦\n#&gt; $ strength          &lt;fct&gt; even, even, even, even, even, even, even, even, evenâ€¦\n#&gt; $ player            &lt;fct&gt; victor_hedman, evgeni_malkin, jason_garrison, ondrejâ€¦\n#&gt; $ player_diff       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n#&gt; $ offense_team      &lt;fct&gt; TBL, PIT, TBL, TBL, TBL, PIT, PIT, PIT, PIT, PIT, PIâ€¦\n#&gt; $ defense_team      &lt;fct&gt; PIT, TBL, PIT, PIT, PIT, TBL, TBL, TBL, TBL, TBL, TBâ€¦\n#&gt; $ offense_goal_diff &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n#&gt; $ game_type         &lt;fct&gt; playoff, playoff, playoff, playoff, playoff, playoffâ€¦\n#&gt; $ position          &lt;fct&gt; defenseman, center, defenseman, left_wing, defensemaâ€¦\n#&gt; $ dow               &lt;fct&gt; Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sat, Sat, Saâ€¦\n#&gt; $ month             &lt;fct&gt; May, May, May, May, May, May, May, May, May, May, Maâ€¦\n#&gt; $ year              &lt;dbl&gt; 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016â€¦"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#data-splitting-strategy",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#data-splitting-strategy",
    "title": "5 - Feature engineering",
    "section": "Data splitting strategy",
    "text": "Data splitting strategy"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#why-a-validation-set",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#why-a-validation-set",
    "title": "5 - Feature engineering",
    "section": "Why a validation set?",
    "text": "Why a validation set?\nRecall that resampling gives us performance measures without using the test set.\nItâ€™s important to get good resampling statistics (e.g.Â \\(R^2\\)).\n\nThat usually means having enough data to estimate performance.\n\nWhen you have â€œa lotâ€ of data, a validation set can be an efficient way to do this."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#splitting-the-nhl-data",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#splitting-the-nhl-data",
    "title": "5 - Feature engineering",
    "section": "Splitting the NHL data ",
    "text": "Splitting the NHL data \n\nset.seed(23)\nnhl_split &lt;- initial_split(season_2015, prop = 3/4)\nnhl_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;9110/3037/12147&gt;\n\nnhl_train_and_val &lt;- training(nhl_split)\nnhl_test  &lt;- testing(nhl_split)\n\n## not testing\nnrow(nhl_train_and_val)\n#&gt; [1] 9110\n \n## testing\nnrow(nhl_test)\n#&gt; [1] 3037"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#validation-split",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#validation-split",
    "title": "5 - Feature engineering",
    "section": "Validation split ",
    "text": "Validation split \nSince there are a lot of observations, weâ€™ll use a validation set:\n\nset.seed(234)\nnhl_val &lt;- validation_split(nhl_train_and_val, prop = 0.80)\nnhl_val\n#&gt; # Validation Set Split (0.8/0.2)  \n#&gt; # A tibble: 1 Ã— 2\n#&gt;   splits              id        \n#&gt;   &lt;list&gt;              &lt;chr&gt;     \n#&gt; 1 &lt;split [7288/1822]&gt; validation\n\n\nRemember that a validation split is a type of resample."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nLetâ€™s explore the training set data.\nUse the function plot_nhl_shots() for nice spatial plots of the data.\n\n\n\nnhl_train &lt;- analysis(nhl_val$splits[[1]])\n\nset.seed(100)\nnhl_train %&gt;% \n  sample_n(200) %&gt;%\n  plot_nhl_shots(emphasis = position)\n\n\n\n\n\n\n\n\n\n\n\n\n08:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#prepare-your-data-for-modeling",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#prepare-your-data-for-modeling",
    "title": "5 - Feature engineering",
    "section": "Prepare your data for modeling ",
    "text": "Prepare your data for modeling \n\nThe recipes package is an extensible framework for pipeable sequences of feature engineering steps that provide preprocessing tools to be applied to data.\n\n\n\nStatistical parameters for the steps can be estimated from an initial data set and then applied to other data sets.\n\n\n\n\nThe resulting processed output can be used as inputs for statistical or machine learning models."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#a-first-recipe",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#a-first-recipe",
    "title": "5 - Feature engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train)\n\n\n\nThe recipe() function assigns columns to roles of â€œoutcomeâ€ or â€œpredictorâ€ using the formula"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#a-first-recipe-1",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#a-first-recipe-1",
    "title": "5 - Feature engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nsummary(nhl_rec)\n#&gt; # A tibble: 17 Ã— 4\n#&gt;    variable          type    role      source  \n#&gt;    &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 period            numeric predictor original\n#&gt;  2 period_type       nominal predictor original\n#&gt;  3 coord_x           numeric predictor original\n#&gt;  4 coord_y           numeric predictor original\n#&gt;  5 game_time         numeric predictor original\n#&gt;  6 strength          nominal predictor original\n#&gt;  7 player            nominal predictor original\n#&gt;  8 player_diff       numeric predictor original\n#&gt;  9 offense_team      nominal predictor original\n#&gt; 10 defense_team      nominal predictor original\n#&gt; 11 offense_goal_diff numeric predictor original\n#&gt; 12 game_type         nominal predictor original\n#&gt; 13 position          nominal predictor original\n#&gt; 14 dow               nominal predictor original\n#&gt; 15 month             nominal predictor original\n#&gt; 16 year              numeric predictor original\n#&gt; 17 on_goal           nominal outcome   original"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#create-indicator-variables",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#create-indicator-variables",
    "title": "5 - Feature engineering",
    "section": "Create indicator variables ",
    "text": "Create indicator variables \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\n\nFor any factor or character predictors, make binary indicators.\nThere are many recipe steps that can convert categorical predictors to numeric columns."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#filter-out-constant-columns",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#filter-out-constant-columns",
    "title": "5 - Feature engineering",
    "section": "Filter out constant columns ",
    "text": "Filter out constant columns \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())\n\n\nIn case there is a factor level that was never observed in the training data (resulting in a column of all 0s), we can delete any zero-variance predictors that have a single unique value.\n\nNote that the selector chooses all columns with a role of â€œpredictorâ€"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#normalization",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#normalization",
    "title": "5 - Feature engineering",
    "section": "Normalization ",
    "text": "Normalization \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\n\n\nThis centers and scales the numeric predictors.\nThe recipe will use the training set to estimate the means and standard deviations of the data.\n\n\n\n\nAll data the recipe is applied to will be normalized using those statistics (there is no re-estimation)."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#reduce-correlation",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#reduce-correlation",
    "title": "5 - Feature engineering",
    "section": "Reduce correlation ",
    "text": "Reduce correlation \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_corr(all_numeric_predictors(), threshold = 0.9)\n\n\nTo deal with highly correlated predictors, find the minimum set of predictor columns that make the pairwise correlations less than the threshold."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#other-possible-steps",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#other-possible-steps",
    "title": "5 - Feature engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_pca(all_numeric_predictors())\n\n\nPCA feature extractionâ€¦"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#other-possible-steps-1",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#other-possible-steps-1",
    "title": "5 - Feature engineering",
    "section": "Other possible steps  ",
    "text": "Other possible steps  \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  embed::step_umap(all_numeric_predictors(), outcome = on_goal)\n\n\nA fancy machine learning supervised dimension reduction techniqueâ€¦\n\nNote that this uses the outcome, and it is from an extension package"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#other-possible-steps-2",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#other-possible-steps-2",
    "title": "5 - Feature engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nnhl_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_ns(coord_y, coord_x, deg_free = 10)\n\n\nNonlinear transforms like natural splines, and so on!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-1",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-1",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a recipe() for the on-goal data to :\n\ncreate one-hot indicator variables\nremove zero-variance variables\n\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#minimal-recipe",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#minimal-recipe",
    "title": "5 - Feature engineering",
    "section": "Minimal recipe ",
    "text": "Minimal recipe \n\nnhl_indicators &lt;-\n  recipe(on_goal ~ ., data = nhl_train) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#using-a-workflow",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#using-a-workflow",
    "title": "5 - Feature engineering",
    "section": "Using a workflow    ",
    "text": "Using a workflow    \n\nset.seed(9)\n\nnhl_glm_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(nhl_indicators) %&gt;%\n  add_model(logistic_reg())\n \nctrl &lt;- control_resamples(save_pred = TRUE)\nnhl_glm_res &lt;-\n  nhl_glm_wflow %&gt;%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_glm_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.555     1      NA Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.558     1      NA Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-2",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-2",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() to fit your workflow with a recipe.\nCollect the predictions from the results.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#holdout-predictions",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#holdout-predictions",
    "title": "5 - Feature engineering",
    "section": "Holdout predictions    ",
    "text": "Holdout predictions    \n\n# Since we used `save_pred = TRUE`\nglm_val_pred &lt;- collect_predictions(nhl_glm_res)\nglm_val_pred %&gt;% slice(1:7)\n#&gt; # A tibble: 7 Ã— 7\n#&gt;   id         .pred_yes .pred_no  .row .pred_class on_goal .config             \n#&gt;   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;   &lt;chr&gt;               \n#&gt; 1 validation     0.198 8.02e- 1    10 no          no      Preprocessor1_Model1\n#&gt; 2 validation     0.264 7.36e- 1    17 no          yes     Preprocessor1_Model1\n#&gt; 3 validation     0.189 8.11e- 1    23 no          no      Preprocessor1_Model1\n#&gt; 4 validation     1.00  8.39e-11    40 yes         yes     Preprocessor1_Model1\n#&gt; 5 validation     0.322 6.78e- 1    41 no          yes     Preprocessor1_Model1\n#&gt; 6 validation     1.00  8.39e-11    46 yes         no      Preprocessor1_Model1\n#&gt; 7 validation     0.354 6.46e- 1    55 no          no      Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#two-class-data-1",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#two-class-data-1",
    "title": "5 - Feature engineering",
    "section": "Two class data",
    "text": "Two class data\nThese definitions assume that we know the threshold for converting â€œsoftâ€ probability predictions into â€œhardâ€ class predictions.\n\nIs a 50% threshold good?\nWhat happens if we say that we need to be 80% sure to declare an event?\n\nsensitivity â¬‡ï¸, specificity â¬†ï¸\n\n\n\nWhat happens for a 20% threshold?\n\nsensitivity â¬†ï¸, specificity â¬‡ï¸"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#roc-curves",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#roc-curves",
    "title": "5 - Feature engineering",
    "section": "ROC curves",
    "text": "ROC curves\nTo make an ROC (receiver operator characteristic) curve, we:\n\ncalculate the sensitivity and specificity for all possible thresholds\nplot false positive rate (x-axis) versus true positive rate (y-axis)\n\n\nWe can use the area under the ROC curve as a classification metric:\n\nROC AUC = 1 ğŸ’¯\nROC AUC = 1/2 ğŸ˜¢\n\n\nROC curves are insensitive to class imbalance."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#roc-curves-1",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#roc-curves-1",
    "title": "5 - Feature engineering",
    "section": "ROC curves ",
    "text": "ROC curves \n\n# Assumes _first_ factor level is event; there are options to change that\nroc_curve_points &lt;- glm_val_pred %&gt;% roc_curve(truth = on_goal, estimate = .pred_yes)\nroc_curve_points %&gt;% slice(1, 50, 100)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .threshold specificity sensitivity\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1   -Inf          0            1    \n#&gt; 2      0.139      0.0303       0.977\n#&gt; 3      0.272      0.0642       0.955\n\nglm_val_pred %&gt;% roc_auc(truth = on_goal, estimate = .pred_yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.558"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#roc-curve-plot",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#roc-curve-plot",
    "title": "5 - Feature engineering",
    "section": "ROC curve plot ",
    "text": "ROC curve plot \n\nautoplot(roc_curve_points)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-3",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-3",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCompute and plot an ROC curve for your current model.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-4",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-4",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nWhat data are being used for this ROC curve plot?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#what-do-we-do-with-the-player-data",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#what-do-we-do-with-the-player-data",
    "title": "5 - Feature engineering",
    "section": "What do we do with the player data? ğŸ’",
    "text": "What do we do with the player data? ğŸ’\nThere are 597 unique player values in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables ğŸ˜³\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the player column with the estimated effect of that predictor\n\n\n\nLetâ€™s use an effect encoding."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#what-is-an-effect-encoding",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#what-is-an-effect-encoding",
    "title": "5 - Feature engineering",
    "section": "What is an effect encoding?",
    "text": "What is an effect encoding?\nWe replace the qualitativeâ€™s predictor data with their effect on the outcome.\n\n\nData before:\n\nbefore\n#&gt; # A tibble: 7 Ã— 3\n#&gt;   on_goal player            .row\n#&gt;   &lt;fct&gt;   &lt;fct&gt;            &lt;int&gt;\n#&gt; 1 yes     brian_dumoulin       1\n#&gt; 2 yes     patric_hornqvist     2\n#&gt; 3 yes     nikita_nesterov      3\n#&gt; 4 yes     jack_eichel          4\n#&gt; 5 yes     justin_williams      5\n#&gt; 6 yes     seth_jones           6\n#&gt; 7 yes     kris_letang          7\n\n\nData after:\n\nafter\n#&gt; # A tibble: 7 Ã— 3\n#&gt;   on_goal  player  .row\n#&gt;   &lt;fct&gt;     &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 yes     -0.114      1\n#&gt; 2 yes      0.631      2\n#&gt; 3 yes      0.142      3\n#&gt; 4 yes      0.220      4\n#&gt; 5 yes      0.248      5\n#&gt; 6 yes      0.0733     6\n#&gt; 7 yes      0.0774     7\n\n\nThe player column is replaced with the log-odds of being on goal.\n\nAs a reminder:\n\\[\\text{log-odds} = log\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right)\\]\nwhere \\(\\hat{p}\\) is the on goal rate estimate.\nFor logistic regression, this is what the predictors are modeling. The log-odds are more likely to be linear with the outcome."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#per-player-statistics",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#per-player-statistics",
    "title": "5 - Feature engineering",
    "section": "Per-player statistics",
    "text": "Per-player statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood statistical methods for estimating these rates use partial pooling.\nPooling borrows strength across players and shrinks extreme values (e.g.Â zero or one) towards the mean for players with very few shots.\nThe embed package has recipe steps for effect encodings.\n\n\n\nPartial pooling gives better estimates for players with fewer shots by shrinking the estimate to the overall on-goal rate (55.2%)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#partial-pooling",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#partial-pooling",
    "title": "5 - Feature engineering",
    "section": "Partial pooling",
    "text": "Partial pooling"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#player-effects",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#player-effects",
    "title": "5 - Feature engineering",
    "section": "Player effects  ",
    "text": "Player effects  \n\nlibrary(embed)\n\nnhl_effect_rec &lt;-\n  recipe(on_goal ~ ., data = nhl_train) %&gt;%\n  step_lencode_mixed(player, outcome = vars(on_goal)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\n\nIt is very important to appropriately validate the effect encoding step to make sure that we are not overfitting."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#recipes-are-estimated",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#recipes-are-estimated",
    "title": "5 - Feature engineering",
    "section": "Recipes are estimated ",
    "text": "Recipes are estimated \nPreprocessing steps in a recipe use the training set to compute quantities.\n\nWhat kind of quantities are computed for preprocessing?\n\nLevels of a factor\nWhether a column has zero variance\nNormalization\nFeature extraction\nEffect encodings\n\n\n\nWhen a recipe is part of a workflow, this estimation occurs when fit() is called."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#effect-encoding-results",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#effect-encoding-results",
    "title": "5 - Feature engineering",
    "section": "Effect encoding results    ",
    "text": "Effect encoding results    \n\nnhl_effect_wflow &lt;-\n  nhl_glm_wflow %&gt;%\n  update_recipe(nhl_effect_rec)\n\nnhl_effect_res &lt;-\n  nhl_effect_wflow %&gt;%\n  fit_resamples(nhl_val, control = ctrl)\n\ncollect_metrics(nhl_effect_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.540     1      NA Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.551     1      NA Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#angle",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#angle",
    "title": "5 - Feature engineering",
    "section": "Angle",
    "text": "Angle\n\nnhl_angle_rec &lt;-\n  nhl_indicators %&gt;%\n  step_mutate(\n    angle = abs(atan2(abs(coord_y), (89 - abs(coord_x))) * (180 / pi))\n  )\n\n\n\nNote the danger of using step_mutate() â€“ easy to have data leakage\ncoord_x is â€œdistance from goalâ€. We subtract it from 89 to get the distance from the center of the ice. The abs() calls account for the fact that the goals might be on either side of (0, 0). The rest of it is the formula for going from (x, y) to angle in degrees."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#distance",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#distance",
    "title": "5 - Feature engineering",
    "section": "Distance",
    "text": "Distance\n\nnhl_distance_rec &lt;-\n  nhl_angle_rec %&gt;%\n  step_mutate(\n    distance = sqrt((89 - abs(coord_x))^2 + abs(coord_y)^2),\n    distance = log(distance)\n  )"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#behind-goal-line",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#behind-goal-line",
    "title": "5 - Feature engineering",
    "section": "Behind goal line",
    "text": "Behind goal line\n\nnhl_behind_rec &lt;-\n  nhl_distance_rec %&gt;%\n  step_mutate(\n    behind_goal_line = ifelse(abs(coord_x) &gt;= 89, 1, 0)\n  )"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#fit-different-recipes",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#fit-different-recipes",
    "title": "5 - Feature engineering",
    "section": "Fit different recipes    ",
    "text": "Fit different recipes    \nA workflow set can cross models and/or preprocessors and then resample them en masse.\n\nset.seed(9)\n\nnhl_glm_set_res &lt;-\n  workflow_set(\n    list(`1_dummy` = nhl_indicators, `2_angle` = nhl_angle_rec, \n         `3_dist` = nhl_distance_rec, `4_bgl` = nhl_behind_rec),\n    list(logistic = logistic_reg())\n  ) %&gt;%\n  workflow_map(fn = \"fit_resamples\", resamples = nhl_val, verbose = TRUE, control = ctrl)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-5",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#your-turn-5",
    "title": "5 - Feature engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a workflow set with 2 or 3 recipes.\n(Consider using recipes weâ€™ve already created.)\nUse workflow_map() to resample the workflow set.\n\n\n\n08:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#compare-recipes",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#compare-recipes",
    "title": "5 - Feature engineering",
    "section": "Compare recipes",
    "text": "Compare recipes\n\nlibrary(forcats)\ncollect_metrics(nhl_glm_set_res) %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  mutate(\n    features = gsub(\"_logistic\", \"\", wflow_id), \n    features = fct_reorder(features, mean)\n  ) %&gt;%\n  ggplot(aes(x = mean, y = features)) +\n  geom_point(size = 3) +\n  labs(y = NULL, x = \"ROC AUC (validation set)\")"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#compare-recipes-1",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#compare-recipes-1",
    "title": "5 - Feature engineering",
    "section": "Compare recipes",
    "text": "Compare recipes"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#debugging-a-recipe",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#debugging-a-recipe",
    "title": "5 - Feature engineering",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g.Â encoded_players) can be estimated manually with a function called prep(). It is analogous to fit().\n\n\n\n\nAnother function (bake()) is analogous to predict(), and gives you the processed data back."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/05-feature-engineering.html#more-on-recipes",
    "href": "archive/2022-07-RStudio-conf/05-feature-engineering.html#more-on-recipes",
    "title": "5 - Feature engineering",
    "section": "More on recipes",
    "text": "More on recipes\n\nOnce fit() is called on a workflow, changing the model does not re-fit the recipe.\n\n\n\nA list of all known steps is at https://www.tidymodels.org/find/recipes/.\n\n\n\n\nSome steps can be skipped when using predict().\n\n\n\n\nThe order of the steps matters."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/07-wrapping-up.html#your-turn",
    "href": "archive/2022-07-RStudio-conf/07-wrapping-up.html#your-turn",
    "title": "7 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/07-wrapping-up.html#resources-to-keep-learning",
    "href": "archive/2022-07-RStudio-conf/07-wrapping-up.html#resources-to-keep-learning",
    "title": "7 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://smltar.com/\n\n\n\nFollow us on Twitter and at the tidyverse blog for updates!"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html",
    "href": "archive/2022-07-RStudio-conf/index.html",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels presented at the 2022 RStudio conference. This workshop provides an introduction to machine learning with R using the tidymodels framework, a collection of packages for modeling and machine learning using tidyverse principles. We will build, evaluate, compare, and tune predictive models. Along the way, weâ€™ll learn about key concepts in machine learning including overfitting, resampling, and feature engineering. Learners will gain knowledge about good predictive modeling practices, as well as hands-on experience using tidymodels packages like parsnip, rsample, recipes, yardstick, tune, and workflows."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#welcome",
    "href": "archive/2022-07-RStudio-conf/index.html#welcome",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels presented at the 2022 RStudio conference. This workshop provides an introduction to machine learning with R using the tidymodels framework, a collection of packages for modeling and machine learning using tidyverse principles. We will build, evaluate, compare, and tune predictive models. Along the way, weâ€™ll learn about key concepts in machine learning including overfitting, resampling, and feature engineering. Learners will gain knowledge about good predictive modeling practices, as well as hands-on experience using tidymodels packages like parsnip, rsample, recipes, yardstick, tune, and workflows."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#is-this-workshop-for-me",
    "href": "archive/2022-07-RStudio-conf/index.html#is-this-workshop-for-me",
    "title": "Machine learning with tidymodels",
    "section": "Is this workshop for me? ",
    "text": "Is this workshop for me? \nThis course assumes intermediate R knowledge. This workshop is for you if:\n\nYou can use the magrittr pipe %&gt;% and/or native pipe |&gt;\nYou are familiar with functions from dplyr, tidyr, and ggplot2\nYou can read data into R, transform and reshape data, and make a wide variety of graphs\n\nWe expect participants to have some exposure to basic statistical concepts, but NOT intermediate or expert familiarity with modeling or machine learning."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#preparation",
    "href": "archive/2022-07-RStudio-conf/index.html#preparation",
    "title": "Machine learning with tidymodels",
    "section": "Preparation",
    "text": "Preparation\nPlease join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2022.02), available at https://www.rstudio.com/download\nThe following R packages, which you can install from the R console:\n\n\ninstall.packages(c(\"DALEXtra\", \"doParallel\", \"embed\", \"forcats\",\n                   \"lme4\", \"ranger\", \"remotes\", \"rpart\", \n                   \"rpart.plot\", \"stacks\", \"tidymodels\",\n                   \"vetiver\", \"xgboost\"))\n\nremotes::install_github(\"topepo/ongoal@v0.0.2\")"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#slides",
    "href": "archive/2022-07-RStudio-conf/index.html#slides",
    "title": "Machine learning with tidymodels",
    "section": "Slides",
    "text": "Slides\nThese slides are designed to use with live teaching and are published for workshop participantsâ€™ convenience. There are not meant as standalone learning materials. For that, we recommend tidymodels.org and Tidy Modeling with R.\n\nDay One\n\n01: Introduction\n02: Your data budget\n03: What makes a model?\n04: Evaluating models\n\n\n\nDay Two\n\n05: Feature engineering\n06: Tuning hyperparameters\n07: Wrapping up\n\nThereâ€™s also a page for slide annotations; these are extra notes for selected slides."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#code",
    "href": "archive/2022-07-RStudio-conf/index.html#code",
    "title": "Machine learning with tidymodels",
    "section": "Code",
    "text": "Code\nQuarto files for working along are available on GitHub. (Donâ€™t worry if you havenâ€™t used Quarto before; it will feel familiar to R Markdown users.)"
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#acknowledgments",
    "href": "archive/2022-07-RStudio-conf/index.html#acknowledgments",
    "title": "Machine learning with tidymodels",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including the slides, is made with Quarto. Please submit an issue on the GitHub repo for this workshop if you find something that could be fixed or improved."
  },
  {
    "objectID": "archive/2022-07-RStudio-conf/index.html#reuse-and-licensing",
    "href": "archive/2022-07-RStudio-conf/index.html#reuse-and-licensing",
    "title": "Machine learning with tidymodels",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e.Â not an original creation and reused from another source), these educational materials are licensed under Creative Commons Attribution CC BY-SA 4.0."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-on-tree-frog-hatching",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-on-tree-frog-hatching",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-on-tree-frog-hatching-1",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-on-tree-frog-hatching-1",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\nRed-eyed tree frog embryos can hatch earlier than their normal ~7 days if they detect potential predator threat!\nType ?stacks::tree_frogs to learn more about this dataset, including references.\nWe are using a slightly modified version from stacks.\n\n\nlibrary(tidymodels)\n\ndata(\"tree_frogs\", package = \"stacks\")\ntree_frogs &lt;- tree_frogs %&gt;%\n  mutate(t_o_d = factor(t_o_d),\n         age = age / 86400) %&gt;%\n  filter(!is.na(latency)) %&gt;%\n  select(-c(clutch, hatched))"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-on-tree-frog-hatching-2",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-on-tree-frog-hatching-2",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\n\n\nN = 572\nA numeric outcome, latency\n4 other variables\n\ntreatment, reflex, and t_o_d are nominal predictors\nage is a numeric predictor\n\n\n\n\n\n\n\nlatency: How long it took the frog to hatch after being stimulated - i.e.Â after being poked by a blunt probe (in seconds).\ntreatment: Whether or not they got gentamicin, a compound that knocks out the embryoâ€™s lateral line (a sensory organ).\nreflex: A measure of ear function (low, mid, full)\nt_o_d: Time that the stimulus was applied (morning, afternoon, night)\nage: Age at the time it was stimulated (in days)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-on-tree-frog-hatching-3",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-on-tree-frog-hatching-3",
    "title": "2 - Your data budget",
    "section": "Data on tree frog hatching",
    "text": "Data on tree frog hatching\n\ntree_frogs\n#&gt; # A tibble: 572 Ã— 5\n#&gt;    treatment  reflex   age t_o_d     latency\n#&gt;    &lt;fct&gt;      &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;\n#&gt;  1 control    full    5.40 morning        22\n#&gt;  2 control    low     4.18 night         360\n#&gt;  3 control    full    4.65 afternoon     106\n#&gt;  4 control    mid     4.14 night         180\n#&gt;  5 control    full    4.6  afternoon      60\n#&gt;  6 gentamicin full    5.36 morning        39\n#&gt;  7 control    full    4.56 afternoon     214\n#&gt;  8 control    full    5.43 morning        50\n#&gt;  9 control    full    4.63 afternoon     224\n#&gt; 10 control    full    5.40 morning        63\n#&gt; # â„¹ 562 more rows"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-splitting-and-spending",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-splitting-and-spending",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nFor machine learning, we typically split data into training and test sets:\n\n\nThe training set is used to estimate model parameters.\nThe test set is used to find an independent assessment of model performance.\n\n\n\nDo not ğŸš« use the test set during training."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-splitting-and-spending-1",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-splitting-and-spending-1",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-splitting-and-spending-2",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-splitting-and-spending-2",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\n\nSpending too much data in training prevents us from computing a good assessment of predictive performance.\n\n\n\nSpending too much data in testing prevents us from computing a good estimate of model parameters."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#your-turn",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#your-turn",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nWhen is a good time to split your data?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-splitting-and-spending-3",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-splitting-and-spending-3",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nfrog_split &lt;- initial_split(tree_frogs)\nfrog_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;429/143/572&gt;\n\n\nHow much data in training vs testing? This function uses a good default, but this depends on your specific goal/data We will talk about more powerful ways of splitting, like stratification, later"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#accessing-the-data",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#accessing-the-data",
    "title": "2 - Your data budget",
    "section": "Accessing the data ",
    "text": "Accessing the data \n\nfrog_train &lt;- training(frog_split)\nfrog_test &lt;- testing(frog_split)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#the-training-set",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#the-training-set",
    "title": "2 - Your data budget",
    "section": "The training set",
    "text": "The training set\n\nfrog_train\n#&gt; # A tibble: 429 Ã— 5\n#&gt;    treatment  reflex   age t_o_d     latency\n#&gt;    &lt;fct&gt;      &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;\n#&gt;  1 control    full    5.36 morning        36\n#&gt;  2 gentamicin full    5.37 morning        72\n#&gt;  3 gentamicin full    4.65 afternoon     141\n#&gt;  4 control    full    5.42 morning        27\n#&gt;  5 control    full    5.43 morning        27\n#&gt;  6 gentamicin full    5.38 morning        73\n#&gt;  7 gentamicin full    5.42 morning        68\n#&gt;  8 gentamicin full    4.75 afternoon     124\n#&gt;  9 control    full    5.00 night          62\n#&gt; 10 control    full    5.39 morning        25\n#&gt; # â„¹ 419 more rows"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#the-test-set",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#the-test-set",
    "title": "2 - Your data budget",
    "section": "The test set ",
    "text": "The test set \n\nfrog_test\n#&gt; # A tibble: 143 Ã— 5\n#&gt;    treatment  reflex   age t_o_d     latency\n#&gt;    &lt;fct&gt;      &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;\n#&gt;  1 control    full    5.40 morning        22\n#&gt;  2 control    low     4.18 night         360\n#&gt;  3 control    full    4.63 afternoon     224\n#&gt;  4 gentamicin full    4.75 afternoon     158\n#&gt;  5 control    mid     4.22 night          91\n#&gt;  6 gentamicin full    4.89 night         301\n#&gt;  7 control    full    5.38 morning         2\n#&gt;  8 control    full    4.80 afternoon      56\n#&gt;  9 control    full    5.36 morning        11\n#&gt; 10 control    full    5.40 morning        64\n#&gt; # â„¹ 133 more rows"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#your-turn-1",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#your-turn-1",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nSplit your data so 20% is held out for the test set.\nTry out different values in set.seed() to see how the results change.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-splitting-and-spending-4",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#data-splitting-and-spending-4",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nfrog_split &lt;- initial_split(tree_frogs, prop = 0.8)\nfrog_train &lt;- training(frog_split)\nfrog_test &lt;- testing(frog_split)\n\nnrow(frog_train)\n#&gt; [1] 457\nnrow(frog_test)\n#&gt; [1] 115"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#section-1",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#section-1",
    "title": "2 - Your data budget",
    "section": "",
    "text": "We will use this tomorrow"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#your-turn-2",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#your-turn-2",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nExplore the frog_train data on your own!\n\nWhatâ€™s the distribution of the outcome, latency?\nWhatâ€™s the distribution of numeric variables like age?\nHow does latency differ across the categorical variables?\n\n\n\n\n08:00\n\n\n\n\nMake a plot or summary and then share with neighbor"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#section-3",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#section-3",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(latency)) +\n  geom_histogram(bins = 20)\n\n\n\nThis histogram brings up a concern. What if in our training set we get unlucky and sample few or none of these large values? That could mean that our model wouldnâ€™t be able to predict such values. Letâ€™s come back to that!"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#section-4",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#section-4",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(latency, treatment, fill = treatment)) +\n  geom_boxplot(alpha = 0.5, show.legend = FALSE)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#section-5",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#section-5",
    "title": "2 - Your data budget",
    "section": "",
    "text": "frog_train %&gt;%\n  ggplot(aes(latency, reflex, fill = reflex)) +\n  geom_boxplot(alpha = 0.3, show.legend = FALSE)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#section-6",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#section-6",
    "title": "2 - Your data budget",
    "section": "",
    "text": "ggplot(frog_train, aes(age, latency, color = reflex)) +\n  geom_point(alpha = .8, size = 2)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#section-7",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#section-7",
    "title": "2 - Your data budget",
    "section": "",
    "text": "Stratified sampling would split within each quartile\n\nBased on our exploration, we realized that stratifying by latency might help get a consistent distribution. For instance, weâ€™d include high and low latency in both the test and training"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/02-data-budget.html#stratification",
    "href": "archive/2022-08-Reykjavik-City/02-data-budget.html#stratification",
    "title": "2 - Your data budget",
    "section": "Stratification",
    "text": "Stratification\nUse strata = latency\n\nset.seed(123)\nfrog_split &lt;- initial_split(tree_frogs, prop = 0.8, strata = latency)\nfrog_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;456/116/572&gt;\n\n\nStratification often helps, with very little downside"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#metrics-for-model-performance",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#metrics-for-model-performance",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\naugment(tree_fit, new_data = frog_test) %&gt;%\n  metrics(latency, .pred)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      59.2  \n#&gt; 2 rsq     standard       0.380\n#&gt; 3 mae     standard      40.2\n\n\n\nRMSE: difference between the predicted and observed values â¬‡ï¸\n\\(R^2\\): squared correlation between the predicted and observed values â¬†ï¸\nMAE: similar to RMSE, but mean absolute error â¬‡ï¸"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#metrics-for-model-performance-1",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#metrics-for-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\naugment(tree_fit, new_data = frog_test) %&gt;%\n  rmse(latency, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard        59.2"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#metrics-for-model-performance-2",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#metrics-for-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\naugment(tree_fit, new_data = frog_test) %&gt;%\n  group_by(reflex) %&gt;%\n  rmse(latency, .pred)\n#&gt; # A tibble: 3 Ã— 4\n#&gt;   reflex .metric .estimator .estimate\n#&gt;   &lt;fct&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 low    rmse    standard        94.3\n#&gt; 2 mid    rmse    standard       101. \n#&gt; 3 full   rmse    standard        51.2"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#metrics-for-model-performance-3",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#metrics-for-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\nfrog_metrics &lt;- metric_set(rmse, msd)\naugment(tree_fit, new_data = frog_test) %&gt;%\n  frog_metrics(latency, .pred)\n#&gt; # A tibble: 2 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      59.2  \n#&gt; 2 msd     standard      -0.908"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-1",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-1",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸",
    "text": "Dangers of overfitting âš ï¸"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-2",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-2",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸",
    "text": "Dangers of overfitting âš ï¸"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-3",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-3",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\ntree_fit %&gt;%\n  augment(frog_train)\n#&gt; # A tibble: 456 Ã— 6\n#&gt;    treatment  reflex   age t_o_d     latency .pred\n#&gt;    &lt;fct&gt;      &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 control    full    5.42 morning        33  39.8\n#&gt;  2 control    full    5.38 morning        19  66.7\n#&gt;  3 control    full    5.38 morning         2  66.7\n#&gt;  4 control    full    5.44 morning        39  39.8\n#&gt;  5 control    full    5.41 morning        42  39.8\n#&gt;  6 control    full    4.75 afternoon      20  59.8\n#&gt;  7 control    full    4.95 night          31  83.1\n#&gt;  8 control    full    5.42 morning        21  39.8\n#&gt;  9 gentamicin full    5.39 morning        30  64.6\n#&gt; 10 control    full    4.55 afternoon      43 174. \n#&gt; # â„¹ 446 more rows\n\nWe call this â€œresubstitutionâ€ or â€œrepredicting the training setâ€"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-4",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-4",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\ntree_fit %&gt;%\n  augment(frog_train) %&gt;%\n  rmse(latency, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard        49.4\n\nWe call this a â€œresubstitution estimateâ€"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-5",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-5",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\ntree_fit %&gt;%\n  augment(frog_train) %&gt;%\n  rmse(latency, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard        49.4"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-6",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-6",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\ntree_fit %&gt;%\n  augment(frog_train) %&gt;%\n  rmse(latency, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard        49.4\n\n\n\ntree_fit %&gt;%\n  augment(frog_test) %&gt;%\n  rmse(latency, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard        59.2\n\n\n\nâš ï¸ Remember that weâ€™re demonstrating overfitting\n\n\nâš ï¸ Donâ€™t use the test set until the end of your modeling analysis"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#your-turn",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#your-turn",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse augment() and metrics() to compute a regression metric like mae().\nCompute the metrics for both training and testing data.\nNotice the evidence of overfitting! âš ï¸\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-7",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#dangers-of-overfitting-7",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\ntree_fit %&gt;%\n  augment(frog_train) %&gt;%\n  metrics(latency, .pred)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      49.4  \n#&gt; 2 rsq     standard       0.494\n#&gt; 3 mae     standard      33.4\n\n\n\ntree_fit %&gt;%\n  augment(frog_test) %&gt;%\n  metrics(latency, .pred)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      59.2  \n#&gt; 2 rsq     standard       0.380\n#&gt; 3 mae     standard      40.2\n\n\n\nWhat if we want to compare more models?\n\n\nAnd/or more model configurations?\n\n\nAnd we want to understand if these are important differences?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation-1",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation-1",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#your-turn-1",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#your-turn-1",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nIf we use 10 folds, what percent of the training data\n\nends up in analysis\nends up in assessment\n\nfor each fold?\n\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation-2",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation-2",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(frog_train) # v = 10 is default\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits           id    \n#&gt;    &lt;list&gt;           &lt;chr&gt; \n#&gt;  1 &lt;split [410/46]&gt; Fold01\n#&gt;  2 &lt;split [410/46]&gt; Fold02\n#&gt;  3 &lt;split [410/46]&gt; Fold03\n#&gt;  4 &lt;split [410/46]&gt; Fold04\n#&gt;  5 &lt;split [410/46]&gt; Fold05\n#&gt;  6 &lt;split [410/46]&gt; Fold06\n#&gt;  7 &lt;split [411/45]&gt; Fold07\n#&gt;  8 &lt;split [411/45]&gt; Fold08\n#&gt;  9 &lt;split [411/45]&gt; Fold09\n#&gt; 10 &lt;split [411/45]&gt; Fold10"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation-3",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation-3",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWhat is in this?\n\nfrog_folds &lt;- vfold_cv(frog_train)\nfrog_folds$splits[1:3]\n#&gt; [[1]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;410/46/456&gt;\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;410/46/456&gt;\n#&gt; \n#&gt; [[3]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;410/46/456&gt;\n\n\nTalk about a list column, storing non-atomic types in dataframe"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation-4",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation-4",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(frog_train, v = 5)\n#&gt; #  5-fold cross-validation \n#&gt; # A tibble: 5 Ã— 2\n#&gt;   splits           id   \n#&gt;   &lt;list&gt;           &lt;chr&gt;\n#&gt; 1 &lt;split [364/92]&gt; Fold1\n#&gt; 2 &lt;split [365/91]&gt; Fold2\n#&gt; 3 &lt;split [365/91]&gt; Fold3\n#&gt; 4 &lt;split [365/91]&gt; Fold4\n#&gt; 5 &lt;split [365/91]&gt; Fold5"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation-5",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation-5",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(frog_train, strata = latency)\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits           id    \n#&gt;    &lt;list&gt;           &lt;chr&gt; \n#&gt;  1 &lt;split [408/48]&gt; Fold01\n#&gt;  2 &lt;split [408/48]&gt; Fold02\n#&gt;  3 &lt;split [408/48]&gt; Fold03\n#&gt;  4 &lt;split [409/47]&gt; Fold04\n#&gt;  5 &lt;split [411/45]&gt; Fold05\n#&gt;  6 &lt;split [412/44]&gt; Fold06\n#&gt;  7 &lt;split [412/44]&gt; Fold07\n#&gt;  8 &lt;split [412/44]&gt; Fold08\n#&gt;  9 &lt;split [412/44]&gt; Fold09\n#&gt; 10 &lt;split [412/44]&gt; Fold10\n\n\nStratification often helps, with very little downside"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation-6",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#cross-validation-6",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWeâ€™ll use this setup:\n\nset.seed(123)\nfrog_folds &lt;- vfold_cv(frog_train, v = 10, strata = latency)\nfrog_folds\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits           id    \n#&gt;    &lt;list&gt;           &lt;chr&gt; \n#&gt;  1 &lt;split [408/48]&gt; Fold01\n#&gt;  2 &lt;split [408/48]&gt; Fold02\n#&gt;  3 &lt;split [408/48]&gt; Fold03\n#&gt;  4 &lt;split [409/47]&gt; Fold04\n#&gt;  5 &lt;split [411/45]&gt; Fold05\n#&gt;  6 &lt;split [412/44]&gt; Fold06\n#&gt;  7 &lt;split [412/44]&gt; Fold07\n#&gt;  8 &lt;split [412/44]&gt; Fold08\n#&gt;  9 &lt;split [412/44]&gt; Fold09\n#&gt; 10 &lt;split [412/44]&gt; Fold10\n\n\nSet the seed when creating resamples"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#fit-our-model-to-the-resamples",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#fit-our-model-to-the-resamples",
    "title": "4 - Evaluating models",
    "section": "Fit our model to the resamples",
    "text": "Fit our model to the resamples\n\ntree_res &lt;- fit_resamples(tree_wflow, frog_folds)\ntree_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 4\n#&gt;    splits           id     .metrics         .notes          \n#&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n#&gt;  1 &lt;split [408/48]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  2 &lt;split [408/48]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  3 &lt;split [408/48]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  4 &lt;split [409/47]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  5 &lt;split [411/45]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  6 &lt;split [412/44]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  7 &lt;split [412/44]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  8 &lt;split [412/44]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  9 &lt;split [412/44]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt; 10 &lt;split [412/44]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#evaluating-model-performance",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#evaluating-model-performance",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\ntree_res %&gt;%\n  collect_metrics()\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard   59.6      10  2.31   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.305    10  0.0342 Preprocessor1_Model1\n\n\nWe can reliably measure performance using only the training data ğŸ‰"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#comparing-metrics",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#comparing-metrics",
    "title": "4 - Evaluating models",
    "section": "Comparing metrics ",
    "text": "Comparing metrics \nHow do the metrics from resampling compare to the metrics from training and testing?\n\n\n\ntree_res %&gt;%\n  collect_metrics() %&gt;% \n  select(.metric, mean, n)\n#&gt; # A tibble: 2 Ã— 3\n#&gt;   .metric   mean     n\n#&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 rmse    59.6      10\n#&gt; 2 rsq      0.305    10\n\n\nThe RMSE previously was\n\n49.36 for the training set\n59.16 for test set\n\n\n\nRemember that:\nâš ï¸ the training set gives you overly optimistic metrics\nâš ï¸ the test set is precious"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#evaluating-model-performance-1",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#evaluating-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nctrl_frog &lt;- control_resamples(save_pred = TRUE)\ntree_res &lt;- fit_resamples(tree_wflow, frog_folds, control = ctrl_frog)\n\ntree_preds &lt;- collect_predictions(tree_res)\ntree_preds\n#&gt; # A tibble: 456 Ã— 5\n#&gt;    id     .pred  .row latency .config             \n#&gt;    &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt;  1 Fold01  39.6     1      33 Preprocessor1_Model1\n#&gt;  2 Fold01  72.1     3       2 Preprocessor1_Model1\n#&gt;  3 Fold01  63.8     9      30 Preprocessor1_Model1\n#&gt;  4 Fold01  72.1    13      46 Preprocessor1_Model1\n#&gt;  5 Fold01  43.3    28      11 Preprocessor1_Model1\n#&gt;  6 Fold01  61.7    35      41 Preprocessor1_Model1\n#&gt;  7 Fold01  39.6    51      43 Preprocessor1_Model1\n#&gt;  8 Fold01 134.     70      20 Preprocessor1_Model1\n#&gt;  9 Fold01  70.6    74      21 Preprocessor1_Model1\n#&gt; 10 Fold01  39.6   106      14 Preprocessor1_Model1\n#&gt; # â„¹ 446 more rows"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#section-3",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#section-3",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "tree_preds %&gt;% \n  ggplot(aes(latency, .pred, color = id)) + \n  geom_abline(lty = 2, col = \"gray\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#where-are-the-fitted-models",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#where-are-the-fitted-models",
    "title": "4 - Evaluating models",
    "section": "Where are the fitted models? ",
    "text": "Where are the fitted models? \n\ntree_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits           id     .metrics         .notes           .predictions     \n#&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;           \n#&gt;  1 &lt;split [408/48]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [48 Ã— 4]&gt;\n#&gt;  2 &lt;split [408/48]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [48 Ã— 4]&gt;\n#&gt;  3 &lt;split [408/48]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [48 Ã— 4]&gt;\n#&gt;  4 &lt;split [409/47]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [47 Ã— 4]&gt;\n#&gt;  5 &lt;split [411/45]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [45 Ã— 4]&gt;\n#&gt;  6 &lt;split [412/44]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt;  7 &lt;split [412/44]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt;  8 &lt;split [412/44]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt;  9 &lt;split [412/44]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n#&gt; 10 &lt;split [412/44]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [44 Ã— 4]&gt;\n\n\nğŸ—‘ï¸"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#bootstrapping",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#bootstrapping",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#bootstrapping-1",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#bootstrapping-1",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(3214)\nbootstraps(frog_train)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 25 Ã— 2\n#&gt;    splits            id         \n#&gt;    &lt;list&gt;            &lt;chr&gt;      \n#&gt;  1 &lt;split [456/163]&gt; Bootstrap01\n#&gt;  2 &lt;split [456/166]&gt; Bootstrap02\n#&gt;  3 &lt;split [456/173]&gt; Bootstrap03\n#&gt;  4 &lt;split [456/177]&gt; Bootstrap04\n#&gt;  5 &lt;split [456/166]&gt; Bootstrap05\n#&gt;  6 &lt;split [456/163]&gt; Bootstrap06\n#&gt;  7 &lt;split [456/164]&gt; Bootstrap07\n#&gt;  8 &lt;split [456/165]&gt; Bootstrap08\n#&gt;  9 &lt;split [456/170]&gt; Bootstrap09\n#&gt; 10 &lt;split [456/177]&gt; Bootstrap10\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#your-turn-2",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#your-turn-2",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCreate:\n\nbootstrap folds (change times from the default)\nvalidation set (use the reference guide to find the function)\n\nDonâ€™t forget to set a seed when you resample!\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#bootstrapping-2",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#bootstrapping-2",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(322)\nbootstraps(frog_train, times = 10)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits            id         \n#&gt;    &lt;list&gt;            &lt;chr&gt;      \n#&gt;  1 &lt;split [456/173]&gt; Bootstrap01\n#&gt;  2 &lt;split [456/168]&gt; Bootstrap02\n#&gt;  3 &lt;split [456/170]&gt; Bootstrap03\n#&gt;  4 &lt;split [456/164]&gt; Bootstrap04\n#&gt;  5 &lt;split [456/176]&gt; Bootstrap05\n#&gt;  6 &lt;split [456/156]&gt; Bootstrap06\n#&gt;  7 &lt;split [456/166]&gt; Bootstrap07\n#&gt;  8 &lt;split [456/168]&gt; Bootstrap08\n#&gt;  9 &lt;split [456/167]&gt; Bootstrap09\n#&gt; 10 &lt;split [456/170]&gt; Bootstrap10"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#validation-set",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#validation-set",
    "title": "4 - Evaluating models",
    "section": "Validation set ",
    "text": "Validation set \n\nset.seed(853)\nvalidation_split(frog_train, strata = latency)\n#&gt; # Validation Set Split (0.75/0.25)  using stratification \n#&gt; # A tibble: 1 Ã— 2\n#&gt;   splits            id        \n#&gt;   &lt;list&gt;            &lt;chr&gt;     \n#&gt; 1 &lt;split [340/116]&gt; validation\n\n\nA validation set is just another type of resample"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#random-forest-1",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#random-forest-1",
    "title": "4 - Evaluating models",
    "section": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ",
    "text": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\n\nEnsemble many decision tree models\nAll the trees vote! ğŸ—³ï¸\nBootstrap aggregating + random predictor sampling\n\n\n\nOften works well without tuning hyperparameters (more on this tomorrow!), as long as there are enough trees"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#create-a-random-forest-model",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#create-a-random-forest-model",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_spec &lt;- rand_forest(trees = 1000, mode = \"regression\")\nrf_spec\n#&gt; Random Forest Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#create-a-random-forest-model-1",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#create-a-random-forest-model-1",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_wflow &lt;- workflow(latency ~ ., rf_spec)\nrf_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; latency ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#your-turn-3",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#your-turn-3",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() and rf_wflow to:\n\nkeep predictions\ncompute metrics\nplot true vs.Â predicted values\n\n\n\n\n08:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#evaluating-model-performance-2",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#evaluating-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nctrl_frog &lt;- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\n\nset.seed(2)\nrf_res &lt;- fit_resamples(rf_wflow, frog_folds, control = ctrl_frog)\ncollect_metrics(rf_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard   55.9      10  1.76   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.372    10  0.0312 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#section-5",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#section-5",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "collect_predictions(rf_res) %&gt;% \n  ggplot(aes(latency, .pred, color = id)) + \n  geom_abline(lty = 2, col = \"gray\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#how-can-we-compare-multiple-model-workflows-at-once",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#how-can-we-compare-multiple-model-workflows-at-once",
    "title": "4 - Evaluating models",
    "section": "How can we compare multiple model workflows at once?",
    "text": "How can we compare multiple model workflows at once?"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#evaluate-a-workflow-set",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#evaluate-a-workflow-set",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec))\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result    \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#evaluate-a-workflow-set-1",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#evaluate-a-workflow-set-1",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec)) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = frog_folds)\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result   \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#evaluate-a-workflow-set-2",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#evaluate-a-workflow-set-2",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(latency ~ .), list(tree_spec, rf_spec)) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = frog_folds) %&gt;%\n  rank_results()\n#&gt; # A tibble: 4 Ã— 9\n#&gt;   wflow_id         .config .metric   mean std_err     n preprocessor model  rank\n#&gt;   &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n#&gt; 1 formula_rand_foâ€¦ Preproâ€¦ rmse    55.7    1.81      10 formula      randâ€¦     1\n#&gt; 2 formula_rand_foâ€¦ Preproâ€¦ rsq      0.375  0.0309    10 formula      randâ€¦     1\n#&gt; 3 formula_decisioâ€¦ Preproâ€¦ rmse    59.6    2.31      10 formula      deciâ€¦     2\n#&gt; 4 formula_decisioâ€¦ Preproâ€¦ rsq      0.305  0.0342    10 formula      deciâ€¦     2\n\nThe first metric of the metric set is used for ranking. Use rank_metric to change that.\n\nLots more available with workflow sets, like collect_metrics(), autoplot() methods, and more!"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#your-turn-4",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#your-turn-4",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nWhen do you think a workflow set would be useful?\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#the-final-fit",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#the-final-fit",
    "title": "4 - Evaluating models",
    "section": "The final fit ",
    "text": "The final fit \nSuppose that we are happy with our random forest model.\nLetâ€™s fit the model on the training set and verify our performance using the test set.\n\nWeâ€™ve shown you fit() and predict() (+ augment()) but there is a shortcut:\n\n# frog_split has train + test info\nfinal_fit &lt;- last_fit(rf_wflow, frog_split) \n\nfinal_fit\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits            id               .metrics .notes   .predictions .workflow \n#&gt;   &lt;list&gt;            &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n#&gt; 1 &lt;split [456/116]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#what-is-in-final_fit",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#what-is-in-final_fit",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_metrics(final_fit)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard      57.3   Preprocessor1_Model1\n#&gt; 2 rsq     standard       0.415 Preprocessor1_Model1\n\n\nThese are metrics computed with the test set"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#what-is-in-final_fit-1",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#what-is-in-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_predictions(final_fit)\n#&gt; # A tibble: 116 Ã— 5\n#&gt;    id               .pred  .row latency .config             \n#&gt;    &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt;  1 train/test split  44.1     1      22 Preprocessor1_Model1\n#&gt;  2 train/test split 101.      3     106 Preprocessor1_Model1\n#&gt;  3 train/test split  75.7     6      39 Preprocessor1_Model1\n#&gt;  4 train/test split  43.1     8      50 Preprocessor1_Model1\n#&gt;  5 train/test split  43.5    10      63 Preprocessor1_Model1\n#&gt;  6 train/test split  43.8    14      25 Preprocessor1_Model1\n#&gt;  7 train/test split  51.2    16      48 Preprocessor1_Model1\n#&gt;  8 train/test split 161.     17      91 Preprocessor1_Model1\n#&gt;  9 train/test split  50.9    32      11 Preprocessor1_Model1\n#&gt; 10 train/test split 177.     33     109 Preprocessor1_Model1\n#&gt; # â„¹ 106 more rows\n\n\nThese are predictions for the test set"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#section-6",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#section-6",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "collect_predictions(final_fit) %&gt;%\n  ggplot(aes(latency, .pred)) + \n  geom_abline(lty = 2, col = \"deeppink4\", size = 1.5) +\n  geom_point(alpha = 0.5) +\n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#what-is-in-final_fit-2",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#what-is-in-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\nextract_workflow(final_fit)\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; latency ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n#&gt; \n#&gt; Type:                             Regression \n#&gt; Number of trees:                  1000 \n#&gt; Sample size:                      456 \n#&gt; Number of independent variables:  4 \n#&gt; Mtry:                             2 \n#&gt; Target node size:                 5 \n#&gt; Variable importance mode:         none \n#&gt; Splitrule:                        variance \n#&gt; OOB prediction error (MSE):       3113.532 \n#&gt; R squared (OOB):                  0.355469\n\n\nUse this for prediction on new data, like for deploying"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#your-turn-5",
    "href": "archive/2022-08-Reykjavik-City/04-evaluating-models.html#your-turn-5",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nEnd of the day discussion!\nWhich model do you think you would decide to use?\nWhat surprised you the most?\nWhat is one thing you are looking forward to for tomorrow?\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#tuning-parameters",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#optimize-tuning-parameters",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#optimize-tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search ğŸ’  which tests a pre-defined set of candidate values\nIterative search ğŸŒ€ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#choosing-tuning-parameters",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#choosing-tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters    ",
    "text": "Choosing tuning parameters    \nLetâ€™s take our previous recipe and add a few changes:\n\nglm_rec &lt;-\n  recipe(on_goal ~ ., data = nhl_train) %&gt;%\n  step_lencode_mixed(shooter, goaltender, outcome = vars(on_goal)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_mutate(\n    angle = abs( atan2(abs(coord_y), (89 - coord_x) ) * (180 / pi) ),\n    defensive_zone = ifelse(coord_x &lt;= -25.5, 1, 0),\n    behind_goal_line = ifelse(coord_x &gt;= 89, 1, 0)\n  ) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_ns(angle, deg_free = tune(\"angle\")) %&gt;%\n  step_ns(coord_x, deg_free = tune(\"coord_x\")) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n\nLetâ€™s tune() our spline terms!"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#choosing-tuning-parameters-1",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#choosing-tuning-parameters-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters    ",
    "text": "Choosing tuning parameters    \nLetâ€™s take our previous recipe and add a few changes:\n\nglm_spline_wflow &lt;-\n  workflow() %&gt;%\n  add_model(logistic_reg()) %&gt;%\n  add_recipe(glm_rec)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#section",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#section",
    "title": "6 - Tuning Hyperparameters",
    "section": "",
    "text": "Splines replace the existing numeric predictor with a set of columns that allow a model to emulate a flexible, nonlinear relationship.\nMore spline terms = more â€œwigglyâ€, i.e.Â flexibly model a nonlinear relationship\nHow many spline terms? This is called degrees of freedom\n2 and 5 look like they underfit; 20 and 100 look like they overfit"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#splines-and-nonlinear-relationships",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#splines-and-nonlinear-relationships",
    "title": "6 - Tuning Hyperparameters",
    "section": "Splines and nonlinear relationships",
    "text": "Splines and nonlinear relationships\n\n\nOur hockey data exhibits nonlinear relationships\nWe can model nonlinearity like this via a model (later this afternoon) or feature engineering\nHow do we decide how â€œwigglyâ€ or flexible to make our spline features? TUNING"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#grid-search",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#grid-search",
    "title": "6 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nParameters\n\nThe tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\nThe extract_parameter_set_dials() function extracts these tuning parameters and the info.\n\n\nGrids\n\nCreate your grid manually or automatically.\nThe grid_*() functions can make a grid.\n\n\n\nMost basic (but very effective) way to tune models"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#create-a-grid",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#create-a-grid",
    "title": "6 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nglm_spline_wflow %&gt;% \n  extract_parameter_set_dials()\n#&gt; Collection of 2 parameters for tuning\n#&gt; \n#&gt;  identifier     type    object\n#&gt;       angle deg_free nparam[+]\n#&gt;     coord_x deg_free nparam[+]\n\n\nA parameter set can be updated (e.g.Â to change the ranges)."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#create-a-grid-1",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#create-a-grid-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\n\n\nset.seed(12)\ngrid &lt;- \n  glm_spline_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#&gt; # A tibble: 25 Ã— 2\n#&gt;    angle coord_x\n#&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt;  1     5       8\n#&gt;  2     1      13\n#&gt;  3     8       6\n#&gt;  4    11       5\n#&gt;  5     4       1\n#&gt;  6     6      10\n#&gt;  7    10      15\n#&gt;  8    12      13\n#&gt;  9     8      13\n#&gt; 10    10       6\n#&gt; # â„¹ 15 more rows\n\n\n\n\nA space-filling design like this tends to perform better than random grids.\nSpace-filling designs are also usually more efficient than regular grids."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#your-turn",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#your-turn",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a grid for our tunable workflow.\nTry creating a regular grid.\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#create-a-grid-2",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#create-a-grid-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nset.seed(12)\ngrid &lt;- \n  glm_spline_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 25)\n\ngrid\n#&gt; # A tibble: 225 Ã— 2\n#&gt;    angle coord_x\n#&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt;  1     1       1\n#&gt;  2     2       1\n#&gt;  3     3       1\n#&gt;  4     4       1\n#&gt;  5     5       1\n#&gt;  6     6       1\n#&gt;  7     7       1\n#&gt;  8     8       1\n#&gt;  9     9       1\n#&gt; 10    10       1\n#&gt; # â„¹ 215 more rows\n\n\nNote that even though we requested 25x25=625 rows, we only got 15x15=225 back, since the deg_free parameters only have a range of 1-&gt;15."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#update-parameter-ranges",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#update-parameter-ranges",
    "title": "6 - Tuning Hyperparameters",
    "section": "Update parameter ranges  ",
    "text": "Update parameter ranges  \n\nset.seed(12)\ngrid &lt;- \n  glm_spline_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  update(angle = spline_degree(c(2L, 50L)),\n         coord_x = spline_degree(c(2L, 50L))) %&gt;% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#&gt; # A tibble: 25 Ã— 2\n#&gt;    angle coord_x\n#&gt;    &lt;int&gt;   &lt;int&gt;\n#&gt;  1    14      27\n#&gt;  2     4      42\n#&gt;  3    26      20\n#&gt;  4    36      16\n#&gt;  5    13       3\n#&gt;  6    20      33\n#&gt;  7    31      49\n#&gt;  8    40      44\n#&gt;  9    24      45\n#&gt; 10    34      18\n#&gt; # â„¹ 15 more rows\n\n\nEven though angle is a deg_free parameter in step_ns(), we donâ€™t use the dials deg_free() object here. We have a special spline_degree() function that has better defaults for splines."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#the-results",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#the-results",
    "title": "6 - Tuning Hyperparameters",
    "section": "The results  ",
    "text": "The results  \n\n\ngrid %&gt;% \n  ggplot(aes(angle, coord_x)) +\n  geom_point(size = 4)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#spline-grid-search",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#spline-grid-search",
    "title": "6 - Tuning Hyperparameters",
    "section": "Spline grid search   ",
    "text": "Spline grid search   \n\nset.seed(9)\nctrl &lt;- control_grid(save_pred = TRUE, parallel_over = \"everything\")\n\nglm_spline_res &lt;-\n  glm_spline_wflow %&gt;%\n  tune_grid(resamples = nhl_val, grid = grid, control = ctrl)\n\nglm_spline_res\n#&gt; # Tuning results\n#&gt; # Validation Set Split (0.8/0.2)  \n#&gt; # A tibble: 1 Ã— 5\n#&gt;   splits              id         .metrics          .notes           .predictions         \n#&gt;   &lt;list&gt;              &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;           &lt;list&gt;               \n#&gt; 1 &lt;split [5348/1338]&gt; validation &lt;tibble [50 Ã— 6]&gt; &lt;tibble [2 Ã— 3]&gt; &lt;tibble [33,450 Ã— 8]&gt;\n#&gt; \n#&gt; There were issues with some computations:\n#&gt; \n#&gt;   - Warning(s) x1: glm.fit: algorithm did not converge, glm.fit: fitted probabilities numerically 0 or 1 occurred\n#&gt;   - Warning(s) x1: glm.fit: fitted probabilities numerically 0 or 1 occurred\n#&gt; \n#&gt; Run `show_notes(.Last.tune.result)` for more information.\n\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#your-turn-1",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#your-turn-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nTune our glm_wflow.\nWhat happens if you donâ€™t supply a grid argument to tune_grid()?\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#grid-results",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#grid-results",
    "title": "6 - Tuning Hyperparameters",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(glm_spline_res)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#tuning-results",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#tuning-results",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(glm_spline_res)\n#&gt; # A tibble: 50 Ã— 8\n#&gt;    angle coord_x .metric  .estimator  mean     n std_err .config              \n#&gt;    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1    14      27 accuracy binary     0.804     1      NA Preprocessor01_Model1\n#&gt;  2    14      27 roc_auc  binary     0.815     1      NA Preprocessor01_Model1\n#&gt;  3     4      42 accuracy binary     0.808     1      NA Preprocessor02_Model1\n#&gt;  4     4      42 roc_auc  binary     0.820     1      NA Preprocessor02_Model1\n#&gt;  5    26      20 accuracy binary     0.805     1      NA Preprocessor03_Model1\n#&gt;  6    26      20 roc_auc  binary     0.819     1      NA Preprocessor03_Model1\n#&gt;  7    36      16 accuracy binary     0.800     1      NA Preprocessor04_Model1\n#&gt;  8    36      16 roc_auc  binary     0.817     1      NA Preprocessor04_Model1\n#&gt;  9    13       3 accuracy binary     0.803     1      NA Preprocessor05_Model1\n#&gt; 10    13       3 roc_auc  binary     0.807     1      NA Preprocessor05_Model1\n#&gt; # â„¹ 40 more rows"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#tuning-results-1",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#tuning-results-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(glm_spline_res, summarize = FALSE)\n#&gt; # A tibble: 50 Ã— 7\n#&gt;    id         angle coord_x .metric  .estimator .estimate .config              \n#&gt;    &lt;chr&gt;      &lt;int&gt;   &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1 validation    14      27 accuracy binary         0.804 Preprocessor01_Model1\n#&gt;  2 validation    14      27 roc_auc  binary         0.815 Preprocessor01_Model1\n#&gt;  3 validation     4      42 accuracy binary         0.808 Preprocessor02_Model1\n#&gt;  4 validation     4      42 roc_auc  binary         0.820 Preprocessor02_Model1\n#&gt;  5 validation    26      20 accuracy binary         0.805 Preprocessor03_Model1\n#&gt;  6 validation    26      20 roc_auc  binary         0.819 Preprocessor03_Model1\n#&gt;  7 validation    36      16 accuracy binary         0.800 Preprocessor04_Model1\n#&gt;  8 validation    36      16 roc_auc  binary         0.817 Preprocessor04_Model1\n#&gt;  9 validation    13       3 accuracy binary         0.803 Preprocessor05_Model1\n#&gt; 10 validation    13       3 roc_auc  binary         0.807 Preprocessor05_Model1\n#&gt; # â„¹ 40 more rows"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#choose-a-parameter-combination",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#choose-a-parameter-combination",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nshow_best(glm_spline_res, metric = \"roc_auc\")\n#&gt; # A tibble: 5 Ã— 8\n#&gt;   angle coord_x .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1     4      42 roc_auc binary     0.820     1      NA Preprocessor02_Model1\n#&gt; 2    26      20 roc_auc binary     0.819     1      NA Preprocessor03_Model1\n#&gt; 3    36      16 roc_auc binary     0.817     1      NA Preprocessor04_Model1\n#&gt; 4    40      44 roc_auc binary     0.817     1      NA Preprocessor08_Model1\n#&gt; 5    37      27 roc_auc binary     0.816     1      NA Preprocessor15_Model1"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \nCreate your own tibble for final parameters or use one of the tune::select_*() functions:\n\nselect_best(glm_spline_res, metric = \"roc_auc\")\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   angle coord_x .config              \n#&gt;   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                \n#&gt; 1     4      42 Preprocessor02_Model1\n\n\nThis best result has:\n\nlow-degree spline for angle (less â€œwigglyâ€, less complex)\nhigher-degree spline for coord_x (more â€œwigglyâ€, more complex)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#boosted-trees-1",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#boosted-trees-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted trees ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ",
    "text": "Boosted trees ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\n\nEnsemble many decision tree models\n\n\nReview how a decision tree model works:\n\nSeries of splits or if/then statements based on predictors\nFirst the tree grows until some condition is met (maximum depth, no more data)\nThen the tree is pruned to reduce its complexity"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#single-decision-tree",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#single-decision-tree",
    "title": "6 - Tuning Hyperparameters",
    "section": "Single decision tree",
    "text": "Single decision tree"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#boosted-trees-2",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#boosted-trees-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted trees ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ",
    "text": "Boosted trees ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\nBoosting methods fit a sequence of tree-based models.\n\n\nEach tree is dependent on the one before and tries to compensate for any poor results in the previous trees.\nThis is like gradient-based steepest ascent methods from calculus."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted tree tuning parameters",
    "text": "Boosted tree tuning parameters\nMost modern boosting methods have a lot of tuning parameters!\n\n\nFor tree growth and pruning (min_n, max_depth, etc)\nFor boosting (trees, stop_iter, learn_rate)\n\n\n\nWeâ€™ll use early stopping to stop boosting when a few iterations produce consecutively worse results."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#comparing-tree-ensembles",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#comparing-tree-ensembles",
    "title": "6 - Tuning Hyperparameters",
    "section": "Comparing tree ensembles",
    "text": "Comparing tree ensembles\n\n\nRandom forest\n\nIndependent trees\nBootstrapped data\nNo pruning\n1000â€™s of trees\n\n\nBoosting\n\nDependent trees\nDifferent case weights\nTune tree parameters\nFar fewer trees\n\n\nThe general consensus for tree-based models is, in terms of performance: boosting &gt; random forest &gt; bagging &gt; single trees."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#boosted-tree-code",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#boosted-tree-code",
    "title": "6 - Tuning Hyperparameters",
    "section": "Boosted tree code",
    "text": "Boosted tree code\n\nxgb_spec &lt;-\n  boost_tree(\n    trees = tune(), min_n = tune(), tree_depth = tune(),\n    learn_rate = tune(), loss_reduction = tune()\n  ) %&gt;%\n  set_mode(\"classification\") %&gt;% \n  set_engine(\"xgboost\") \n\nxgb_rec &lt;- \n  recipe(on_goal ~ ., data = nhl_train) %&gt;% \n  step_lencode_mixed(shooter, goaltender, outcome = vars(on_goal)) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nxgb_wflow &lt;- \n  workflow() %&gt;% \n  add_model(xgb_spec) %&gt;% \n  add_recipe(xgb_rec)\n\n\nvalidation is an argument to parsnip::xgb_train(), not directly to xgboost. It generates a validation set that is used by xgboost when evaluating model performance. It is eventually assigned to xgb.train(watchlist = list(validation = data)).\nSee translate(xgb_spec) to see where it is passed to parsnip::xgb_train()."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#your-turn-2",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#your-turn-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate your boosted tree workflow.\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#running-in-parallel",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#running-in-parallel",
    "title": "6 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\n\n\n\nGrid search, combined with resampling, requires fitting a lot of models!\nThese models donâ€™t depend on one another and can be run in parallel.\n\nWe can use a parallel backend to do this:\n\ncores &lt;- parallelly::availableCores(logical = FALSE)\ncl &lt;- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#running-in-parallel-1",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#running-in-parallel-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\nFaceted on the expensiveness of preprocessing used."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#tuning",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#tuning",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning ",
    "text": "Tuning \nThis will take some time to run â³\n\nset.seed(9)\n\nxgb_res &lt;-\n  xgb_wflow %&gt;%\n  tune_grid(resamples = nhl_val, grid = 30, control = ctrl) # automatic grid now!"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#your-turn-3",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#your-turn-3",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nStart tuning the boosted tree model!\nWe wonâ€™t wait for everyoneâ€™s tuning to finish, but take this time to get it started before we move on.\n\n\n\n03:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#tuning-results-2",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#tuning-results-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\nxgb_res\n#&gt; # Tuning results\n#&gt; # Validation Set Split (0.8/0.2)  \n#&gt; # A tibble: 1 Ã— 5\n#&gt;   splits              id         .metrics          .notes           .predictions          \n#&gt;   &lt;list&gt;              &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;           &lt;list&gt;                \n#&gt; 1 &lt;split [5348/1338]&gt; validation &lt;tibble [60 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [40,140 Ã— 11]&gt;"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#tuning-results-3",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#tuning-results-3",
    "title": "6 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\nautoplot(xgb_res)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#again-with-the-location-features",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#again-with-the-location-features",
    "title": "6 - Tuning Hyperparameters",
    "section": "Again with the location features",
    "text": "Again with the location features\n\ncoord_rec &lt;- \n  xgb_rec %&gt;%\n  step_mutate(\n    angle = abs( atan2(abs(coord_y), (89 - coord_x) ) * (180 / pi) ),\n    defensive_zone = ifelse(coord_x &lt;= -25.5, 1, 0),\n    behind_goal_line = ifelse(coord_x &gt;= 89, 1, 0)\n  )\n\nxgb_coord_wflow &lt;- \n  workflow() %&gt;% \n  add_model(xgb_spec) %&gt;% \n  add_recipe(coord_rec)\n\nset.seed(9)\nxgb_coord_res &lt;-\n  xgb_coord_wflow %&gt;%\n  tune_grid(resamples = nhl_val, grid = 30, control = ctrl)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#did-the-machine-figure-it-out",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#did-the-machine-figure-it-out",
    "title": "6 - Tuning Hyperparameters",
    "section": "Did the machine figure it out?",
    "text": "Did the machine figure it out?\nNo extra features:\n\nshow_best(xgb_res, metric = \"roc_auc\", n = 3)\n#&gt; # A tibble: 3 Ã— 11\n#&gt;   trees min_n tree_depth learn_rate loss_reduction .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1  1697     3          3    0.0116       0.674     roc_auc binary     0.804     1      NA Preprocessor1_Model02\n#&gt; 2  1264    16          2    0.00732      0.000340  roc_auc binary     0.803     1      NA Preprocessor1_Model12\n#&gt; 3   569    39          4    0.0272       0.0000288 roc_auc binary     0.800     1      NA Preprocessor1_Model30\n\nWith additional coordinate features:\n\nshow_best(xgb_coord_res, metric = \"roc_auc\", n = 3)\n#&gt; # A tibble: 3 Ã— 11\n#&gt;   trees min_n tree_depth learn_rate loss_reduction .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1  1697     3          3    0.0116    0.674        roc_auc binary     0.807     1      NA Preprocessor1_Model02\n#&gt; 2  1264    16          2    0.00732   0.000340     roc_auc binary     0.804     1      NA Preprocessor1_Model12\n#&gt; 3   427    31          2    0.249     0.0000000773 roc_auc binary     0.803     1      NA Preprocessor1_Model23"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#compare-models",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#compare-models",
    "title": "6 - Tuning Hyperparameters",
    "section": "Compare models",
    "text": "Compare models\nBest logistic regression results:\n\nglm_spline_res %&gt;% \n  show_best(metric = \"roc_auc\", n = 1) %&gt;% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#&gt; # A tibble: 1 Ã— 6\n#&gt;   .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1 roc_auc binary     0.820     1      NA Preprocessor02_Model1\n\n\nBest boosting results:\n\nxgb_res %&gt;% \n  show_best(metric = \"roc_auc\", n = 1) %&gt;% \n  select(.metric, .estimator, mean, n, std_err, .config)\n#&gt; # A tibble: 1 Ã— 6\n#&gt;   .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1 roc_auc binary     0.804     1      NA Preprocessor1_Model02"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#updating-the-workflow",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#updating-the-workflow",
    "title": "6 - Tuning Hyperparameters",
    "section": "Updating the workflow  ",
    "text": "Updating the workflow  \n\nbest_auc &lt;- select_best(glm_spline_res, metric = \"roc_auc\")\nbest_auc\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   angle coord_x .config              \n#&gt;   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                \n#&gt; 1     4      42 Preprocessor02_Model1\n\nglm_spline_wflow &lt;-\n  glm_spline_wflow %&gt;% \n  finalize_workflow(best_auc)\n\nglm_spline_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Recipe\n#&gt; Model: logistic_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; 7 Recipe Steps\n#&gt; \n#&gt; â€¢ step_lencode_mixed()\n#&gt; â€¢ step_dummy()\n#&gt; â€¢ step_mutate()\n#&gt; â€¢ step_zv()\n#&gt; â€¢ step_ns()\n#&gt; â€¢ step_ns()\n#&gt; â€¢ step_normalize()\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#the-final-fit-to-the-nhl-data",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#the-final-fit-to-the-nhl-data",
    "title": "6 - Tuning Hyperparameters",
    "section": "The final fit to the NHL data  ",
    "text": "The final fit to the NHL data  \n\ntest_res &lt;- \n  glm_spline_wflow %&gt;% \n  last_fit(split = nhl_split)\n\ntest_res\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits              id               .metrics         .notes           .predictions         .workflow \n#&gt;   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;           &lt;list&gt;               &lt;list&gt;    \n#&gt; 1 &lt;split [6686/2229]&gt; train/test split &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [2,229 Ã— 6]&gt; &lt;workflow&gt;\n\n\nRemember that last_fit() fits one time with the combined training and validation set, then evaluates one time with the testing set."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#your-turn-4",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#your-turn-4",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nFinalize your workflow with the best parameters.\nCreate a final fit.\n\n\n\n08:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#estimates-of-roc-auc",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#estimates-of-roc-auc",
    "title": "6 - Tuning Hyperparameters",
    "section": "Estimates of ROC AUC ",
    "text": "Estimates of ROC AUC \nValidation results from tuning:\n\nglm_spline_res %&gt;% \n  show_best(metric = \"roc_auc\", n = 1) %&gt;% \n  select(.metric, mean, n, std_err)\n#&gt; # A tibble: 1 Ã— 4\n#&gt;   .metric  mean     n std_err\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n#&gt; 1 roc_auc 0.820     1      NA\n\n\nTest set results:\n\ntest_res %&gt;% collect_metrics()\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric  .estimator .estimate .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary         0.800 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary         0.807 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#final-fitted-workflow",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#final-fitted-workflow",
    "title": "6 - Tuning Hyperparameters",
    "section": "Final fitted workflow",
    "text": "Final fitted workflow\nExtract the final fitted workflow, fit using the training set:\n\nfinal_glm_spline_wflow &lt;- \n  test_res %&gt;% \n  extract_workflow()\n\n# use this object to predict or deploy\npredict(final_glm_spline_wflow, nhl_test[1:3,])\n#&gt; # A tibble: 3 Ã— 1\n#&gt;   .pred_class\n#&gt;   &lt;fct&gt;      \n#&gt; 1 yes        \n#&gt; 2 yes        \n#&gt; 3 no"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#next-steps",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#next-steps",
    "title": "6 - Tuning Hyperparameters",
    "section": "Next steps",
    "text": "Next steps\n\nDocument the model.\n\n\n\nDeploy the model.\n\n\n\n\nCreate an applicability domain model to help monitor our data over time.\n\n\n\n\nUse explainers to characterize the model and the predictions."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#explain-yourself",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#explain-yourself",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain yourself",
    "text": "Explain yourself\nThere are two categories of model explanations, global and local.\n\n\nGlobal model explanations provide an overall understanding aggregated over a whole set of observations.\nLocal model explanations provide information about a prediction for a single observation.\n\n\n\nYou can also build global model explanations by aggregating local model explanations."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#a-tidymodels-explainer",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#a-tidymodels-explainer",
    "title": "6 - Tuning Hyperparameters",
    "section": "A tidymodels explainer",
    "text": "A tidymodels explainer\nWe can build explainers using:\n\noriginal, basic predictors\nderived features\n\n\nlibrary(DALEXtra)\n\nglm_explainer &lt;- explain_tidymodels(\n  final_glm_spline_wflow,\n  data = dplyr::select(nhl_train, -on_goal),\n  # DALEX required an integer for factors:\n  y = as.integer(nhl_train$on_goal) - 1,\n  verbose = FALSE\n)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#explain-the-x-coordinates",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#explain-the-x-coordinates",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain the x coordinates",
    "text": "Explain the x coordinates\nWith our explainer, letâ€™s create partial dependence profiles:\n\nset.seed(123)\npdp_coord_x &lt;- model_profile(\n  glm_explainer,\n  variables = \"coord_x\",\n  N = 500,\n  groups = \"strength\"\n)\n\n\nYou can use the default plot() method or create your own visualization."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#explain-the-x-coordinates-1",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#explain-the-x-coordinates-1",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain the x coordinates",
    "text": "Explain the x coordinates"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#explain-the-x-coordinates-2",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#explain-the-x-coordinates-2",
    "title": "6 - Tuning Hyperparameters",
    "section": "Explain the x coordinates",
    "text": "Explain the x coordinates"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#your-turn-5",
    "href": "archive/2022-08-Reykjavik-City/06-tuning-hyperparameters.html#your-turn-5",
    "title": "6 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate an explainer for our glm model.\nTry using other variables, like extra_attacker or game_seconds.\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/08-wrapping-up.html#your-turn",
    "href": "archive/2022-08-Reykjavik-City/08-wrapping-up.html#your-turn",
    "title": "8 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\n05:00"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/08-wrapping-up.html#resources-to-keep-learning",
    "href": "archive/2022-08-Reykjavik-City/08-wrapping-up.html#resources-to-keep-learning",
    "title": "8 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://smltar.com/\n\n\n\nFollow us on Twitter and at the tidyverse blog for updates!"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/index.html",
    "href": "archive/2022-08-Reykjavik-City/index.html",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels presented in ReykjaviÌk Iceland. This workshop provides an introduction to machine learning with R using the tidymodels framework, a collection of packages for modeling and machine learning using tidyverse principles. We will build, evaluate, compare, and tune predictive models. Along the way, weâ€™ll learn about key concepts in machine learning including overfitting, resampling, and feature engineering. Learners will gain knowledge about good predictive modeling practices, as well as hands-on experience using tidymodels packages like parsnip, rsample, recipes, yardstick, tune, and workflows."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/index.html#welcome",
    "href": "archive/2022-08-Reykjavik-City/index.html#welcome",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels presented in ReykjaviÌk Iceland. This workshop provides an introduction to machine learning with R using the tidymodels framework, a collection of packages for modeling and machine learning using tidyverse principles. We will build, evaluate, compare, and tune predictive models. Along the way, weâ€™ll learn about key concepts in machine learning including overfitting, resampling, and feature engineering. Learners will gain knowledge about good predictive modeling practices, as well as hands-on experience using tidymodels packages like parsnip, rsample, recipes, yardstick, tune, and workflows."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/index.html#is-this-workshop-for-me",
    "href": "archive/2022-08-Reykjavik-City/index.html#is-this-workshop-for-me",
    "title": "Machine learning with tidymodels",
    "section": "Is this workshop for me? ",
    "text": "Is this workshop for me? \nThis course assumes intermediate R knowledge. This workshop is for you if:\n\nYou can use the magrittr pipe %&gt;% and/or native pipe |&gt;\nYou are familiar with functions from dplyr, tidyr, and ggplot2\nYou can read data into R, transform and reshape data, and make a wide variety of graphs\n\nWe expect participants to have some exposure to basic statistical concepts, but NOT intermediate or expert familiarity with modeling or machine learning."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/index.html#preparation",
    "href": "archive/2022-08-Reykjavik-City/index.html#preparation",
    "title": "Machine learning with tidymodels",
    "section": "Preparation",
    "text": "Preparation\nPlease join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2022.02), available at https://www.rstudio.com/download\nThe following R packages, which you can install from the R console:\n\n\ninstall.packages(c(\"Cubist\", \"DALEXtra\", \"doParallel\", \"earth\", \"embed\", \n                   \"forcats\", \"lme4\", \"parallelly\", \"ranger\", \"remotes\", \"rpart\", \n                   \"rpart.plot\", \"rules\", \"stacks\", \"tidymodels\",\n                   \"vetiver\", \"xgboost\"))\n\nremotes::install_github(\"topepo/ongoal@hockeyR\")"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/index.html#slides",
    "href": "archive/2022-08-Reykjavik-City/index.html#slides",
    "title": "Machine learning with tidymodels",
    "section": "Slides",
    "text": "Slides\nThese slides are designed to use with live teaching and are published for workshop participantsâ€™ convenience. There are not meant as standalone learning materials. For that, we recommend tidymodels.org and Tidy Modeling with R.\n\nDay One\n\n01: Introduction\n02: Your data budget\n03: What makes a model?\n04: Evaluating models\n\n\n\nDay Two\n\n05: Feature engineering\n06: Tuning hyperparameters\n07: Transit Case Study\n08: Wrapping up\n\nThereâ€™s also a page for slide annotations; these are extra notes for selected slides."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/index.html#code",
    "href": "archive/2022-08-Reykjavik-City/index.html#code",
    "title": "Machine learning with tidymodels",
    "section": "Code",
    "text": "Code\nQuarto files (version 1.4.104) for working along are available on GitHub. (Donâ€™t worry if you havenâ€™t used Quarto before; it will feel familiar to R Markdown users.)"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/index.html#past-workshops",
    "href": "archive/2022-08-Reykjavik-City/index.html#past-workshops",
    "title": "Machine learning with tidymodels",
    "section": "Past workshops",
    "text": "Past workshops\n\n25-26 July 2022 at rstudio::conf()\n16-17 August 2022 in Reykjavik Iceland"
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/index.html#acknowledgments",
    "href": "archive/2022-08-Reykjavik-City/index.html#acknowledgments",
    "title": "Machine learning with tidymodels",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including the slides, is made with Quarto. Please submit an issue on the GitHub repo for this workshop if you find something that could be fixed or improved."
  },
  {
    "objectID": "archive/2022-08-Reykjavik-City/index.html#reuse-and-licensing",
    "href": "archive/2022-08-Reykjavik-City/index.html#reuse-and-licensing",
    "title": "Machine learning with tidymodels",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e.Â not an original creation and reused from another source), these educational materials are licensed under Creative Commons Attribution CC BY-SA 4.0."
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#data-on-chicago-taxi-trips",
    "href": "archive/2023-07-nyr/02-data-budget.html#data-on-chicago-taxi-trips",
    "title": "2 - Your data budget",
    "section": "Data on Chicago taxi trips",
    "text": "Data on Chicago taxi trips\n\n\n\nThe city of Chicago releases anonymized trip-level data on taxi trips in the city.\nWe pulled a sample of 10,000 rides occurring in early 2022.\nType ?modeldatatoo::data_taxi() to learn more about this dataset, including references.\n\n\n\n\n\nCredit: https://www.svgrepo.com/svg/8322/taxi"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#which-of-these-variables-can-we-use",
    "href": "archive/2023-07-nyr/02-data-budget.html#which-of-these-variables-can-we-use",
    "title": "2 - Your data budget",
    "section": "Which of these variables can we use?",
    "text": "Which of these variables can we use?\n\nlibrary(tidymodels)\nlibrary(modeldatatoo)\n\ntaxi &lt;- data_taxi()\n\nnames(taxi)\n#&gt;  [1] \"tip\"          \"id\"           \"duration\"     \"distance\"     \"fare\"        \n#&gt;  [6] \"tolls\"        \"extras\"       \"total_cost\"   \"payment_type\" \"company\"     \n#&gt; [11] \"local\"        \"dow\"          \"month\"        \"hour\""
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#checklist-for-predictors",
    "href": "archive/2023-07-nyr/02-data-budget.html#checklist-for-predictors",
    "title": "2 - Your data budget",
    "section": "Checklist for predictors",
    "text": "Checklist for predictors\n\nIs it ethical to use this variable? (Or even legal?)\nWill this variable be available at prediction time?\nDoes this variable contribute to explainability?"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#data-on-chicago-taxi-trips-1",
    "href": "archive/2023-07-nyr/02-data-budget.html#data-on-chicago-taxi-trips-1",
    "title": "2 - Your data budget",
    "section": "Data on Chicago taxi trips",
    "text": "Data on Chicago taxi trips\nWe are using a slightly modified version from the modeldatatoo data.\n\ntaxi &lt;- taxi %&gt;%\n  mutate(month = factor(month, levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\"))) %&gt;% \n  select(-c(id, duration, fare, tolls, extras, total_cost, payment_type)) %&gt;% \n  drop_na()"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#data-on-chicago-taxi-trips-2",
    "href": "archive/2023-07-nyr/02-data-budget.html#data-on-chicago-taxi-trips-2",
    "title": "2 - Your data budget",
    "section": "Data on Chicago taxi trips",
    "text": "Data on Chicago taxi trips\n\n\n\nN = 10,000\nA nominal outcome, tip, with levels \"yes\" and \"no\"\n6 other variables\n\ncompany, local, and dow, and month are nominal predictors\ndistance and hours are numeric predictors\n\n\n\n\n\n\nCredit: https://unsplash.com/photos/7_r85l4eht8\n\n\ntip: Whether the rider left a tip. A factor with levels â€œyesâ€ and â€œnoâ€.\ndistance: The trip distance, in odometer miles.\ncompany: The taxi company, as a factor. Companies that occurred few times were binned as â€œotherâ€.\nlocal: Whether the trip started in the same community area as it began. See the source data for community area values.\ndow: The day of the week in which the trip began, as a factor.\nmonth: The month in which the trip began, as a factor.\nhour: The hour of the day in which the trip began, as a numeric."
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#data-on-chicago-taxi-trips-3",
    "href": "archive/2023-07-nyr/02-data-budget.html#data-on-chicago-taxi-trips-3",
    "title": "2 - Your data budget",
    "section": "Data on Chicago taxi trips",
    "text": "Data on Chicago taxi trips\n\ntaxi\n#&gt; # A tibble: 8,807 Ã— 7\n#&gt;    tip   distance company      local dow   month  hour\n#&gt;    &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;        &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n#&gt;  1 yes       1.24 Sun Taxi     no    Thu   Feb      13\n#&gt;  2 no        5.39 Flash Cab    no    Sat   Mar      12\n#&gt;  3 yes       3.01 City Service no    Wed   Feb      17\n#&gt;  4 no       18.4  Sun Taxi     no    Sat   Apr       6\n#&gt;  5 yes       1.76 Sun Taxi     no    Sun   Jan      15\n#&gt;  6 yes      13.6  Sun Taxi     no    Mon   Feb      17\n#&gt;  7 yes       3.71 City Service no    Mon   Mar      21\n#&gt;  8 yes       4.8  other        no    Tue   Mar       9\n#&gt;  9 yes      18.0  City Service no    Fri   Jan      19\n#&gt; 10 no       17.5  other        yes   Thu   Apr      12\n#&gt; # â„¹ 8,797 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#data-splitting-and-spending",
    "href": "archive/2023-07-nyr/02-data-budget.html#data-splitting-and-spending",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nFor machine learning, we typically split data into training and test sets:\n\n\nThe training set is used to estimate model parameters.\nThe test set is used to find an independent assessment of model performance.\n\n\n\nDo not ğŸš« use the test set during training."
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#data-splitting-and-spending-1",
    "href": "archive/2023-07-nyr/02-data-budget.html#data-splitting-and-spending-1",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#data-splitting-and-spending-2",
    "href": "archive/2023-07-nyr/02-data-budget.html#data-splitting-and-spending-2",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\n\nSpending too much data in training prevents us from computing a good assessment of predictive performance.\n\n\n\nSpending too much data in testing prevents us from computing a good estimate of model parameters."
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#your-turn",
    "href": "archive/2023-07-nyr/02-data-budget.html#your-turn",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nWhen is a good time to split your data?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#the-initial-split",
    "href": "archive/2023-07-nyr/02-data-budget.html#the-initial-split",
    "title": "2 - Your data budget",
    "section": "The initial split ",
    "text": "The initial split \n\nset.seed(123)\ntaxi_split &lt;- initial_split(taxi)\ntaxi_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;6605/2202/8807&gt;\n\n\nHow much data in training vs testing? This function uses a good default, but this depends on your specific goal/data We will talk about more powerful ways of splitting, like stratification, later"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#accessing-the-data",
    "href": "archive/2023-07-nyr/02-data-budget.html#accessing-the-data",
    "title": "2 - Your data budget",
    "section": "Accessing the data ",
    "text": "Accessing the data \n\ntaxi_train &lt;- training(taxi_split)\ntaxi_test &lt;- testing(taxi_split)"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#the-training-set",
    "href": "archive/2023-07-nyr/02-data-budget.html#the-training-set",
    "title": "2 - Your data budget",
    "section": "The training set",
    "text": "The training set\n\ntaxi_train\n#&gt; # A tibble: 6,605 Ã— 7\n#&gt;    tip   distance company                   local dow   month  hour\n#&gt;    &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;                     &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n#&gt;  1 yes       4.54 City Service              no    Sat   Mar      16\n#&gt;  2 no       10.2  Flash Cab                 no    Mon   Feb       8\n#&gt;  3 yes      12.4  other                     no    Sun   Apr      15\n#&gt;  4 yes      15.3  Sun Taxi                  no    Mon   Apr      18\n#&gt;  5 no        6.41 Flash Cab                 no    Wed   Apr      14\n#&gt;  6 yes       1.56 other                     no    Tue   Jan      13\n#&gt;  7 yes       3.13 Flash Cab                 no    Sun   Apr      12\n#&gt;  8 yes       7.54 other                     no    Tue   Apr       8\n#&gt;  9 yes       6.98 Flash Cab                 no    Tue   Apr       5\n#&gt; 10 yes       0.7  Taxi Affiliation Services no    Tue   Jan       9\n#&gt; # â„¹ 6,595 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#the-test-set",
    "href": "archive/2023-07-nyr/02-data-budget.html#the-test-set",
    "title": "2 - Your data budget",
    "section": "The test set ",
    "text": "The test set \nğŸ™ˆ\n\nThere are 2202 rows and 7 columns in the test set."
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#your-turn-1",
    "href": "archive/2023-07-nyr/02-data-budget.html#your-turn-1",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nSplit your data so 20% is held out for the test set.\nTry out different values in set.seed() to see how the results change.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#data-splitting-and-spending-3",
    "href": "archive/2023-07-nyr/02-data-budget.html#data-splitting-and-spending-3",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\ntaxi_split &lt;- initial_split(taxi, prop = 0.8)\ntaxi_train &lt;- training(taxi_split)\ntaxi_test &lt;- testing(taxi_split)\n\nnrow(taxi_train)\n#&gt; [1] 7045\nnrow(taxi_test)\n#&gt; [1] 1762"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#section-1",
    "href": "archive/2023-07-nyr/02-data-budget.html#section-1",
    "title": "2 - Your data budget",
    "section": "",
    "text": "We will use this tomorrow"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#your-turn-2",
    "href": "archive/2023-07-nyr/02-data-budget.html#your-turn-2",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nExplore the taxi_train data on your own!\n\nWhatâ€™s the distribution of the outcome, tip?\nWhatâ€™s the distribution of numeric variables like distance?\nHow does tip differ across the categorical variables?\n\n\n\n\nâˆ’+\n08:00\n\n\n\n\nMake a plot or summary and then share with neighbor"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#section-3",
    "href": "archive/2023-07-nyr/02-data-budget.html#section-3",
    "title": "2 - Your data budget",
    "section": "",
    "text": "taxi_train %&gt;% \n  ggplot(aes(x = tip)) +\n  geom_bar()"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#section-4",
    "href": "archive/2023-07-nyr/02-data-budget.html#section-4",
    "title": "2 - Your data budget",
    "section": "",
    "text": "taxi_train %&gt;% \n  ggplot(aes(x = tip, fill = local)) +\n  geom_bar() +\n  scale_fill_viridis_d(end = .5)"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#section-5",
    "href": "archive/2023-07-nyr/02-data-budget.html#section-5",
    "title": "2 - Your data budget",
    "section": "",
    "text": "taxi_train %&gt;% \n  mutate(tip = forcats::fct_rev(tip)) %&gt;% \n  ggplot(aes(x = hour, fill = tip)) +\n  geom_bar()"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#section-6",
    "href": "archive/2023-07-nyr/02-data-budget.html#section-6",
    "title": "2 - Your data budget",
    "section": "",
    "text": "taxi_train %&gt;% \n  mutate(tip = forcats::fct_rev(tip)) %&gt;% \n  ggplot(aes(x = hour, fill = tip)) +\n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#section-7",
    "href": "archive/2023-07-nyr/02-data-budget.html#section-7",
    "title": "2 - Your data budget",
    "section": "",
    "text": "taxi_train %&gt;% \n  mutate(tip = forcats::fct_rev(tip)) %&gt;% \n  ggplot(aes(x = distance)) +\n  geom_histogram(bins = 100) +\n  facet_grid(vars(tip))"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#section-8",
    "href": "archive/2023-07-nyr/02-data-budget.html#section-8",
    "title": "2 - Your data budget",
    "section": "",
    "text": "Stratified sampling would split within response values\n\nBased on our EDA, we know that the source data contains fewer \"no\" tip values than \"yes\". We want to make sure we allot equal proportions of those responses so that both the training and testing data have enough of each to give accurate estimates."
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#stratification",
    "href": "archive/2023-07-nyr/02-data-budget.html#stratification",
    "title": "2 - Your data budget",
    "section": "Stratification",
    "text": "Stratification\nUse strata = tip\n\nset.seed(123)\ntaxi_split &lt;- initial_split(taxi, prop = 0.8, strata = tip)\ntaxi_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;7045/1762/8807&gt;"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#stratification-1",
    "href": "archive/2023-07-nyr/02-data-budget.html#stratification-1",
    "title": "2 - Your data budget",
    "section": "Stratification",
    "text": "Stratification\nStratification often helps, with very little downside"
  },
  {
    "objectID": "archive/2023-07-nyr/02-data-budget.html#the-whole-game---status-update",
    "href": "archive/2023-07-nyr/02-data-budget.html#the-whole-game---status-update",
    "title": "2 - Your data budget",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#looking-at-predictions",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#looking-at-predictions",
    "title": "4 - Evaluating models",
    "section": "Looking at predictions",
    "text": "Looking at predictions\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  relocate(tip, .pred_class, .pred_yes, .pred_no)\n#&gt; # A tibble: 7,045 Ã— 10\n#&gt;    tip   .pred_class .pred_yes .pred_no distance company local dow   month  hour\n#&gt;    &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n#&gt;  1 no    no             0.0625   0.937      5.39 Flash â€¦ no    Sat   Mar      12\n#&gt;  2 no    yes            0.924    0.0758    18.4  Sun Taâ€¦ no    Sat   Apr       6\n#&gt;  3 no    no             0.391    0.609      5.8  other   no    Tue   Jan      10\n#&gt;  4 no    no             0.112    0.888      6.85 Flash â€¦ no    Fri   Apr       8\n#&gt;  5 no    no             0.129    0.871      9.5  City Sâ€¦ no    Wed   Jan       7\n#&gt;  6 no    no             0.326    0.674     12    other   no    Fri   Apr      11\n#&gt;  7 no    no             0.0917   0.908      8.9  Taxi Aâ€¦ no    Mon   Feb      14\n#&gt;  8 no    yes            0.902    0.0980     1.38 other   no    Fri   Apr      16\n#&gt;  9 no    no             0.0917   0.908      9.12 Flash â€¦ no    Wed   Apr       9\n#&gt; 10 no    yes            0.933    0.0668     2.28 City Sâ€¦ no    Thu   Apr      16\n#&gt; # â„¹ 7,035 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#confusion-matrix",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#confusion-matrix",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#confusion-matrix-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#confusion-matrix-1",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix \n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  conf_mat(truth = tip, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction  yes   no\n#&gt;        yes 4639  660\n#&gt;        no   337 1409"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#confusion-matrix-2",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#confusion-matrix-2",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix \n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  conf_mat(truth = tip, estimate = .pred_class) %&gt;%\n  autoplot(type = \"heatmap\")"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#metrics-for-model-performance",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#metrics-for-model-performance",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  accuracy(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.858"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-accuracy",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-accuracy",
    "title": "4 - Evaluating models",
    "section": "Dangers of accuracy ",
    "text": "Dangers of accuracy \nWe need to be careful of using accuracy() since it can give â€œgoodâ€ performance by only predicting one way with imbalanced data\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  mutate(.pred_class = factor(\"yes\", levels = c(\"yes\", \"no\"))) %&gt;%\n  accuracy(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.706"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#metrics-for-model-performance-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#metrics-for-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  sensitivity(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.932"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#metrics-for-model-performance-2",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#metrics-for-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  sensitivity(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.932\n\n\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  specificity(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 specificity binary         0.681"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#metrics-for-model-performance-3",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#metrics-for-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \nWe can use metric_set() to combine multiple calculations into one\n\ntaxi_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  taxi_metrics(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy    binary         0.858\n#&gt; 2 specificity binary         0.681\n#&gt; 3 sensitivity binary         0.932"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#metrics-for-model-performance-4",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#metrics-for-model-performance-4",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\ntaxi_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  group_by(local) %&gt;%\n  taxi_metrics(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 6 Ã— 4\n#&gt;   local .metric     .estimator .estimate\n#&gt;   &lt;fct&gt; &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 yes   accuracy    binary         0.840\n#&gt; 2 no    accuracy    binary         0.862\n#&gt; 3 yes   specificity binary         0.346\n#&gt; 4 no    specificity binary         0.719\n#&gt; 5 yes   sensitivity binary         0.969\n#&gt; 6 no    sensitivity binary         0.925"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#two-class-data",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#two-class-data",
    "title": "4 - Evaluating models",
    "section": "Two class data",
    "text": "Two class data\nThese metrics assume that we know the threshold for converting â€œsoftâ€ probability predictions into â€œhardâ€ class predictions.\n\nIs a 50% threshold good?\nWhat happens if we say that we need to be 80% sure to declare an event?\n\nsensitivity â¬‡ï¸, specificity â¬†ï¸\n\n\n\nWhat happens for a 20% threshold?\n\nsensitivity â¬†ï¸, specificity â¬‡ï¸"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#varying-the-threshold",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#varying-the-threshold",
    "title": "4 - Evaluating models",
    "section": "Varying the threshold",
    "text": "Varying the threshold"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#roc-curves",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#roc-curves",
    "title": "4 - Evaluating models",
    "section": "ROC curves",
    "text": "ROC curves\nTo make an ROC (receiver operator characteristic) curve, we:\n\ncalculate the sensitivity and specificity for all possible thresholds\nplot false positive rate (x-axis) versus true positive rate (y-axis)\n\ngiven that sensitivity is the true positive rate, and specificity is the true negative rate. Hence 1 - specificity is the false positive rate.\n\nWe can use the area under the ROC curve as a classification metric:\n\nROC AUC = 1 ğŸ’¯\nROC AUC = 1/2 ğŸ˜¢\n\n\nROC curves are insensitive to class imbalance."
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#roc-curves-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#roc-curves-1",
    "title": "4 - Evaluating models",
    "section": "ROC curves ",
    "text": "ROC curves \n\n# Assumes _first_ factor level is event; there are options to change that\naugment(taxi_fit, new_data = taxi_train) %&gt;% \n  roc_curve(truth = tip, .pred_yes) %&gt;%\n  slice(1, 20, 50)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .threshold specificity sensitivity\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1    -Inf          0           1    \n#&gt; 2       0.25       0.486       0.972\n#&gt; 3       0.6        0.705       0.920\n\naugment(taxi_fit, new_data = taxi_train) %&gt;% \n  roc_auc(truth = tip, .pred_yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.868"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#roc-curve-plot",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#roc-curve-plot",
    "title": "4 - Evaluating models",
    "section": "ROC curve plot ",
    "text": "ROC curve plot \n\n\naugment(taxi_fit, new_data = taxi_train) %&gt;% \n  roc_curve(truth = tip, .pred_yes) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#your-turn",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#your-turn",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCompute and plot an ROC curve for your current model.\nWhat data are being used for this ROC curve plot?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-1",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸",
    "text": "Dangers of overfitting âš ï¸"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-2",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-2",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸",
    "text": "Dangers of overfitting âš ï¸"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-3",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-3",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\ntaxi_fit %&gt;%\n  augment(taxi_train)\n#&gt; # A tibble: 7,045 Ã— 10\n#&gt;    tip   distance company local dow   month  hour .pred_class .pred_yes .pred_no\n#&gt;    &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 no        5.39 Flash â€¦ no    Sat   Mar      12 no             0.0625   0.937 \n#&gt;  2 no       18.4  Sun Taâ€¦ no    Sat   Apr       6 yes            0.924    0.0758\n#&gt;  3 no        5.8  other   no    Tue   Jan      10 no             0.391    0.609 \n#&gt;  4 no        6.85 Flash â€¦ no    Fri   Apr       8 no             0.112    0.888 \n#&gt;  5 no        9.5  City Sâ€¦ no    Wed   Jan       7 no             0.129    0.871 \n#&gt;  6 no       12    other   no    Fri   Apr      11 no             0.326    0.674 \n#&gt;  7 no        8.9  Taxi Aâ€¦ no    Mon   Feb      14 no             0.0917   0.908 \n#&gt;  8 no        1.38 other   no    Fri   Apr      16 yes            0.902    0.0980\n#&gt;  9 no        9.12 Flash â€¦ no    Wed   Apr       9 no             0.0917   0.908 \n#&gt; 10 no        2.28 City Sâ€¦ no    Thu   Apr      16 yes            0.933    0.0668\n#&gt; # â„¹ 7,035 more rows\n\nWe call this â€œresubstitutionâ€ or â€œrepredicting the training setâ€"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-4",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-4",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\ntaxi_fit %&gt;%\n  augment(taxi_train) %&gt;%\n  accuracy(tip, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.858\n\nWe call this a â€œresubstitution estimateâ€"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-5",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-5",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\ntaxi_fit %&gt;%\n  augment(taxi_train) %&gt;%\n  accuracy(tip, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.858"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-6",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-6",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\ntaxi_fit %&gt;%\n  augment(taxi_train) %&gt;%\n  accuracy(tip, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.858\n\n\n\ntaxi_fit %&gt;%\n  augment(taxi_test) %&gt;%\n  accuracy(tip, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.795\n\n\n\nâš ï¸ Remember that weâ€™re demonstrating overfitting\n\n\nâš ï¸ Donâ€™t use the test set until the end of your modeling analysis"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#your-turn-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#your-turn-1",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse augment() and and a metric function to compute a classification metric like brier_class().\nCompute the metrics for both training and testing data to demonstrate overfitting!\nNotice the evidence of overfitting! âš ï¸\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-7",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#dangers-of-overfitting-7",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\ntaxi_fit %&gt;%\n  augment(taxi_train) %&gt;%\n  brier_class(tip, .pred_yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary         0.113\n\n\n\ntaxi_fit %&gt;%\n  augment(taxi_test) %&gt;%\n  brier_class(tip, .pred_yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary         0.152\n\n\n\nWhat if we want to compare more models?\n\n\nAnd/or more model configurations?\n\n\nAnd we want to understand if these are important differences?"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation-1",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#your-turn-2",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#your-turn-2",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nIf we use 10 folds, what percent of the training data\n\nends up in analysis\nends up in assessment\n\nfor each fold?\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation-2",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation-2",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(taxi_train) # v = 10 is default\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [6340/705]&gt; Fold01\n#&gt;  2 &lt;split [6340/705]&gt; Fold02\n#&gt;  3 &lt;split [6340/705]&gt; Fold03\n#&gt;  4 &lt;split [6340/705]&gt; Fold04\n#&gt;  5 &lt;split [6340/705]&gt; Fold05\n#&gt;  6 &lt;split [6341/704]&gt; Fold06\n#&gt;  7 &lt;split [6341/704]&gt; Fold07\n#&gt;  8 &lt;split [6341/704]&gt; Fold08\n#&gt;  9 &lt;split [6341/704]&gt; Fold09\n#&gt; 10 &lt;split [6341/704]&gt; Fold10"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation-3",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation-3",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWhat is in this?\n\ntaxi_folds &lt;- vfold_cv(taxi_train)\ntaxi_folds$splits[1:3]\n#&gt; [[1]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;6340/705/7045&gt;\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;6340/705/7045&gt;\n#&gt; \n#&gt; [[3]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;6340/705/7045&gt;\n\n\nTalk about a list column, storing non-atomic types in dataframe"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation-4",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation-4",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(taxi_train, v = 5)\n#&gt; #  5-fold cross-validation \n#&gt; # A tibble: 5 Ã— 2\n#&gt;   splits              id   \n#&gt;   &lt;list&gt;              &lt;chr&gt;\n#&gt; 1 &lt;split [5636/1409]&gt; Fold1\n#&gt; 2 &lt;split [5636/1409]&gt; Fold2\n#&gt; 3 &lt;split [5636/1409]&gt; Fold3\n#&gt; 4 &lt;split [5636/1409]&gt; Fold4\n#&gt; 5 &lt;split [5636/1409]&gt; Fold5"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation-5",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation-5",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(taxi_train, strata = tip)\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [6340/705]&gt; Fold01\n#&gt;  2 &lt;split [6340/705]&gt; Fold02\n#&gt;  3 &lt;split [6340/705]&gt; Fold03\n#&gt;  4 &lt;split [6340/705]&gt; Fold04\n#&gt;  5 &lt;split [6340/705]&gt; Fold05\n#&gt;  6 &lt;split [6340/705]&gt; Fold06\n#&gt;  7 &lt;split [6341/704]&gt; Fold07\n#&gt;  8 &lt;split [6341/704]&gt; Fold08\n#&gt;  9 &lt;split [6341/704]&gt; Fold09\n#&gt; 10 &lt;split [6342/703]&gt; Fold10\n\n\nStratification often helps, with very little downside"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation-6",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#cross-validation-6",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWeâ€™ll use this setup:\n\nset.seed(123)\ntaxi_folds &lt;- vfold_cv(taxi_train, v = 10, strata = tip)\ntaxi_folds\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [6340/705]&gt; Fold01\n#&gt;  2 &lt;split [6340/705]&gt; Fold02\n#&gt;  3 &lt;split [6340/705]&gt; Fold03\n#&gt;  4 &lt;split [6340/705]&gt; Fold04\n#&gt;  5 &lt;split [6340/705]&gt; Fold05\n#&gt;  6 &lt;split [6340/705]&gt; Fold06\n#&gt;  7 &lt;split [6341/704]&gt; Fold07\n#&gt;  8 &lt;split [6341/704]&gt; Fold08\n#&gt;  9 &lt;split [6341/704]&gt; Fold09\n#&gt; 10 &lt;split [6342/703]&gt; Fold10\n\n\nSet the seed when creating resamples"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#fit-our-model-to-the-resamples",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#fit-our-model-to-the-resamples",
    "title": "4 - Evaluating models",
    "section": "Fit our model to the resamples",
    "text": "Fit our model to the resamples\n\ntaxi_res &lt;- fit_resamples(taxi_wflow, taxi_folds)\ntaxi_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 4\n#&gt;    splits             id     .metrics         .notes          \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n#&gt;  1 &lt;split [6340/705]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  2 &lt;split [6340/705]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  3 &lt;split [6340/705]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  4 &lt;split [6340/705]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  5 &lt;split [6340/705]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  6 &lt;split [6340/705]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  7 &lt;split [6341/704]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  8 &lt;split [6341/704]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  9 &lt;split [6341/704]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt; 10 &lt;split [6342/703]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#evaluating-model-performance",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#evaluating-model-performance",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\ntaxi_res %&gt;%\n  collect_metrics()\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.793    10 0.00293 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.809    10 0.00461 Preprocessor1_Model1\n\n\nWe can reliably measure performance using only the training data ğŸ‰"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#comparing-metrics",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#comparing-metrics",
    "title": "4 - Evaluating models",
    "section": "Comparing metrics ",
    "text": "Comparing metrics \nHow do the metrics from resampling compare to the metrics from training and testing?\n\n\n\ntaxi_res %&gt;%\n  collect_metrics() %&gt;% \n  select(.metric, mean, n)\n#&gt; # A tibble: 2 Ã— 3\n#&gt;   .metric   mean     n\n#&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 accuracy 0.793    10\n#&gt; 2 roc_auc  0.809    10\n\n\nThe ROC AUC previously was\n\n0.87 for the training set\n0.81 for test set\n\n\n\nRemember that:\nâš ï¸ the training set gives you overly optimistic metrics\nâš ï¸ the test set is precious"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#evaluating-model-performance-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#evaluating-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nctrl_taxi &lt;- control_resamples(save_pred = TRUE)\ntaxi_res &lt;- fit_resamples(taxi_wflow, taxi_folds, control = ctrl_taxi)\n\ntaxi_preds &lt;- collect_predictions(taxi_res)\ntaxi_preds\n#&gt; # A tibble: 7,045 Ã— 7\n#&gt;    id     .pred_yes .pred_no  .row .pred_class tip   .config             \n#&gt;    &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt; &lt;chr&gt;               \n#&gt;  1 Fold01    0.936    0.0638    10 yes         no    Preprocessor1_Model1\n#&gt;  2 Fold01    0.898    0.102     20 yes         no    Preprocessor1_Model1\n#&gt;  3 Fold01    0.898    0.102     47 yes         no    Preprocessor1_Model1\n#&gt;  4 Fold01    0.101    0.899     51 no          no    Preprocessor1_Model1\n#&gt;  5 Fold01    0.871    0.129     59 yes         no    Preprocessor1_Model1\n#&gt;  6 Fold01    0.0815   0.918     60 no          no    Preprocessor1_Model1\n#&gt;  7 Fold01    0.162    0.838     92 no          no    Preprocessor1_Model1\n#&gt;  8 Fold01    0.26     0.74      97 no          no    Preprocessor1_Model1\n#&gt;  9 Fold01    0.274    0.726     98 no          no    Preprocessor1_Model1\n#&gt; 10 Fold01    0.804    0.196    104 yes         no    Preprocessor1_Model1\n#&gt; # â„¹ 7,035 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#evaluating-model-performance-2",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#evaluating-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\ntaxi_preds %&gt;% \n  group_by(id) %&gt;%\n  taxi_metrics(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 30 Ã— 4\n#&gt;    id     .metric  .estimator .estimate\n#&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt;  1 Fold01 accuracy binary         0.793\n#&gt;  2 Fold02 accuracy binary         0.8  \n#&gt;  3 Fold03 accuracy binary         0.786\n#&gt;  4 Fold04 accuracy binary         0.804\n#&gt;  5 Fold05 accuracy binary         0.796\n#&gt;  6 Fold06 accuracy binary         0.789\n#&gt;  7 Fold07 accuracy binary         0.793\n#&gt;  8 Fold08 accuracy binary         0.808\n#&gt;  9 Fold09 accuracy binary         0.783\n#&gt; 10 Fold10 accuracy binary         0.780\n#&gt; # â„¹ 20 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#where-are-the-fitted-models",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#where-are-the-fitted-models",
    "title": "4 - Evaluating models",
    "section": "Where are the fitted models? ",
    "text": "Where are the fitted models? \n\ntaxi_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [6340/705]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [6340/705]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [6340/705]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [6340/705]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [6340/705]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [6340/705]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [6341/704]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [6341/704]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [6341/704]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [6342/703]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;\n\n\nğŸ—‘ï¸"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#bootstrapping",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#bootstrapping",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#bootstrapping-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#bootstrapping-1",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(3214)\nbootstraps(taxi_train)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 25 Ã— 2\n#&gt;    splits              id         \n#&gt;    &lt;list&gt;              &lt;chr&gt;      \n#&gt;  1 &lt;split [7045/2561]&gt; Bootstrap01\n#&gt;  2 &lt;split [7045/2577]&gt; Bootstrap02\n#&gt;  3 &lt;split [7045/2648]&gt; Bootstrap03\n#&gt;  4 &lt;split [7045/2616]&gt; Bootstrap04\n#&gt;  5 &lt;split [7045/2616]&gt; Bootstrap05\n#&gt;  6 &lt;split [7045/2599]&gt; Bootstrap06\n#&gt;  7 &lt;split [7045/2654]&gt; Bootstrap07\n#&gt;  8 &lt;split [7045/2593]&gt; Bootstrap08\n#&gt;  9 &lt;split [7045/2624]&gt; Bootstrap09\n#&gt; 10 &lt;split [7045/2615]&gt; Bootstrap10\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#the-whole-game---status-update",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#the-whole-game---status-update",
    "title": "4 - Evaluating models",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#your-turn-3",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#your-turn-3",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCreate:\n\nMonte Carlo Cross-Validation sets\nvalidation set\n\n(use the reference guide to find the function)\nDonâ€™t forget to set a seed when you resample!\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#monte-carlo-cross-validation",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#monte-carlo-cross-validation",
    "title": "4 - Evaluating models",
    "section": "Monte Carlo Cross-Validation ",
    "text": "Monte Carlo Cross-Validation \n\nset.seed(322)\nmc_cv(taxi_train, times = 10)\n#&gt; # Monte Carlo cross-validation (0.75/0.25) with 10 resamples  \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits              id        \n#&gt;    &lt;list&gt;              &lt;chr&gt;     \n#&gt;  1 &lt;split [5283/1762]&gt; Resample01\n#&gt;  2 &lt;split [5283/1762]&gt; Resample02\n#&gt;  3 &lt;split [5283/1762]&gt; Resample03\n#&gt;  4 &lt;split [5283/1762]&gt; Resample04\n#&gt;  5 &lt;split [5283/1762]&gt; Resample05\n#&gt;  6 &lt;split [5283/1762]&gt; Resample06\n#&gt;  7 &lt;split [5283/1762]&gt; Resample07\n#&gt;  8 &lt;split [5283/1762]&gt; Resample08\n#&gt;  9 &lt;split [5283/1762]&gt; Resample09\n#&gt; 10 &lt;split [5283/1762]&gt; Resample10"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#validation-set",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#validation-set",
    "title": "4 - Evaluating models",
    "section": "Validation set ",
    "text": "Validation set \n\nset.seed(853)\nvalidation_split(taxi_train, strata = tip)\n#&gt; # Validation Set Split (0.75/0.25)  using stratification \n#&gt; # A tibble: 1 Ã— 2\n#&gt;   splits              id        \n#&gt;   &lt;list&gt;              &lt;chr&gt;     \n#&gt; 1 &lt;split [5283/1762]&gt; validation\n\n\nA validation set is just another type of resample"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#random-forest-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#random-forest-1",
    "title": "4 - Evaluating models",
    "section": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ",
    "text": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\n\nEnsemble many decision tree models\nAll the trees vote! ğŸ—³ï¸\nBootstrap aggregating + random predictor sampling\n\n\n\nOften works well without tuning hyperparameters (more on this tomorrow!), as long as there are enough trees"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#create-a-random-forest-model",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#create-a-random-forest-model",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_spec &lt;- rand_forest(trees = 1000, mode = \"classification\")\nrf_spec\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#create-a-random-forest-model-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#create-a-random-forest-model-1",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_wflow &lt;- workflow(tip ~ ., rf_spec)\nrf_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; tip ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#your-turn-4",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#your-turn-4",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() and rf_wflow to:\n\nkeep predictions\ncompute metrics\n\n\n\n\nâˆ’+\n08:00"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#evaluating-model-performance-3",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#evaluating-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nctrl_taxi &lt;- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\n\nset.seed(2)\nrf_res &lt;- fit_resamples(rf_wflow, taxi_folds, control = ctrl_taxi)\ncollect_metrics(rf_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.813    10 0.00305 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.832    10 0.00513 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#how-can-we-compare-multiple-model-workflows-at-once",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#how-can-we-compare-multiple-model-workflows-at-once",
    "title": "4 - Evaluating models",
    "section": "How can we compare multiple model workflows at once?",
    "text": "How can we compare multiple model workflows at once?"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#evaluate-a-workflow-set",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#evaluate-a-workflow-set",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(tip ~ .), list(tree_spec, rf_spec))\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result    \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#evaluate-a-workflow-set-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#evaluate-a-workflow-set-1",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(tip ~ .), list(tree_spec, rf_spec)) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = taxi_folds)\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result   \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#evaluate-a-workflow-set-2",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#evaluate-a-workflow-set-2",
    "title": "4 - Evaluating models",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nworkflow_set(list(tip ~ .), list(tree_spec, rf_spec)) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = taxi_folds) %&gt;%\n  rank_results()\n#&gt; # A tibble: 4 Ã— 9\n#&gt;   wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n#&gt;   &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n#&gt; 1 formula_rand_forâ€¦ Preproâ€¦ accuraâ€¦ 0.813 0.00339    10 formula      randâ€¦     1\n#&gt; 2 formula_rand_forâ€¦ Preproâ€¦ roc_auc 0.833 0.00528    10 formula      randâ€¦     1\n#&gt; 3 formula_decisionâ€¦ Preproâ€¦ accuraâ€¦ 0.793 0.00293    10 formula      deciâ€¦     2\n#&gt; 4 formula_decisionâ€¦ Preproâ€¦ roc_auc 0.809 0.00461    10 formula      deciâ€¦     2\n\nThe first metric of the metric set is used for ranking. Use rank_metric to change that.\n\nLots more available with workflow sets, like collect_metrics(), autoplot() methods, and more!"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#your-turn-5",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#your-turn-5",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nWhen do you think a workflow set would be useful?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#the-whole-game---status-update-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#the-whole-game---status-update-1",
    "title": "4 - Evaluating models",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#the-final-fit",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#the-final-fit",
    "title": "4 - Evaluating models",
    "section": "The final fit ",
    "text": "The final fit \nSuppose that we are happy with our random forest model.\nLetâ€™s fit the model on the training set and verify our performance using the test set.\n\nWeâ€™ve shown you fit() and predict() (+ augment()) but there is a shortcut:\n\n# taxi_split has train + test info\nfinal_fit &lt;- last_fit(rf_wflow, taxi_split) \n\nfinal_fit\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits              id               .metrics .notes   .predictions .workflow \n#&gt;   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n#&gt; 1 &lt;split [7045/1762]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#what-is-in-final_fit",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#what-is-in-final_fit",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_metrics(final_fit)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric  .estimator .estimate .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary         0.810 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary         0.817 Preprocessor1_Model1\n\n\nThese are metrics computed with the test set"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#what-is-in-final_fit-1",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#what-is-in-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_predictions(final_fit)\n#&gt; # A tibble: 1,762 Ã— 7\n#&gt;    id               .pred_yes .pred_no  .row .pred_class tip   .config          \n#&gt;    &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt; &lt;chr&gt;            \n#&gt;  1 train/test split     0.732   0.268     10 yes         no    Preprocessor1_Moâ€¦\n#&gt;  2 train/test split     0.827   0.173     29 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  3 train/test split     0.899   0.101     35 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  4 train/test split     0.914   0.0856    42 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  5 train/test split     0.911   0.0889    47 yes         no    Preprocessor1_Moâ€¦\n#&gt;  6 train/test split     0.848   0.152     54 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  7 train/test split     0.580   0.420     59 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  8 train/test split     0.912   0.0876    62 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  9 train/test split     0.810   0.190     63 yes         yes   Preprocessor1_Moâ€¦\n#&gt; 10 train/test split     0.960   0.0402    69 yes         yes   Preprocessor1_Moâ€¦\n#&gt; # â„¹ 1,752 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#what-is-in-final_fit-2",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#what-is-in-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\nextract_workflow(final_fit)\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; tip ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  1000 \n#&gt; Sample size:                      7045 \n#&gt; Number of independent variables:  6 \n#&gt; Mtry:                             2 \n#&gt; Target node size:                 10 \n#&gt; Variable importance mode:         none \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.1373147\n\n\nUse this for prediction on new data, like for deploying"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#the-whole-game",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#the-whole-game",
    "title": "4 - Evaluating models",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-07-nyr/04-evaluating-models.html#your-turn-6",
    "href": "archive/2023-07-nyr/04-evaluating-models.html#your-turn-6",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nEnd of the day discussion!\nWhich model do you think you would decide to use?\nWhat surprised you the most?\nWhat is one thing you are looking forward to for tomorrow?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#working-with-our-predictors",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#working-with-our-predictors",
    "title": "1 - Feature Engineering",
    "section": "Working with our predictors",
    "text": "Working with our predictors\nWe might want to modify our predictors columns for a few reasons:\n\nThe model requires them in a different format (e.g.Â dummy variables for linear regression).\nThe model needs certain data qualities (e.g.Â same units for K-NN).\nThe outcome is better predicted when one or more columns are transformed in some way (a.k.a â€œfeature engineeringâ€).\n\n\nThe first two reasons are fairly predictable (next page).\nThe last one depends on your modeling problem."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#what-is-feature-engineering",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#what-is-feature-engineering",
    "title": "1 - Feature Engineering",
    "section": "What is feature engineering?",
    "text": "What is feature engineering?\nThink of a feature as some representation of a predictor that will be used in a model.\n\nExample representations:\n\nInteractions\nPolynomial expansions/splines\nPrincipal component analysis (PCA) feature extraction\n\nThere are a lot of examples in Feature Engineering and Selection (FES)."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#example-dates",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#example-dates",
    "title": "1 - Feature Engineering",
    "section": "Example: Dates",
    "text": "Example: Dates\nHow can we represent date columns for our model?\n\nWhen we use a date column in its native format, most models in R convert it to an integer.\n\n\nWe can re-engineer it as:\n\nDays since a reference date\nDay of the week\nMonth\nYear\nIndicators for holidays\n\n\nThe main point is that we try to maximize performance with different versions of the predictors.\nMention that, for the Chicago data, the day or the week features are usually the most important ones in the model."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#general-definitions",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#general-definitions",
    "title": "1 - Feature Engineering",
    "section": "General definitions ",
    "text": "General definitions \n\nData preprocessing steps allow your model to fit.\nFeature engineering steps help the model do the least work to predict the outcome as well as possible.\n\nThe recipes package can handle both!\n\nThese terms are often used interchangeably in the ML community but we want to distinguish them."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#hotel-data",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#hotel-data",
    "title": "1 - Feature Engineering",
    "section": "Hotel Data  ",
    "text": "Hotel Data  \nWeâ€™ll use data on hotels to predict the cost of a room.\nThe data are in the modeldatatoo package. Weâ€™ll sample down the data and refactor some columns:\n\n\n\nlibrary(tidymodels)\nlibrary(modeldatatoo)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\nset.seed(295)\nhotel_rates &lt;- \n  data_hotel_rates() %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date_num, -arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#data-splitting-strategy",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#data-splitting-strategy",
    "title": "1 - Feature Engineering",
    "section": "Data splitting strategy",
    "text": "Data splitting strategy"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#data-spending",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#data-spending",
    "title": "1 - Feature Engineering",
    "section": "Data Spending ",
    "text": "Data Spending \nLetâ€™s split the data into a training set (75%) and testing set (25%):\n\nset.seed(4028)\nhotel_split &lt;-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_tr &lt;- training(hotel_split)\nhotel_te &lt;- testing(hotel_split)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#your-turn",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#your-turn",
    "title": "1 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\nLetâ€™s take some time and investigate the training data. The outcome is avg_price_per_room.\nAre there any interesting characteristics of the data?\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#resampling-strategy",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#resampling-strategy",
    "title": "1 - Feature Engineering",
    "section": "Resampling Strategy",
    "text": "Resampling Strategy"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#resampling-strategy-1",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#resampling-strategy-1",
    "title": "1 - Feature Engineering",
    "section": "Resampling Strategy ",
    "text": "Resampling Strategy \nWeâ€™ll use simple 10-fold cross-validation (stratified sampling):\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_tr, strata = avg_price_per_room)\nhotel_rs\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [3372/377]&gt; Fold01\n#&gt;  2 &lt;split [3373/376]&gt; Fold02\n#&gt;  3 &lt;split [3373/376]&gt; Fold03\n#&gt;  4 &lt;split [3373/376]&gt; Fold04\n#&gt;  5 &lt;split [3373/376]&gt; Fold05\n#&gt;  6 &lt;split [3374/375]&gt; Fold06\n#&gt;  7 &lt;split [3375/374]&gt; Fold07\n#&gt;  8 &lt;split [3376/373]&gt; Fold08\n#&gt;  9 &lt;split [3376/373]&gt; Fold09\n#&gt; 10 &lt;split [3376/373]&gt; Fold10"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#prepare-your-data-for-modeling",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#prepare-your-data-for-modeling",
    "title": "1 - Feature Engineering",
    "section": "Prepare your data for modeling ",
    "text": "Prepare your data for modeling \n\nThe recipes package is an extensible framework for pipeable sequences of preprocessing and feature engineering steps.\n\n\n\nStatistical parameters for the steps can be estimated from an initial data set and then applied to other data sets.\n\n\n\n\nThe resulting processed output can be used as inputs for statistical or machine learning models."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#a-first-recipe",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#a-first-recipe",
    "title": "1 - Feature Engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_tr)\n\n\n\nThe recipe() function assigns columns to roles of â€œoutcomeâ€ or â€œpredictorâ€ using the formula"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#a-first-recipe-1",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#a-first-recipe-1",
    "title": "1 - Feature Engineering",
    "section": "A first recipe ",
    "text": "A first recipe \n\nsummary(hotel_rec)\n#&gt; # A tibble: 28 Ã— 4\n#&gt;    variable                  type      role      source  \n#&gt;    &lt;chr&gt;                     &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 lead_time                 &lt;chr [2]&gt; predictor original\n#&gt;  2 arrival_date_day_of_month &lt;chr [2]&gt; predictor original\n#&gt;  3 stays_in_weekend_nights   &lt;chr [2]&gt; predictor original\n#&gt;  4 stays_in_week_nights      &lt;chr [2]&gt; predictor original\n#&gt;  5 adults                    &lt;chr [2]&gt; predictor original\n#&gt;  6 children                  &lt;chr [2]&gt; predictor original\n#&gt;  7 babies                    &lt;chr [2]&gt; predictor original\n#&gt;  8 meal                      &lt;chr [3]&gt; predictor original\n#&gt;  9 country                   &lt;chr [3]&gt; predictor original\n#&gt; 10 market_segment            &lt;chr [3]&gt; predictor original\n#&gt; # â„¹ 18 more rows\n\nThe type column contains information on the variables"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#your-turn-1",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#your-turn-1",
    "title": "1 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\nWhat do you think are in the type vectors for the lead_time and country columns?\n\n\n\nâˆ’+\n02:00"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#create-indicator-variables",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#create-indicator-variables",
    "title": "1 - Feature Engineering",
    "section": "Create indicator variables ",
    "text": "Create indicator variables \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\n\nFor any factor or character predictors, make binary indicators.\nThere are many recipe steps that can convert categorical predictors to numeric columns.\nstep_dummy() records the levels of the categorical predictors in the training set."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#filter-out-constant-columns",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#filter-out-constant-columns",
    "title": "1 - Feature Engineering",
    "section": "Filter out constant columns ",
    "text": "Filter out constant columns \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())\n\n\nIn case there is a factor level that was never observed in the training data (resulting in a column of all 0s), we can delete any zero-variance predictors that have a single unique value.\n\nNote that the selector chooses all columns with a role of â€œpredictorâ€"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#normalization",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#normalization",
    "title": "1 - Feature Engineering",
    "section": "Normalization ",
    "text": "Normalization \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\n\n\nThis centers and scales the numeric predictors.\nThe recipe will use the training set to estimate the means and standard deviations of the data.\n\n\n\n\nAll data the recipe is applied to will be normalized using those statistics (there is no re-estimation)."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#reduce-correlation",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#reduce-correlation",
    "title": "1 - Feature Engineering",
    "section": "Reduce correlation ",
    "text": "Reduce correlation \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_corr(all_numeric_predictors(), threshold = 0.9)\n\n\nTo deal with highly correlated predictors, find the minimum set of predictor columns that make the pairwise correlations less than the threshold."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#other-possible-steps",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#other-possible-steps",
    "title": "1 - Feature Engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_pca(all_numeric_predictors())\n\n\nPCA feature extractionâ€¦"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#other-possible-steps-1",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#other-possible-steps-1",
    "title": "1 - Feature Engineering",
    "section": "Other possible steps  ",
    "text": "Other possible steps  \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  embed::step_umap(all_numeric_predictors(), outcome = avg_price_per_room)\n\n\nA fancy machine learning supervised dimension reduction techniqueâ€¦\n\nNote that this uses the outcome, and it is from an extension package"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#other-possible-steps-2",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#other-possible-steps-2",
    "title": "1 - Feature Engineering",
    "section": "Other possible steps ",
    "text": "Other possible steps \n\nhotel_rec &lt;- \n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_spline_natural(year_day, deg_free = 10)\n\n\nNonlinear transforms like natural splines, and so on!"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#your-turn-2",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#your-turn-2",
    "title": "1 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a recipe() for the hotel data to:\n\nuse a Yeo-Johnson (YJ) transformation on lead_time\nconvert factors to indicator variables\nremove zero-variance variables\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#minimal-recipe",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#minimal-recipe",
    "title": "1 - Feature Engineering",
    "section": "Minimal recipe ",
    "text": "Minimal recipe \n\nhotel_indicators &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;% \n  step_YeoJohnson(lead_time) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#measuring-performance",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#measuring-performance",
    "title": "1 - Feature Engineering",
    "section": "Measuring Performance ",
    "text": "Measuring Performance \nWeâ€™ll compute two measures: mean absolute error and the coefficient of determination (a.k.a \\(R^2\\)).\n\\[\\begin{align}\nMAE &= \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i| \\notag \\\\\nR^2 &= cor(y_i, \\hat{y}_i)^2\n\\end{align}\\]\nThe focus will be on MAE for parameter optimization. Weâ€™ll use a metric set to compute these:\n\nreg_metrics &lt;- metric_set(mae, rsq)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#using-a-workflow",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#using-a-workflow",
    "title": "1 - Feature Engineering",
    "section": "Using a workflow    ",
    "text": "Using a workflow    \n\nset.seed(9)\n\nhotel_lm_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(hotel_indicators) %&gt;%\n  add_model(linear_reg())\n \nctrl &lt;- control_resamples(save_pred = TRUE)\nhotel_lm_res &lt;-\n  hotel_lm_wflow %&gt;%\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\ncollect_metrics(hotel_lm_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   17.3      10 0.199   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.874    10 0.00400 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#your-turn-3",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#your-turn-3",
    "title": "1 - Feature Engineering",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() to fit your workflow with a recipe.\nCollect the predictions from the results.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#holdout-predictions",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#holdout-predictions",
    "title": "1 - Feature Engineering",
    "section": "Holdout predictions    ",
    "text": "Holdout predictions    \n\n# Since we used `save_pred = TRUE`\nlm_val_pred &lt;- collect_predictions(hotel_lm_res)\nlm_val_pred %&gt;% slice(1:7)\n#&gt; # A tibble: 7 Ã— 5\n#&gt;   id     .pred  .row avg_price_per_room .config             \n#&gt;   &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt;              &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 Fold01  62.1    20                 40 Preprocessor1_Model1\n#&gt; 2 Fold01  48.0    28                 54 Preprocessor1_Model1\n#&gt; 3 Fold01  64.6    45                 50 Preprocessor1_Model1\n#&gt; 4 Fold01  45.8    49                 42 Preprocessor1_Model1\n#&gt; 5 Fold01  45.8    61                 49 Preprocessor1_Model1\n#&gt; 6 Fold01  30.0    66                 40 Preprocessor1_Model1\n#&gt; 7 Fold01  38.8    88                 49 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#calibration-plot",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#calibration-plot",
    "title": "1 - Feature Engineering",
    "section": "Calibration Plot ",
    "text": "Calibration Plot \n\nlibrary(probably)\n\ncal_plot_regression(hotel_lm_res, alpha = 1 / 5)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#what-do-we-do-with-the-agent-and-company-data",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#what-do-we-do-with-the-agent-and-company-data",
    "title": "1 - Feature Engineering",
    "section": "What do we do with the agent and company data?",
    "text": "What do we do with the agent and company data?\nThere are 98 unique agent values and 100 unique companies in our training set. How can we include this information in our model?\n\nWe could:\n\nmake the full set of indicator variables ğŸ˜³\nlump agents and companies that rarely occur into an â€œotherâ€ group\nuse feature hashing to create a smaller set of indicator variables\nuse effect encoding to replace the agent and company columns with the estimated effect of that predictor (in the extra materials)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#per-agent-statistics",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#per-agent-statistics",
    "title": "1 - Feature Engineering",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#collapsing-factor-levels",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#collapsing-factor-levels",
    "title": "1 - Feature Engineering",
    "section": "Collapsing factor levels ",
    "text": "Collapsing factor levels \nThere is a recipe step that will redefine factor levels based on their frequency in the training set:\n\nhotel_other_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;% \n  step_YeoJohnson(lead_time) %&gt;%\n  step_other(agent, threshold = 0.001) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_zv(all_predictors())\n\nUsing this code, 34 agents (out of 98) were collapsed into â€œotherâ€ based on the training set.\nWe could try to optimize the threshold for collapsing (see the next set of slides on model tuning)."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#does-othering-help",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#does-othering-help",
    "title": "1 - Feature Engineering",
    "section": "Does othering help?  ",
    "text": "Does othering help?  \n\nhotel_other_wflow &lt;-\n  hotel_lm_wflow %&gt;%\n  update_recipe(hotel_other_rec)\n\nhotel_other_res &lt;-\n  hotel_other_wflow %&gt;%\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\ncollect_metrics(hotel_other_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   17.4      10 0.205   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.874    10 0.00417 Preprocessor1_Model1\n\nAabout the same MAE and much faster to complete.\nNow letâ€™s look at a more sophisticated tool called effect feature hashing."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#feature-hashing",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#feature-hashing",
    "title": "1 - Feature Engineering",
    "section": "Feature Hashing",
    "text": "Feature Hashing\nBetween agent and company, simple dummy variables would create 198 new columns (that are mostly zeros).\nAnother option is to have a binary indicator that combines some levels of these variables.\nFeature hashing (for more see FES, SMLTAR, and TMwR):\n\nuses the character values of the levels\nconverts them to integer hash values\nuses the integers to assign them to a specific indicator column."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#feature-hashing-1",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#feature-hashing-1",
    "title": "1 - Feature Engineering",
    "section": "Feature Hashing",
    "text": "Feature Hashing\nSuppose we want to use 32 indicator variables for agent.\nFor a agent with value â€œMax_Kuhnâ€, a hashing function converts it to an integer (say 210397726).\nTo assign it to one of the 32 columns, we would use modular arithmetic to assign it to a column:\n\n# For \"Max_Kuhn\" put a '1' in column: \n210397726 %% 32\n#&gt; [1] 30\n\nHash functions are meant to emulate randomness."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#feature-hashing-pros",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#feature-hashing-pros",
    "title": "1 - Feature Engineering",
    "section": "Feature Hashing Pros",
    "text": "Feature Hashing Pros\n\nThe procedure will automatically work on new values of the predictors.\nIt is fast.\nâ€œSignedâ€ hashes add a sign to help avoid aliasing."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#feature-hashing-cons",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#feature-hashing-cons",
    "title": "1 - Feature Engineering",
    "section": "Feature Hashing Cons",
    "text": "Feature Hashing Cons\n\nThere is no real logic behind which factor levels are combined.\nWe donâ€™t know how many columns to add (more in the next section).\nSome columns may have all zeros.\nIf a indicator column is important to the model, we canâ€™t easily determine why.\n\n\nThe signed hash make it slightly more possible to differentiate between confounded levels"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#feature-hashing-in-recipes",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#feature-hashing-in-recipes",
    "title": "1 - Feature Engineering",
    "section": "Feature Hashing in recipes   ",
    "text": "Feature Hashing in recipes   \nThe textrecipes package has a step that can be added to the recipe:\n\nlibrary(textrecipes)\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  # Defaults to 32 signed indicator columns\n  step_dummy_hash(agent) %&gt;%\n  step_dummy_hash(company) %&gt;%\n  # Regular indicators for the others\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())\n\nhotel_hash_wflow &lt;-\n  hotel_lm_wflow %&gt;%\n  update_recipe(hash_rec)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#feature-hashing-in-recipes-1",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#feature-hashing-in-recipes-1",
    "title": "1 - Feature Engineering",
    "section": "Feature Hashing in recipes   ",
    "text": "Feature Hashing in recipes   \n\nhotel_hash_res &lt;-\n  hotel_hash_wflow %&gt;%\n  fit_resamples(hotel_rs, control = ctrl, metrics = reg_metrics)\n\ncollect_metrics(hotel_hash_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard   17.5      10 0.256   Preprocessor1_Model1\n#&gt; 2 rsq     standard    0.872    10 0.00395 Preprocessor1_Model1\n\nAbout the same performance but now we can handle new values."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#debugging-a-recipe",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#debugging-a-recipe",
    "title": "1 - Feature Engineering",
    "section": "Debugging a recipe",
    "text": "Debugging a recipe\n\nTypically, you will want to use a workflow to estimate and apply a recipe.\n\n\n\nIf you have an error and need to debug your recipe, the original recipe object (e.g.Â hash_rec) can be estimated manually with a function called prep(). It is analogous to fit(). See TMwR section 16.4\n\n\n\n\nAnother function (bake()) is analogous to predict(), and gives you the processed data back.\n\n\n\n\nThe tidy() function can be used to get specific results from the recipe."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#example",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#example",
    "title": "1 - Feature Engineering",
    "section": "Example  ",
    "text": "Example  \n\nhash_rec_fit &lt;- prep(hash_rec)\n\n# Get the transformation coefficient\ntidy(hash_rec_fit, number = 1)\n\n# Get the processed data\nbake(hash_rec_fit, hotel_tr %&gt;% slice(1:3), contains(\"_agent_\"))"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-01-feature-engineering.html#more-on-recipes",
    "href": "archive/2023-07-nyr/advanced-01-feature-engineering.html#more-on-recipes",
    "title": "1 - Feature Engineering",
    "section": "More on recipes",
    "text": "More on recipes\n\nOnce fit() is called on a workflow, changing the model does not re-fit the recipe.\n\n\n\nA list of all known steps is at https://www.tidymodels.org/find/recipes/.\n\n\n\n\nSome steps can be skipped when using predict().\n\n\n\n\nThe order of the steps matters."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#previously---setup",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#previously---setup",
    "title": "3 - Grid Search via Racing",
    "section": "Previously - Setup ",
    "text": "Previously - Setup \n\n\n\nlibrary(tidymodels)\nlibrary(modeldatatoo)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\n\n\nset.seed(295)\nhotel_rates &lt;- \n  data_hotel_rates() %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date_num, -arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#previously---data-usage",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#previously---data-usage",
    "title": "3 - Grid Search via Racing",
    "section": "Previously - Data Usage ",
    "text": "Previously - Data Usage \n\nset.seed(4028)\nhotel_split &lt;-\n  initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_tr &lt;- training(hotel_split)\nhotel_te &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_tr, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#previously---boosting-model",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#previously---boosting-model",
    "title": "3 - Grid Search via Racing",
    "section": "Previously - Boosting Model    ",
    "text": "Previously - Boosting Model    \n\nhotel_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_tr) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) %&gt;%\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) %&gt;%\n  step_zv(all_predictors())\n\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune(), min_n = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\")\n\nlgbm_wflow &lt;- workflow(hotel_rec, lgbm_spec)\n\nlgbm_param &lt;-\n  lgbm_wflow %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#making-grid-search-more-efficient",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#making-grid-search-more-efficient",
    "title": "3 - Grid Search via Racing",
    "section": "Making Grid Search More Efficient",
    "text": "Making Grid Search More Efficient\nIn the last section, we evaluated 250 models (25 candidates times 10 resamples).\nWe can make this go faster using parallel processing.\nAlso, for some models, we can fit far fewer models than the number that are being evaluated.\n\nFor boosting, a model with X trees can often predict on candidates with less than X trees.\n\nBoth of these methods can lead to enormous speed-ups."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#model-racing",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#model-racing",
    "title": "3 - Grid Search via Racing",
    "section": "Model Racing",
    "text": "Model Racing\nRacing is an old tool that we can use to go even faster.\n\nEvaluate all of the candidate models but only for a few resamples.\nDetermine which candidates have a low probability of being selected.\nEliminate poor candidates.\nRepeat with next resample (until no more resamples remain)\n\nThis can result in fitting a small number of models."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#discarding-candidates",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#discarding-candidates",
    "title": "3 - Grid Search via Racing",
    "section": "Discarding Candidates",
    "text": "Discarding Candidates\nHow do we eliminate tuning parameter combinations?\nThere are a few methods to do so. Weâ€™ll use one based on analysis of variance (ANOVA).\nHoweverâ€¦ there is typically a large difference between resamples in the results."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#resampling-results-non-racing",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#resampling-results-non-racing",
    "title": "3 - Grid Search via Racing",
    "section": "Resampling Results (Non-Racing)",
    "text": "Resampling Results (Non-Racing)\n\n\nHere are some realistic (but simulated) examples of two candidate models.\nAn error estimate is measured for each of 10 resamples.\n\nThe lines connect resamples.\n\nThere is usually a significant resample-to-resample effect (rank corr: 0.83)."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#are-candidates-different",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#are-candidates-different",
    "title": "3 - Grid Search via Racing",
    "section": "Are Candidates Different?",
    "text": "Are Candidates Different?\nOne way to evaluate these models is to do a paired t-test\n\nor a t-test on their differences matched by resamples\n\nWith \\(n = 10\\) resamples, the confidence interval is (0.99, 2.8), indicating that candidate number 2 has smaller error.\nWhat if we were to compare each model candidate to the current best at each resample?\nOne shows superiority when 4 resamples have been evaluated."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#evaluating-differences-in-candidates",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#evaluating-differences-in-candidates",
    "title": "3 - Grid Search via Racing",
    "section": "Evaluating Differences in Candidates",
    "text": "Evaluating Differences in Candidates"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#interim-analysis-of-results",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#interim-analysis-of-results",
    "title": "3 - Grid Search via Racing",
    "section": "Interim Analysis of Results",
    "text": "Interim Analysis of Results\nOne version of racing uses a mixed model ANOVA to construct one-sided confidence intervals for each candidate versus the current best.\nAny candidates whose bound does not include zero are discarded. Here is an animation.\nThe resamples are analyzed in a random order.\n\nKuhn (2014) has examples and simulations to show that the method works.\nThe finetune package has functions tune_race_anova() and tune_race_win_loss()."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#racing",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#racing",
    "title": "3 - Grid Search via Racing",
    "section": "Racing     ",
    "text": "Racing     \n\n# Let's use a larger grid\nset.seed(8945)\nlgbm_grid &lt;- \n  lgbm_param %&gt;% \n  grid_latin_hypercube(size = 50)\n\nlibrary(finetune)\n\nset.seed(9)\nlgbm_race_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_race_anova(\n    resamples = hotel_rs,\n    grid = lgbm_grid, \n    metrics = reg_metrics\n  )\n\nThe syntax and helper functions are extremely similar to those shown for tune_grid()."
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#racing-results",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#racing-results",
    "title": "3 - Grid Search via Racing",
    "section": "Racing Results ",
    "text": "Racing Results \n\nshow_best(lgbm_race_res, metric = \"mae\")\n#&gt; # A tibble: 2 Ã— 11\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1  1014     5     0.0791           35            181 mae     standard    10.3    10   0.202 Preprocessor06_Model1\n#&gt; 2  1516     7     0.0421          176             12 mae     standard    10.4    10   0.200 Preprocessor42_Model1"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#racing-results-1",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#racing-results-1",
    "title": "3 - Grid Search via Racing",
    "section": "Racing Results ",
    "text": "Racing Results \n\n\nOnly 170 models were fit (out of 500).\nselect_best() never considers candidate models that did not get to the end of the race.\nThere is a helper function to see how candidate models were removed from consideration.\n\n\nplot_race(lgbm_race_res) + \n  scale_x_continuous(breaks = pretty_breaks())"
  },
  {
    "objectID": "archive/2023-07-nyr/advanced-03-racing.html#your-turn",
    "href": "archive/2023-07-nyr/advanced-03-racing.html#your-turn",
    "title": "3 - Grid Search via Racing",
    "section": "Your turn",
    "text": "Your turn\n\nRun tune_race_anova() with a different seed.\nDid you get the same or similar results?\n\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2023-07-nyr/annotations.html#section",
    "href": "archive/2023-07-nyr/annotations.html#section",
    "title": "Annotations",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€\nThis page contains annotations for selected slides.\nThereâ€™s a lot that we want to tell you. We donâ€™t want people to have to frantically scribble down things that we say that are not on the slides.\nWeâ€™ve added sections to this document with longer explanations and links to other resources."
  },
  {
    "objectID": "archive/2023-07-nyr/annotations.html#the-initial-split",
    "href": "archive/2023-07-nyr/annotations.html#the-initial-split",
    "title": "Annotations",
    "section": "The initial split",
    "text": "The initial split\nWhat does set.seed() do?\nWeâ€™ll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random).\nThink of PRN as a box that takes a starting value (the â€œseedâ€) that produces random numbers using that starting value as an input into its process.\nIf we know a seed value, we can reproduce our â€œrandomâ€ numbers. To use a different set of random numbers, choose a different seed value.\nFor example:\n\nset.seed(1)\nrunif(3)\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\n# Get a new set of random numbers:\nset.seed(2)\nrunif(3)\n#&gt; [1] 0.1848823 0.7023740 0.5733263\n\n# We can reproduce the old ones with the same seed\nset.seed(1)\nrunif(3)\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\nIf we donâ€™t set the seed, R uses the clock time and the process ID to create a seed. This isnâ€™t reproducible.\nSince we want our code to be reproducible, we set the seeds before random numbers are used.\nIn theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same and we donâ€™t get reproducible results.\nThe value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to â€œspread the randomness aroundâ€. It is basically:\n\ncat(paste0(\"set.seed(\", sample.int(10000, 5), \")\", collapse = \"\\n\"))\n#&gt; set.seed(9725)\n#&gt; set.seed(8462)\n#&gt; set.seed(4050)\n#&gt; set.seed(8789)\n#&gt; set.seed(1301)"
  },
  {
    "objectID": "archive/2023-07-nyr/annotations.html#what-is-wrong-with-this",
    "href": "archive/2023-07-nyr/annotations.html#what-is-wrong-with-this",
    "title": "Annotations",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?\nIf we treat the preprocessing as a separate task, it raises the risk that we might accidentally overfit to the data at hand.\nFor example, someone might estimate something from the entire data set (such as the principle components) and treat that data as if it were known (and not estimated). Depending on the what was done with the data, consequences in doing that could be:\n\nYour performance metrics are slightly-to-moderately optimistic (e.g.Â you might think your accuracy is 85% when it is actually 75%)\nA consequential component of the analysis is not right and the model just doesnâ€™t work.\n\nThe big issue here is that you wonâ€™t be able to figure this out until you get a new piece of data, such as the test set.\nA really good example of this is in â€˜Selection bias in gene extraction on the basis of microarray gene-expression dataâ€™. The authors re-analyze a previous publication and show that the original researchers did not include feature selection in the workflow. Because of that, their performance statistics were extremely optimistic. In one case, they could do the original analysis on complete noise and still achieve zero errors.\nGenerally speaking, this problem is referred to as data leakage. Some other references:\n\nOverfitting to Predictors and External Validation\nAre We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning\nNavigating the pitfalls of applying machine learning in genomics\nA review of feature selection techniques in bioinformatics\nOn Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation"
  },
  {
    "objectID": "archive/2023-07-nyr/annotations.html#where-are-the-fitted-models",
    "href": "archive/2023-07-nyr/annotations.html#where-are-the-fitted-models",
    "title": "Annotations",
    "section": "Where are the fitted models?",
    "text": "Where are the fitted models?\nThe primary purpose of resampling is to estimate model performance. The models are almost never needed again.\nAlso, if the data set is large, the model object may require a lot of memory to save so, by default, we donâ€™t keep them.\nFor more advanced use cases, you can extract and save them. See:\n\nhttps://www.tmwr.org/resampling.html#extract\nhttps://www.tidymodels.org/learn/models/coefficients/ (an example)"
  },
  {
    "objectID": "archive/2023-07-nyr/annotations.html#validation-set",
    "href": "archive/2023-07-nyr/annotations.html#validation-set",
    "title": "Annotations",
    "section": "Validation set",
    "text": "Validation set\nThe upcoming version of the rsample package (1.2.0) will have a new set of functions specific to validation sets. They will allow you to make an initial three-way split and still use a validation set with the tune package."
  },
  {
    "objectID": "archive/2023-07-nyr/annotations.html#update-parameter-ranges",
    "href": "archive/2023-07-nyr/annotations.html#update-parameter-ranges",
    "title": "Annotations",
    "section": "Update parameter ranges",
    "text": "Update parameter ranges\nIn about 90% of the cases, the dials function that you use to update the parameter range has the same name as the argument. For example, if you were to update the mtry parameter in a random forests model, the code would look like\n\nparameter_object %&gt;% \n  update(mtry = mtry(c(1, 100)))\n\nThere are some cases where the parameter function, or its associated values, are different from the argument name.\nFor example, with step_spline_naturall(), we might want to tune the deg_free argument (for the degrees of freedom of a spline function. ). In this case, the argument name is deg_free but we update it with spline_degree().\ndeg_free represents the general concept of degrees of freedom and could be associated with many different things. For example, if we ever had an argument that was the number of degrees of freedom for a \\(t\\) distribution, we would call that argument deg_free.\nFor splines, we probably want a wider range for the degrees of freedom. We made a specialized function called spline_degree() to be used in these cases.\nHow can you tell when this happens? There is a helper function called tunable() and that gives information on how we make the default ranges for parameters. There is a column in these objects names call_info:\n\nlibrary(tidymodels)\nns_tunable &lt;- \n  recipe(mpg ~ ., data = mtcars) %&gt;% \n  step_spline_natural(dis, deg_free = tune()) %&gt;% \n  tunable()\n\nns_tunable\n#&gt; # A tibble: 1 Ã— 5\n#&gt;   name     call_info        source component           component_id        \n#&gt;   &lt;chr&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;chr&gt;               &lt;chr&gt;               \n#&gt; 1 deg_free &lt;named list [3]&gt; recipe step_spline_natural spline_natural_P1Tjg\nns_tunable$call_info\n#&gt; [[1]]\n#&gt; [[1]]$pkg\n#&gt; [1] \"dials\"\n#&gt; \n#&gt; [[1]]$fun\n#&gt; [1] \"spline_degree\"\n#&gt; \n#&gt; [[1]]$range\n#&gt; [1]  2 15"
  },
  {
    "objectID": "archive/2023-07-nyr/annotations.html#early-stopping-for-boosted-trees",
    "href": "archive/2023-07-nyr/annotations.html#early-stopping-for-boosted-trees",
    "title": "Annotations",
    "section": "Early stopping for boosted trees",
    "text": "Early stopping for boosted trees\nWhen deciding on the number of boosting iterations, there are two main strategies:\n\nDirectly tune it (trees = tune())\nSet it to one value and tune the number of early stopping iterations (trees = 500, stop_iter = tune()).\n\nEarly stopping is when we monitor the performance of the model. If the model doesnâ€™t make any improvements for stop_iter iterations, training stops.\nHereâ€™s an example where, after eleven iterations, performance starts to get worse.\n\n\n\n\n\n\n\n\n\nThis is likely due to over-fitting so we stop the model at eleven boosting iterations.\nEarly stopping usually has good results and takes far less time.\nWe could an engine argument called validation here. Thatâ€™s not an argument to any function in the lightgbm package.\nbonsai has its own wrapper around (lightgbm::lgb.train()) called bonsai::train_lightgbm(). We use that here and it has a validation argument.\nHow would you know that? There are a few different ways:\n\nLook at the documentation in ?boost_tree and click on the lightgbm entry in the engine list.\nCheck out the pkgdown reference website https://parsnip.tidymodels.org/reference/index.html\nRun the translate() function on the parsnip specification object.\n\nThe first two options are best since they tell you a lot more about the particularities of each model engine (there are a lot for lightgbm)."
  },
  {
    "objectID": "archive/2023-07-nyr/annotations.html#per-agent-statistics",
    "href": "archive/2023-07-nyr/annotations.html#per-agent-statistics",
    "title": "Annotations",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics\nThe effect encoding method essentially takes the effect of a variable, like agent, and makes a data column for that effect. In our example, affect of the agent on the ADR is quantified by a model and then added as a data column to be used in the model.\nSuppose agent Max has a single reservation in the data and it had an ADR of â‚¬200. If we used a naive estimate for Maxâ€™s effect, the model is being told that Max should always produce an effect of â‚¬200. Thatâ€™s a very poor estimate since it is from a single data point.\nContrast this with seasoned agent Davis, who has taken 250 reservations with an average ADR of â‚¬100. Davisâ€™s mean is more predictive because it is estimated with better data (i.e., more total reservations). Partial pooling leverages the entire data set and can borrow strength from all of the agents. It is a common tool in Bayesian estimation and non-Bayesian mixed models. If a agentâ€™s data is of good quality, the partial pooling effect estimate is closer to the raw mean. Maxâ€™s data is not great and is â€œshrunkâ€ towards the center of the overall average. Since there is so little known about Maxâ€™s reservation history, this is a better effect estimate (until more data is available for him).\nThe Stan documentation has a pretty good vignette on this: https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html\nAlso, Bayes Rules! has a nice section on this: https://www.bayesrulesbook.com/chapter-15.html\nSince this example has a numeric outcome, partial pooling is very similar to the Jamesâ€“Stein estimator: https://en.wikipedia.org/wiki/Jamesâ€“Stein_estimator"
  },
  {
    "objectID": "archive/2023-07-nyr/annotations.html#agent-effects",
    "href": "archive/2023-07-nyr/annotations.html#agent-effects",
    "title": "Annotations",
    "section": "Agent effects",
    "text": "Agent effects\nEffect encoding might result in a somewhat circular argument: the column is more likely to be important to the model since it is the output of a separate model. The risk here is that we might over-fit the effect to the data. For this reason, it is super important to make sure that we verify that we arenâ€™t overfitting by checking with resampling (or a validation set).\nPartial pooling somewhat lowers the risk of overfitting since it tends to correct for agents with small sample sizes. It canâ€™t correct for improper data usage or data leakage though."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#chicago-l-train-data",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#chicago-l-train-data",
    "title": "Case Study on Transportation",
    "section": "Chicago L-Train data",
    "text": "Chicago L-Train data\nSeveral years worth of pre-pandemic data were assembled to try to predict the daily number of people entering the Clark and Lake elevated (â€œLâ€) train station in Chicago.\nMore information:\n\nSeveral Chapters in Feature Engineering and Selection.\n\nStart with Section 4.1\nSee Section 1.3\n\nVideo: The Global Pandemic Ruined My Favorite Data Set"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#predictors",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#predictors",
    "title": "Case Study on Transportation",
    "section": "Predictors",
    "text": "Predictors\n\nthe 14-day lagged ridership at this and other stations (units: thousands of rides/day)\nweather data\nhome/away game schedules for Chicago teams\nthe date\n\nThe data are in modeldata. See ?Chicago."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#l-train-locations",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#l-train-locations",
    "title": "Case Study on Transportation",
    "section": "L Train Locations",
    "text": "L Train Locations"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#your-turn-explore-the-data",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#your-turn-explore-the-data",
    "title": "Case Study on Transportation",
    "section": "Your turn: Explore the Data",
    "text": "Your turn: Explore the Data\nTake a look at these data for a few minutes and see if you can find any interesting characteristics in the predictors or the outcome.\n\nlibrary(tidymodels)\nlibrary(rules)\ndata(\"Chicago\")\ndim(Chicago)\n#&gt; [1] 5698   50\nstations\n#&gt;  [1] \"Austin\"           \"Quincy_Wells\"     \"Belmont\"          \"Archer_35th\"     \n#&gt;  [5] \"Oak_Park\"         \"Western\"          \"Clark_Lake\"       \"Clinton\"         \n#&gt;  [9] \"Merchandise_Mart\" \"Irving_Park\"      \"Washington_Wells\" \"Harlem\"          \n#&gt; [13] \"Monroe\"           \"Polk\"             \"Ashland\"          \"Kedzie\"          \n#&gt; [17] \"Addison\"          \"Jefferson_Park\"   \"Montrose\"         \"California\"\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#splitting-with-chicago-data",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#splitting-with-chicago-data",
    "title": "Case Study on Transportation",
    "section": "Splitting with Chicago data ",
    "text": "Splitting with Chicago data \nLetâ€™s put the last two weeks of data into the test set. initial_time_split() can be used for this purpose:\n\ndata(Chicago)\n\nchi_split &lt;- initial_time_split(Chicago, prop = 1 - (14/nrow(Chicago)))\nchi_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;5684/14/5698&gt;\n\nchi_train &lt;- training(chi_split)\nchi_test  &lt;- testing(chi_split)\n\n## training\nnrow(chi_train)\n#&gt; [1] 5684\n \n## testing\nnrow(chi_test)\n#&gt; [1] 14"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#time-series-resampling",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#time-series-resampling",
    "title": "Case Study on Transportation",
    "section": "Time series resampling",
    "text": "Time series resampling\nOur Chicago data is over time. Regular cross-validation, which uses random sampling, may not be the best idea.\nWe can emulate our training/test split by making similar resamples.\n\nFold 1: Take the first X years of data as the analysis set, the next 2 weeks as the assessment set.\nFold 2: Take the first X years + 2 weeks of data as the analysis set, the next 2 weeks as the assessment set.\nand so on"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#rolling-forecast-origin-resampling",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#rolling-forecast-origin-resampling",
    "title": "Case Study on Transportation",
    "section": "Rolling forecast origin resampling",
    "text": "Rolling forecast origin resampling\n\n\nThis image shows overlapping assessment sets. We will use non-overlapping data but it could be done wither way."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#times-series-resampling",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#times-series-resampling",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n\n\n\n\n  )\n\nUse the date column to find the date data."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#times-series-resampling-1",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#times-series-resampling-1",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n\n\n\n  )\n\nOur units will be weeks."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#times-series-resampling-2",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#times-series-resampling-2",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15  \n    \n    \n  )\n\nEvery analysis set has 15 years of data"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#times-series-resampling-3",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#times-series-resampling-3",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n\n  )\n\nEvery assessment set has 2 weeks of data"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#times-series-resampling-4",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#times-series-resampling-4",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n    step = 2 \n  )\n\nIncrement by 2 weeks so that there are no overlapping assessment sets.\n\nchi_rs$splits[[1]] %&gt;% assessment() %&gt;% pluck(\"date\") %&gt;% range()\n#&gt; [1] \"2016-01-07\" \"2016-01-20\"\nchi_rs$splits[[2]] %&gt;% assessment() %&gt;% pluck(\"date\") %&gt;% range()\n#&gt; [1] \"2016-01-21\" \"2016-02-03\""
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#our-resampling-object",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#our-resampling-object",
    "title": "Case Study on Transportation",
    "section": "Our resampling object ",
    "text": "Our resampling object \n\n\n\nchi_rs\n#&gt; # Sliding period resampling \n#&gt; # A tibble: 16 Ã— 2\n#&gt;    splits            id     \n#&gt;    &lt;list&gt;            &lt;chr&gt;  \n#&gt;  1 &lt;split [5463/14]&gt; Slice01\n#&gt;  2 &lt;split [5467/14]&gt; Slice02\n#&gt;  3 &lt;split [5467/14]&gt; Slice03\n#&gt;  4 &lt;split [5467/14]&gt; Slice04\n#&gt;  5 &lt;split [5467/14]&gt; Slice05\n#&gt;  6 &lt;split [5467/14]&gt; Slice06\n#&gt;  7 &lt;split [5467/14]&gt; Slice07\n#&gt;  8 &lt;split [5467/14]&gt; Slice08\n#&gt;  9 &lt;split [5467/14]&gt; Slice09\n#&gt; 10 &lt;split [5467/14]&gt; Slice10\n#&gt; 11 &lt;split [5467/14]&gt; Slice11\n#&gt; 12 &lt;split [5467/14]&gt; Slice12\n#&gt; 13 &lt;split [5467/14]&gt; Slice13\n#&gt; 14 &lt;split [5467/14]&gt; Slice14\n#&gt; 15 &lt;split [5467/14]&gt; Slice15\n#&gt; 16 &lt;split [5467/11]&gt; Slice16\n\n\n\n\nWe will fit 16 models on 16 slightly different analysis sets.\nEach will produce a separate performance metrics.\nWe will average the 16 metrics to get the resampling estimate of that statistic."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#feature-engineering-with-recipes",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#feature-engineering-with-recipes",
    "title": "Case Study on Transportation",
    "section": "Feature engineering with recipes ",
    "text": "Feature engineering with recipes \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train)\n\nBased on the formula, the function assigns columns to roles of â€œoutcomeâ€ or â€œpredictorâ€"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#a-recipe",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#a-recipe",
    "title": "Case Study on Transportation",
    "section": "A recipe",
    "text": "A recipe\n\nsummary(chi_rec)\n#&gt; # A tibble: 50 Ã— 4\n#&gt;    variable         type      role      source  \n#&gt;    &lt;chr&gt;            &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 Austin           &lt;chr [2]&gt; predictor original\n#&gt;  2 Quincy_Wells     &lt;chr [2]&gt; predictor original\n#&gt;  3 Belmont          &lt;chr [2]&gt; predictor original\n#&gt;  4 Archer_35th      &lt;chr [2]&gt; predictor original\n#&gt;  5 Oak_Park         &lt;chr [2]&gt; predictor original\n#&gt;  6 Western          &lt;chr [2]&gt; predictor original\n#&gt;  7 Clark_Lake       &lt;chr [2]&gt; predictor original\n#&gt;  8 Clinton          &lt;chr [2]&gt; predictor original\n#&gt;  9 Merchandise_Mart &lt;chr [2]&gt; predictor original\n#&gt; 10 Irving_Park      &lt;chr [2]&gt; predictor original\n#&gt; # â„¹ 40 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#a-recipe---work-with-dates",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#a-recipe---work-with-dates",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) \n\nThis creates three new columns in the data based on the date. Note that the day-of-the-week column is a factor."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#a-recipe---work-with-dates-1",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#a-recipe---work-with-dates-1",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %&gt;% \n  step_holiday(date) \n\nAdd indicators for major holidays. Specific holidays, especially those non-USA, can also be generated.\nAt this point, we donâ€™t need date anymore. Instead of deleting it (there is a step for that) we will change its role to be an identification variable.\n\nWe might want to change the role (instead of removing the column) because it will stay in the data set (even when resampled) and might be useful for diagnosing issues."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#a-recipe---work-with-dates-2",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#a-recipe---work-with-dates-2",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %&gt;% \n  step_holiday(date) %&gt;% \n  update_role(date, new_role = \"id\") %&gt;%\n  update_role_requirements(role = \"id\", bake = TRUE)\n\ndate is still in the data set but tidymodels knows not to treat it as an analysis column.\nupdate_role_requirements() is needed to make sure that this column is required when making new data points."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#a-recipe---remove-constant-columns",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#a-recipe---remove-constant-columns",
    "title": "Case Study on Transportation",
    "section": "A recipe - remove constant columns ",
    "text": "A recipe - remove constant columns \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %&gt;% \n  step_holiday(date) %&gt;% \n  update_role(date, new_role = \"id\") %&gt;%\n  update_role_requirements(role = \"id\", bake = TRUE) %&gt;% \n  step_zv(all_nominal_predictors())"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#a-recipe---handle-correlations",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#a-recipe---handle-correlations",
    "title": "Case Study on Transportation",
    "section": "A recipe - handle correlations ",
    "text": "A recipe - handle correlations \nThe station columns have a very high degree of correlation.\nWe might want to decorrelated them with principle component analysis to help the model fits go more easily.\nThe vector stations contains all station names and can be used to identify all the relevant columns.\n\nchi_pca_rec &lt;- \n  chi_rec %&gt;% \n  step_normalize(all_of(!!stations)) %&gt;% \n  step_pca(all_of(!!stations), num_comp = tune())\n\nWeâ€™ll tune the number of PCA components for (default) values of one to four."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#make-some-models",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#make-some-models",
    "title": "Case Study on Transportation",
    "section": "Make some models     ",
    "text": "Make some models     \nLetâ€™s try three models. The first one requires the rules package (loaded earlier).\n\ncb_spec &lt;- cubist_rules(committees = 25, neighbors = tune())\nmars_spec &lt;- mars(prod_degree = tune()) %&gt;% set_mode(\"regression\")\nlm_spec &lt;- linear_reg()\n\nchi_set &lt;- \n  workflow_set(\n    list(pca = chi_pca_rec, basic = chi_rec), \n    list(cubist = cb_spec, mars = mars_spec, lm = lm_spec)\n  ) %&gt;% \n  # Evaluate models using mean absolute errors\n  option_add(metrics = metric_set(mae))\n\n\nBriefly talk about Cubist being a (sort of) boosted rule-based model and MARS being a nonlinear regression model. Both incorporate feature selection nicely."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#process-them-on-the-resamples",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#process-them-on-the-resamples",
    "title": "Case Study on Transportation",
    "section": "Process them on the resamples",
    "text": "Process them on the resamples\n\n# Set up some objects for stacking ensembles (in a few slides)\ngrid_ctrl &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\nchi_res &lt;- \n  chi_set %&gt;% \n  workflow_map(\n    resamples = chi_rs,\n    grid = 10,\n    control = grid_ctrl,\n    verbose = TRUE,\n    seed = 12\n  )"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#how-do-the-results-look",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#how-do-the-results-look",
    "title": "Case Study on Transportation",
    "section": "How do the results look?",
    "text": "How do the results look?\n\nrank_results(chi_res)\n#&gt; # A tibble: 31 Ã— 9\n#&gt;    wflow_id     .config              .metric  mean std_err     n preprocessor model   rank\n#&gt;    &lt;chr&gt;        &lt;chr&gt;                &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;  &lt;int&gt;\n#&gt;  1 pca_cubist   Preprocessor1_Model1 mae     0.798   0.104    16 recipe       cubisâ€¦     1\n#&gt;  2 pca_cubist   Preprocessor3_Model3 mae     0.978   0.110    16 recipe       cubisâ€¦     2\n#&gt;  3 pca_cubist   Preprocessor4_Model2 mae     0.983   0.122    16 recipe       cubisâ€¦     3\n#&gt;  4 pca_cubist   Preprocessor4_Model1 mae     0.991   0.127    16 recipe       cubisâ€¦     4\n#&gt;  5 pca_cubist   Preprocessor3_Model2 mae     0.991   0.113    16 recipe       cubisâ€¦     5\n#&gt;  6 pca_cubist   Preprocessor2_Model2 mae     1.02    0.118    16 recipe       cubisâ€¦     6\n#&gt;  7 pca_cubist   Preprocessor1_Model3 mae     1.05    0.134    16 recipe       cubisâ€¦     7\n#&gt;  8 basic_cubist Preprocessor1_Model8 mae     1.07    0.115    16 recipe       cubisâ€¦     8\n#&gt;  9 basic_cubist Preprocessor1_Model7 mae     1.07    0.112    16 recipe       cubisâ€¦     9\n#&gt; 10 basic_cubist Preprocessor1_Model6 mae     1.07    0.114    16 recipe       cubisâ€¦    10\n#&gt; # â„¹ 21 more rows"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#plot-the-results",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#plot-the-results",
    "title": "Case Study on Transportation",
    "section": "Plot the results  ",
    "text": "Plot the results  \n\nautoplot(chi_res)"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#pull-out-specific-results",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#pull-out-specific-results",
    "title": "Case Study on Transportation",
    "section": "Pull out specific results  ",
    "text": "Pull out specific results  \nWe can also pull out the specific tuning results and look at them:\n\nchi_res %&gt;% \n  extract_workflow_set_result(\"pca_cubist\") %&gt;% \n  autoplot()"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#why-choose-just-one-final_fit",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#why-choose-just-one-final_fit",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit? \nModel stacks generate predictions that are informed by several models."
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#why-choose-just-one-final_fit-1",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#why-choose-just-one-final_fit-1",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#why-choose-just-one-final_fit-2",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#why-choose-just-one-final_fit-2",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#why-choose-just-one-final_fit-3",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#why-choose-just-one-final_fit-3",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#why-choose-just-one-final_fit-4",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#why-choose-just-one-final_fit-4",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#why-choose-just-one-final_fit-5",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#why-choose-just-one-final_fit-5",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#building-a-model-stack",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#building-a-model-stack",
    "title": "Case Study on Transportation",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlibrary(stacks)\n\n\nDefine candidate members\nInitialize a data stack object\nAdd candidate ensemble members to the data stack\nEvaluate how to combine their predictions\nFit candidate ensemble members with non-zero stacking coefficients\nPredict on new data!"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#start-the-stack-and-add-members",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#start-the-stack-and-add-members",
    "title": "Case Study on Transportation",
    "section": "Start the stack and add members ",
    "text": "Start the stack and add members \nCollect all of the resampling results for all model configurations.\n\nchi_stack &lt;- \n  stacks() %&gt;% \n  add_candidates(chi_res)"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#estimate-weights-for-each-candidate",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#estimate-weights-for-each-candidate",
    "title": "Case Study on Transportation",
    "section": "Estimate weights for each candidate ",
    "text": "Estimate weights for each candidate \nWhich configurations should be retained? Uses a penalized linear model:\n\nset.seed(122)\nchi_stack_res &lt;- blend_predictions(chi_stack)\n\nchi_stack_res\n#&gt; # A tibble: 5 Ã— 3\n#&gt;   member           type         weight\n#&gt;   &lt;chr&gt;            &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 pca_cubist_1_1   cubist_rules  0.343\n#&gt; 2 pca_cubist_3_2   cubist_rules  0.236\n#&gt; 3 basic_cubist_1_4 cubist_rules  0.189\n#&gt; 4 pca_lm_4_1       linear_reg    0.163\n#&gt; 5 pca_cubist_3_3   cubist_rules  0.109"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#how-did-it-do",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#how-did-it-do",
    "title": "Case Study on Transportation",
    "section": "How did it do?  ",
    "text": "How did it do?  \nThe overall results of the penalized model:\n\nautoplot(chi_stack_res)"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#what-does-it-use",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#what-does-it-use",
    "title": "Case Study on Transportation",
    "section": "What does it use?  ",
    "text": "What does it use?  \n\nautoplot(chi_stack_res, type = \"weights\")"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#fit-the-required-candidate-models",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#fit-the-required-candidate-models",
    "title": "Case Study on Transportation",
    "section": "Fit the required candidate models",
    "text": "Fit the required candidate models\nFor each model we retain in the stack, we need their model fit on the entire training set.\n\nchi_stack_res &lt;- fit_members(chi_stack_res)"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#the-test-set-best-cubist-model",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#the-test-set-best-cubist-model",
    "title": "Case Study on Transportation",
    "section": "The test set: best Cubist model  ",
    "text": "The test set: best Cubist model  \nWe can pull out the results and the workflow to fit the single best cubist model.\n\nbest_cubist &lt;- \n  chi_res %&gt;% \n  extract_workflow_set_result(\"pca_cubist\") %&gt;% \n  select_best()\n\ncubist_res &lt;- \n  chi_res %&gt;% \n  extract_workflow(\"pca_cubist\") %&gt;% \n  finalize_workflow(best_cubist) %&gt;% \n  last_fit(split = chi_split, metrics = metric_set(mae))"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#the-test-set-stack-ensemble",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#the-test-set-stack-ensemble",
    "title": "Case Study on Transportation",
    "section": "The test set: stack ensemble",
    "text": "The test set: stack ensemble\nWe donâ€™t have last_fit() for stacks (yet) so we manually make predictions.\n\nstack_pred &lt;- \n  predict(chi_stack_res, chi_test) %&gt;% \n  bind_cols(chi_test)"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#compare-the-results",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#compare-the-results",
    "title": "Case Study on Transportation",
    "section": "Compare the results  ",
    "text": "Compare the results  \nSingle best versus the stack:\n\ncollect_metrics(cubist_res)\n#&gt; # A tibble: 1 Ã— 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard       0.670 Preprocessor1_Model1\n\nstack_pred %&gt;% mae(ridership, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       0.689"
  },
  {
    "objectID": "archive/2023-07-nyr/extras-transit-case-study.html#plot-the-test-set",
    "href": "archive/2023-07-nyr/extras-transit-case-study.html#plot-the-test-set",
    "title": "Case Study on Transportation",
    "section": "Plot the test set  ",
    "text": "Plot the test set  \n\n\nlibrary(probably)\ncubist_res %&gt;% \n  collect_predictions() %&gt;% \n  ggplot(aes(ridership, .pred)) + \n  geom_point(alpha = 1 / 2) + \n  geom_abline(lty = 2, col = \"green\") + \n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#workshop-policies",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#workshop-policies",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease do not photograph people wearing red lanyards\nThere are gender-neutral bathrooms located are among the Grand Suite Bathrooms\nThere are two meditation/prayer rooms: Grand Suite 2A and 2B\nA lactation room is located in Grand Suite 1\nThe meditation/prayer and lactation rooms are open\nSun - Tue 7:30am - 7:00pm, Wed 8:00am - 6:00pm"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#workshop-policies-1",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#workshop-policies-1",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease review the code of conduct and COVID policies, which apply to all workshops: https://posit.co/code-of-conduct/.\nCoC site has info on how to report a problem (in person, email, phone)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#who-are-you",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %&gt;% or base R |&gt; pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have exposure to basic statistical concepts\nYou do not need intermediate or expert familiarity with modeling or ML\nYou have used some tidymodels packages\nYou have some experience with evaluating statistical models using resampling techniques"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#who-are-tidymodels",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#who-are-tidymodels",
    "title": "1 - Introduction",
    "section": "Who are tidymodels?",
    "text": "Who are tidymodels?\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\nIjeamaka Anyene (Day 1) and Edgar Ruiz (Day 2) are TAing!\n\n\nMany thanks to Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill, and DesirÃ©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#asking-for-help",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#asking-for-help",
    "title": "1 - Introduction",
    "section": "Asking for help",
    "text": "Asking for help\n\nğŸŸª â€œIâ€™m stuck and need help!â€\n\n\nğŸŸ© â€œI finished the exerciseâ€"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#section-2",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#section-2",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#tentative-plan-for-this-workshop",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#tentative-plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Tentative plan for this workshop",
    "text": "Tentative plan for this workshop\n\nFeature engineering with recipes\nModel optimization by tuning\n\nGrid search\nRacing\nIterative methods\n\nExtras (time permitting)\n\nEffect encodings\nA case study"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#section-3",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#section-3",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors ğŸ‘‹\n\n Log in to Posit Cloud (free):\nCheck the workshop channel on Discord for the link!"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#lets-install-some-packages",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Letâ€™s install some packages",
    "text": "Letâ€™s install some packages\nIf you are using your own laptop instead of RStudio Cloud:\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"bonsai\", \"doParallel\", \"embed\", \"finetune\", \"lightgbm\", \"lme4\",\n    \"plumber\", \"probably\", \"ranger\", \"rpart\", \"rpart.plot\", \"rules\",\n    \"splines2\", \"stacks\", \"text2vec\", \"textrecipes\", \"tidymodels\", \n    \"vetiver\", \"remotes\")\n\ninstall.packages(pkgs)\n\n\n\n Or log in to Posit Cloud\nLink in our Discord channel!"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#hotel-data",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#hotel-data",
    "title": "1 - Introduction",
    "section": "Hotel Data  ",
    "text": "Hotel Data  \nWeâ€™ll use data on hotels to predict the cost of a room.\nThe data are in the modeldata package. Weâ€™ll sample down the data and refactor some columns:\n\n\n\nlibrary(tidymodels)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#hotel-date-columns",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#hotel-date-columns",
    "title": "1 - Introduction",
    "section": "Hotel date columns",
    "text": "Hotel date columns\n\nnames(hotel_rates)\n#&gt;  [1] \"avg_price_per_room\"             \"lead_time\"                     \n#&gt;  [3] \"stays_in_weekend_nights\"        \"stays_in_week_nights\"          \n#&gt;  [5] \"adults\"                         \"children\"                      \n#&gt;  [7] \"babies\"                         \"meal\"                          \n#&gt;  [9] \"country\"                        \"market_segment\"                \n#&gt; [11] \"distribution_channel\"           \"is_repeated_guest\"             \n#&gt; [13] \"previous_cancellations\"         \"previous_bookings_not_canceled\"\n#&gt; [15] \"reserved_room_type\"             \"assigned_room_type\"            \n#&gt; [17] \"booking_changes\"                \"agent\"                         \n#&gt; [19] \"company\"                        \"days_in_waiting_list\"          \n#&gt; [21] \"customer_type\"                  \"required_car_parking_spaces\"   \n#&gt; [23] \"total_of_special_requests\"      \"arrival_date_num\"              \n#&gt; [25] \"near_christmas\"                 \"near_new_years\"                \n#&gt; [27] \"historical_adr\""
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#data-splitting-strategy",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#data-splitting-strategy",
    "title": "1 - Introduction",
    "section": "Data splitting strategy",
    "text": "Data splitting strategy"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#data-spending",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#data-spending",
    "title": "1 - Introduction",
    "section": "Data Spending ",
    "text": "Data Spending \nLetâ€™s split the data into a training set (75%) and testing set (25%) using stratification:\n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#your-turn",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\nLetâ€™s take some time and investigate the training data. The outcome is avg_price_per_room.\nAre there any interesting characteristics of the data?\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-01-introduction.html#our-versions",
    "href": "archive/2023-09-posit-conf/advanced-01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\nR version 4.2.2 (2022-10-31), Quarto (1.4.104)\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nbonsai\n0.2.1\n\n\nbroom\n1.0.5\n\n\ndials\n1.2.0\n\n\ndoParallel\n1.0.17\n\n\ndplyr\n1.1.3\n\n\nembed\n1.1.2\n\n\nfinetune\n1.1.0\n\n\nggplot2\n3.4.3\n\n\nlightgbm\n3.3.5\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nlme4\n1.1-34\n\n\nmodeldata\n1.2.0\n\n\nparsnip\n1.1.1\n\n\nplumber\n1.2.1\n\n\nprobably\n1.0.2\n\n\npurrr\n1.0.2\n\n\nranger\n0.15.1\n\n\nrecipes\n1.0.8\n\n\nremotes\n2.4.2.1\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nrpart\n4.1.19\n\n\nrpart.plot\n3.1.1\n\n\nrsample\n1.2.0\n\n\nrules\n1.0.2\n\n\nscales\n1.2.1\n\n\nsplines2\n0.5.1\n\n\nstacks\n1.0.2\n\n\ntext2vec\n0.6.3\n\n\ntextrecipes\n1.0.4\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\ntibble\n3.2.1\n\n\ntidymodels\n1.1.1\n\n\ntidyr\n1.3.0\n\n\ntune\n1.1.2\n\n\nvetiver\n0.2.4\n\n\nworkflows\n1.1.3\n\n\nworkflowsets\n1.0.1\n\n\nyardstick\n1.2.0"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#previously---setup",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#previously---setup",
    "title": "2 - Tuning Hyperparameters",
    "section": "Previously - Setup ",
    "text": "Previously - Setup \n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#previously---data-usage",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#previously---data-usage",
    "title": "2 - Tuning Hyperparameters",
    "section": "Previously - Data Usage ",
    "text": "Previously - Data Usage \n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#previously---feature-engineering",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#previously---feature-engineering",
    "title": "2 - Tuning Hyperparameters",
    "section": "Previously - Feature engineering  ",
    "text": "Previously - Feature engineering  \n\nlibrary(textrecipes)\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  # Defaults to 32 signed indicator columns\n  step_dummy_hash(agent) %&gt;%\n  step_dummy_hash(company) %&gt;%\n  # Regular indicators for the others\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#tuning-parameters",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#tuning-parameters",
    "title": "2 - Tuning Hyperparameters",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#optimize-tuning-parameters",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#optimize-tuning-parameters",
    "title": "2 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#tagging-parameters-for-tuning",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#tagging-parameters-for-tuning",
    "title": "2 - Tuning Hyperparameters",
    "section": "Tagging parameters for tuning ",
    "text": "Tagging parameters for tuning \nWith tidymodels, you can mark the parameters that you want to optimize with a value of tune().\n\nThe function itself just returnsâ€¦ itself:\n\ntune()\n#&gt; tune()\nstr(tune())\n#&gt;  language tune()\n\n# optionally add a label\ntune(\"I hope that the workshop is going well\")\n#&gt; tune(\"I hope that the workshop is going well\")\n\n\nFor exampleâ€¦"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#optimizing-the-hash-features",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#optimizing-the-hash-features",
    "title": "2 - Tuning Hyperparameters",
    "section": "Optimizing the hash features   ",
    "text": "Optimizing the hash features   \nOur new recipe is:\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) %&gt;%\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) %&gt;%\n  step_zv(all_predictors())\n\n\nWe will be using a tree-based model in a minute.\n\nThe other categorical predictors are left as-is.\nThatâ€™s why there is no step_dummy()."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-trees",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-trees",
    "title": "2 - Tuning Hyperparameters",
    "section": "Boosted Trees",
    "text": "Boosted Trees\nThese are popular ensemble methods that build a sequence of tree models.\n\nEach tree uses the results of the previous tree to better predict samples, especially those that have been poorly predicted.\n\nEach tree in the ensemble is saved and new samples are predicted using a weighted average of the votes of each tree in the ensemble.\n\nWeâ€™ll focus on the popular lightgbm implementation."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "title": "2 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\nSome possible parameters:\n\nmtry: The number of predictors randomly sampled at each split (in \\([1, ncol(x)]\\) or \\((0, 1]\\)).\ntrees: The number of trees (\\([1, \\infty]\\), but usually up to thousands)\nmin_n: The number of samples needed to further split (\\([1, n]\\)).\nlearn_rate: The rate that each tree adapts from previous iterations (\\((0, \\infty]\\), usual maximum is 0.1).\nstop_iter: The number of iterations of boosting where no improvement was shown before stopping (\\([1, trees]\\))"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters-1",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\nTBH it is usually not difficult to optimize these models.\n\nOften, there are multiple candidate tuning parameter combinations that have very good results.\n\nTo demonstrate simple concepts, weâ€™ll look at optimizing the number of trees in the ensemble (between 1 and 100) and the learning rate (\\(10^{-5}\\) to \\(10^{-1}\\))."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters-2",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters-2",
    "title": "2 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters   ",
    "text": "Boosted Tree Tuning Parameters   \nWeâ€™ll need to load the bonsai package. This has the information needed to use lightgbm\n\nlibrary(bonsai)\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow &lt;- workflow(hash_rec, lgbm_spec)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search ğŸ’  which tests a pre-defined set of candidate values\nIterative search ğŸŒ€ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search",
    "title": "2 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nA small grid of points trying to minimize the error via learning rate:"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search-1",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nIn reality we would probably sample the space more densely:"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#iterative-search",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#iterative-search",
    "title": "2 - Tuning Hyperparameters",
    "section": "Iterative Search",
    "text": "Iterative Search\nWe could start with a few points and search the space:"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#parameters",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#parameters",
    "title": "2 - Tuning Hyperparameters",
    "section": "Parameters",
    "text": "Parameters\n\nThe tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\nThe extract_parameter_set_dials() function extracts these tuning parameters and the info.\n\n\nGrids\n\nCreate your grid manually or automatically.\nThe grid_*() functions can make a grid.\n\n\n\nMost basic (but very effective) way to tune models"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#create-a-grid",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#create-a-grid",
    "title": "2 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nlgbm_wflow %&gt;% \n  extract_parameter_set_dials()\n#&gt; Collection of 4 parameters for tuning\n#&gt; \n#&gt;    identifier       type    object\n#&gt;         trees      trees nparam[+]\n#&gt;    learn_rate learn_rate nparam[+]\n#&gt;    agent hash  num_terms nparam[+]\n#&gt;  company hash  num_terms nparam[+]\n\n# Individual functions: \ntrees()\n#&gt; # Trees (quantitative)\n#&gt; Range: [1, 2000]\nlearn_rate()\n#&gt; Learning Rate (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-10, -1]\n\n\nA parameter set can be updated (e.g.Â to change the ranges)."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#create-a-grid-1",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#create-a-grid-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\n\n\nset.seed(12)\ngrid &lt;- \n  lgbm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#&gt; # A tibble: 25 Ã— 4\n#&gt;    trees    learn_rate `agent hash` `company hash`\n#&gt;    &lt;int&gt;         &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1  1629 0.00000440             524           1454\n#&gt;  2  1746 0.0000000751          1009           2865\n#&gt;  3    53 0.0000180             2313            367\n#&gt;  4   442 0.000000445            347            460\n#&gt;  5  1413 0.0000000208          3232            553\n#&gt;  6  1488 0.0000578             3692            639\n#&gt;  7   906 0.000385               602            332\n#&gt;  8  1884 0.00000000101         1127            567\n#&gt;  9  1812 0.0239                 961           1183\n#&gt; 10   393 0.000000117            487           1783\n#&gt; # â„¹ 15 more rows\n\n\n\n\nA space-filling design tends to perform better than random grids.\nSpace-filling designs are also usually more efficient than regular grids."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#your-turn",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#your-turn",
    "title": "2 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a grid for our tunable workflow.\nTry creating a regular grid.\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#create-a-regular-grid",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#create-a-regular-grid",
    "title": "2 - Tuning Hyperparameters",
    "section": "Create a regular grid  ",
    "text": "Create a regular grid  \n\nset.seed(12)\ngrid &lt;- \n  lgbm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 4)\n\ngrid\n#&gt; # A tibble: 256 Ã— 4\n#&gt;    trees   learn_rate `agent hash` `company hash`\n#&gt;    &lt;int&gt;        &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1     1 0.0000000001          256            256\n#&gt;  2   667 0.0000000001          256            256\n#&gt;  3  1333 0.0000000001          256            256\n#&gt;  4  2000 0.0000000001          256            256\n#&gt;  5     1 0.0000001             256            256\n#&gt;  6   667 0.0000001             256            256\n#&gt;  7  1333 0.0000001             256            256\n#&gt;  8  2000 0.0000001             256            256\n#&gt;  9     1 0.0001                256            256\n#&gt; 10   667 0.0001                256            256\n#&gt; # â„¹ 246 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#your-turn-1",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#your-turn-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\n\nWhat advantage would a regular grid have?"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#update-parameter-ranges",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#update-parameter-ranges",
    "title": "2 - Tuning Hyperparameters",
    "section": "Update parameter ranges  ",
    "text": "Update parameter ranges  \n\nlgbm_param &lt;- \n  lgbm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  update(trees = trees(c(1L, 100L)),\n         learn_rate = learn_rate(c(-5, -1)))\n\nset.seed(712)\ngrid &lt;- \n  lgbm_param %&gt;% \n  grid_latin_hypercube(size = 25)\n\ngrid\n#&gt; # A tibble: 25 Ã— 4\n#&gt;    trees learn_rate `agent hash` `company hash`\n#&gt;    &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1    75  0.000312          2991           1250\n#&gt;  2     4  0.0000337          899           3088\n#&gt;  3    15  0.0295             520           1578\n#&gt;  4     8  0.0997            1256           3592\n#&gt;  5    80  0.000622           419            258\n#&gt;  6    70  0.000474          2499           1089\n#&gt;  7    35  0.000165           287           2376\n#&gt;  8    64  0.00137            389            359\n#&gt;  9    58  0.0000250          616            881\n#&gt; 10    84  0.0639            2311           2635\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#the-results",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#the-results",
    "title": "2 - Tuning Hyperparameters",
    "section": "The results  ",
    "text": "The results  \n\n\ngrid %&gt;% \n  ggplot(aes(trees, learn_rate)) +\n  geom_point(size = 4) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\nNote that the learning rates are uniform on the log-10 scale."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#choosing-tuning-parameters",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#choosing-tuning-parameters",
    "title": "2 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters     ",
    "text": "Choosing tuning parameters     \nLetâ€™s take our previous model and tune more parameters:\n\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune(),  min_n = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow &lt;- workflow(hash_rec, lgbm_spec)\n\n# Update the feature hash ranges (log-2 units)\nlgbm_param &lt;-\n  lgbm_wflow %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search-3",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search-3",
    "title": "2 - Tuning Hyperparameters",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nset.seed(9)\nctrl &lt;- control_grid(save_pred = TRUE)\n\nlgbm_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = 25,\n    # The options below are not required by default\n    param_info = lgbm_param, \n    control = ctrl,\n    metrics = reg_metrics\n  )\n\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search-4",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search-4",
    "title": "2 - Tuning Hyperparameters",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nlgbm_res \n#&gt; # Tuning results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics          .notes           .predictions        \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;              \n#&gt;  1 &lt;split [3372/377]&gt; Fold01 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,425 Ã— 9]&gt;\n#&gt;  2 &lt;split [3373/376]&gt; Fold02 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  3 &lt;split [3373/376]&gt; Fold03 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  4 &lt;split [3373/376]&gt; Fold04 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  5 &lt;split [3373/376]&gt; Fold05 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  6 &lt;split [3374/375]&gt; Fold06 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,375 Ã— 9]&gt;\n#&gt;  7 &lt;split [3375/374]&gt; Fold07 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,350 Ã— 9]&gt;\n#&gt;  8 &lt;split [3376/373]&gt; Fold08 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,325 Ã— 9]&gt;\n#&gt;  9 &lt;split [3376/373]&gt; Fold09 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,325 Ã— 9]&gt;\n#&gt; 10 &lt;split [3376/373]&gt; Fold10 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [9,325 Ã— 9]&gt;"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#grid-results",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#grid-results",
    "title": "2 - Tuning Hyperparameters",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(lgbm_res)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#tuning-results",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#tuning-results",
    "title": "2 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(lgbm_res)\n#&gt; # A tibble: 50 Ã— 11\n#&gt;    trees min_n learn_rate `agent hash` `company hash` .metric .estimator   mean     n std_err .config              \n#&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1   298    19   4.15e- 9          222             36 mae     standard   53.2      10 0.427   Preprocessor01_Model1\n#&gt;  2   298    19   4.15e- 9          222             36 rsq     standard    0.810    10 0.00686 Preprocessor01_Model1\n#&gt;  3  1394     5   5.82e- 6           28             21 mae     standard   52.9      10 0.424   Preprocessor02_Model1\n#&gt;  4  1394     5   5.82e- 6           28             21 rsq     standard    0.810    10 0.00800 Preprocessor02_Model1\n#&gt;  5   774    12   4.41e- 2           27             95 mae     standard    9.77     10 0.155   Preprocessor03_Model1\n#&gt;  6   774    12   4.41e- 2           27             95 rsq     standard    0.946    10 0.00341 Preprocessor03_Model1\n#&gt;  7  1342     7   6.84e-10           71             17 mae     standard   53.2      10 0.427   Preprocessor04_Model1\n#&gt;  8  1342     7   6.84e-10           71             17 rsq     standard    0.811    10 0.00785 Preprocessor04_Model1\n#&gt;  9   669    39   8.62e- 7          141            145 mae     standard   53.2      10 0.426   Preprocessor05_Model1\n#&gt; 10   669    39   8.62e- 7          141            145 rsq     standard    0.807    10 0.00639 Preprocessor05_Model1\n#&gt; # â„¹ 40 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#tuning-results-1",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#tuning-results-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(lgbm_res, summarize = FALSE)\n#&gt; # A tibble: 500 Ã— 10\n#&gt;    id     trees min_n    learn_rate `agent hash` `company hash` .metric .estimator .estimate .config              \n#&gt;    &lt;chr&gt;  &lt;int&gt; &lt;int&gt;         &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1 Fold01   298    19 0.00000000415          222             36 mae     standard      51.8   Preprocessor01_Model1\n#&gt;  2 Fold01   298    19 0.00000000415          222             36 rsq     standard       0.821 Preprocessor01_Model1\n#&gt;  3 Fold02   298    19 0.00000000415          222             36 mae     standard      52.1   Preprocessor01_Model1\n#&gt;  4 Fold02   298    19 0.00000000415          222             36 rsq     standard       0.804 Preprocessor01_Model1\n#&gt;  5 Fold03   298    19 0.00000000415          222             36 mae     standard      52.2   Preprocessor01_Model1\n#&gt;  6 Fold03   298    19 0.00000000415          222             36 rsq     standard       0.786 Preprocessor01_Model1\n#&gt;  7 Fold04   298    19 0.00000000415          222             36 mae     standard      51.7   Preprocessor01_Model1\n#&gt;  8 Fold04   298    19 0.00000000415          222             36 rsq     standard       0.826 Preprocessor01_Model1\n#&gt;  9 Fold05   298    19 0.00000000415          222             36 mae     standard      55.2   Preprocessor01_Model1\n#&gt; 10 Fold05   298    19 0.00000000415          222             36 rsq     standard       0.845 Preprocessor01_Model1\n#&gt; # â„¹ 490 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#choose-a-parameter-combination",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#choose-a-parameter-combination",
    "title": "2 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nshow_best(lgbm_res, metric = \"rsq\")\n#&gt; # A tibble: 5 Ã— 11\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1  1890    10    0.0159           115            174 rsq     standard   0.948    10 0.00334 Preprocessor12_Model1\n#&gt; 2   774    12    0.0441            27             95 rsq     standard   0.946    10 0.00341 Preprocessor03_Model1\n#&gt; 3  1638    36    0.0409            15            120 rsq     standard   0.945    10 0.00384 Preprocessor16_Model1\n#&gt; 4   963    23    0.00556          157             13 rsq     standard   0.937    10 0.00320 Preprocessor06_Model1\n#&gt; 5   590     5    0.00320           85             73 rsq     standard   0.908    10 0.00465 Preprocessor24_Model1"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \nCreate your own tibble for final parameters or use one of the tune::select_*() functions:\n\nlgbm_best &lt;- select_best(lgbm_res, metric = \"mae\")\nlgbm_best\n#&gt; # A tibble: 1 Ã— 6\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;                \n#&gt; 1  1890    10     0.0159          115            174 Preprocessor12_Model1"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#checking-calibration",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#checking-calibration",
    "title": "2 - Tuning Hyperparameters",
    "section": "Checking Calibration  ",
    "text": "Checking Calibration  \n\n\nlibrary(probably)\nlgbm_res %&gt;%\n  collect_predictions(\n    parameters = lgbm_best\n  ) %&gt;%\n  cal_plot_regression(\n    truth = avg_price_per_room,\n    estimate = .pred,\n    alpha = 1 / 3\n  )"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#running-in-parallel",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#running-in-parallel",
    "title": "2 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\n\n\n\nGrid search, combined with resampling, requires fitting a lot of models!\nThese models donâ€™t depend on one another and can be run in parallel.\n\nWe can use a parallel backend to do this:\n\ncores &lt;- parallelly::availableCores(logical = FALSE)\ncl &lt;- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#running-in-parallel-1",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#running-in-parallel-1",
    "title": "2 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\nFaceted on the expensiveness of preprocessing used."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#early-stopping-for-boosted-trees",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#early-stopping-for-boosted-trees",
    "title": "2 - Tuning Hyperparameters",
    "section": "Early stopping for boosted trees",
    "text": "Early stopping for boosted trees\nWe have directly optimized the number of trees as a tuning parameter.\nInstead we could\n\nSet the number of trees to a single large number.\nStop adding trees when performance gets worse.\n\nThis is known as â€œearly stoppingâ€ and there is a parameter for that: stop_iter.\nEarly stopping has a potential to decrease the tuning time."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#your-turn-2",
    "href": "archive/2023-09-posit-conf/advanced-03-tuning-hyperparameters.html#your-turn-2",
    "title": "2 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\n\nSet trees = 2000 and tune the stop_iter parameter.\nNote that you will need to regenerate lgbm_param with your new workflow!\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#previously---setup",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#previously---setup",
    "title": "4 - Iterative Search",
    "section": "Previously - Setup",
    "text": "Previously - Setup\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;%  \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#previously---data-usage",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#previously---data-usage",
    "title": "4 - Iterative Search",
    "section": "Previously - Data Usage",
    "text": "Previously - Data Usage\n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#previously---boosting-model",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#previously---boosting-model",
    "title": "4 - Iterative Search",
    "section": "Previously - Boosting Model",
    "text": "Previously - Boosting Model\n\nhotel_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) %&gt;%\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) %&gt;%\n  step_zv(all_predictors())\n\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune(), min_n = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow &lt;- workflow(hotel_rec, lgbm_spec)\n\nlgbm_param &lt;-\n  lgbm_wflow %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#iterative-search",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#iterative-search",
    "title": "4 - Iterative Search",
    "section": "Iterative Search",
    "text": "Iterative Search\nInstead of pre-defining a grid of candidate points, we can model our current results to predict what the next candidate point should be.\n\nSuppose that we are only tuning the learning rate in our boosted tree.\n\nWe could do something like:\nmae_pred &lt;- lm(mae ~ learn_rate, data = resample_results)\nand use this to predict and rank new learning rate candidates."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#iterative-search-1",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#iterative-search-1",
    "title": "4 - Iterative Search",
    "section": "Iterative Search",
    "text": "Iterative Search\nA linear model probably isnâ€™t the best choice though (more in a minute).\nTo illustrate the process, we resampled a large grid of learning rate values for our data to show what the relationship is between MAE and learning rate.\nNow suppose that we used a grid of three points in the parameter range for learning rateâ€¦"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#a-large-grid",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#a-large-grid",
    "title": "4 - Iterative Search",
    "section": "A Large Grid",
    "text": "A Large Grid"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#a-three-point-grid",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#a-three-point-grid",
    "title": "4 - Iterative Search",
    "section": "A Three Point Grid",
    "text": "A Three Point Grid"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#gaussian-processes-and-optimization",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#gaussian-processes-and-optimization",
    "title": "4 - Iterative Search",
    "section": "Gaussian Processes and Optimization",
    "text": "Gaussian Processes and Optimization\nWe can make a â€œmeta-modelâ€ with a small set of historical performance results.\nGaussian Processes (GP) models are a good choice to model performance.\n\nIt is a Bayesian model so we are using Bayesian Optimization (BO).\nFor regression, we can assume that our data are multivariate normal.\nWe also define a covariance function for the variance relationship between data points. A common one is:\n\n\\[\\operatorname{cov}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\exp\\left(-\\frac{1}{2}|\\boldsymbol{x}_i - \\boldsymbol{x}_j|^2\\right) + \\sigma^2_{ij}\\]\n\nGPs are good because\n\nthey are flexible regression models (in the sense that splines are flexible).\nwe need to get mean and variance predictions (and they are Bayesian)\ntheir variability is based on spatial distances.\n\nSome people use random forests (with conformal variance estimates) or other methods but GPs are most popular."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#predicting-candidates",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#predicting-candidates",
    "title": "4 - Iterative Search",
    "section": "Predicting Candidates",
    "text": "Predicting Candidates\nThe GP model can take candidate tuning parameter combinations as inputs and make predictions for performance (e.g.Â MAE)\n\nThe mean performance\nThe variance of performance\n\nThe variance is mostly driven by spatial variability (the previous equation).\nThe predicted variance is zero at locations of actual data points and becomes very high when far away from any observed data."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#your-turn",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#your-turn",
    "title": "4 - Iterative Search",
    "section": "Your turn",
    "text": "Your turn\n\n\nYour GP makes predictions on two new candidate tuning parameters.\nWe want to minimize MAE.\nWhich should we choose?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#gp-fit-ribbon-is-mean---1sd",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#gp-fit-ribbon-is-mean---1sd",
    "title": "4 - Iterative Search",
    "section": "GP Fit (ribbon is mean +/- 1SD)",
    "text": "GP Fit (ribbon is mean +/- 1SD)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#choosing-new-candidates",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#choosing-new-candidates",
    "title": "4 - Iterative Search",
    "section": "Choosing New Candidates",
    "text": "Choosing New Candidates\nThis isnâ€™t a very good fit but we can still use it.\nHow can we use the outputs to choose the next point to measure?\n\nAcquisition functions take the predicted mean and variance and use them to balance:\n\nexploration: new candidates should explore new areas.\nexploitation: new candidates must stay near existing values.\n\nExploration focuses on the variance, exploitation is about the mean."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#acquisition-functions",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#acquisition-functions",
    "title": "4 - Iterative Search",
    "section": "Acquisition Functions",
    "text": "Acquisition Functions\nWeâ€™ll use an acquisition function to select a new candidate.\nThe most popular method appears to be expected improvement (EI) above the current best results.\n\nZero at existing data points.\nThe expected improvement is integrated over all possible improvement (â€œexpectedâ€ in the probability sense).\n\nWe would probably pick the point with the largest EI as the next point.\n(There are other functions beyond EI.)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#expected-improvement",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#expected-improvement",
    "title": "4 - Iterative Search",
    "section": "Expected Improvement",
    "text": "Expected Improvement"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#iteration",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#iteration",
    "title": "4 - Iterative Search",
    "section": "Iteration",
    "text": "Iteration\nOnce we pick the candidate point, we measure performance for it (e.g.Â resampling).\n\nAnother GP is fit, EI is recomputed, and so on.\n\nWe stop when we have completed the allowed number of iterations or if we donâ€™t see any improvement after a pre-set number of attempts."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#gp-fit-with-four-points",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#gp-fit-with-four-points",
    "title": "4 - Iterative Search",
    "section": "GP Fit with four points",
    "text": "GP Fit with four points"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#expected-improvement-1",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#expected-improvement-1",
    "title": "4 - Iterative Search",
    "section": "Expected Improvement",
    "text": "Expected Improvement"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#gp-evolution",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#gp-evolution",
    "title": "4 - Iterative Search",
    "section": "GP Evolution",
    "text": "GP Evolution"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#expected-improvement-evolution",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#expected-improvement-evolution",
    "title": "4 - Iterative Search",
    "section": "Expected Improvement Evolution",
    "text": "Expected Improvement Evolution"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#bo-in-tidymodels",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#bo-in-tidymodels",
    "title": "4 - Iterative Search",
    "section": "BO in tidymodels",
    "text": "BO in tidymodels\nWeâ€™ll use a function called tune_bayes() that has very similar syntax to tune_grid().\n\nIt has an additional initial argument for the initial set of performance estimates and parameter combinations for the GP model."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#initial-grid-points",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#initial-grid-points",
    "title": "4 - Iterative Search",
    "section": "Initial grid points",
    "text": "Initial grid points\ninitial can be the results of another tune_*() function or an integer (in which case tune_grid() is used under to hood to make such an initial set of results).\n\nWeâ€™ll run the optimization more than once, so letâ€™s make an initial grid of results to serve as the substrate for the BO.\nI suggest at least the number of tuning parameters plus two as the initial grid for BO."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#an-initial-grid",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#an-initial-grid",
    "title": "4 - Iterative Search",
    "section": "An Initial Grid",
    "text": "An Initial Grid\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\nset.seed(12)\ninit_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = nrow(lgbm_param) + 2,\n    param_info = lgbm_param,\n    metrics = reg_metrics\n  )\n\nshow_best(init_res, metric = \"mae\")\n#&gt; # A tibble: 5 Ã— 11\n#&gt;   trees min_n   learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config             \n#&gt;   &lt;int&gt; &lt;int&gt;        &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1   390    10 0.0139                 13             62 mae     standard    11.3    10   0.202 Preprocessor1_Model1\n#&gt; 2   718    31 0.00112                72             25 mae     standard    29.0    10   0.335 Preprocessor4_Model1\n#&gt; 3  1236    22 0.0000261              11             17 mae     standard    51.8    10   0.416 Preprocessor7_Model1\n#&gt; 4  1044    25 0.00000832             34             12 mae     standard    52.8    10   0.424 Preprocessor5_Model1\n#&gt; 5  1599     7 0.0000000402          254            179 mae     standard    53.2    10   0.427 Preprocessor6_Model1"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#bo-using-tidymodels",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#bo-using-tidymodels",
    "title": "4 - Iterative Search",
    "section": "BO using tidymodels",
    "text": "BO using tidymodels\n\nset.seed(15)\nlgbm_bayes_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_bayes(\n    resamples = hotel_rs,\n    initial = init_res,     # &lt;- initial results\n    iter = 20,\n    param_info = lgbm_param,\n    metrics = reg_metrics\n  )\n\nshow_best(lgbm_bayes_res, metric = \"mae\")\n#&gt; # A tibble: 5 Ã— 12\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config .iter\n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n#&gt; 1  1769     2     0.0299          114            245 mae     standard    9.41    10   0.160 Iter13     13\n#&gt; 2  1969     3     0.0270          240             99 mae     standard    9.49    10   0.189 Iter11     11\n#&gt; 3  1780     5     0.0403           27             78 mae     standard    9.54    10   0.164 Iter17     17\n#&gt; 4  1454     3     0.0414          114             10 mae     standard    9.55    10   0.144 Iter10     10\n#&gt; 5  1253     2     0.0312          131            207 mae     standard    9.56    10   0.145 Iter19     19"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#plotting-bo-results",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#plotting-bo-results",
    "title": "4 - Iterative Search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(lgbm_bayes_res, metric = \"mae\")"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#plotting-bo-results-1",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#plotting-bo-results-1",
    "title": "4 - Iterative Search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"parameters\")"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#plotting-bo-results-2",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#plotting-bo-results-2",
    "title": "4 - Iterative Search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"performance\")"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#enhance",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#enhance",
    "title": "4 - Iterative Search",
    "section": "ENHANCE",
    "text": "ENHANCE\n\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"performance\") +\n  ylim(c(9, 14))"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#your-turn-1",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#your-turn-1",
    "title": "4 - Iterative Search",
    "section": "Your turn",
    "text": "Your turn\nLetâ€™s try a different acquisition function: conf_bound(kappa).\nWeâ€™ll use the objective argument to set it.\nChoose your own kappa value:\n\nLarger values will explore the space more.\nâ€œLargeâ€ values are usually less than one.\n\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#notes",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#notes",
    "title": "4 - Iterative Search",
    "section": "Notes",
    "text": "Notes\n\nStopping tune_bayes() will return the current results.\nParallel processing can still be used to more efficiently measure each candidate point.\nThere are a lot of other iterative methods that you can use.\nThe finetune package also has functions for simulated annealing search."
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#finalizing-the-model",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#finalizing-the-model",
    "title": "4 - Iterative Search",
    "section": "Finalizing the Model",
    "text": "Finalizing the Model\nLetâ€™s say that weâ€™ve tried a lot of different models and we like our lightgbm model the most.\nWhat do we do now?\n\nFinalize the workflow by choosing the values for the tuning parameters.\nFit the model on the entire training set.\nVerify performance using the test set.\nDocument and publish the model(?)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#locking-down-the-tuning-parameters",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#locking-down-the-tuning-parameters",
    "title": "4 - Iterative Search",
    "section": "Locking Down the Tuning Parameters",
    "text": "Locking Down the Tuning Parameters\nWe can take the results of the Bayesian optimization and accept the best results:\n\nbest_param &lt;- select_best(lgbm_bayes_res, metric = \"mae\")\nfinal_wflow &lt;- \n  lgbm_wflow %&gt;% \n  finalize_workflow(best_param)\nfinal_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Recipe\n#&gt; Model: boost_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; 4 Recipe Steps\n#&gt; \n#&gt; â€¢ step_YeoJohnson()\n#&gt; â€¢ step_dummy_hash()\n#&gt; â€¢ step_dummy_hash()\n#&gt; â€¢ step_zv()\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Boosted Tree Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1769\n#&gt;   min_n = 2\n#&gt;   learn_rate = 0.0299391312149257\n#&gt; \n#&gt; Engine-Specific Arguments:\n#&gt;   num_threads = 1\n#&gt; \n#&gt; Computational engine: lightgbm"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#the-final-fit",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#the-final-fit",
    "title": "4 - Iterative Search",
    "section": "The Final Fit",
    "text": "The Final Fit\nWe can use individual functions:\nfinal_fit &lt;- final_wflow %&gt;% fit(data = hotel_train)\n\n# then predict() or augment() \n# then compute metrics\n\nRemember that there is also a convenience function to do all of this:\n\nset.seed(3893)\nfinal_res &lt;- final_wflow %&gt;% last_fit(hotel_split, metrics = reg_metrics)\nfinal_res\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits              id               .metrics         .notes           .predictions         .workflow \n#&gt;   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;           &lt;list&gt;               &lt;list&gt;    \n#&gt; 1 &lt;split [3749/1251]&gt; train/test split &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1,251 Ã— 4]&gt; &lt;workflow&gt;"
  },
  {
    "objectID": "archive/2023-09-posit-conf/advanced-05-iterative.html#test-set-results",
    "href": "archive/2023-09-posit-conf/advanced-05-iterative.html#test-set-results",
    "title": "4 - Iterative Search",
    "section": "Test Set Results",
    "text": "Test Set Results\n\n\n\nfinal_res %&gt;% \n  collect_predictions() %&gt;% \n  cal_plot_regression(\n    truth = avg_price_per_room, \n    estimate = .pred, \n    alpha = 1 / 4)\n\nTest set performance:\n\nfinal_res %&gt;% collect_metrics()\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard       9.63  Preprocessor1_Model1\n#&gt; 2 rsq     standard       0.948 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2023-09-posit-conf/annotations.html#section",
    "href": "archive/2023-09-posit-conf/annotations.html#section",
    "title": "Annotations",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€\nThis page contains annotations for selected slides.\nThereâ€™s a lot that we want to tell you. We donâ€™t want people to have to frantically scribble down things that we say that are not on the slides.\nWeâ€™ve added sections to this document with longer explanations and links to other resources."
  },
  {
    "objectID": "archive/2023-09-posit-conf/annotations.html#the-initial-split",
    "href": "archive/2023-09-posit-conf/annotations.html#the-initial-split",
    "title": "Annotations",
    "section": "The initial split",
    "text": "The initial split\nWhat does set.seed() do?\nWeâ€™ll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random).\nThink of PRN as a box that takes a starting value (the â€œseedâ€) that produces random numbers using that starting value as an input into its process.\nIf we know a seed value, we can reproduce our â€œrandomâ€ numbers. To use a different set of random numbers, choose a different seed value.\nFor example:\n\nset.seed(1)\nrunif(3)\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\n# Get a new set of random numbers:\nset.seed(2)\nrunif(3)\n#&gt; [1] 0.1848823 0.7023740 0.5733263\n\n# We can reproduce the old ones with the same seed\nset.seed(1)\nrunif(3)\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\nIf we donâ€™t set the seed, R uses the clock time and the process ID to create a seed. This isnâ€™t reproducible.\nSince we want our code to be reproducible, we set the seeds before random numbers are used.\nIn theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same and we donâ€™t get reproducible results.\nThe value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to â€œspread the randomness aroundâ€. It is basically:\n\ncat(paste0(\"set.seed(\", sample.int(10000, 5), \")\", collapse = \"\\n\"))\n#&gt; set.seed(9725)\n#&gt; set.seed(8462)\n#&gt; set.seed(4050)\n#&gt; set.seed(8789)\n#&gt; set.seed(1301)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/annotations.html#what-is-wrong-with-this",
    "href": "archive/2023-09-posit-conf/annotations.html#what-is-wrong-with-this",
    "title": "Annotations",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?\nIf we treat the preprocessing as a separate task, it raises the risk that we might accidentally overfit to the data at hand.\nFor example, someone might estimate something from the entire data set (such as the principle components) and treat that data as if it were known (and not estimated). Depending on the what was done with the data, consequences in doing that could be:\n\nYour performance metrics are slightly-to-moderately optimistic (e.g.Â you might think your accuracy is 85% when it is actually 75%)\nA consequential component of the analysis is not right and the model just doesnâ€™t work.\n\nThe big issue here is that you wonâ€™t be able to figure this out until you get a new piece of data, such as the test set.\nA really good example of this is in â€˜Selection bias in gene extraction on the basis of microarray gene-expression dataâ€™. The authors re-analyze a previous publication and show that the original researchers did not include feature selection in the workflow. Because of that, their performance statistics were extremely optimistic. In one case, they could do the original analysis on complete noise and still achieve zero errors.\nGenerally speaking, this problem is referred to as data leakage. Some other references:\n\nOverfitting to Predictors and External Validation\nAre We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning\nNavigating the pitfalls of applying machine learning in genomics\nA review of feature selection techniques in bioinformatics\nOn Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation"
  },
  {
    "objectID": "archive/2023-09-posit-conf/annotations.html#where-are-the-fitted-models",
    "href": "archive/2023-09-posit-conf/annotations.html#where-are-the-fitted-models",
    "title": "Annotations",
    "section": "Where are the fitted models?",
    "text": "Where are the fitted models?\nThe primary purpose of resampling is to estimate model performance. The models are almost never needed again.\nAlso, if the data set is large, the model object may require a lot of memory to save so, by default, we donâ€™t keep them.\nFor more advanced use cases, you can extract and save them. See:\n\nhttps://www.tmwr.org/resampling.html#extract\nhttps://www.tidymodels.org/learn/models/coefficients/ (an example)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/annotations.html#update-parameter-ranges",
    "href": "archive/2023-09-posit-conf/annotations.html#update-parameter-ranges",
    "title": "Annotations",
    "section": "Update parameter ranges",
    "text": "Update parameter ranges\nIn about 90% of the cases, the dials function that you use to update the parameter range has the same name as the argument. For example, if you were to update the mtry parameter in a random forests model, the code would look like\n\nparameter_object %&gt;% \n  update(mtry = mtry(c(1, 100)))\n\nThere are some cases where the parameter function, or its associated values, are different from the argument name.\nFor example, with step_spline_naturall(), we might want to tune the deg_free argument (for the degrees of freedom of a spline function. ). In this case, the argument name is deg_free but we update it with spline_degree().\ndeg_free represents the general concept of degrees of freedom and could be associated with many different things. For example, if we ever had an argument that was the number of degrees of freedom for a \\(t\\) distribution, we would call that argument deg_free.\nFor splines, we probably want a wider range for the degrees of freedom. We made a specialized function called spline_degree() to be used in these cases.\nHow can you tell when this happens? There is a helper function called tunable() and that gives information on how we make the default ranges for parameters. There is a column in these objects names call_info:\n\nlibrary(tidymodels)\nns_tunable &lt;- \n  recipe(mpg ~ ., data = mtcars) %&gt;% \n  step_spline_natural(dis, deg_free = tune()) %&gt;% \n  tunable()\n\nns_tunable\n#&gt; # A tibble: 1 Ã— 5\n#&gt;   name     call_info        source component           component_id        \n#&gt;   &lt;chr&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;chr&gt;               &lt;chr&gt;               \n#&gt; 1 deg_free &lt;named list [3]&gt; recipe step_spline_natural spline_natural_P1Tjg\nns_tunable$call_info\n#&gt; [[1]]\n#&gt; [[1]]$pkg\n#&gt; [1] \"dials\"\n#&gt; \n#&gt; [[1]]$fun\n#&gt; [1] \"spline_degree\"\n#&gt; \n#&gt; [[1]]$range\n#&gt; [1]  2 15"
  },
  {
    "objectID": "archive/2023-09-posit-conf/annotations.html#early-stopping-for-boosted-trees",
    "href": "archive/2023-09-posit-conf/annotations.html#early-stopping-for-boosted-trees",
    "title": "Annotations",
    "section": "Early stopping for boosted trees",
    "text": "Early stopping for boosted trees\nWhen deciding on the number of boosting iterations, there are two main strategies:\n\nDirectly tune it (trees = tune())\nSet it to one value and tune the number of early stopping iterations (trees = 500, stop_iter = tune()).\n\nEarly stopping is when we monitor the performance of the model. If the model doesnâ€™t make any improvements for stop_iter iterations, training stops.\nHereâ€™s an example where, after eleven iterations, performance starts to get worse.\n\n\n\n\n\n\n\n\n\nThis is likely due to over-fitting so we stop the model at eleven boosting iterations.\nEarly stopping usually has good results and takes far less time.\nWe could an engine argument called validation here. Thatâ€™s not an argument to any function in the lightgbm package.\nbonsai has its own wrapper around (lightgbm::lgb.train()) called bonsai::train_lightgbm(). We use that here and it has a validation argument.\nHow would you know that? There are a few different ways:\n\nLook at the documentation in ?boost_tree and click on the lightgbm entry in the engine list.\nCheck out the pkgdown reference website https://parsnip.tidymodels.org/reference/index.html\nRun the translate() function on the parsnip specification object.\n\nThe first two options are best since they tell you a lot more about the particularities of each model engine (there are a lot for lightgbm)."
  },
  {
    "objectID": "archive/2023-09-posit-conf/annotations.html#per-agent-statistics",
    "href": "archive/2023-09-posit-conf/annotations.html#per-agent-statistics",
    "title": "Annotations",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics\nThe effect encoding method essentially takes the effect of a variable, like agent, and makes a data column for that effect. In our example, affect of the agent on the ADR is quantified by a model and then added as a data column to be used in the model.\nSuppose agent Max has a single reservation in the data and it had an ADR of â‚¬200. If we used a naive estimate for Maxâ€™s effect, the model is being told that Max should always produce an effect of â‚¬200. Thatâ€™s a very poor estimate since it is from a single data point.\nContrast this with seasoned agent Davis, who has taken 250 reservations with an average ADR of â‚¬100. Davisâ€™s mean is more predictive because it is estimated with better data (i.e., more total reservations). Partial pooling leverages the entire data set and can borrow strength from all of the agents. It is a common tool in Bayesian estimation and non-Bayesian mixed models. If a agentâ€™s data is of good quality, the partial pooling effect estimate is closer to the raw mean. Maxâ€™s data is not great and is â€œshrunkâ€ towards the center of the overall average. Since there is so little known about Maxâ€™s reservation history, this is a better effect estimate (until more data is available for him).\nThe Stan documentation has a pretty good vignette on this: https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html\nAlso, Bayes Rules! has a nice section on this: https://www.bayesrulesbook.com/chapter-15.html\nSince this example has a numeric outcome, partial pooling is very similar to the Jamesâ€“Stein estimator: https://en.wikipedia.org/wiki/Jamesâ€“Stein_estimator"
  },
  {
    "objectID": "archive/2023-09-posit-conf/annotations.html#agent-effects",
    "href": "archive/2023-09-posit-conf/annotations.html#agent-effects",
    "title": "Annotations",
    "section": "Agent effects",
    "text": "Agent effects\nEffect encoding might result in a somewhat circular argument: the column is more likely to be important to the model since it is the output of a separate model. The risk here is that we might over-fit the effect to the data. For this reason, it is super important to make sure that we verify that we arenâ€™t overfitting by checking with resampling (or a validation set).\nPartial pooling somewhat lowers the risk of overfitting since it tends to correct for agents with small sample sizes. It canâ€™t correct for improper data usage or data leakage though."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#chicago-l-train-data",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#chicago-l-train-data",
    "title": "Case Study on Transportation",
    "section": "Chicago L-Train data",
    "text": "Chicago L-Train data\nSeveral years worth of pre-pandemic data were assembled to try to predict the daily number of people entering the Clark and Lake elevated (â€œLâ€) train station in Chicago.\nMore information:\n\nSeveral Chapters in Feature Engineering and Selection.\n\nStart with Section 4.1\nSee Section 1.3\n\nVideo: The Global Pandemic Ruined My Favorite Data Set"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#predictors",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#predictors",
    "title": "Case Study on Transportation",
    "section": "Predictors",
    "text": "Predictors\n\nthe 14-day lagged ridership at this and other stations (units: thousands of rides/day)\nweather data\nhome/away game schedules for Chicago teams\nthe date\n\nThe data are in modeldata. See ?Chicago."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#l-train-locations",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#l-train-locations",
    "title": "Case Study on Transportation",
    "section": "L Train Locations",
    "text": "L Train Locations"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#your-turn-explore-the-data",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#your-turn-explore-the-data",
    "title": "Case Study on Transportation",
    "section": "Your turn: Explore the Data",
    "text": "Your turn: Explore the Data\nTake a look at these data for a few minutes and see if you can find any interesting characteristics in the predictors or the outcome.\n\nlibrary(tidymodels)\nlibrary(rules)\ndata(\"Chicago\")\ndim(Chicago)\n#&gt; [1] 5698   50\nstations\n#&gt;  [1] \"Austin\"           \"Quincy_Wells\"     \"Belmont\"          \"Archer_35th\"     \n#&gt;  [5] \"Oak_Park\"         \"Western\"          \"Clark_Lake\"       \"Clinton\"         \n#&gt;  [9] \"Merchandise_Mart\" \"Irving_Park\"      \"Washington_Wells\" \"Harlem\"          \n#&gt; [13] \"Monroe\"           \"Polk\"             \"Ashland\"          \"Kedzie\"          \n#&gt; [17] \"Addison\"          \"Jefferson_Park\"   \"Montrose\"         \"California\"\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#splitting-with-chicago-data",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#splitting-with-chicago-data",
    "title": "Case Study on Transportation",
    "section": "Splitting with Chicago data ",
    "text": "Splitting with Chicago data \nLetâ€™s put the last two weeks of data into the test set. initial_time_split() can be used for this purpose:\n\ndata(Chicago)\n\nchi_split &lt;- initial_time_split(Chicago, prop = 1 - (14/nrow(Chicago)))\nchi_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;5684/14/5698&gt;\n\nchi_train &lt;- training(chi_split)\nchi_test  &lt;- testing(chi_split)\n\n## training\nnrow(chi_train)\n#&gt; [1] 5684\n \n## testing\nnrow(chi_test)\n#&gt; [1] 14"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#time-series-resampling",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#time-series-resampling",
    "title": "Case Study on Transportation",
    "section": "Time series resampling",
    "text": "Time series resampling\nOur Chicago data is over time. Regular cross-validation, which uses random sampling, may not be the best idea.\nWe can emulate our training/test split by making similar resamples.\n\nFold 1: Take the first X years of data as the analysis set, the next 2 weeks as the assessment set.\nFold 2: Take the first X years + 2 weeks of data as the analysis set, the next 2 weeks as the assessment set.\nand so on"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#rolling-forecast-origin-resampling",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#rolling-forecast-origin-resampling",
    "title": "Case Study on Transportation",
    "section": "Rolling forecast origin resampling",
    "text": "Rolling forecast origin resampling\n\n\nThis image shows overlapping assessment sets. We will use non-overlapping data but it could be done wither way."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#times-series-resampling",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#times-series-resampling",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n\n\n\n\n  )\n\nUse the date column to find the date data."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#times-series-resampling-1",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#times-series-resampling-1",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n\n\n\n  )\n\nOur units will be weeks."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#times-series-resampling-2",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#times-series-resampling-2",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15  \n    \n    \n  )\n\nEvery analysis set has 15 years of data"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#times-series-resampling-3",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#times-series-resampling-3",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n\n  )\n\nEvery assessment set has 2 weeks of data"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#times-series-resampling-4",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#times-series-resampling-4",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n    step = 2 \n  )\n\nIncrement by 2 weeks so that there are no overlapping assessment sets.\n\nchi_rs$splits[[1]] %&gt;% assessment() %&gt;% pluck(\"date\") %&gt;% range()\n#&gt; [1] \"2016-01-07\" \"2016-01-20\"\nchi_rs$splits[[2]] %&gt;% assessment() %&gt;% pluck(\"date\") %&gt;% range()\n#&gt; [1] \"2016-01-21\" \"2016-02-03\""
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#our-resampling-object",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#our-resampling-object",
    "title": "Case Study on Transportation",
    "section": "Our resampling object ",
    "text": "Our resampling object \n\n\n\nchi_rs\n#&gt; # Sliding period resampling \n#&gt; # A tibble: 16 Ã— 2\n#&gt;    splits            id     \n#&gt;    &lt;list&gt;            &lt;chr&gt;  \n#&gt;  1 &lt;split [5463/14]&gt; Slice01\n#&gt;  2 &lt;split [5467/14]&gt; Slice02\n#&gt;  3 &lt;split [5467/14]&gt; Slice03\n#&gt;  4 &lt;split [5467/14]&gt; Slice04\n#&gt;  5 &lt;split [5467/14]&gt; Slice05\n#&gt;  6 &lt;split [5467/14]&gt; Slice06\n#&gt;  7 &lt;split [5467/14]&gt; Slice07\n#&gt;  8 &lt;split [5467/14]&gt; Slice08\n#&gt;  9 &lt;split [5467/14]&gt; Slice09\n#&gt; 10 &lt;split [5467/14]&gt; Slice10\n#&gt; 11 &lt;split [5467/14]&gt; Slice11\n#&gt; 12 &lt;split [5467/14]&gt; Slice12\n#&gt; 13 &lt;split [5467/14]&gt; Slice13\n#&gt; 14 &lt;split [5467/14]&gt; Slice14\n#&gt; 15 &lt;split [5467/14]&gt; Slice15\n#&gt; 16 &lt;split [5467/11]&gt; Slice16\n\n\n\n\nWe will fit 16 models on 16 slightly different analysis sets.\nEach will produce a separate performance metrics.\nWe will average the 16 metrics to get the resampling estimate of that statistic."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#feature-engineering-with-recipes",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#feature-engineering-with-recipes",
    "title": "Case Study on Transportation",
    "section": "Feature engineering with recipes ",
    "text": "Feature engineering with recipes \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train)\n\nBased on the formula, the function assigns columns to roles of â€œoutcomeâ€ or â€œpredictorâ€"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#a-recipe",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#a-recipe",
    "title": "Case Study on Transportation",
    "section": "A recipe",
    "text": "A recipe\n\nsummary(chi_rec)\n#&gt; # A tibble: 50 Ã— 4\n#&gt;    variable         type      role      source  \n#&gt;    &lt;chr&gt;            &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 Austin           &lt;chr [2]&gt; predictor original\n#&gt;  2 Quincy_Wells     &lt;chr [2]&gt; predictor original\n#&gt;  3 Belmont          &lt;chr [2]&gt; predictor original\n#&gt;  4 Archer_35th      &lt;chr [2]&gt; predictor original\n#&gt;  5 Oak_Park         &lt;chr [2]&gt; predictor original\n#&gt;  6 Western          &lt;chr [2]&gt; predictor original\n#&gt;  7 Clark_Lake       &lt;chr [2]&gt; predictor original\n#&gt;  8 Clinton          &lt;chr [2]&gt; predictor original\n#&gt;  9 Merchandise_Mart &lt;chr [2]&gt; predictor original\n#&gt; 10 Irving_Park      &lt;chr [2]&gt; predictor original\n#&gt; # â„¹ 40 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#a-recipe---work-with-dates",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#a-recipe---work-with-dates",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) \n\nThis creates three new columns in the data based on the date. Note that the day-of-the-week column is a factor."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#a-recipe---work-with-dates-1",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#a-recipe---work-with-dates-1",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %&gt;% \n  step_holiday(date) \n\nAdd indicators for major holidays. Specific holidays, especially those non-USA, can also be generated.\nAt this point, we donâ€™t need date anymore. Instead of deleting it (there is a step for that) we will change its role to be an identification variable.\n\nWe might want to change the role (instead of removing the column) because it will stay in the data set (even when resampled) and might be useful for diagnosing issues."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#a-recipe---work-with-dates-2",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#a-recipe---work-with-dates-2",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %&gt;% \n  step_holiday(date) %&gt;% \n  update_role(date, new_role = \"id\") %&gt;%\n  update_role_requirements(role = \"id\", bake = TRUE)\n\ndate is still in the data set but tidymodels knows not to treat it as an analysis column.\nupdate_role_requirements() is needed to make sure that this column is required when making new data points."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#a-recipe---remove-constant-columns",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#a-recipe---remove-constant-columns",
    "title": "Case Study on Transportation",
    "section": "A recipe - remove constant columns ",
    "text": "A recipe - remove constant columns \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %&gt;% \n  step_holiday(date) %&gt;% \n  update_role(date, new_role = \"id\") %&gt;%\n  update_role_requirements(role = \"id\", bake = TRUE) %&gt;% \n  step_zv(all_nominal_predictors())"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#a-recipe---handle-correlations",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#a-recipe---handle-correlations",
    "title": "Case Study on Transportation",
    "section": "A recipe - handle correlations ",
    "text": "A recipe - handle correlations \nThe station columns have a very high degree of correlation.\nWe might want to decorrelated them with principle component analysis to help the model fits go more easily.\nThe vector stations contains all station names and can be used to identify all the relevant columns.\n\nchi_pca_rec &lt;- \n  chi_rec %&gt;% \n  step_normalize(all_of(!!stations)) %&gt;% \n  step_pca(all_of(!!stations), num_comp = tune())\n\nWeâ€™ll tune the number of PCA components for (default) values of one to four."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#make-some-models",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#make-some-models",
    "title": "Case Study on Transportation",
    "section": "Make some models     ",
    "text": "Make some models     \nLetâ€™s try three models. The first one requires the rules package (loaded earlier).\n\ncb_spec &lt;- cubist_rules(committees = 25, neighbors = tune())\nmars_spec &lt;- mars(prod_degree = tune()) %&gt;% set_mode(\"regression\")\nlm_spec &lt;- linear_reg()\n\nchi_set &lt;- \n  workflow_set(\n    list(pca = chi_pca_rec, basic = chi_rec), \n    list(cubist = cb_spec, mars = mars_spec, lm = lm_spec)\n  ) %&gt;% \n  # Evaluate models using mean absolute errors\n  option_add(metrics = metric_set(mae))\n\n\nBriefly talk about Cubist being a (sort of) boosted rule-based model and MARS being a nonlinear regression model. Both incorporate feature selection nicely."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#process-them-on-the-resamples",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#process-them-on-the-resamples",
    "title": "Case Study on Transportation",
    "section": "Process them on the resamples",
    "text": "Process them on the resamples\n\n# Set up some objects for stacking ensembles (in a few slides)\ngrid_ctrl &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\nchi_res &lt;- \n  chi_set %&gt;% \n  workflow_map(\n    resamples = chi_rs,\n    grid = 10,\n    control = grid_ctrl,\n    verbose = TRUE,\n    seed = 12\n  )"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#how-do-the-results-look",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#how-do-the-results-look",
    "title": "Case Study on Transportation",
    "section": "How do the results look?",
    "text": "How do the results look?\n\nrank_results(chi_res)\n#&gt; # A tibble: 31 Ã— 9\n#&gt;    wflow_id     .config              .metric  mean std_err     n preprocessor model   rank\n#&gt;    &lt;chr&gt;        &lt;chr&gt;                &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;  &lt;int&gt;\n#&gt;  1 pca_cubist   Preprocessor1_Model1 mae     0.798   0.104    16 recipe       cubisâ€¦     1\n#&gt;  2 pca_cubist   Preprocessor3_Model3 mae     0.978   0.110    16 recipe       cubisâ€¦     2\n#&gt;  3 pca_cubist   Preprocessor4_Model2 mae     0.983   0.122    16 recipe       cubisâ€¦     3\n#&gt;  4 pca_cubist   Preprocessor4_Model1 mae     0.991   0.127    16 recipe       cubisâ€¦     4\n#&gt;  5 pca_cubist   Preprocessor3_Model2 mae     0.991   0.113    16 recipe       cubisâ€¦     5\n#&gt;  6 pca_cubist   Preprocessor2_Model2 mae     1.02    0.118    16 recipe       cubisâ€¦     6\n#&gt;  7 pca_cubist   Preprocessor1_Model3 mae     1.05    0.134    16 recipe       cubisâ€¦     7\n#&gt;  8 basic_cubist Preprocessor1_Model8 mae     1.07    0.115    16 recipe       cubisâ€¦     8\n#&gt;  9 basic_cubist Preprocessor1_Model7 mae     1.07    0.112    16 recipe       cubisâ€¦     9\n#&gt; 10 basic_cubist Preprocessor1_Model6 mae     1.07    0.114    16 recipe       cubisâ€¦    10\n#&gt; # â„¹ 21 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#plot-the-results",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#plot-the-results",
    "title": "Case Study on Transportation",
    "section": "Plot the results  ",
    "text": "Plot the results  \n\nautoplot(chi_res)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#pull-out-specific-results",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#pull-out-specific-results",
    "title": "Case Study on Transportation",
    "section": "Pull out specific results  ",
    "text": "Pull out specific results  \nWe can also pull out the specific tuning results and look at them:\n\nchi_res %&gt;% \n  extract_workflow_set_result(\"pca_cubist\") %&gt;% \n  autoplot()"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit? \nModel stacks generate predictions that are informed by several models."
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-1",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-1",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-2",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-2",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-3",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-3",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-4",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-4",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-5",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-5",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#building-a-model-stack",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#building-a-model-stack",
    "title": "Case Study on Transportation",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlibrary(stacks)\n\n\nDefine candidate members\nInitialize a data stack object\nAdd candidate ensemble members to the data stack\nEvaluate how to combine their predictions\nFit candidate ensemble members with non-zero stacking coefficients\nPredict on new data!"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#start-the-stack-and-add-members",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#start-the-stack-and-add-members",
    "title": "Case Study on Transportation",
    "section": "Start the stack and add members ",
    "text": "Start the stack and add members \nCollect all of the resampling results for all model configurations.\n\nchi_stack &lt;- \n  stacks() %&gt;% \n  add_candidates(chi_res)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#estimate-weights-for-each-candidate",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#estimate-weights-for-each-candidate",
    "title": "Case Study on Transportation",
    "section": "Estimate weights for each candidate ",
    "text": "Estimate weights for each candidate \nWhich configurations should be retained? Uses a penalized linear model:\n\nset.seed(122)\nchi_stack_res &lt;- blend_predictions(chi_stack)\n\nchi_stack_res\n#&gt; # A tibble: 5 Ã— 3\n#&gt;   member           type         weight\n#&gt;   &lt;chr&gt;            &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 pca_cubist_1_1   cubist_rules  0.343\n#&gt; 2 pca_cubist_3_2   cubist_rules  0.236\n#&gt; 3 basic_cubist_1_4 cubist_rules  0.189\n#&gt; 4 pca_lm_4_1       linear_reg    0.163\n#&gt; 5 pca_cubist_3_3   cubist_rules  0.109"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#how-did-it-do",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#how-did-it-do",
    "title": "Case Study on Transportation",
    "section": "How did it do?  ",
    "text": "How did it do?  \nThe overall results of the penalized model:\n\nautoplot(chi_stack_res)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#what-does-it-use",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#what-does-it-use",
    "title": "Case Study on Transportation",
    "section": "What does it use?  ",
    "text": "What does it use?  \n\nautoplot(chi_stack_res, type = \"weights\")"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#fit-the-required-candidate-models",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#fit-the-required-candidate-models",
    "title": "Case Study on Transportation",
    "section": "Fit the required candidate models",
    "text": "Fit the required candidate models\nFor each model we retain in the stack, we need their model fit on the entire training set.\n\nchi_stack_res &lt;- fit_members(chi_stack_res)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#the-test-set-best-cubist-model",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#the-test-set-best-cubist-model",
    "title": "Case Study on Transportation",
    "section": "The test set: best Cubist model  ",
    "text": "The test set: best Cubist model  \nWe can pull out the results and the workflow to fit the single best cubist model.\n\nbest_cubist &lt;- \n  chi_res %&gt;% \n  extract_workflow_set_result(\"pca_cubist\") %&gt;% \n  select_best()\n\ncubist_res &lt;- \n  chi_res %&gt;% \n  extract_workflow(\"pca_cubist\") %&gt;% \n  finalize_workflow(best_cubist) %&gt;% \n  last_fit(split = chi_split, metrics = metric_set(mae))"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#the-test-set-stack-ensemble",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#the-test-set-stack-ensemble",
    "title": "Case Study on Transportation",
    "section": "The test set: stack ensemble",
    "text": "The test set: stack ensemble\nWe donâ€™t have last_fit() for stacks (yet) so we manually make predictions.\n\nstack_pred &lt;- \n  predict(chi_stack_res, chi_test) %&gt;% \n  bind_cols(chi_test)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#compare-the-results",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#compare-the-results",
    "title": "Case Study on Transportation",
    "section": "Compare the results  ",
    "text": "Compare the results  \nSingle best versus the stack:\n\ncollect_metrics(cubist_res)\n#&gt; # A tibble: 1 Ã— 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard       0.670 Preprocessor1_Model1\n\nstack_pred %&gt;% mae(ridership, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       0.689"
  },
  {
    "objectID": "archive/2023-09-posit-conf/extras-transit-case-study.html#plot-the-test-set",
    "href": "archive/2023-09-posit-conf/extras-transit-case-study.html#plot-the-test-set",
    "title": "Case Study on Transportation",
    "section": "Plot the test set  ",
    "text": "Plot the test set  \n\n\nlibrary(probably)\ncubist_res %&gt;% \n  collect_predictions() %&gt;% \n  ggplot(aes(ridership, .pred)) + \n  geom_point(alpha = 1 / 2) + \n  geom_abline(lty = 2, col = \"green\") + \n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2023-09-posit-conf/index.html",
    "href": "archive/2023-09-posit-conf/index.html",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels offered at posit::conf(2023). The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. This website hosts the materials for both the Introduction to tidymodels and Advanced tidymodels courses.\nIntroduction to tidymodels will teach you core tidymodels packages and their uses: data splitting/resampling with rsample, model fitting with parsnip, measuring model performance with yardstick, and basic pre-processing with recipes. Time permitting, youâ€™ll be introduced to model optimization using the tune package. Youâ€™ll learn tidymodels syntax as well as the process of predictive modeling for tabular data.\nAdvanced tidymodels will teach you about model optimization using the tune and finetune packages, including racing and iterative methods. Youâ€™ll be able to do more sophisticated feature engineering with recipes. Time permitting, model ensembles via stacking will be introduced. This course is focused on the analysis of tabular data and does not include deep learning methods."
  },
  {
    "objectID": "archive/2023-09-posit-conf/index.html#welcome",
    "href": "archive/2023-09-posit-conf/index.html#welcome",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels offered at posit::conf(2023). The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. This website hosts the materials for both the Introduction to tidymodels and Advanced tidymodels courses.\nIntroduction to tidymodels will teach you core tidymodels packages and their uses: data splitting/resampling with rsample, model fitting with parsnip, measuring model performance with yardstick, and basic pre-processing with recipes. Time permitting, youâ€™ll be introduced to model optimization using the tune package. Youâ€™ll learn tidymodels syntax as well as the process of predictive modeling for tabular data.\nAdvanced tidymodels will teach you about model optimization using the tune and finetune packages, including racing and iterative methods. Youâ€™ll be able to do more sophisticated feature engineering with recipes. Time permitting, model ensembles via stacking will be introduced. This course is focused on the analysis of tabular data and does not include deep learning methods."
  },
  {
    "objectID": "archive/2023-09-posit-conf/index.html#is-this-workshop-for-me",
    "href": "archive/2023-09-posit-conf/index.html#is-this-workshop-for-me",
    "title": "Machine learning with tidymodels",
    "section": "Is this workshop for me? ",
    "text": "Is this workshop for me? \nDepending on your background, one of Introduction to tidymodels or Advanced tidymodels might serve you better than the other.\n\nIntroduction to tidymodels\nThis workshop is for you if you:\n\nare comfortable using tidyverse packages to read data into R, transform and reshape data, and make a variety of graphs, and\nhave had some exposure to basic statistical concepts such as linear models, residuals, etc.\n\nIntermediate or expert familiarity with modeling or machine learning is not required. Interested students who have intermediate or expert familiarity with modeling or machine learning may be interested in the Advanced tidymodels workshop.\n\n\nAdvanced tidymodels\nThis workshop is for you if you:\n\nhave the prerequisite skills listed for the Introduction to tidymodels workshops,\nhave used tidymodels packages like recipes, rsample, and parsnip, and\nhave some experience with evaluating statistical models using resampling techniques like v-fold cross-validation or the bootstrap.\n\nParticipants who are new to tidymodels or machine learning will benefit from taking the Introduction to tidymodels workshop before joining this one. Participants who have completed the â€œIntroduction to tidymodelsâ€ workshop will be well-prepared for this course."
  },
  {
    "objectID": "archive/2023-09-posit-conf/index.html#preparation",
    "href": "archive/2023-09-posit-conf/index.html#preparation",
    "title": "Machine learning with tidymodels",
    "section": "Preparation",
    "text": "Preparation\nThe process to set up your computer for either workshop will look the same. Please join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2022.02), available at https://posit.co/download/rstudio-desktop/\nThe following R packages, which you can install from the R console:\n\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"bonsai\", \"doParallel\", \"embed\", \"finetune\", \"lightgbm\", \"lme4\",\n    \"plumber\", \"probably\", \"ranger\", \"rpart\", \"rpart.plot\", \"rules\",\n    \"splines2\", \"stacks\", \"text2vec\", \"textrecipes\", \"tidymodels\", \n    \"vetiver\", \"remotes\")\n\ninstall.packages(pkgs)\n\nIf youâ€™re a Windows user and encounter an error message during installation noting a missing Rtools installation, install Rtools using the installer linked here."
  },
  {
    "objectID": "archive/2023-09-posit-conf/index.html#slides",
    "href": "archive/2023-09-posit-conf/index.html#slides",
    "title": "Machine learning with tidymodels",
    "section": "Slides",
    "text": "Slides\nThese slides are designed to use with live teaching and are published for workshop participantsâ€™ convenience. There are not meant as standalone learning materials. For that, we recommend tidymodels.org and Tidy Modeling with R.\n\nIntroduction to tidymodels\n\n01: Introduction\n02: Your data budget\n03: What makes a model?\n04: Evaluating models\n05: Tuning models\n06: Wrapping up\n\n\n\nAdvanced tidymodels\n\n01: Introduction\n02: Feature engineering using recipes\n03: Tuning hyperparameters (grid search)\n04: Grid search via racing\n05: Iterative search\n06: Wrapping up\n\n\n\nExtra content (time permitting)\n\nIntro: Time-based splitting\nIntro: Using workflowsets\nIntro: Using recipes\nAdvanced: Transit case study (includes stacking)\nAdvanced: Effect encoding\nAdvanced: Model deployment\n\nThereâ€™s also a page for slide annotations; these are extra notes for selected slides."
  },
  {
    "objectID": "archive/2023-09-posit-conf/index.html#code",
    "href": "archive/2023-09-posit-conf/index.html#code",
    "title": "Machine learning with tidymodels",
    "section": "Code",
    "text": "Code\nQuarto files for working along are available on GitHub. (Donâ€™t worry if you havenâ€™t used Quarto before; it will feel familiar to R Markdown users.)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/index.html#past-workshops",
    "href": "archive/2023-09-posit-conf/index.html#past-workshops",
    "title": "Machine learning with tidymodels",
    "section": "Past workshops",
    "text": "Past workshops\n\nJuly 2022 at rstudio::conf()\nAugust 2022 in Reykjavik\nJuly 2023 at the New York R Conference"
  },
  {
    "objectID": "archive/2023-09-posit-conf/index.html#acknowledgments",
    "href": "archive/2023-09-posit-conf/index.html#acknowledgments",
    "title": "Machine learning with tidymodels",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including the slides, is made with Quarto. Please submit an issue on the GitHub repo for this workshop if you find something that could be fixed or improved."
  },
  {
    "objectID": "archive/2023-09-posit-conf/index.html#reuse-and-licensing",
    "href": "archive/2023-09-posit-conf/index.html#reuse-and-licensing",
    "title": "Machine learning with tidymodels",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e.Â not an original creation and reused from another source), these educational materials are licensed under Creative Commons Attribution CC BY-SA 4.0."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-on-chicago-taxi-trips",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-on-chicago-taxi-trips",
    "title": "2 - Your data budget",
    "section": "Data on Chicago taxi trips",
    "text": "Data on Chicago taxi trips\n\n\n\nThe city of Chicago releases anonymized trip-level data on taxi trips in the city.\nWe pulled a sample of 10,000 rides occurring in early 2022.\nType ?taxi to learn more about this dataset, including references.\n\n\n\n\n\nCredit: https://www.svgrepo.com/svg/8322/taxi"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-on-chicago-taxi-trips-1",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-on-chicago-taxi-trips-1",
    "title": "2 - Your data budget",
    "section": "Data on Chicago taxi trips",
    "text": "Data on Chicago taxi trips\n\n\n\nN = 10,000\nA nominal outcome, tip, with levels \"yes\" and \"no\"\nSeveral nominal variables like pickup & dropoff location, taxi ID, and payment type.\nSeveral numeric variables like trip length and fare subtotals.\n\n\n\n\n\nCredit: https://unsplash.com/photos/7_r85l4eht8\n\n\nâ€œFare subtotalsâ€ refers to the fare itself, tax, tolls, tip amount.\nActual variables in our data:\ntip: Whether the rider left a tip. A factor with levels â€œyesâ€ and â€œnoâ€.\ndistance: The trip distance, in odometer miles.\ncompany: The taxi company, as a factor. Companies that occurred few times were binned as â€œotherâ€.\nlocal: Whether the trip started in the same community area as it began. See the source data for community area values.\ndow: The day of the week in which the trip began, as a factor.\nmonth: The month in which the trip began, as a factor.\nhour: The hour of the day in which the trip began, as a numeric."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#checklist-for-predictors",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#checklist-for-predictors",
    "title": "2 - Your data budget",
    "section": "Checklist for predictors",
    "text": "Checklist for predictors\n\nIs it ethical to use this variable? (Or even legal?)\nWill this variable be available at prediction time?\nDoes this variable contribute to explainability?"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-on-chicago-taxi-trips-2",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-on-chicago-taxi-trips-2",
    "title": "2 - Your data budget",
    "section": "Data on Chicago taxi trips",
    "text": "Data on Chicago taxi trips\n\nlibrary(tidymodels)\n\ntaxi\n#&gt; # A tibble: 10,000 Ã— 7\n#&gt;    tip   distance company                      local dow   month  hour\n#&gt;    &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;                        &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n#&gt;  1 yes      17.2  Chicago Independents         no    Thu   Feb      16\n#&gt;  2 yes       0.88 City Service                 yes   Thu   Mar       8\n#&gt;  3 yes      18.1  other                        no    Mon   Feb      18\n#&gt;  4 yes      20.7  Chicago Independents         no    Mon   Apr       8\n#&gt;  5 yes      12.2  Chicago Independents         no    Sun   Mar      21\n#&gt;  6 yes       0.94 Sun Taxi                     yes   Sat   Apr      23\n#&gt;  7 yes      17.5  Flash Cab                    no    Fri   Mar      12\n#&gt;  8 yes      17.7  other                        no    Sun   Jan       6\n#&gt;  9 yes       1.85 Taxicab Insurance Agency Llc no    Fri   Apr      12\n#&gt; 10 yes       1.47 City Service                 no    Tue   Mar      14\n#&gt; # â„¹ 9,990 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-splitting-and-spending",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-splitting-and-spending",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nFor machine learning, we typically split data into training and test sets:\n\n\nThe training set is used to estimate model parameters.\nThe test set is used to find an independent assessment of model performance.\n\n\n\nDo not ğŸš« use the test set during training."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-splitting-and-spending-1",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-splitting-and-spending-1",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-splitting-and-spending-2",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-splitting-and-spending-2",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\n\nSpending too much data in training prevents us from computing a good assessment of predictive performance.\n\n\n\nSpending too much data in testing prevents us from computing a good estimate of model parameters."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#your-turn",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#your-turn",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nWhen is a good time to split your data?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#the-initial-split",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#the-initial-split",
    "title": "2 - Your data budget",
    "section": "The initial split ",
    "text": "The initial split \n\nset.seed(123)\ntaxi_split &lt;- initial_split(taxi)\ntaxi_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;7500/2500/10000&gt;\n\n\nHow much data in training vs testing? This function uses a good default, but this depends on your specific goal/data We will talk about more powerful ways of splitting, like stratification, later"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#what-is-set.seed",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#what-is-set.seed",
    "title": "2 - Your data budget",
    "section": "What is set.seed()?",
    "text": "What is set.seed()?\nTo create that split of the data, R generates â€œpseudo-randomâ€ numbers: while they are made to behave like random numbers, their generation is deterministic give a â€œseedâ€.\nThis allows us to reproduce results by setting that seed.\nWhich seed you pick doesnâ€™t matter, as long as you donâ€™t try a bunch of seeds and pick the one that gives you the best performance."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#accessing-the-data",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#accessing-the-data",
    "title": "2 - Your data budget",
    "section": "Accessing the data ",
    "text": "Accessing the data \n\ntaxi_train &lt;- training(taxi_split)\ntaxi_test &lt;- testing(taxi_split)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#the-training-set",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#the-training-set",
    "title": "2 - Your data budget",
    "section": "The training set",
    "text": "The training set\n\ntaxi_train\n#&gt; # A tibble: 7,500 Ã— 7\n#&gt;    tip   distance company                   local dow   month  hour\n#&gt;    &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;                     &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n#&gt;  1 yes       0.7  Taxi Affiliation Services yes   Tue   Mar      18\n#&gt;  2 yes       0.99 Sun Taxi                  yes   Tue   Jan       8\n#&gt;  3 yes       1.78 other                     no    Sat   Mar      22\n#&gt;  4 yes       0    Taxi Affiliation Services yes   Wed   Apr      15\n#&gt;  5 yes       0    Taxi Affiliation Services no    Sun   Jan      21\n#&gt;  6 yes       2.3  other                     no    Sat   Apr      21\n#&gt;  7 yes       6.35 Sun Taxi                  no    Wed   Mar      16\n#&gt;  8 yes       2.79 other                     no    Sun   Feb      14\n#&gt;  9 yes      16.6  other                     no    Sun   Apr      18\n#&gt; 10 yes       0.02 Chicago Independents      yes   Sun   Apr      15\n#&gt; # â„¹ 7,490 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#the-test-set",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#the-test-set",
    "title": "2 - Your data budget",
    "section": "The test set ",
    "text": "The test set \nğŸ™ˆ\n\nThere are 2500 rows and 7 columns in the test set."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#your-turn-1",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#your-turn-1",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nSplit your data so 20% is held out for the test set.\nTry out different values in set.seed() to see how the results change.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-splitting-and-spending-3",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#data-splitting-and-spending-3",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\ntaxi_split &lt;- initial_split(taxi, prop = 0.8)\ntaxi_train &lt;- training(taxi_split)\ntaxi_test &lt;- testing(taxi_split)\n\nnrow(taxi_train)\n#&gt; [1] 8000\nnrow(taxi_test)\n#&gt; [1] 2000"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#validation-set",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#validation-set",
    "title": "2 - Your data budget",
    "section": "Validation set",
    "text": "Validation set\n\nset.seed(123)\ninitial_validation_split(taxi, prop = c(0.6, 0.2))\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;6000/2000/2000/10000&gt;"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#your-turn-2",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#your-turn-2",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nExplore the taxi_train data on your own!\n\nWhatâ€™s the distribution of the outcome, tip?\nWhatâ€™s the distribution of numeric variables like distance?\nHow does tip differ across the categorical variables?\n\n\n\n\nâˆ’+\n08:00\n\n\n\n\nMake a plot or summary and then share with neighbor"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#section-2",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#section-2",
    "title": "2 - Your data budget",
    "section": "",
    "text": "taxi_train %&gt;% \n  ggplot(aes(x = tip)) +\n  geom_bar()"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#section-3",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#section-3",
    "title": "2 - Your data budget",
    "section": "",
    "text": "taxi_train %&gt;% \n  ggplot(aes(x = tip, fill = local)) +\n  geom_bar() +\n  scale_fill_viridis_d(end = .5)"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#section-4",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#section-4",
    "title": "2 - Your data budget",
    "section": "",
    "text": "taxi_train %&gt;% \n  ggplot(aes(x = hour, fill = tip)) +\n  geom_bar()"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#section-5",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#section-5",
    "title": "2 - Your data budget",
    "section": "",
    "text": "taxi_train %&gt;% \n  ggplot(aes(x = hour, fill = tip)) +\n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#section-6",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#section-6",
    "title": "2 - Your data budget",
    "section": "",
    "text": "taxi_train %&gt;% \n  ggplot(aes(x = distance)) +\n  geom_histogram(bins = 100) +\n  facet_grid(vars(tip))"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#section-7",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#section-7",
    "title": "2 - Your data budget",
    "section": "",
    "text": "Stratified sampling would split within response values\n\nBased on our EDA, we know that the source data contains fewer \"no\" tip values than \"yes\". We want to make sure we allot equal proportions of those responses so that both the training and testing data have enough of each to give accurate estimates."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#stratification",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#stratification",
    "title": "2 - Your data budget",
    "section": "Stratification",
    "text": "Stratification\nUse strata = tip\n\nset.seed(123)\ntaxi_split &lt;- initial_split(taxi, prop = 0.8, strata = tip)\ntaxi_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;8000/2000/10000&gt;"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#stratification-1",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#stratification-1",
    "title": "2 - Your data budget",
    "section": "Stratification",
    "text": "Stratification\nStratification often helps, with very little downside"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-02-data-budget.html#the-whole-game---status-update",
    "href": "archive/2023-09-posit-conf/intro-02-data-budget.html#the-whole-game---status-update",
    "title": "2 - Your data budget",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#looking-at-predictions",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#looking-at-predictions",
    "title": "4 - Evaluating models",
    "section": "Looking at predictions",
    "text": "Looking at predictions\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  relocate(tip, .pred_class, .pred_yes, .pred_no)\n#&gt; # A tibble: 8,000 Ã— 10\n#&gt;    tip   .pred_class .pred_yes .pred_no distance company local dow   month  hour\n#&gt;    &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n#&gt;  1 yes   yes             0.967   0.0333    17.2  Chicagâ€¦ no    Thu   Feb      16\n#&gt;  2 yes   yes             0.935   0.0646     0.88 City Sâ€¦ yes   Thu   Mar       8\n#&gt;  3 yes   yes             0.967   0.0333    18.1  other   no    Mon   Feb      18\n#&gt;  4 yes   yes             0.949   0.0507    12.2  Chicagâ€¦ no    Sun   Mar      21\n#&gt;  5 yes   yes             0.821   0.179      0.94 Sun Taâ€¦ yes   Sat   Apr      23\n#&gt;  6 yes   yes             0.967   0.0333    17.5  Flash â€¦ no    Fri   Mar      12\n#&gt;  7 yes   yes             0.967   0.0333    17.7  other   no    Sun   Jan       6\n#&gt;  8 yes   yes             0.938   0.0616     1.85 Taxicaâ€¦ no    Fri   Apr      12\n#&gt;  9 yes   yes             0.938   0.0616     0.53 Sun Taâ€¦ no    Tue   Mar      18\n#&gt; 10 yes   yes             0.931   0.0694     6.65 Taxicaâ€¦ no    Sun   Apr      11\n#&gt; # â„¹ 7,990 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#confusion-matrix",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#confusion-matrix",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#confusion-matrix-1",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#confusion-matrix-1",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix \n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  conf_mat(truth = tip, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction  yes   no\n#&gt;        yes 7341  536\n#&gt;        no    43   80"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#confusion-matrix-2",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#confusion-matrix-2",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix \n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  conf_mat(truth = tip, estimate = .pred_class) %&gt;%\n  autoplot(type = \"heatmap\")"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  accuracy(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.928"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-accuracy",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-accuracy",
    "title": "4 - Evaluating models",
    "section": "Dangers of accuracy ",
    "text": "Dangers of accuracy \nWe need to be careful of using accuracy() since it can give â€œgoodâ€ performance by only predicting one way with imbalanced data\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  mutate(.pred_class = factor(\"yes\", levels = c(\"yes\", \"no\"))) %&gt;%\n  accuracy(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.923"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-1",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  sensitivity(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.994"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-2",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  sensitivity(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.994\n\n\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  specificity(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 specificity binary         0.130"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-3",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \nWe can use metric_set() to combine multiple calculations into one\n\ntaxi_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  taxi_metrics(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy    binary         0.928\n#&gt; 2 specificity binary         0.130\n#&gt; 3 sensitivity binary         0.994"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-4",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-4",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\ntaxi_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\naugment(taxi_fit, new_data = taxi_train) %&gt;%\n  group_by(local) %&gt;%\n  taxi_metrics(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 6 Ã— 4\n#&gt;   local .metric     .estimator .estimate\n#&gt;   &lt;fct&gt; &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 yes   accuracy    binary         0.898\n#&gt; 2 no    accuracy    binary         0.935\n#&gt; 3 yes   specificity binary         0.169\n#&gt; 4 no    specificity binary         0.116\n#&gt; 5 yes   sensitivity binary         0.987\n#&gt; 6 no    sensitivity binary         0.996"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#two-class-data",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#two-class-data",
    "title": "4 - Evaluating models",
    "section": "Two class data",
    "text": "Two class data\nThese metrics assume that we know the threshold for converting â€œsoftâ€ probability predictions into â€œhardâ€ class predictions.\n\nIs a 50% threshold good?\nWhat happens if we say that we need to be 80% sure to declare an event?\n\nsensitivity â¬‡ï¸, specificity â¬†ï¸\n\n\n\nWhat happens for a 20% threshold?\n\nsensitivity â¬†ï¸, specificity â¬‡ï¸"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#varying-the-threshold",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#varying-the-threshold",
    "title": "4 - Evaluating models",
    "section": "Varying the threshold",
    "text": "Varying the threshold"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#roc-curves",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#roc-curves",
    "title": "4 - Evaluating models",
    "section": "ROC curves",
    "text": "ROC curves\nTo make an ROC (receiver operator characteristic) curve, we:\n\ncalculate the sensitivity and specificity for all possible thresholds\nplot false positive rate (x-axis) versus true positive rate (y-axis)\n\ngiven that sensitivity is the true positive rate, and specificity is the true negative rate. Hence 1 - specificity is the false positive rate.\n\nWe can use the area under the ROC curve as a classification metric:\n\nROC AUC = 1 ğŸ’¯\nROC AUC = 1/2 ğŸ˜¢\n\n\nROC curves are insensitive to class imbalance."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#roc-curves-1",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#roc-curves-1",
    "title": "4 - Evaluating models",
    "section": "ROC curves ",
    "text": "ROC curves \n\n# Assumes _first_ factor level is event; there are options to change that\naugment(taxi_fit, new_data = taxi_train) %&gt;% \n  roc_curve(truth = tip, .pred_yes) %&gt;%\n  slice(1, 20, 50)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .threshold specificity sensitivity\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1   -Inf           0         1      \n#&gt; 2      0.783       0.209     0.981  \n#&gt; 3      1           1         0.00135\n\naugment(taxi_fit, new_data = taxi_train) %&gt;% \n  roc_auc(truth = tip, .pred_yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.691"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#roc-curve-plot",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#roc-curve-plot",
    "title": "4 - Evaluating models",
    "section": "ROC curve plot ",
    "text": "ROC curve plot \n\n\naugment(taxi_fit, new_data = taxi_train) %&gt;% \n  roc_curve(truth = tip, .pred_yes) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#your-turn",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#your-turn",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCompute and plot an ROC curve for your current model.\nWhat data are being used for this ROC curve plot?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-1",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-1",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸",
    "text": "Dangers of overfitting âš ï¸"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-2",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-2",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸",
    "text": "Dangers of overfitting âš ï¸"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-3",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-3",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\ntaxi_fit %&gt;%\n  augment(taxi_train)\n#&gt; # A tibble: 8,000 Ã— 10\n#&gt;    tip   distance company local dow   month  hour .pred_class .pred_yes .pred_no\n#&gt;    &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 yes      17.2  Chicagâ€¦ no    Thu   Feb      16 yes             0.967   0.0333\n#&gt;  2 yes       0.88 City Sâ€¦ yes   Thu   Mar       8 yes             0.935   0.0646\n#&gt;  3 yes      18.1  other   no    Mon   Feb      18 yes             0.967   0.0333\n#&gt;  4 yes      12.2  Chicagâ€¦ no    Sun   Mar      21 yes             0.949   0.0507\n#&gt;  5 yes       0.94 Sun Taâ€¦ yes   Sat   Apr      23 yes             0.821   0.179 \n#&gt;  6 yes      17.5  Flash â€¦ no    Fri   Mar      12 yes             0.967   0.0333\n#&gt;  7 yes      17.7  other   no    Sun   Jan       6 yes             0.967   0.0333\n#&gt;  8 yes       1.85 Taxicaâ€¦ no    Fri   Apr      12 yes             0.938   0.0616\n#&gt;  9 yes       0.53 Sun Taâ€¦ no    Tue   Mar      18 yes             0.938   0.0616\n#&gt; 10 yes       6.65 Taxicaâ€¦ no    Sun   Apr      11 yes             0.931   0.0694\n#&gt; # â„¹ 7,990 more rows\n\nWe call this â€œresubstitutionâ€ or â€œrepredicting the training setâ€"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-4",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-4",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\ntaxi_fit %&gt;%\n  augment(taxi_train) %&gt;%\n  accuracy(tip, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.928\n\nWe call this a â€œresubstitution estimateâ€"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-5",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-5",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\ntaxi_fit %&gt;%\n  augment(taxi_train) %&gt;%\n  accuracy(tip, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.928"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-6",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-6",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\ntaxi_fit %&gt;%\n  augment(taxi_train) %&gt;%\n  accuracy(tip, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.928\n\n\n\ntaxi_fit %&gt;%\n  augment(taxi_test) %&gt;%\n  accuracy(tip, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.908\n\n\n\nâš ï¸ Remember that weâ€™re demonstrating overfitting\n\n\nâš ï¸ Donâ€™t use the test set until the end of your modeling analysis"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#your-turn-1",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#your-turn-1",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse augment() and a metric function to compute a classification metric like brier_class().\nCompute the metrics for both training and testing data to demonstrate overfitting!\nNotice the evidence of overfitting! âš ï¸\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-7",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-7",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\ntaxi_fit %&gt;%\n  augment(taxi_train) %&gt;%\n  brier_class(tip, .pred_yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary        0.0632\n\n\n\ntaxi_fit %&gt;%\n  augment(taxi_test) %&gt;%\n  brier_class(tip, .pred_yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary        0.0782\n\n\n\nWhat if we want to compare more models?\n\n\nAnd/or more model configurations?\n\n\nAnd we want to understand if these are important differences?"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation-1",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation-1",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#your-turn-2",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#your-turn-2",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nIf we use 10 folds, what percent of the training data\n\nends up in analysis\nends up in assessment\n\nfor each fold?\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation-2",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation-2",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(taxi_train) # v = 10 is default\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [7200/800]&gt; Fold01\n#&gt;  2 &lt;split [7200/800]&gt; Fold02\n#&gt;  3 &lt;split [7200/800]&gt; Fold03\n#&gt;  4 &lt;split [7200/800]&gt; Fold04\n#&gt;  5 &lt;split [7200/800]&gt; Fold05\n#&gt;  6 &lt;split [7200/800]&gt; Fold06\n#&gt;  7 &lt;split [7200/800]&gt; Fold07\n#&gt;  8 &lt;split [7200/800]&gt; Fold08\n#&gt;  9 &lt;split [7200/800]&gt; Fold09\n#&gt; 10 &lt;split [7200/800]&gt; Fold10"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation-3",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation-3",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWhat is in this?\n\ntaxi_folds &lt;- vfold_cv(taxi_train)\ntaxi_folds$splits[1:3]\n#&gt; [[1]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;7200/800/8000&gt;\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;7200/800/8000&gt;\n#&gt; \n#&gt; [[3]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;7200/800/8000&gt;\n\n\nTalk about a list column, storing non-atomic types in dataframe"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation-4",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation-4",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(taxi_train, v = 5)\n#&gt; #  5-fold cross-validation \n#&gt; # A tibble: 5 Ã— 2\n#&gt;   splits              id   \n#&gt;   &lt;list&gt;              &lt;chr&gt;\n#&gt; 1 &lt;split [6400/1600]&gt; Fold1\n#&gt; 2 &lt;split [6400/1600]&gt; Fold2\n#&gt; 3 &lt;split [6400/1600]&gt; Fold3\n#&gt; 4 &lt;split [6400/1600]&gt; Fold4\n#&gt; 5 &lt;split [6400/1600]&gt; Fold5"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation-5",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation-5",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(taxi_train, strata = tip)\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [7200/800]&gt; Fold01\n#&gt;  2 &lt;split [7200/800]&gt; Fold02\n#&gt;  3 &lt;split [7200/800]&gt; Fold03\n#&gt;  4 &lt;split [7200/800]&gt; Fold04\n#&gt;  5 &lt;split [7200/800]&gt; Fold05\n#&gt;  6 &lt;split [7200/800]&gt; Fold06\n#&gt;  7 &lt;split [7200/800]&gt; Fold07\n#&gt;  8 &lt;split [7200/800]&gt; Fold08\n#&gt;  9 &lt;split [7200/800]&gt; Fold09\n#&gt; 10 &lt;split [7200/800]&gt; Fold10\n\n\nStratification often helps, with very little downside"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation-6",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#cross-validation-6",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWeâ€™ll use this setup:\n\nset.seed(123)\ntaxi_folds &lt;- vfold_cv(taxi_train, v = 10, strata = tip)\ntaxi_folds\n#&gt; #  10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [7200/800]&gt; Fold01\n#&gt;  2 &lt;split [7200/800]&gt; Fold02\n#&gt;  3 &lt;split [7200/800]&gt; Fold03\n#&gt;  4 &lt;split [7200/800]&gt; Fold04\n#&gt;  5 &lt;split [7200/800]&gt; Fold05\n#&gt;  6 &lt;split [7200/800]&gt; Fold06\n#&gt;  7 &lt;split [7200/800]&gt; Fold07\n#&gt;  8 &lt;split [7200/800]&gt; Fold08\n#&gt;  9 &lt;split [7200/800]&gt; Fold09\n#&gt; 10 &lt;split [7200/800]&gt; Fold10\n\n\nSet the seed when creating resamples"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#fit-our-model-to-the-resamples",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#fit-our-model-to-the-resamples",
    "title": "4 - Evaluating models",
    "section": "Fit our model to the resamples",
    "text": "Fit our model to the resamples\n\ntaxi_res &lt;- fit_resamples(taxi_wflow, taxi_folds)\ntaxi_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 4\n#&gt;    splits             id     .metrics         .notes          \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n#&gt;  1 &lt;split [7200/800]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  2 &lt;split [7200/800]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  3 &lt;split [7200/800]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  4 &lt;split [7200/800]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  5 &lt;split [7200/800]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  6 &lt;split [7200/800]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  7 &lt;split [7200/800]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  8 &lt;split [7200/800]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  9 &lt;split [7200/800]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt; 10 &lt;split [7200/800]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\ntaxi_res %&gt;%\n  collect_metrics()\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.915    10 0.00309 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.624    10 0.0105  Preprocessor1_Model1\n\n\ncollect_metrics() is one of a suite of collect_*() functions that can be used to work with columns of tuning results. Most columns in a tuning result prefixed with . have a corresponding collect_*() function with options for common summaries.\n\n\nWe can reliably measure performance using only the training data ğŸ‰"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#comparing-metrics",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#comparing-metrics",
    "title": "4 - Evaluating models",
    "section": "Comparing metrics ",
    "text": "Comparing metrics \nHow do the metrics from resampling compare to the metrics from training and testing?\n\n\n\ntaxi_res %&gt;%\n  collect_metrics() %&gt;% \n  select(.metric, mean, n)\n#&gt; # A tibble: 2 Ã— 3\n#&gt;   .metric   mean     n\n#&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 accuracy 0.915    10\n#&gt; 2 roc_auc  0.624    10\n\n\nThe ROC AUC previously was\n\n0.69 for the training set\n0.64 for test set\n\n\n\nRemember that:\nâš ï¸ the training set gives you overly optimistic metrics\nâš ï¸ the test set is precious"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-1",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nctrl_taxi &lt;- control_resamples(save_pred = TRUE)\ntaxi_res &lt;- fit_resamples(taxi_wflow, taxi_folds, control = ctrl_taxi)\n\ntaxi_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [7200/800]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [7200/800]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [7200/800]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [7200/800]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [7200/800]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [7200/800]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [7200/800]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [7200/800]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [7200/800]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [7200/800]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-2",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\ntaxi_preds &lt;- collect_predictions(taxi_res)\ntaxi_preds\n#&gt; # A tibble: 8,000 Ã— 7\n#&gt;    id     .pred_yes .pred_no  .row .pred_class tip   .config             \n#&gt;    &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt; &lt;chr&gt;               \n#&gt;  1 Fold01     0.938   0.0615    14 yes         yes   Preprocessor1_Model1\n#&gt;  2 Fold01     0.946   0.0544    19 yes         yes   Preprocessor1_Model1\n#&gt;  3 Fold01     0.973   0.0269    33 yes         yes   Preprocessor1_Model1\n#&gt;  4 Fold01     0.903   0.0971    43 yes         yes   Preprocessor1_Model1\n#&gt;  5 Fold01     0.973   0.0269    74 yes         yes   Preprocessor1_Model1\n#&gt;  6 Fold01     0.903   0.0971   103 yes         yes   Preprocessor1_Model1\n#&gt;  7 Fold01     0.915   0.0851   104 yes         no    Preprocessor1_Model1\n#&gt;  8 Fold01     0.903   0.0971   124 yes         yes   Preprocessor1_Model1\n#&gt;  9 Fold01     0.667   0.333    126 yes         yes   Preprocessor1_Model1\n#&gt; 10 Fold01     0.949   0.0510   128 yes         yes   Preprocessor1_Model1\n#&gt; # â„¹ 7,990 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-3",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\ntaxi_preds %&gt;% \n  group_by(id) %&gt;%\n  taxi_metrics(truth = tip, estimate = .pred_class)\n#&gt; # A tibble: 30 Ã— 4\n#&gt;    id     .metric  .estimator .estimate\n#&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt;  1 Fold01 accuracy binary         0.905\n#&gt;  2 Fold02 accuracy binary         0.925\n#&gt;  3 Fold03 accuracy binary         0.926\n#&gt;  4 Fold04 accuracy binary         0.915\n#&gt;  5 Fold05 accuracy binary         0.902\n#&gt;  6 Fold06 accuracy binary         0.912\n#&gt;  7 Fold07 accuracy binary         0.906\n#&gt;  8 Fold08 accuracy binary         0.91 \n#&gt;  9 Fold09 accuracy binary         0.918\n#&gt; 10 Fold10 accuracy binary         0.931\n#&gt; # â„¹ 20 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#where-are-the-fitted-models",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#where-are-the-fitted-models",
    "title": "4 - Evaluating models",
    "section": "Where are the fitted models? ",
    "text": "Where are the fitted models? \n\ntaxi_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [7200/800]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [7200/800]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [7200/800]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [7200/800]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [7200/800]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [7200/800]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [7200/800]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [7200/800]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [7200/800]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [7200/800]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;\n\n\nğŸ—‘ï¸"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#bootstrapping",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#bootstrapping",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#bootstrapping-1",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#bootstrapping-1",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(3214)\nbootstraps(taxi_train)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 25 Ã— 2\n#&gt;    splits              id         \n#&gt;    &lt;list&gt;              &lt;chr&gt;      \n#&gt;  1 &lt;split [8000/2902]&gt; Bootstrap01\n#&gt;  2 &lt;split [8000/2916]&gt; Bootstrap02\n#&gt;  3 &lt;split [8000/3004]&gt; Bootstrap03\n#&gt;  4 &lt;split [8000/2979]&gt; Bootstrap04\n#&gt;  5 &lt;split [8000/2961]&gt; Bootstrap05\n#&gt;  6 &lt;split [8000/2962]&gt; Bootstrap06\n#&gt;  7 &lt;split [8000/3026]&gt; Bootstrap07\n#&gt;  8 &lt;split [8000/2926]&gt; Bootstrap08\n#&gt;  9 &lt;split [8000/2972]&gt; Bootstrap09\n#&gt; 10 &lt;split [8000/2972]&gt; Bootstrap10\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#the-whole-game---status-update",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#the-whole-game---status-update",
    "title": "4 - Evaluating models",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#your-turn-3",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#your-turn-3",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCreate:\n\nMonte Carlo Cross-Validation sets\nvalidation set\n\n(use the reference guide to find the functions)\nDonâ€™t forget to set a seed when you resample!\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#monte-carlo-cross-validation",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#monte-carlo-cross-validation",
    "title": "4 - Evaluating models",
    "section": "Monte Carlo Cross-Validation ",
    "text": "Monte Carlo Cross-Validation \n\nset.seed(322)\nmc_cv(taxi_train, times = 10)\n#&gt; # Monte Carlo cross-validation (0.75/0.25) with 10 resamples  \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits              id        \n#&gt;    &lt;list&gt;              &lt;chr&gt;     \n#&gt;  1 &lt;split [6000/2000]&gt; Resample01\n#&gt;  2 &lt;split [6000/2000]&gt; Resample02\n#&gt;  3 &lt;split [6000/2000]&gt; Resample03\n#&gt;  4 &lt;split [6000/2000]&gt; Resample04\n#&gt;  5 &lt;split [6000/2000]&gt; Resample05\n#&gt;  6 &lt;split [6000/2000]&gt; Resample06\n#&gt;  7 &lt;split [6000/2000]&gt; Resample07\n#&gt;  8 &lt;split [6000/2000]&gt; Resample08\n#&gt;  9 &lt;split [6000/2000]&gt; Resample09\n#&gt; 10 &lt;split [6000/2000]&gt; Resample10"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#validation-set",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#validation-set",
    "title": "4 - Evaluating models",
    "section": "Validation set ",
    "text": "Validation set \n\nset.seed(853)\ntaxi_val_split &lt;- initial_validation_split(taxi, strata = tip)\nvalidation_set(taxi_val_split)\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   splits              id        \n#&gt;   &lt;list&gt;              &lt;chr&gt;     \n#&gt; 1 &lt;split [6000/2000]&gt; validation\n\n\nA validation set is just another type of resample"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#random-forest-1",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#random-forest-1",
    "title": "4 - Evaluating models",
    "section": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ",
    "text": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\n\nEnsemble many decision tree models\nAll the trees vote! ğŸ—³ï¸\nBootstrap aggregating + random predictor sampling\n\n\n\nOften works well without tuning hyperparameters (more on this in Advanced tidymodels!), as long as there are enough trees"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#create-a-random-forest-model",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#create-a-random-forest-model",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_spec &lt;- rand_forest(trees = 1000, mode = \"classification\")\nrf_spec\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#create-a-random-forest-model-1",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#create-a-random-forest-model-1",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_wflow &lt;- workflow(tip ~ ., rf_spec)\nrf_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; tip ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#your-turn-4",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#your-turn-4",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() and rf_wflow to:\n\nkeep predictions\ncompute metrics\n\n\n\n\nâˆ’+\n08:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-4",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-4",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nctrl_taxi &lt;- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\n\nset.seed(2)\nrf_res &lt;- fit_resamples(rf_wflow, taxi_folds, control = ctrl_taxi)\ncollect_metrics(rf_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric  .estimator  mean     n std_err .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary     0.923    10 0.00317 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary     0.616    10 0.0147  Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#the-whole-game---status-update-1",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#the-whole-game---status-update-1",
    "title": "4 - Evaluating models",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#the-final-fit",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#the-final-fit",
    "title": "4 - Evaluating models",
    "section": "The final fit ",
    "text": "The final fit \nSuppose that we are happy with our random forest model.\nLetâ€™s fit the model on the training set and verify our performance using the test set.\n\nWeâ€™ve shown you fit() and predict() (+ augment()) but there is a shortcut:\n\n# taxi_split has train + test info\nfinal_fit &lt;- last_fit(rf_wflow, taxi_split) \n\nfinal_fit\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits              id               .metrics .notes   .predictions .workflow \n#&gt;   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n#&gt; 1 &lt;split [8000/2000]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#what-is-in-final_fit",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#what-is-in-final_fit",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_metrics(final_fit)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric  .estimator .estimate .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary         0.914 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary         0.638 Preprocessor1_Model1\n\n\nThese are metrics computed with the test set"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#what-is-in-final_fit-1",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#what-is-in-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_predictions(final_fit)\n#&gt; # A tibble: 2,000 Ã— 7\n#&gt;    id               .pred_yes .pred_no  .row .pred_class tip   .config          \n#&gt;    &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt; &lt;chr&gt;            \n#&gt;  1 train/test split     0.957   0.0426     4 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  2 train/test split     0.938   0.0621    10 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  3 train/test split     0.958   0.0416    19 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  4 train/test split     0.894   0.106     23 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  5 train/test split     0.943   0.0573    28 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  6 train/test split     0.979   0.0213    34 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  7 train/test split     0.954   0.0463    35 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  8 train/test split     0.928   0.0722    38 yes         yes   Preprocessor1_Moâ€¦\n#&gt;  9 train/test split     0.985   0.0147    40 yes         yes   Preprocessor1_Moâ€¦\n#&gt; 10 train/test split     0.948   0.0523    42 yes         no    Preprocessor1_Moâ€¦\n#&gt; # â„¹ 1,990 more rows"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#what-is-in-final_fit-2",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#what-is-in-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\nextract_workflow(final_fit)\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; tip ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  1000 \n#&gt; Sample size:                      8000 \n#&gt; Number of independent variables:  6 \n#&gt; Mtry:                             2 \n#&gt; Target node size:                 10 \n#&gt; Variable importance mode:         none \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.07069778\n\n\nUse this for prediction on new data, like for deploying"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#the-whole-game",
    "href": "archive/2023-09-posit-conf/intro-04-evaluating-models.html#the-whole-game",
    "title": "4 - Evaluating models",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-06-wrapping-up.html#your-turn",
    "href": "archive/2023-09-posit-conf/intro-06-wrapping-up.html#your-turn",
    "title": "6 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-06-wrapping-up.html#resources-to-keep-learning",
    "href": "archive/2023-09-posit-conf/intro-06-wrapping-up.html#resources-to-keep-learning",
    "title": "6 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://smltar.com/\n\n\n\nFollow us on Mastodon and at the tidyverse blog for updates!"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-06-wrapping-up.html#feedback",
    "href": "archive/2023-09-posit-conf/intro-06-wrapping-up.html#feedback",
    "title": "6 - Wrapping up",
    "section": "Feedback",
    "text": "Feedback\n\nPlease go to http://pos.it/conf-workshop-survey. Your feedback is crucial!\nData from the survey informs curriculum and format decisions for future conf workshops, and we really appreciate you taking the time to provide it."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-time-splits.html#the-raw-taxi-data-set",
    "href": "archive/2023-09-posit-conf/intro-extra-time-splits.html#the-raw-taxi-data-set",
    "title": "Extras - Time-based data splitting",
    "section": "The raw taxi data set",
    "text": "The raw taxi data set\nWe prepared the data set specifically for this introductory workshop.\nIt looked similar to this:\n\nglimpse(taxi_raw)\n#&gt; Rows: 10,000\n#&gt; Columns: 24\n#&gt; $ trip_id                    &lt;chr&gt; \"3ac8d4412642a35e9b9a493285814d7983d5a159\",â€¦\n#&gt; $ taxi_id                    &lt;chr&gt; \"391317d70c5d06deec744062c4595dc1958b200fdaâ€¦\n#&gt; $ trip_start_timestamp       &lt;dttm&gt; 2023-06-10 18:45:00, 2023-05-21 21:30:00, â€¦\n#&gt; $ trip_end_timestamp         &lt;dttm&gt; 2023-06-10 19:30:00, 2023-05-21 21:45:00, â€¦\n#&gt; $ trip_seconds               &lt;dbl&gt; 3258, 839, 476, 2220, 1588, 2270, 1575, 267â€¦\n#&gt; $ trip_miles                 &lt;dbl&gt; 17.02, 2.16, 1.05, 17.40, 17.62, 16.36, 18.â€¦\n#&gt; $ pickup_census_tract        &lt;dbl&gt; 17031980000, 17031839100, 17031320100, 1703â€¦\n#&gt; $ dropoff_census_tract       &lt;dbl&gt; 17031081403, 17031081300, 17031081403, 1703â€¦\n#&gt; $ pickup_community_area      &lt;dbl&gt; 76, 32, 32, 76, 32, 76, 76, 32, 32, 33, 8, â€¦\n#&gt; $ dropoff_community_area     &lt;dbl&gt; 8, 8, 8, 32, 76, 8, 32, 76, 28, 32, 7, 33, â€¦\n#&gt; $ fare                       &lt;dbl&gt; 44.50, 10.00, 6.75, 45.00, 44.25, 41.25, 44â€¦\n#&gt; $ tips                       &lt;dbl&gt; 12.25, 4.00, 2.00, 9.90, 8.00, 9.15, 12.31,â€¦\n#&gt; $ tolls                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n#&gt; $ extras                     &lt;dbl&gt; 4.0, 1.0, 0.0, 4.0, 2.0, 4.0, 4.0, 0.0, 0.0â€¦\n#&gt; $ trip_total                 &lt;dbl&gt; 61.25, 15.50, 9.25, 58.90, 54.75, 54.90, 61â€¦\n#&gt; $ payment_type               &lt;chr&gt; \"Credit Card\", \"Credit Card\", \"Credit Card\"â€¦\n#&gt; $ company                    &lt;chr&gt; \"Taxicab Insurance Agency Llc\", \"Chicago Inâ€¦\n#&gt; $ pickup_centroid_latitude   &lt;dbl&gt; 41.97907, 41.88099, 41.88499, 41.97907, 41.â€¦\n#&gt; $ pickup_centroid_longitude  &lt;dbl&gt; -87.90304, -87.63275, -87.62099, -87.90304,â€¦\n#&gt; $ pickup_centroid_location   &lt;chr&gt; \"POINT (-87.9030396611 41.9790708201)\", \"POâ€¦\n#&gt; $ dropoff_centroid_latitude  &lt;dbl&gt; 41.89092, 41.89833, 41.89092, 41.87102, 41.â€¦\n#&gt; $ dropoff_centroid_longitude &lt;dbl&gt; -87.61887, -87.62076, -87.61887, -87.63141,â€¦\n#&gt; $ dropoff_centroid_location  &lt;chr&gt; \"POINT (-87.6188683546 41.8909220259)\", \"POâ€¦\n#&gt; $ tip                        &lt;fct&gt; yes, yes, yes, yes, yes, yes, yes, yes, yesâ€¦\n\n\ntrip_start_time has date and time rounded to â€œquarters of the hourâ€."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-time-splits.html#time-nature-of-the-data",
    "href": "archive/2023-09-posit-conf/intro-extra-time-splits.html#time-nature-of-the-data",
    "title": "Extras - Time-based data splitting",
    "section": "Time nature of the data",
    "text": "Time nature of the data\nWe assumed only the month, day of the week, and hour mattered and treated each observation as independent.\n\nIf the data have a strong time component, all your data splitting strategies should support the model in estimating temporal trends.\n\n\nThus, donâ€™t sample randomly because this breaks up the time component!"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-time-splits.html#splitting-with-time-component",
    "href": "archive/2023-09-posit-conf/intro-extra-time-splits.html#splitting-with-time-component",
    "title": "Extras - Time-based data splitting",
    "section": "Splitting with time component ",
    "text": "Splitting with time component \nThe more recent observations are assumed to be more similar to new data, so initial_time_split() puts them into the test set.\nThe function assumes that the data are already ordered.\n\ntaxi_raw &lt;- taxi_raw %&gt;%\n  arrange(trip_start_timestamp)\n\ntaxi_split &lt;- initial_time_split(taxi_raw, prop = 3 / 4)\ntaxi_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;7500/2500/10000&gt;\n\ntaxi_train &lt;- training(taxi_split)\ntaxi_test  &lt;- testing(taxi_split)\n\nnrow(taxi_train)\n#&gt; [1] 7500\n \nnrow(taxi_test)\n#&gt; [1] 2500"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-time-splits.html#time-series-resampling",
    "href": "archive/2023-09-posit-conf/intro-extra-time-splits.html#time-series-resampling",
    "title": "Extras - Time-based data splitting",
    "section": "Time series resampling",
    "text": "Time series resampling\nThe same idea also applies to resampling: the newer observations go into the assessment set.\nFor example:\n\nFold 1: Take the first X weeks of data as the analysis set, and the next 3 weeks as the assessment set.\nFold 2: Take weeks 2 to X + 1 as the analysis set, and the next 3 weeks as the assessment set.\nand so on"
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-time-splits.html#rolling-origin-forecast-resampling",
    "href": "archive/2023-09-posit-conf/intro-extra-time-splits.html#rolling-origin-forecast-resampling",
    "title": "Extras - Time-based data splitting",
    "section": "Rolling origin forecast resampling",
    "text": "Rolling origin forecast resampling\n\n\nThis image shows overlapping assessment sets. We will use non-overlapping data but it could be done either way."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-time-splits.html#times-series-resampling",
    "href": "archive/2023-09-posit-conf/intro-extra-time-splits.html#times-series-resampling",
    "title": "Extras - Time-based data splitting",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\ntaxi_rs &lt;-\n  taxi_train %&gt;%\n  sliding_period(\n    index = \"trip_start_timestamp\",  \n\n\n\n\n  )\n\nUse the trip_start_timestamp column to find the date data."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-time-splits.html#times-series-resampling-1",
    "href": "archive/2023-09-posit-conf/intro-extra-time-splits.html#times-series-resampling-1",
    "title": "Extras - Time-based data splitting",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\ntaxi_rs &lt;-\n  taxi_train %&gt;%\n  sliding_period(\n    index = \"trip_start_timestamp\",  \n    period = \"week\",\n\n\n\n  )\n\nOur units will be in weeks."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-time-splits.html#times-series-resampling-2",
    "href": "archive/2023-09-posit-conf/intro-extra-time-splits.html#times-series-resampling-2",
    "title": "Extras - Time-based data splitting",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\ntaxi_rs &lt;-\n  taxi_train %&gt;%\n  sliding_period(\n    index = \"trip_start_timestamp\",  \n    period = \"week\",\n    lookback = 8\n    \n    \n  )\n\nEvery analysis set has 8 weeks of data."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-time-splits.html#times-series-resampling-3",
    "href": "archive/2023-09-posit-conf/intro-extra-time-splits.html#times-series-resampling-3",
    "title": "Extras - Time-based data splitting",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\ntaxi_rs &lt;-\n  taxi_train %&gt;%\n  sliding_period(\n    index = \"trip_start_timestamp\",  \n    period = \"week\",\n    lookback = 8,\n    assess_stop = 3,\n\n  )\n\nEvery assessment set has 3 weeks of data."
  },
  {
    "objectID": "archive/2023-09-posit-conf/intro-extra-time-splits.html#times-series-resampling-4",
    "href": "archive/2023-09-posit-conf/intro-extra-time-splits.html#times-series-resampling-4",
    "title": "Extras - Time-based data splitting",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\ntaxi_rs &lt;-\n  taxi_train %&gt;%\n  sliding_period(\n    index = \"trip_start_timestamp\",  \n    period = \"week\",\n    lookback = 8,\n    assess_stop = 3,\n    step = 1\n  )\n\nIncrement by 1 week\n\n\ntaxi_rs$splits[[1]] %&gt;% assessment() %&gt;% pluck(\"trip_start_timestamp\") %&gt;% range()\n#&gt; [1] \"2023-03-02 05:15:00 UTC\" \"2023-03-22 22:00:00 UTC\"\n\ntaxi_rs$splits[[2]] %&gt;% assessment() %&gt;% pluck(\"trip_start_timestamp\") %&gt;% range()\n#&gt; [1] \"2023-03-09 07:00:00 UTC\" \"2023-03-29 21:15:00 UTC\"\n\ntaxi_rs$splits[[3]] %&gt;% assessment() %&gt;% pluck(\"trip_start_timestamp\") %&gt;% range()\n#&gt; [1] \"2023-03-16 06:30:00 UTC\" \"2023-04-05 23:45:00 UTC\""
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#normas-del-taller",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#normas-del-taller",
    "title": "1 - IntroducciÃ³n",
    "section": "Normas del taller",
    "text": "Normas del taller\n\nCÃ³digo de conducta: TODO-ADD-LATER"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#quien-eres",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#quien-eres",
    "title": "1 - IntroducciÃ³n",
    "section": "Quien eres?",
    "text": "Quien eres?\n\nSabe utilizar la â€œpipaâ€ de magritr (%&gt;%) o R (|&gt;)\nConoce las funciones de dplyr, tidyr y ggplot2\nEntiende conceptos estadÃ­sticos bÃ¡sicos\nNo necesitarÃ¡ ser experto en modelaje o aprendizaje automÃ¡tico"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#el-equipo-tidymodels",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#el-equipo-tidymodels",
    "title": "1 - IntroducciÃ³n",
    "section": "El equipo Tidymodels",
    "text": "El equipo Tidymodels\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\nAgradecimientos especiales para: Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill y DesirÃ©e De Leon"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#section-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#section-2",
    "title": "1 - IntroducciÃ³n",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#que-planeamos-hacer-en-este-taller",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#que-planeamos-hacer-en-este-taller",
    "title": "1 - IntroducciÃ³n",
    "section": "Que planeamos hacer en este taller",
    "text": "Que planeamos hacer en este taller\n\nTu presupuesto de datos\nLas partes de un modelo\nEvaluar modelos\nAfinar modelos\n\n\nThis workshop will well-prepare folks going on to the Advanced tidymodels workshop, which will cover feature engineering and much more on hyperparameter tuning."
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#section-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#section-3",
    "title": "1 - IntroducciÃ³n",
    "section": "",
    "text": "Salude a sus vecinos ğŸ‘‹"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#que-es-aprendizaje-automÃ¡tico",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#que-es-aprendizaje-automÃ¡tico",
    "title": "1 - IntroducciÃ³n",
    "section": "Â¿Que es aprendizaje automÃ¡tico?",
    "text": "Â¿Que es aprendizaje automÃ¡tico?"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#que-es-aprendizaje-automÃ¡tico-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#que-es-aprendizaje-automÃ¡tico-1",
    "title": "1 - IntroducciÃ³n",
    "section": "Â¿Que es aprendizaje automÃ¡tico?",
    "text": "Â¿Que es aprendizaje automÃ¡tico?\n\n\nÂ¿Este es tu sistema para aprendizaje automÃ¡tico?\nSÃ­, le tiramos los datos a este monton de algebra linear, y despuÃ©s tomamos las repuestas que salen\nÂ¿Y si las respuestas estÃ¡n equivocadas?\nLo empezamos a mezclar hasta que algo se ve como correcto\n\n\nhttps://xkcd.com/1838/"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#que-es-aprendizaje-automÃ¡tico-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#que-es-aprendizaje-automÃ¡tico-2",
    "title": "1 - IntroducciÃ³n",
    "section": "Â¿Que es aprendizaje automÃ¡tico?",
    "text": "Â¿Que es aprendizaje automÃ¡tico?\n\n\nhttps://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#que-es-aprendizaje-automÃ¡tico-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#que-es-aprendizaje-automÃ¡tico-3",
    "title": "1 - IntroducciÃ³n",
    "section": "Â¿Que es aprendizaje automÃ¡tico?",
    "text": "Â¿Que es aprendizaje automÃ¡tico?\n\n\n\n\n\nflowchart TB\n  au[Aprendizaje\\nAutomÃ¡tico\\nClÃ¡sico]\n  sp[Supervisada]\n  au--Datos numÃ©ricos o categÃ³ricos--&gt;sp\n  us[No supervisada]\n  au--Datos no estÃ¡n clasificados--&gt;us\n  cl[ClasificaciÃ³n]\n  sp--Predice categorÃ­a--&gt;cl\n  rs[RegresiÃ³n]\n  sp--Predice numero--&gt;rs\n  ag[AgrupaciÃ³n]\n  us--Divide por similitudes--&gt;ag\n  rd[ReducciÃ³n\\n de dimensiones]\n  us--Busca dependencias\\nescondidas--&gt;rd\n  as[AsociaciÃ³n]\n  us--Identifica secuencias--&gt;as\n  \n\n\n\n\n\n\n\nhttps://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#tu-turno",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#tu-turno",
    "title": "1 - IntroducciÃ³n",
    "section": "Tu turno",
    "text": "Tu turno\n\n\nÂ¿Como se relacionan las estadÃ­sticas y el aprendizaje automÃ¡tico?\nÂ¿Como se parecen? Â¿Cuales son sus diferencias?\n\n\n\nâˆ’+\n03:00\n\n\n\n\nthe â€œtwo culturesâ€\nmodel first vs.Â data first\ninference vs.Â prediction"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#que-es-tidymodels",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#que-es-tidymodels",
    "title": "1 - IntroducciÃ³n",
    "section": "Â¿Que es Tidymodels? ",
    "text": "Â¿Que es Tidymodels? \n\nlibrary(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.1.1 â”€â”€\n#&gt; âœ” broom        1.0.5      âœ” rsample      1.2.0 \n#&gt; âœ” dials        1.2.1      âœ” tibble       3.2.1 \n#&gt; âœ” dplyr        1.1.4      âœ” tidyr        1.3.1 \n#&gt; âœ” infer        1.0.6      âœ” tune         1.1.2 \n#&gt; âœ” modeldata    1.3.0      âœ” workflows    1.1.4 \n#&gt; âœ” parsnip      1.2.0      âœ” workflowsets 1.0.1 \n#&gt; âœ” purrr        1.0.2      âœ” yardstick    1.3.0 \n#&gt; âœ” recipes      1.0.10\n#&gt; â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\n#&gt; âœ– purrr::discard() masks scales::discard()\n#&gt; âœ– dplyr::filter()  masks stats::filter()\n#&gt; âœ– dplyr::lag()     masks stats::lag()\n#&gt; âœ– recipes::step()  masks stats::step()\n#&gt; â€¢ Learn how to get started at https://www.tidymodels.org/start/"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller",
    "title": "1 - IntroducciÃ³n",
    "section": "Expectativas del taller",
    "text": "Expectativas del taller\n\nEl â€œmapaâ€ de hoy\nProcesos bÃ¡sicos de los modelos predictivos\nVer la ingenieria de caraterÃ­sticas (feature engineering) y afinamiento como extenciones cÃ­clicas"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-1",
    "title": "1 - IntroducciÃ³n",
    "section": "Expectativas del taller",
    "text": "Expectativas del taller\n\n\n\n\n\nflowchart LR\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-2",
    "title": "1 - IntroducciÃ³n",
    "section": "Expectativas del taller",
    "text": "Expectativas del taller\n\n\n\n\n\nflowchart LR\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts\n  dt[Arbol de\\nDecisiÃ³n]\n  style dt fill:#FDF4E3,stroke:#666,color:#000\n  tr --&gt; dt"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-3",
    "title": "1 - IntroducciÃ³n",
    "section": "Expectativas del taller",
    "text": "Expectativas del taller\n\n\n\n\n\nflowchart LR\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts\n  lg[RegresiÃ³n\\nlogÃ­stica]\n  style lg fill:#FDF4E3,stroke:#666,color:#000\n  tr --&gt; lg\n  dt[Arbol de\\nDecisiÃ³n]\n  style dt fill:#FDF4E3,stroke:#666,color:#000\n  tr --&gt; dt\n  rf[Bosque\\nAleatorio]\n  style rf fill:#FDF4E3,stroke:#666,color:#000\n  tr --&gt; rf"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-4",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-4",
    "title": "1 - IntroducciÃ³n",
    "section": "Expectativas del taller",
    "text": "Expectativas del taller\n\n\n\n\n\nflowchart LR\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts\n  rs[Remuestreo]\n  style rs fill:#FDF4E3,stroke:#666,color:#000\n  tr --&gt; rs\n  lg[RegresiÃ³n\\nlogÃ­stica]\n  style lg fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; lg\n  dt[Arbol de\\nDecisiÃ³n]\n  style dt fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; dt\n  rf[Bosque\\nAleatorio]\n  style rf fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; rf"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-5",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-5",
    "title": "1 - IntroducciÃ³n",
    "section": "Expectativas del taller",
    "text": "Expectativas del taller\n\n\n\n\n\nflowchart LR\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts\n  rs[Remuestreo]\n  style rs fill:#FDF4E3,stroke:#666,color:#000\n  tr --&gt; rs\n  lg[RegresiÃ³n\\nlogÃ­stica]\n  style lg fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; lg\n  dt[Arbol de\\nDecisiÃ³n]\n  style dt fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; dt\n  rf[Bosque\\nAleatorio]\n  style rf fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; rf\n  sm[Seleccionar\\nmodelo]\n  style sm fill:#FDF4E3,stroke:#666,color:#000\n  lg --&gt; sm\n  dt --&gt; sm\n  rf --&gt; sm"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-6",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-6",
    "title": "1 - IntroducciÃ³n",
    "section": "Expectativas del taller",
    "text": "Expectativas del taller\n\n\n\n\n\nflowchart LR\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts\n  rs[Remuestreo]\n  style rs fill:#FDF4E3,stroke:#666,color:#000\n  tr --&gt; rs\n  lg[RegresiÃ³n\\nlogÃ­stica]\n  style lg fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; lg\n  dt[Arbol de\\nDecisiÃ³n]\n  style dt fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; dt\n  rf[Bosque\\nAleatorio]\n  style rf fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; rf\n  sm[Seleccionar\\nmodelo]\n  style sm fill:#FDF4E3,stroke:#666,color:#000\n  lg --&gt; sm\n  dt --&gt; sm\n  rf --&gt; sm\n  fm[Entrenar modelo\\nselecionado]\n  style fm fill:#FBE9BF,stroke:#666,color:#000\n  sm --&gt; fm\n  tr --&gt; fm"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-7",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#expectativas-del-taller-7",
    "title": "1 - IntroducciÃ³n",
    "section": "Expectativas del taller",
    "text": "Expectativas del taller\n\n\n\n\n\nflowchart LR\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts\n  rs[Remuestreo]\n  style rs fill:#FDF4E3,stroke:#666,color:#000\n  tr --&gt; rs\n  lg[RegresiÃ³n\\nlogÃ­stica]\n  style lg fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; lg\n  dt[Arbol de\\nDecisiÃ³n]\n  style dt fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; dt\n  rf[Bosque\\nAleatorio]\n  style rf fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; rf\n  sm[Seleccionar\\nmodelo]\n  style sm fill:#FDF4E3,stroke:#666,color:#000\n  lg --&gt; sm\n  dt --&gt; sm\n  rf --&gt; sm\n  fm[Entrenar modelo\\nselecionado]\n  style fm fill:#FBE9BF,stroke:#666,color:#000\n  sm --&gt; fm\n  tr --&gt; fm\n  vm[Verificar la\\ncalidad]\n  style vm fill:#E5E7FD,stroke:#666,color:#000\n  fm --&gt; vm\n  ts --&gt; vm"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#instalemos-unos-paquetes",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#instalemos-unos-paquetes",
    "title": "1 - IntroducciÃ³n",
    "section": "Instalemos unos paquetes",
    "text": "Instalemos unos paquetes\n\npkgs &lt;- \n  c(\"bonsai\", \"doParallel\", \"embed\", \"finetune\", \"lightgbm\", \"lme4\",\n    \"plumber\", \"probably\", \"ranger\", \"rpart\", \"rpart.plot\", \"rules\",\n    \"splines2\", \"stacks\", \"text2vec\", \"textrecipes\", \"tidymodels\", \n    \"vetiver\", \"remotes\")\n\ninstall.packages(pkgs)"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#nuestras-versiones",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-01-introduccion.html#nuestras-versiones",
    "title": "1 - IntroducciÃ³n",
    "section": "Nuestras versiones",
    "text": "Nuestras versiones\nR version 4.3.2 (2023-10-31), Quarto (1.4.550)\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nbonsai\n0.2.1\n\n\nbroom\n1.0.5\n\n\ndials\n1.2.1\n\n\ndoParallel\n1.0.17\n\n\ndplyr\n1.1.4\n\n\nembed\n1.1.3\n\n\nfinetune\n1.1.0\n\n\nggplot2\n3.5.0\n\n\nlightgbm\n4.3.0\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nlme4\n1.1-35.1\n\n\nmodeldata\n1.3.0\n\n\nparsnip\n1.2.0\n\n\nplumber\n1.2.1\n\n\nprobably\n1.0.3\n\n\npurrr\n1.0.2\n\n\nranger\n0.16.0\n\n\nrecipes\n1.0.10\n\n\nremotes\n2.4.2.1\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nrpart\n4.1.23\n\n\nrpart.plot\n3.1.2\n\n\nrsample\n1.2.0\n\n\nrules\n1.0.2\n\n\nscales\n1.3.0\n\n\nsplines2\n0.5.1\n\n\nstacks\n1.0.3\n\n\ntext2vec\n0.6.4\n\n\ntextrecipes\n1.0.6\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\ntibble\n3.2.1\n\n\ntidymodels\n1.1.1\n\n\ntidyr\n1.3.1\n\n\ntune\n1.1.2\n\n\nvetiver\n0.2.5\n\n\nworkflows\n1.1.4\n\n\nworkflowsets\n1.0.1\n\n\nyardstick\n1.3.0"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#tu-turno",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#tu-turno",
    "title": "3 - Las partes de un modelo",
    "section": "Tu turno",
    "text": "Tu turno\n\nÂ¿Como ajustar un modelo linear en R?\nÂ¿De cuantas maneras sabes como ajustar este tipo de modelo?\n\n\n\nâˆ’+\n03:00\n\n\n\n\n\nlm para regresiÃ³n linear\nglm para regresiÃ³n linear generalizada\nglmnet para regresiÃ³n regularizada\nkeras para regresiÃ³n dentro de Tensorflow\nstan para regresiÃ³n bayensiana\nspark para datos â€œgrandesâ€"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo",
    "title": "3 - Las partes de un modelo",
    "section": "Para especificar un modelo ",
    "text": "Para especificar un modelo \n\n\n\n\nElije un modelo\nEspecifica el â€œmotorâ€\nEstablece el modo"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-1",
    "title": "3 - Las partes de un modelo",
    "section": "Para especificar un modelo ",
    "text": "Para especificar un modelo \n\nlogistic_reg()\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\n\nModels have default engines"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-2",
    "title": "3 - Las partes de un modelo",
    "section": "Para especificar un modelo ",
    "text": "Para especificar un modelo \n\n\n\nElije un modelo\nEspecifica el â€œmotorâ€\nEstablece el modo"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-3",
    "title": "3 - Las partes de un modelo",
    "section": "Para especificar un modelo ",
    "text": "Para especificar un modelo \n\nlogistic_reg() %&gt;%\n  set_engine(\"glmnet\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glmnet"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-4",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-4",
    "title": "3 - Las partes de un modelo",
    "section": "Para especificar un modelo ",
    "text": "Para especificar un modelo \n\nlogistic_reg() %&gt;%\n  set_engine(\"stan\")\n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: stan"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-5",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-5",
    "title": "3 - Las partes de un modelo",
    "section": "Para especificar un modelo ",
    "text": "Para especificar un modelo \n\n\n\nElije un modelo\nEspecifica el â€œmotorâ€\nEstablece el modo"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-6",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-6",
    "title": "3 - Las partes de un modelo",
    "section": "Para especificar un modelo ",
    "text": "Para especificar un modelo \n\ndecision_tree()\n#&gt; Decision Tree Model Specification (unknown mode)\n#&gt; \n#&gt; Computational engine: rpart\n\n\nSome models have a default mode"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-7",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#para-especificar-un-modelo-7",
    "title": "3 - Las partes de un modelo",
    "section": "Para especificar un modelo ",
    "text": "Para especificar un modelo \n\ndecision_tree() %&gt;% \n  set_mode(\"classification\")\n#&gt; Decision Tree Model Specification (classification)\n#&gt; \n#&gt; Computational engine: rpart\n\n\n\n\nEl listado de modelos se encuentra aquÃ­: https://www.tidymodels.org/find/parsnip/"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#to-specify-a-model",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#to-specify-a-model",
    "title": "3 - Las partes de un modelo",
    "section": "To specify a model ",
    "text": "To specify a model \n\n\n\nElije un modelo\nEspecifica el â€œmotorâ€\nEstablece el modo"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#tu-turno-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#tu-turno-1",
    "title": "3 - Las partes de un modelo",
    "section": "Tu turno",
    "text": "Tu turno\n\nCorre el â€œchunkâ€ arbol_espec en tu archivo .qmd.\nCambia el cÃ³digo para que utilize una regresiÃ³n linear\n\nEl listado de modelos se encuentra aquÃ­: https://www.tidymodels.org/find/parsnip/\n\n\nReto: Edita el cÃ³digo para usar otro tipo de modelo. Por ejemplo, trata de usar el Ã¡rbol disponible dentro del paquete partykit, o trata un modelo de tipo diferente\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#los-modelos-que-usaremos-hoy",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#los-modelos-que-usaremos-hoy",
    "title": "3 - Las partes de un modelo",
    "section": "Los modelos que usaremos hoy",
    "text": "Los modelos que usaremos hoy\n\nRegresiÃ³n logÃ­stica\nArboles de decisiÃ³n"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#regresiÃ³n-logÃ­stica",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#regresiÃ³n-logÃ­stica",
    "title": "3 - Las partes de un modelo",
    "section": "RegresiÃ³n logÃ­stica",
    "text": "RegresiÃ³n logÃ­stica"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#regresiÃ³n-logÃ­stica-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#regresiÃ³n-logÃ­stica-1",
    "title": "3 - Las partes de un modelo",
    "section": "RegresiÃ³n logÃ­stica",
    "text": "RegresiÃ³n logÃ­stica"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#regresiÃ³n-logÃ­stica-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#regresiÃ³n-logÃ­stica-2",
    "title": "3 - Las partes de un modelo",
    "section": "RegresiÃ³n logÃ­stica",
    "text": "RegresiÃ³n logÃ­stica\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl logit del la probabilidad del resultado\n\n\\(log(\\frac{p}{1 - p}) = \\beta_0 + \\beta_1\\cdot \\text{A}\\)\n\nCalcular la lÃ­nea de sigmoideo que separan las dos clases"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#arboles-de-decisiÃ³n",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#arboles-de-decisiÃ³n",
    "title": "3 - Las partes de un modelo",
    "section": "Arboles de decisiÃ³n",
    "text": "Arboles de decisiÃ³n"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#arboles-de-decisiÃ³n-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#arboles-de-decisiÃ³n-1",
    "title": "3 - Las partes de un modelo",
    "section": "Arboles de decisiÃ³n",
    "text": "Arboles de decisiÃ³n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl resultado es decidido usando una serie de decisiones basados en las variables predictivas\nPrimero, el arbol crece hasta que se llega a una decision\nDespuÃ©s, el arbol es recortado para que no sea tan complejo"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#arboles-de-decisiÃ³n-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#arboles-de-decisiÃ³n-2",
    "title": "3 - Las partes de un modelo",
    "section": "Arboles de decisiÃ³n",
    "text": "Arboles de decisiÃ³n"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#todos-los-modelos-estÃ¡n-equivocados-pero-algunos-pueden-ser-utÃ­l",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#todos-los-modelos-estÃ¡n-equivocados-pero-algunos-pueden-ser-utÃ­l",
    "title": "3 - Las partes de un modelo",
    "section": "Todos los modelos estÃ¡n equivocados, pero algunos pueden ser utÃ­l",
    "text": "Todos los modelos estÃ¡n equivocados, pero algunos pueden ser utÃ­l\n\n\nRegresiÃ³n logÃ­stica\n\n\n\n\n\n\n\n\n\n\nArboles de decisiÃ³n"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#los-flujos-de-trabajo-combinan-el-preprocesamiento-y-el-modelo",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#los-flujos-de-trabajo-combinan-el-preprocesamiento-y-el-modelo",
    "title": "3 - Las partes de un modelo",
    "section": "Los flujos de trabajo combinan el â€œpreprocesamientoâ€ y el modelo",
    "text": "Los flujos de trabajo combinan el â€œpreprocesamientoâ€ y el modelo\n\n\n\n\n\nflowchart LR\n  dt[Datos]\n  pr[Predictores]\n  subgraph fl[Flujo del modelo]\n    pca[PCA]\n    ls[Menos cuadrados]\n    pca--&gt;ls\n  end\n  dt--&gt;fl\n  pr--&gt;fl\n  ft[Modelo ajustado]\n  fl--&gt;ft\n  style fl fill:#fff,stroke:#666,color:#000\n\n\n\n\n\n\n\nExplain that PCA that is a preprocessor / dimensionality reduction, used to decorrelate data"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#que-no-estÃ¡-bien-en-este-flujo",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#que-no-estÃ¡-bien-en-este-flujo",
    "title": "3 - Las partes de un modelo",
    "section": "Â¿Que no estÃ¡ bien en este flujo?",
    "text": "Â¿Que no estÃ¡ bien en este flujo?\n\n\n\n\n\nflowchart LR\n  dt[Datos]\n  pr[Predictores]\n  pca[PCA]\n  subgraph fl[Flujo del modelo]\n    ls[Menos cuadrados]\n  end\n  pca--&gt;fl\n  dt--&gt;pca\n  pr--&gt;pca\n  ft[Modelo ajustado]\n  fl--&gt;ft\n  style fl fill:#fff,stroke:#666,color:#000"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#por-que-utilizar-workflow",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#por-que-utilizar-workflow",
    "title": "3 - Las partes de un modelo",
    "section": "Â¿Por que utilizar workflow()? ",
    "text": "Â¿Por que utilizar workflow()? \n\n\nworkflow maneja nuevos niveles factoriales mejor que las herramientas regulares de R\n\n\n\n\nPuedes utilizar â€œpreprocesadoresâ€ que no son formulas\n\n\n\n\nAyudan a organizar un proyecto que utiliza varios modelos\n\n\n\n\nAÃºn mÃ¡s importante, un flujo de workflow encaja todo el processo de modelamiento, desde la creaciÃ³n (fit()) del modelo, hasta el uso del modelo (predict())\n\n\nTwo ways workflows handle levels better than base R:\n\nEnforces that new levels are not allowed at prediction time (this is an optional check that can be turned off)\nRestores missing levels that were present at fit time, but happen to be missing at prediction time (like, if your â€œnewâ€ data just doesnâ€™t have an instance of that level)"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#el-flujo-de-trabajo-de-un-modelo-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#el-flujo-de-trabajo-de-un-modelo-1",
    "title": "3 - Las partes de un modelo",
    "section": "El flujo de trabajo de un modelo  ",
    "text": "El flujo de trabajo de un modelo  \n\narbol_espec &lt;-\n  decision_tree(cost_complexity = 0.002) %&gt;% \n  set_mode(\"classification\")\n\narbol_espec %&gt;% \n  fit(propina ~ ., data = taxi_entrenar) \n#&gt; parsnip model object\n#&gt; \n#&gt; n= 8000 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 8000 616 si (0.92300000 0.07700000)  \n#&gt;    2) distancia&gt;=14.12 2041  68 si (0.96668300 0.03331700) *\n#&gt;    3) distancia&lt; 14.12 5959 548 si (0.90803826 0.09196174)  \n#&gt;      6) distancia&lt; 5.275 5419 450 si (0.91695885 0.08304115) *\n#&gt;      7) distancia&gt;=5.275 540  98 si (0.81851852 0.18148148)  \n#&gt;       14) compania=Chicago Independents,City Service,otra,Sun Taxi,Taxi Affiliation Services,Taxicab Insurance Agency Llc 478  68 si (0.85774059 0.14225941) *\n#&gt;       15) compania=Flash Cab 62  30 si (0.51612903 0.48387097)  \n#&gt;         30) dia=Jue 12   2 si (0.83333333 0.16666667) *\n#&gt;         31) dia=Lun,Mar,Mie,Vie,Sab,Dom 50  22 no (0.44000000 0.56000000)  \n#&gt;           62) distancia&gt;=11.77 14   4 si (0.71428571 0.28571429) *\n#&gt;           63) distancia&lt; 11.77 36  12 no (0.33333333 0.66666667) *"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#el-flujo-de-trabajo-de-un-modelo-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#el-flujo-de-trabajo-de-un-modelo-2",
    "title": "3 - Las partes de un modelo",
    "section": "El flujo de trabajo de un modelo  ",
    "text": "El flujo de trabajo de un modelo  \n\narbol_espec &lt;-\n  decision_tree(cost_complexity = 0.002) %&gt;% \n  set_mode(\"classification\")\n\nworkflow() %&gt;%\n  add_formula(propina ~ .) %&gt;%\n  add_model(arbol_espec) %&gt;%\n  fit(data = taxi_entrenar) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; propina ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 8000 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 8000 616 si (0.92300000 0.07700000)  \n#&gt;    2) distancia&gt;=14.12 2041  68 si (0.96668300 0.03331700) *\n#&gt;    3) distancia&lt; 14.12 5959 548 si (0.90803826 0.09196174)  \n#&gt;      6) distancia&lt; 5.275 5419 450 si (0.91695885 0.08304115) *\n#&gt;      7) distancia&gt;=5.275 540  98 si (0.81851852 0.18148148)  \n#&gt;       14) compania=Chicago Independents,City Service,otra,Sun Taxi,Taxi Affiliation Services,Taxicab Insurance Agency Llc 478  68 si (0.85774059 0.14225941) *\n#&gt;       15) compania=Flash Cab 62  30 si (0.51612903 0.48387097)  \n#&gt;         30) dia=Jue 12   2 si (0.83333333 0.16666667) *\n#&gt;         31) dia=Lun,Mar,Mie,Vie,Sab,Dom 50  22 no (0.44000000 0.56000000)  \n#&gt;           62) distancia&gt;=11.77 14   4 si (0.71428571 0.28571429) *\n#&gt;           63) distancia&lt; 11.77 36  12 no (0.33333333 0.66666667) *"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#el-flujo-de-trabajo-de-un-modelo-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#el-flujo-de-trabajo-de-un-modelo-3",
    "title": "3 - Las partes de un modelo",
    "section": "El flujo de trabajo de un modelo  ",
    "text": "El flujo de trabajo de un modelo  \n\narbol_espec &lt;-\n  decision_tree(cost_complexity = 0.002) %&gt;% \n  set_mode(\"classification\")\n\nworkflow(propina ~ ., arbol_espec) %&gt;% \n  fit(data = taxi_entrenar) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: decision_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; propina ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; n= 8000 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 8000 616 si (0.92300000 0.07700000)  \n#&gt;    2) distancia&gt;=14.12 2041  68 si (0.96668300 0.03331700) *\n#&gt;    3) distancia&lt; 14.12 5959 548 si (0.90803826 0.09196174)  \n#&gt;      6) distancia&lt; 5.275 5419 450 si (0.91695885 0.08304115) *\n#&gt;      7) distancia&gt;=5.275 540  98 si (0.81851852 0.18148148)  \n#&gt;       14) compania=Chicago Independents,City Service,otra,Sun Taxi,Taxi Affiliation Services,Taxicab Insurance Agency Llc 478  68 si (0.85774059 0.14225941) *\n#&gt;       15) compania=Flash Cab 62  30 si (0.51612903 0.48387097)  \n#&gt;         30) dia=Jue 12   2 si (0.83333333 0.16666667) *\n#&gt;         31) dia=Lun,Mar,Mie,Vie,Sab,Dom 50  22 no (0.44000000 0.56000000)  \n#&gt;           62) distancia&gt;=11.77 14   4 si (0.71428571 0.28571429) *\n#&gt;           63) distancia&lt; 11.77 36  12 no (0.33333333 0.66666667) *"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#tu-turno-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#tu-turno-2",
    "title": "3 - Las partes de un modelo",
    "section": "Tu turno",
    "text": "Tu turno\n\nCorre el â€œchunkâ€ arbol_flujo en tu archivo .qmd.\nEdita el codigo para crear un flujo con un modelo que tÃº elijas\n\nReto: Â¿Que otros â€œpreprocesadoresâ€, aparte de formulas, podemos usar en workflow?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#predecir-usando-tu-modelo",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#predecir-usando-tu-modelo",
    "title": "3 - Las partes de un modelo",
    "section": "Predecir usando tu modelo  ",
    "text": "Predecir usando tu modelo  \nÂ¿Como usar su nuevo modelo arbol_flujo?\n\narbol_espec &lt;-\n  decision_tree(cost_complexity = 0.002) %&gt;% \n  set_mode(\"classification\")\n\narbol_flujo &lt;-\n  workflow(propina ~ ., arbol_espec) %&gt;% \n  fit(data = taxi_entrenar)"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#tu-turno-3",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#tu-turno-3",
    "title": "3 - Las partes de un modelo",
    "section": "Tu turno",
    "text": "Tu turno\n\nCorre:\npredict(arbol_flujo, new_data = taxi_prueba)\nÂ¿Que resultado te dio?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#tu-turno-4",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#tu-turno-4",
    "title": "3 - Las partes de un modelo",
    "section": "Tu turno",
    "text": "Tu turno\n\nCorre:\naugment(arbol_flujo, new_data = taxi_prueba)\nÂ¿Que resultado te dio?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#entiendedo-tÃº-modelo",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#entiendedo-tÃº-modelo",
    "title": "3 - Las partes de un modelo",
    "section": "Entiendedo tÃº modelo  ",
    "text": "Entiendedo tÃº modelo  \nÂ¿Como podemos entender nuestro nuevo modelo arbol_flujo?"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#entiendedo-tÃº-modelo-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#entiendedo-tÃº-modelo-1",
    "title": "3 - Las partes de un modelo",
    "section": "Entiendedo tÃº modelo  ",
    "text": "Entiendedo tÃº modelo  \nÂ¿Como podemos entender nuestro nuevo modelo arbol_flujo?\n\nlibrary(rpart.plot)\narbol_flujo %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot(roundint = FALSE)\n\nPuedes extraer varios componentes de tu flujo usando las funciones que empiezan con extract_*()\n\nâš ï¸ Â¡Nunca trate de predecir usando los componentes extraÃ­dos!\n\nroundint = FALSE is only to quiet a warning"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#entiendedo-tÃº-modelo-2",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#entiendedo-tÃº-modelo-2",
    "title": "3 - Las partes de un modelo",
    "section": "Entiendedo tÃº modelo  ",
    "text": "Entiendedo tÃº modelo  \nÂ¿Como podemos entender nuestro nuevo modelo arbol_flujo?\n\nPuedes usar tu flujo ajustado para obtener explicaciones del modelo o de las predicciones:\n\n\n\nEl paquete vip puede dar la importancia de cada variable\n\n\n\n\nPara obtener explicaciones del modelo, puede usar el paquete DALEXtra\n\n\n\nPara aprender mÃ¡s: https://www.tmwr.org/explain.html"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#tu-turno-5",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#tu-turno-5",
    "title": "3 - Las partes de un modelo",
    "section": "Tu turno",
    "text": "Tu turno\n\n\nExtrae el â€œmotorâ€ del modelo de tu flujo y examÃ­nalo\n\n\n\nâˆ’+\n05:00\n\n\n\n\nAfterward, ask what kind of object people got from the extraction, and what they did with it (e.g.Â give it to summary(), plot(), broom::tidy() ). Live code along"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#expectativas-del-taller---donde-estamos",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-03-partes-del-modelo.html#expectativas-del-taller---donde-estamos",
    "title": "3 - Las partes de un modelo",
    "section": "Expectativas del taller - Donde estamos",
    "text": "Expectativas del taller - Donde estamos\n\n\n\n\n\nflowchart LR\n  ad[Todos\\nlos datos]\n  style ad fill:#fff,stroke:#666,color:#000\n  tr[Entrenamiento]\n  style tr fill:#FBE9BF,stroke:#666,color:#000\n  ts[Prueba]\n  style ts fill:#E5E7FD,stroke:#666,color:#000\n  ad --&gt; tr\n  ad --&gt; ts\n  rs[Remuestreo]\n  style rs fill:#fff,stroke:#eee,color:#ddd\n  tr --&gt; dt\n  lg[RegresiÃ³n\\nlogÃ­stica]\n  style lg fill:#fff,stroke:#eee,color:#ddd\n  rs --&gt; lg\n  dt[Arbol de\\nDecisiÃ³n]\n  style dt fill:#FDF4E3,stroke:#666,color:#000\n  rs --&gt; dt\n  rf[Bosque\\nAleatorio]\n  style rf fill:#fff,stroke:#eee,color:#ddd\n  rs --&gt; rf\n  sm[Seleccionar\\nmodelo]\n  style sm fill:#fff,stroke:#eee,color:#ddd\n  lg --&gt; sm\n  dt --&gt; sm\n  rf --&gt; sm\n  fm[Entrenar modelo\\nselecionado]\n  style fm fill:#fff,stroke:#eee,color:#ddd\n  sm --&gt; fm\n  tr --&gt; fm\n  vm[Verificar la\\ncalidad]\n  style vm fill:#fff,stroke:#eee,color:#ddd\n  fm --&gt; vm\n  ts --&gt; vm\n\n\n\n\n\n\n\n\nStress that fitting a model on the entire training set was only for illustrating how to fit a model"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#parÃ¡metros-de-afinamiento",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#parÃ¡metros-de-afinamiento",
    "title": "5 - Afinando modelos",
    "section": "ParÃ¡metros de afinamiento",
    "text": "ParÃ¡metros de afinamiento\nAlgunos valores de los parametros de los modelos o de los preprocesadores no se pueden estimar directamente desde los datos\n\nPor ejemplo:\n\nLa profundidad de los Ã¡rboles de decisiÃ³n\nLa cantidad de vecinos en un modelo de vecinos K-nearest"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#optimiza-los-parÃ¡metros-de-afinamiento",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#optimiza-los-parÃ¡metros-de-afinamiento",
    "title": "5 - Afinando modelos",
    "section": "Optimiza los parÃ¡metros de afinamiento",
    "text": "Optimiza los parÃ¡metros de afinamiento\n\nPrueba valores diferentes para medir la calidad del modelo\n\n\n\nEncuentra buenos valores para los parametros\n\n\n\n\nUna vez los valores de los parametros han sido determinados, el modelo final se puede ajustar usando todos los datos en el set de entrenamiento"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#optimiza-los-parÃ¡metros-de-afinamiento-1",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#optimiza-los-parÃ¡metros-de-afinamiento-1",
    "title": "5 - Afinando modelos",
    "section": "Optimiza los parÃ¡metros de afinamiento",
    "text": "Optimiza los parÃ¡metros de afinamiento\nHay dos estrategias principales para optimizar:\n\n\nBusqueda de cuadrÃ­cula (Grid search) ğŸ’  que prueba un set de valores pre-elejidos\nBusqueda iterativa ğŸŒ€ que sugiere nuevos valores del parametro para probar"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#especificando-los-parametros-de-afinamiento",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#especificando-los-parametros-de-afinamiento",
    "title": "5 - Afinando modelos",
    "section": "Especificando los parametros de afinamiento",
    "text": "Especificando los parametros de afinamiento\nTomemos nueso modelo de bosque aleatorio y tratemos de encontrar el nÃºmero mÃ­nimo de datos min_n\n\narbol_espec &lt;- rand_forest(min_n = tune()) %&gt;% \n  set_mode(\"classification\")\n\narbol_flujo &lt;- workflow(propina ~ ., arbol_espec)\narbol_flujo\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; propina ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   min_n = tune()\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#try-out-multiple-values",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#try-out-multiple-values",
    "title": "5 - Afinando modelos",
    "section": "Try out multiple values",
    "text": "Try out multiple values"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#prueba-varios-valores",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#prueba-varios-valores",
    "title": "5 - Afinando modelos",
    "section": "Prueba varios valores",
    "text": "Prueba varios valores\ntune_grid() funciona parecido a fit_resamples() pero puede calcular multiples valores del parÃ¡metro:\n\nset.seed(22)\nrf_res &lt;- tune_grid(\n  arbol_flujo,\n  taxi_plieges,\n  grid = 5\n)"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#compara-los-resultados",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#compara-los-resultados",
    "title": "5 - Afinando modelos",
    "section": "Compara los resultados",
    "text": "Compara los resultados\nInspecciona los resultados, y selecciona los mejores parÃ¡metros:\n\nshow_best(rf_res)\n#&gt; # A tibble: 5 Ã— 7\n#&gt;   min_n .metric .estimator  mean     n std_err .config             \n#&gt;   &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1    33 roc_auc binary     0.653    10  0.0159 Preprocessor1_Model1\n#&gt; 2    31 roc_auc binary     0.650    10  0.0167 Preprocessor1_Model3\n#&gt; 3    21 roc_auc binary     0.646    10  0.0165 Preprocessor1_Model4\n#&gt; 4    13 roc_auc binary     0.646    10  0.0143 Preprocessor1_Model5\n#&gt; 5     6 roc_auc binary     0.631    10  0.0140 Preprocessor1_Model2\n\nparametro_mejor &lt;- select_best(rf_res)\nparametro_mejor\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   min_n .config             \n#&gt;   &lt;int&gt; &lt;chr&gt;               \n#&gt; 1    33 Preprocessor1_Model1\n\nTambiÃ©n puedes usar collect_metrics() y autoplot()"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#el-ajuste-final",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#el-ajuste-final",
    "title": "5 - Afinando modelos",
    "section": "El ajuste final",
    "text": "El ajuste final\n\narbol_flujo &lt;- finalize_workflow(arbol_flujo, parametro_mejor)\n\najuste_final &lt;- last_fit(arbol_flujo, taxi_separar) \n\ncollect_metrics(ajuste_final)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric  .estimator .estimate .config             \n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy binary         0.912 Preprocessor1_Model1\n#&gt; 2 roc_auc  binary         0.636 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#tu-turno",
    "href": "archive/2024-03-conectaR-spanish/presentacion/intro-05-afinando-modelos.html#tu-turno",
    "title": "5 - Afinando modelos",
    "section": "Tu turno",
    "text": "Tu turno\n\nModifica tu flujo de modelo para usar por lo menos un parÃ¡metro\nUsa busqueda de cuadricula para encontrar los mejores parametros\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#venue-information",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#venue-information",
    "title": "1 - Introduction",
    "section": "Venue information",
    "text": "Venue information\n\nThere are gender neutral bathrooms located on levels 3, 4, 5, 6 & 7\nA meditation/prayer room is located in 503\n(Mon & Tue 7am - 7pm, and Wed 7am - 5pm)\nA lactation room is located in 509\n(Mon & Tue 7am - 7pm, and Wed 7am - 5pm)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#workshop-policies",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#workshop-policies",
    "title": "1 - Introduction",
    "section": "Workshop policies",
    "text": "Workshop policies\n\nPlease review the posit::conf code of conduct, which applies to all workshops: https://posit.co/code-of-conduct\nCoC site has info on how to report a problem (in person, email, phone)\nPlease do not photograph people wearing red lanyards"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#who-are-you",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#who-are-you",
    "title": "1 - Introduction",
    "section": "Who are you?",
    "text": "Who are you?\n\nYou can use the magrittr %&gt;% or base R |&gt; pipe\nYou are familiar with functions from dplyr, tidyr, ggplot2\nYou have exposure to basic statistical concepts\nYou do not need intermediate or expert familiarity with modeling or ML\nYou have used some tidymodels packages\nYou have some experience with evaluating statistical models using resampling techniques"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#who-are-tidymodels",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#who-are-tidymodels",
    "title": "1 - Introduction",
    "section": "Who are tidymodels?",
    "text": "Who are tidymodels?\n\nSimon Couch\nHannah Frick\nEmil Hvitfeldt\nMax Kuhn\n\n\nMany thanks to Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill, and DesirÃ©e De Leon for their role in creating these materials!"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#asking-for-help",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#asking-for-help",
    "title": "1 - Introduction",
    "section": "Asking for help",
    "text": "Asking for help\n\nğŸŸª â€œIâ€™m stuck and need help!â€\n\n\nğŸŸ© â€œI finished the exerciseâ€"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#discord",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#discord",
    "title": "1 - Introduction",
    "section": "Discord ",
    "text": "Discord \n\npos.it/conf-event-portal (login)\nClick on â€œJoin Discord, the virtual networking platform!â€\nBrowse Channels -&gt; #workshop-tidymodels-advanced"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#section-2",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#section-2",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#section-3",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#section-3",
    "title": "1 - Introduction",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€\n\n\nhttps://workshops.tidymodels.org"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#tentative-plan-for-this-workshop",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#tentative-plan-for-this-workshop",
    "title": "1 - Introduction",
    "section": "Tentative plan for this workshop",
    "text": "Tentative plan for this workshop\n\nFeature engineering with recipes\nModel optimization by tuning\n\nGrid search\nRacing\nIterative methods\n\nExtras (time permitting)\n\nEffect encodings\nA case study"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#section-4",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#section-4",
    "title": "1 - Introduction",
    "section": "",
    "text": "Introduce yourself to your neighbors ğŸ‘‹\n\n Log in to Posit Cloud (free): TODO-ADD-LATER"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#lets-install-some-packages",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#lets-install-some-packages",
    "title": "1 - Introduction",
    "section": "Letâ€™s install some packages",
    "text": "Letâ€™s install some packages\nIf you are using your own laptop instead of Posit Cloud:\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"bonsai\", \"Cubist\", \"doParallel\", \"earth\", \"embed\", \"finetune\", \n    \"forested\", \"lightgbm\", \"lme4\", \"parallelly\", \"plumber\", \"probably\", \n    \"ranger\", \"rpart\", \"rpart.plot\", \"rules\", \"splines2\", \"stacks\", \n    \"text2vec\", \"textrecipes\", \"tidymodels\", \"vetiver\")\n\ninstall.packages(pkgs)\n\nAlso, you should install the newest version of the dials package (version 1.3.0). To check this, you can run:\n\nrlang::check_installed(\"dials\", version = \"1.3.0\")"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#hotel-data",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#hotel-data",
    "title": "1 - Introduction",
    "section": "Hotel Data  ",
    "text": "Hotel Data  \nWeâ€™ll use data on hotels to predict the cost of a room.\nThe data are in the modeldata package. Weâ€™ll sample down the data and refactor some columns:\n\n\n\nlibrary(tidymodels)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#hotel-date-columns",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#hotel-date-columns",
    "title": "1 - Introduction",
    "section": "Hotel date columns",
    "text": "Hotel date columns\n\nnames(hotel_rates)\n#&gt;  [1] \"avg_price_per_room\"             \"lead_time\"                     \n#&gt;  [3] \"stays_in_weekend_nights\"        \"stays_in_week_nights\"          \n#&gt;  [5] \"adults\"                         \"children\"                      \n#&gt;  [7] \"babies\"                         \"meal\"                          \n#&gt;  [9] \"country\"                        \"market_segment\"                \n#&gt; [11] \"distribution_channel\"           \"is_repeated_guest\"             \n#&gt; [13] \"previous_cancellations\"         \"previous_bookings_not_canceled\"\n#&gt; [15] \"reserved_room_type\"             \"assigned_room_type\"            \n#&gt; [17] \"booking_changes\"                \"agent\"                         \n#&gt; [19] \"company\"                        \"days_in_waiting_list\"          \n#&gt; [21] \"customer_type\"                  \"required_car_parking_spaces\"   \n#&gt; [23] \"total_of_special_requests\"      \"arrival_date_num\"              \n#&gt; [25] \"near_christmas\"                 \"near_new_years\"                \n#&gt; [27] \"historical_adr\""
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#data-splitting-strategy",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#data-splitting-strategy",
    "title": "1 - Introduction",
    "section": "Data splitting strategy",
    "text": "Data splitting strategy"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#data-spending",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#data-spending",
    "title": "1 - Introduction",
    "section": "Data Spending ",
    "text": "Data Spending \nLetâ€™s split the data into a training set (75%) and testing set (25%) using stratification:\n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#your-turn",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#your-turn",
    "title": "1 - Introduction",
    "section": "Your turn",
    "text": "Your turn\nLetâ€™s take some time and investigate the training data. The outcome is avg_price_per_room.\nAre there any interesting characteristics of the data?\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-01-introduction.html#our-versions",
    "href": "archive/2024-08-posit-conf/advanced-01-introduction.html#our-versions",
    "title": "1 - Introduction",
    "section": "Our versions",
    "text": "Our versions\nR version 4.4.1 (2024-06-14), Quarto (1.6.1)\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nbonsai\n0.3.1\n\n\nbroom\n1.0.6\n\n\nCubist\n0.4.4\n\n\ndials\n1.3.0\n\n\ndoParallel\n1.0.17\n\n\ndplyr\n1.1.4\n\n\nearth\n5.3.3\n\n\nembed\n1.1.4\n\n\nfinetune\n1.2.0\n\n\nFormula\n1.2-5\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nggplot2\n3.5.1\n\n\nlattice\n0.22-6\n\n\nlightgbm\n4.5.0\n\n\nlme4\n1.1-35.5\n\n\nmodeldata\n1.4.0\n\n\nparallelly\n1.38.0\n\n\nparsnip\n1.2.1\n\n\nplotmo\n3.6.3\n\n\nplotrix\n3.8-4\n\n\nplumber\n1.2.2\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\nprobably\n1.0.3\n\n\npurrr\n1.0.2\n\n\nrecipes\n1.1.0\n\n\nrsample\n1.2.1\n\n\nrules\n1.0.2\n\n\nscales\n1.3.0\n\n\nsplines2\n0.5.3\n\n\nstacks\n1.0.5\n\n\ntext2vec\n0.6.4\n\n\ntextrecipes\n1.0.6\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\n\n\n\n\ntibble\n3.2.1\n\n\ntidymodels\n1.2.0\n\n\ntidyr\n1.3.1\n\n\ntune\n1.2.1\n\n\nvetiver\n0.2.5\n\n\nworkflows\n1.1.4\n\n\nworkflowsets\n1.1.0\n\n\nyardstick\n1.3.1"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#previously---setup",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#previously---setup",
    "title": "3 - Tuning Hyperparameters",
    "section": "Previously - Setup ",
    "text": "Previously - Setup \n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;% \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#previously---data-usage",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#previously---data-usage",
    "title": "3 - Tuning Hyperparameters",
    "section": "Previously - Data Usage ",
    "text": "Previously - Data Usage \n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#previously---feature-engineering",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#previously---feature-engineering",
    "title": "3 - Tuning Hyperparameters",
    "section": "Previously - Feature engineering  ",
    "text": "Previously - Feature engineering  \n\nlibrary(textrecipes)\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  # Defaults to 32 signed indicator columns\n  step_dummy_hash(agent) %&gt;%\n  step_dummy_hash(company) %&gt;%\n  # Regular indicators for the others\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#tuning-parameters",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#tuning-parameters",
    "title": "3 - Tuning Hyperparameters",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\nSome examples:\n\nTree depth in decision trees\nNumber of neighbors in a K-nearest neighbor model"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#optimize-tuning-parameters",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#optimize-tuning-parameters",
    "title": "3 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\n\nTry different values and measure their performance.\n\n\n\nFind good values for these parameters.\n\n\n\n\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#tagging-parameters-for-tuning",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#tagging-parameters-for-tuning",
    "title": "3 - Tuning Hyperparameters",
    "section": "Tagging parameters for tuning ",
    "text": "Tagging parameters for tuning \nWith tidymodels, you can mark the parameters that you want to optimize with a value of tune().\n\nThe function itself just returnsâ€¦ itself:\n\ntune()\n#&gt; tune()\nstr(tune())\n#&gt;  language tune()\n\n# optionally add a label\ntune(\"I hope that the workshop is going well\")\n#&gt; tune(\"I hope that the workshop is going well\")\n\n\nFor exampleâ€¦"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#optimizing-the-hash-features",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#optimizing-the-hash-features",
    "title": "3 - Tuning Hyperparameters",
    "section": "Optimizing the hash features   ",
    "text": "Optimizing the hash features   \nOur new recipe is:\n\nhash_rec &lt;-\n  recipe(avg_price_per_room ~ ., data = hotel_train) %&gt;%\n  step_YeoJohnson(lead_time) %&gt;%\n  step_dummy_hash(agent,   num_terms = tune(\"agent hash\")) %&gt;%\n  step_dummy_hash(company, num_terms = tune(\"company hash\")) %&gt;%\n  step_zv(all_predictors())\n\n\nWe will be using a tree-based model in a minute.\n\nThe other categorical predictors are left as-is.\nThatâ€™s why there is no step_dummy()."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-trees",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-trees",
    "title": "3 - Tuning Hyperparameters",
    "section": "Boosted Trees",
    "text": "Boosted Trees\nThese are popular ensemble methods that build a sequence of tree models.\n\nEach tree uses the results of the previous tree to better predict samples, especially those that have been poorly predicted.\n\nEach tree in the ensemble is saved and new samples are predicted using a weighted average of the votes of each tree in the ensemble.\n\nWeâ€™ll focus on the popular lightgbm implementation."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters",
    "title": "3 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\nSome possible parameters:\n\nmtry: The number of predictors randomly sampled at each split (in \\([1, ncol(x)]\\) or \\((0, 1]\\)).\ntrees: The number of trees (\\([1, \\infty]\\), but usually up to thousands)\nmin_n: The number of samples needed to further split (\\([1, n]\\)).\nlearn_rate: The rate that each tree adapts from previous iterations (\\((0, \\infty]\\), usual maximum is 0.1).\nstop_iter: The number of iterations of boosting where no improvement was shown before stopping (\\([1, trees]\\))"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters-1",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters",
    "text": "Boosted Tree Tuning Parameters\nTBH it is usually not difficult to optimize these models.\n\nOften, there are multiple candidate tuning parameter combinations that have very good results.\n\nTo demonstrate simple concepts, weâ€™ll look at optimizing the number of trees in the ensemble (between 1 and 100) and the learning rate (\\(10^{-5}\\) to \\(10^{-1}\\))."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters-2",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#boosted-tree-tuning-parameters-2",
    "title": "3 - Tuning Hyperparameters",
    "section": "Boosted Tree Tuning Parameters   ",
    "text": "Boosted Tree Tuning Parameters   \nWeâ€™ll need to load the bonsai package. This has the information needed to use lightgbm\n\nlibrary(bonsai)\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow &lt;- workflow(hash_rec, lgbm_spec)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#optimize-tuning-parameters-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Optimize tuning parameters",
    "text": "Optimize tuning parameters\nThe main two strategies for optimization are:\n\n\nGrid search ğŸ’  which tests a pre-defined set of candidate values\nIterative search ğŸŒ€ which suggests/estimates new values of candidate parameters to evaluate"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search",
    "title": "3 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nA small grid of points trying to minimize the error via learning rate:"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search-1",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Grid search",
    "text": "Grid search\nIn reality we would probably sample the space more densely:"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#iterative-search",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#iterative-search",
    "title": "3 - Tuning Hyperparameters",
    "section": "Iterative Search",
    "text": "Iterative Search\nWe could start with a few points and search the space:"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#parameters",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#parameters",
    "title": "3 - Tuning Hyperparameters",
    "section": "Parameters",
    "text": "Parameters\n\nThe tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\nThe extract_parameter_set_dials() function extracts these tuning parameters and the info.\n\n\nGrids\n\nCreate your grid manually or automatically.\nThe grid_*() functions can make a grid.\n\n\n\nMost basic (but very effective) way to tune models"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#different-types-of-grids",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#different-types-of-grids",
    "title": "3 - Tuning Hyperparameters",
    "section": "Different types of grids ",
    "text": "Different types of grids \n\n\n\n\n\n\n\n\n\nSpace-filling designs (SFD) attempt to cover the parameter space without redundant candidates. We recommend these the most."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#create-a-grid",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#create-a-grid",
    "title": "3 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nlgbm_wflow %&gt;% \n  extract_parameter_set_dials()\n#&gt; Collection of 4 parameters for tuning\n#&gt; \n#&gt;    identifier       type    object\n#&gt;         trees      trees nparam[+]\n#&gt;    learn_rate learn_rate nparam[+]\n#&gt;    agent hash  num_terms nparam[+]\n#&gt;  company hash  num_terms nparam[+]\n\n# Individual functions: \ntrees()\n#&gt; # Trees (quantitative)\n#&gt; Range: [1, 2000]\nlearn_rate()\n#&gt; Learning Rate (quantitative)\n#&gt; Transformer: log-10 [1e-100, Inf]\n#&gt; Range (transformed scale): [-10, -1]\n\n\nA parameter set can be updated (e.g.Â to change the ranges)."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#create-a-grid-1",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#create-a-grid-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Create a grid  ",
    "text": "Create a grid  \n\nset.seed(12)\ngrid &lt;- \n  lgbm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  grid_space_filling(size = 25)\n\ngrid\n#&gt; # A tibble: 25 Ã— 4\n#&gt;    trees learn_rate `agent hash` `company hash`\n#&gt;    &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1     1   7.50e- 6          574            574\n#&gt;  2    84   1.78e- 5         2048           2298\n#&gt;  3   167   5.62e-10         1824            912\n#&gt;  4   250   4.22e- 5         3250            512\n#&gt;  5   334   1.78e- 8          512           2896\n#&gt;  6   417   1.33e- 3          322           1625\n#&gt;  7   500   1   e- 1         1448           1149\n#&gt;  8   584   1   e- 7         1290            256\n#&gt;  9   667   2.37e-10          456            724\n#&gt; 10   750   1.78e- 2          645            322\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#your-turn",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#your-turn",
    "title": "3 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\nCreate a grid for our tunable workflow.\nTry creating a regular grid.\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#create-a-regular-grid",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#create-a-regular-grid",
    "title": "3 - Tuning Hyperparameters",
    "section": "Create a regular grid  ",
    "text": "Create a regular grid  \n\nset.seed(12)\ngrid &lt;- \n  lgbm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 4)\n\ngrid\n#&gt; # A tibble: 256 Ã— 4\n#&gt;    trees   learn_rate `agent hash` `company hash`\n#&gt;    &lt;int&gt;        &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1     1 0.0000000001          256            256\n#&gt;  2   667 0.0000000001          256            256\n#&gt;  3  1333 0.0000000001          256            256\n#&gt;  4  2000 0.0000000001          256            256\n#&gt;  5     1 0.0000001             256            256\n#&gt;  6   667 0.0000001             256            256\n#&gt;  7  1333 0.0000001             256            256\n#&gt;  8  2000 0.0000001             256            256\n#&gt;  9     1 0.0001                256            256\n#&gt; 10   667 0.0001                256            256\n#&gt; # â„¹ 246 more rows"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#your-turn-1",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#your-turn-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\n\nWhat advantage would a regular grid have?"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#update-parameter-ranges",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#update-parameter-ranges",
    "title": "3 - Tuning Hyperparameters",
    "section": "Update parameter ranges  ",
    "text": "Update parameter ranges  \n\nlgbm_param &lt;- \n  lgbm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  update(trees = trees(c(1L, 100L)),\n         learn_rate = learn_rate(c(-5, -1)))\n\nset.seed(712)\ngrid &lt;- \n  lgbm_param %&gt;% \n  grid_space_filling(size = 25)\n\ngrid\n#&gt; # A tibble: 25 Ã— 4\n#&gt;    trees learn_rate `agent hash` `company hash`\n#&gt;    &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1     1  0.00147            574            574\n#&gt;  2     5  0.00215           2048           2298\n#&gt;  3     9  0.0000215         1824            912\n#&gt;  4    13  0.00316           3250            512\n#&gt;  5    17  0.0001             512           2896\n#&gt;  6    21  0.0147             322           1625\n#&gt;  7    25  0.1               1448           1149\n#&gt;  8    29  0.000215          1290            256\n#&gt;  9    34  0.0000147          456            724\n#&gt; 10    38  0.0464             645            322\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#the-results",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#the-results",
    "title": "3 - Tuning Hyperparameters",
    "section": "The results  ",
    "text": "The results  \n\n\ngrid %&gt;% \n  ggplot(aes(trees, learn_rate)) +\n  geom_point(size = 4) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\nNote that the learning rates are uniform on the log-10 scale and this shows 2 of 4 dimensions."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#choosing-tuning-parameters",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#choosing-tuning-parameters",
    "title": "3 - Tuning Hyperparameters",
    "section": "Choosing tuning parameters     ",
    "text": "Choosing tuning parameters     \nLetâ€™s take our previous model and tune more parameters:\n\nlgbm_spec &lt;- \n  boost_tree(trees = tune(), learn_rate = tune(),  min_n = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow &lt;- workflow(hash_rec, lgbm_spec)\n\n# Update the feature hash ranges (log-2 units)\nlgbm_param &lt;-\n  lgbm_wflow %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(`agent hash`   = num_hash(c(3, 8)),\n         `company hash` = num_hash(c(3, 8)))"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search-3",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search-3",
    "title": "3 - Tuning Hyperparameters",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nset.seed(9)\nctrl &lt;- control_grid(save_pred = TRUE)\n\nlgbm_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = 25,\n    # The options below are not required by default\n    param_info = lgbm_param, \n    control = ctrl,\n    metrics = reg_metrics\n  )\n\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search-4",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#grid-search-4",
    "title": "3 - Tuning Hyperparameters",
    "section": "Grid Search   ",
    "text": "Grid Search   \n\nlgbm_res \n#&gt; # Tuning results\n#&gt; # 10-fold cross-validation using stratification \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics          .notes           .predictions        \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;              \n#&gt;  1 &lt;split [3372/377]&gt; Fold01 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,425 Ã— 9]&gt;\n#&gt;  2 &lt;split [3373/376]&gt; Fold02 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  3 &lt;split [3373/376]&gt; Fold03 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  4 &lt;split [3373/376]&gt; Fold04 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  5 &lt;split [3373/376]&gt; Fold05 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,400 Ã— 9]&gt;\n#&gt;  6 &lt;split [3374/375]&gt; Fold06 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,375 Ã— 9]&gt;\n#&gt;  7 &lt;split [3375/374]&gt; Fold07 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,350 Ã— 9]&gt;\n#&gt;  8 &lt;split [3376/373]&gt; Fold08 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,325 Ã— 9]&gt;\n#&gt;  9 &lt;split [3376/373]&gt; Fold09 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,325 Ã— 9]&gt;\n#&gt; 10 &lt;split [3376/373]&gt; Fold10 &lt;tibble [50 Ã— 9]&gt; &lt;tibble [0 Ã— 4]&gt; &lt;tibble [9,325 Ã— 9]&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#grid-results",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#grid-results",
    "title": "3 - Tuning Hyperparameters",
    "section": "Grid results ",
    "text": "Grid results \n\nautoplot(lgbm_res)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#tuning-results",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#tuning-results",
    "title": "3 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(lgbm_res)\n#&gt; # A tibble: 50 Ã— 11\n#&gt;    trees min_n learn_rate `agent hash` `company hash` .metric .estimator   mean     n std_err .config              \n#&gt;    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1   298    19   4.15e- 9          222             36 mae     standard   53.2      10 0.427   Preprocessor01_Model1\n#&gt;  2   298    19   4.15e- 9          222             36 rsq     standard    0.810    10 0.00686 Preprocessor01_Model1\n#&gt;  3  1394     5   5.82e- 6           28             21 mae     standard   52.9      10 0.424   Preprocessor02_Model1\n#&gt;  4  1394     5   5.82e- 6           28             21 rsq     standard    0.810    10 0.00800 Preprocessor02_Model1\n#&gt;  5   774    12   4.41e- 2           27             95 mae     standard    9.77     10 0.155   Preprocessor03_Model1\n#&gt;  6   774    12   4.41e- 2           27             95 rsq     standard    0.946    10 0.00341 Preprocessor03_Model1\n#&gt;  7  1342     7   6.84e-10           71             17 mae     standard   53.2      10 0.427   Preprocessor04_Model1\n#&gt;  8  1342     7   6.84e-10           71             17 rsq     standard    0.811    10 0.00785 Preprocessor04_Model1\n#&gt;  9   669    39   8.62e- 7          141            145 mae     standard   53.2      10 0.426   Preprocessor05_Model1\n#&gt; 10   669    39   8.62e- 7          141            145 rsq     standard    0.807    10 0.00639 Preprocessor05_Model1\n#&gt; # â„¹ 40 more rows"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#tuning-results-1",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#tuning-results-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Tuning results ",
    "text": "Tuning results \n\ncollect_metrics(lgbm_res, summarize = FALSE)\n#&gt; # A tibble: 500 Ã— 10\n#&gt;    id     trees min_n    learn_rate `agent hash` `company hash` .metric .estimator .estimate .config              \n#&gt;    &lt;chr&gt;  &lt;int&gt; &lt;int&gt;         &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1 Fold01   298    19 0.00000000415          222             36 mae     standard      51.8   Preprocessor01_Model1\n#&gt;  2 Fold01   298    19 0.00000000415          222             36 rsq     standard       0.821 Preprocessor01_Model1\n#&gt;  3 Fold02   298    19 0.00000000415          222             36 mae     standard      52.1   Preprocessor01_Model1\n#&gt;  4 Fold02   298    19 0.00000000415          222             36 rsq     standard       0.804 Preprocessor01_Model1\n#&gt;  5 Fold03   298    19 0.00000000415          222             36 mae     standard      52.2   Preprocessor01_Model1\n#&gt;  6 Fold03   298    19 0.00000000415          222             36 rsq     standard       0.786 Preprocessor01_Model1\n#&gt;  7 Fold04   298    19 0.00000000415          222             36 mae     standard      51.7   Preprocessor01_Model1\n#&gt;  8 Fold04   298    19 0.00000000415          222             36 rsq     standard       0.826 Preprocessor01_Model1\n#&gt;  9 Fold05   298    19 0.00000000415          222             36 mae     standard      55.2   Preprocessor01_Model1\n#&gt; 10 Fold05   298    19 0.00000000415          222             36 rsq     standard       0.845 Preprocessor01_Model1\n#&gt; # â„¹ 490 more rows"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#choose-a-parameter-combination",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#choose-a-parameter-combination",
    "title": "3 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \n\nshow_best(lgbm_res, metric = \"rsq\")\n#&gt; # A tibble: 5 Ã— 11\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .metric .estimator  mean     n std_err .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt; 1  1890    10    0.0159           115            174 rsq     standard   0.948    10 0.00334 Preprocessor12_Model1\n#&gt; 2   774    12    0.0441            27             95 rsq     standard   0.946    10 0.00341 Preprocessor03_Model1\n#&gt; 3  1638    36    0.0409            15            120 rsq     standard   0.945    10 0.00384 Preprocessor16_Model1\n#&gt; 4   963    23    0.00556          157             13 rsq     standard   0.937    10 0.00320 Preprocessor06_Model1\n#&gt; 5   590     5    0.00320           85             73 rsq     standard   0.908    10 0.00465 Preprocessor24_Model1"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#choose-a-parameter-combination-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Choose a parameter combination ",
    "text": "Choose a parameter combination \nCreate your own tibble for final parameters or use one of the tune::select_*() functions:\n\nlgbm_best &lt;- select_best(lgbm_res, metric = \"mae\")\nlgbm_best\n#&gt; # A tibble: 1 Ã— 6\n#&gt;   trees min_n learn_rate `agent hash` `company hash` .config              \n#&gt;   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt; &lt;chr&gt;                \n#&gt; 1  1890    10     0.0159          115            174 Preprocessor12_Model1"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#checking-calibration",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#checking-calibration",
    "title": "3 - Tuning Hyperparameters",
    "section": "Checking Calibration  ",
    "text": "Checking Calibration  \n\n\nlibrary(probably)\nlgbm_res %&gt;%\n  collect_predictions(\n    parameters = lgbm_best\n  ) %&gt;%\n  cal_plot_regression(\n    truth = avg_price_per_room,\n    estimate = .pred\n  )"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#running-in-parallel",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#running-in-parallel",
    "title": "3 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\n\n\n\nGrid search, combined with resampling, requires fitting a lot of models!\nThese models donâ€™t depend on one another and can be run in parallel.\n\nWe can use a parallel backend to do this:\n\ncores &lt;- parallelly::availableCores(logical = FALSE)\ncl &lt;- parallel::makePSOCKcluster(cores)\ndoParallel::registerDoParallel(cl)\n\n# Now call `tune_grid()`!\n\n# Shut it down with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#running-in-parallel-1",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#running-in-parallel-1",
    "title": "3 - Tuning Hyperparameters",
    "section": "Running in parallel",
    "text": "Running in parallel\nSpeed-ups are fairly linear up to the number of physical cores (10 here).\n\n\nFaceted on the expensiveness of preprocessing used."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#the-future-of-parallel-processing",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#the-future-of-parallel-processing",
    "title": "3 - Tuning Hyperparameters",
    "section": "The â€˜futureâ€™ of parallel processing",
    "text": "The â€˜futureâ€™ of parallel processing\nWe have relied on the foreach package for parallel processing.\nWe will start the transition to using the future package in the upcoming version of the tune package (version 1.3.0).\nThere will be a period of backward compatibility where you can still use foreach with future via the doFuture package. After that, the transition to future will occur.\nOverall, there will be minimal changes to your code."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#early-stopping-for-boosted-trees",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#early-stopping-for-boosted-trees",
    "title": "3 - Tuning Hyperparameters",
    "section": "Early stopping for boosted trees",
    "text": "Early stopping for boosted trees\nWe have directly optimized the number of trees as a tuning parameter.\nInstead we could\n\nSet the number of trees to a single large number.\nStop adding trees when performance gets worse.\n\nThis is known as â€œearly stoppingâ€ and there is a parameter for that: stop_iter.\nEarly stopping has a potential to decrease the tuning time."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#your-turn-2",
    "href": "archive/2024-08-posit-conf/advanced-03-tuning-hyperparameters.html#your-turn-2",
    "title": "3 - Tuning Hyperparameters",
    "section": "Your turn",
    "text": "Your turn\n\n\nSet trees = 2000 and tune the stop_iter parameter.\nNote that you will need to regenerate lgbm_param with your new workflow!\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#previously---setup",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#previously---setup",
    "title": "5 - Iterative Search",
    "section": "Previously - Setup",
    "text": "Previously - Setup\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(bonsai)\n\n# Max's usual settings: \ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(\n  pillar.advice = FALSE, \n  pillar.min_title_chars = Inf\n)\n\n\n\ndata(hotel_rates)\nset.seed(295)\nhotel_rates &lt;- \n  hotel_rates %&gt;% \n  sample_n(5000) %&gt;% \n  arrange(arrival_date) %&gt;% \n  select(-arrival_date) %&gt;%  \n  mutate(\n    company = factor(as.character(company)),\n    country = factor(as.character(country)),\n    agent = factor(as.character(agent))\n  )"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#previously---data-usage",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#previously---data-usage",
    "title": "5 - Iterative Search",
    "section": "Previously - Data Usage",
    "text": "Previously - Data Usage\n\nset.seed(4028)\nhotel_split &lt;- initial_split(hotel_rates, strata = avg_price_per_room)\n\nhotel_train &lt;- training(hotel_split)\nhotel_test &lt;- testing(hotel_split)\n\nset.seed(472)\nhotel_rs &lt;- vfold_cv(hotel_train, strata = avg_price_per_room)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#our-boosting-model",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#our-boosting-model",
    "title": "5 - Iterative Search",
    "section": "Our Boosting Model",
    "text": "Our Boosting Model\nWe used feature hashing to generate a smaller set of indicator columns to deal with the large number of levels for the agent and country predictors.\n\nTree-based models (and a few others) donâ€™t require indicators for categorical predictors. They can split on these variables as-is.\n\nWeâ€™ll keep all categorical predictors as factors and focus on optimizing additional boosting parameters."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#our-boosting-model-1",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#our-boosting-model-1",
    "title": "5 - Iterative Search",
    "section": "Our Boosting Model",
    "text": "Our Boosting Model\n\nlgbm_spec &lt;- \n  boost_tree(trees = 1000, learn_rate = tune(), min_n = tune(), \n             tree_depth = tune(), loss_reduction = tune(), \n             stop_iter = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"lightgbm\", num_threads = 1)\n\nlgbm_wflow &lt;- workflow(avg_price_per_room ~ ., lgbm_spec)\n\nlgbm_param &lt;- \n  lgbm_wflow %&gt;%\n    extract_parameter_set_dials() %&gt;%\n    update(learn_rate = learn_rate(c(-5, -1)))"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#iterative-search",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#iterative-search",
    "title": "5 - Iterative Search",
    "section": "Iterative Search",
    "text": "Iterative Search\nInstead of pre-defining a grid of candidate points, we can model our current results to predict what the next candidate point should be.\n\nSuppose that we are only tuning the learning rate in our boosted tree.\n\nWe could do something like:\nmae_pred &lt;- lm(mae ~ learn_rate, data = resample_results)\nand use this to predict and rank new learning rate candidates."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#iterative-search-1",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#iterative-search-1",
    "title": "5 - Iterative Search",
    "section": "Iterative Search",
    "text": "Iterative Search\nA linear model probably isnâ€™t the best choice though (more in a minute).\nTo illustrate the process, we resampled a large grid of learning rate values for our data to show what the relationship is between MAE and learning rate.\nNow suppose that we used a grid of three points in the parameter range for learning rateâ€¦"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#a-large-grid",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#a-large-grid",
    "title": "5 - Iterative Search",
    "section": "A Large Grid",
    "text": "A Large Grid"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#a-three-point-grid",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#a-three-point-grid",
    "title": "5 - Iterative Search",
    "section": "A Three Point Grid",
    "text": "A Three Point Grid"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#gaussian-processes-and-optimization",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#gaussian-processes-and-optimization",
    "title": "5 - Iterative Search",
    "section": "Gaussian Processes and Optimization",
    "text": "Gaussian Processes and Optimization\nWe can make a â€œmeta-modelâ€ with a small set of historical performance results.\nGaussian Processes (GP) models are a good choice to model performance.\n\nIt is a Bayesian model so we are using Bayesian Optimization (BO).\nFor regression, we can assume that our data are multivariate normal.\nWe also define a covariance function for the variance relationship between data points. A common one is:\n\n\\[\\operatorname{cov}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\exp\\left(-\\frac{1}{2}|\\boldsymbol{x}_i - \\boldsymbol{x}_j|^2\\right) + \\sigma^2_{ij}\\]\n\nGPs are good because\n\nthey are flexible regression models (in the sense that splines are flexible).\nwe need to get mean and variance predictions (and they are Bayesian)\ntheir variability is based on spatial distances.\n\nSome people use random forests (with conformal variance estimates) or other methods but GPs are most popular."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#predicting-candidates",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#predicting-candidates",
    "title": "5 - Iterative Search",
    "section": "Predicting Candidates",
    "text": "Predicting Candidates\nThe GP model can take candidate tuning parameter combinations as inputs and make predictions for performance (e.g.Â MAE)\n\nThe mean performance\nThe variance of performance\n\nThe variance is mostly driven by spatial variability (the previous equation).\nThe predicted variance is zero at locations of actual data points and becomes very high when far away from any observed data."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#your-turn",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#your-turn",
    "title": "5 - Iterative Search",
    "section": "Your turn",
    "text": "Your turn\n\n\nYour GP makes predictions on two new candidate tuning parameters.\nWe want to minimize MAE.\nWhich should we choose?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#gp-fit-ribbon-is-mean---1sd",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#gp-fit-ribbon-is-mean---1sd",
    "title": "5 - Iterative Search",
    "section": "GP Fit (ribbon is mean +/- 1SD)",
    "text": "GP Fit (ribbon is mean +/- 1SD)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#choosing-new-candidates",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#choosing-new-candidates",
    "title": "5 - Iterative Search",
    "section": "Choosing New Candidates",
    "text": "Choosing New Candidates\nThis isnâ€™t a very good fit but we can still use it.\nHow can we use the outputs to choose the next point to measure?\n\nAcquisition functions take the predicted mean and variance and use them to balance:\n\nexploration: new candidates should explore new areas.\nexploitation: new candidates must stay near existing values.\n\nExploration focuses on the variance, exploitation is about the mean."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#acquisition-functions",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#acquisition-functions",
    "title": "5 - Iterative Search",
    "section": "Acquisition Functions",
    "text": "Acquisition Functions\nWeâ€™ll use an acquisition function to select a new candidate.\nThe most popular method appears to be expected improvement (EI) above the current best results.\n\nZero at existing data points.\nThe expected improvement is integrated over all possible improvement (â€œexpectedâ€ in the probability sense).\n\nWe would probably pick the point with the largest EI as the next point.\n(There are other functions beyond EI.)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#expected-improvement",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#expected-improvement",
    "title": "5 - Iterative Search",
    "section": "Expected Improvement",
    "text": "Expected Improvement"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#iteration",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#iteration",
    "title": "5 - Iterative Search",
    "section": "Iteration",
    "text": "Iteration\nOnce we pick the candidate point, we measure performance for it (e.g.Â resampling).\n\nAnother GP is fit, EI is recomputed, and so on.\n\nWe stop when we have completed the allowed number of iterations or if we donâ€™t see any improvement after a pre-set number of attempts."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#gp-fit-with-four-points",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#gp-fit-with-four-points",
    "title": "5 - Iterative Search",
    "section": "GP Fit with four points",
    "text": "GP Fit with four points"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#expected-improvement-1",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#expected-improvement-1",
    "title": "5 - Iterative Search",
    "section": "Expected Improvement",
    "text": "Expected Improvement"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#gp-evolution",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#gp-evolution",
    "title": "5 - Iterative Search",
    "section": "GP Evolution",
    "text": "GP Evolution"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#expected-improvement-evolution",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#expected-improvement-evolution",
    "title": "5 - Iterative Search",
    "section": "Expected Improvement Evolution",
    "text": "Expected Improvement Evolution"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#bo-in-tidymodels",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#bo-in-tidymodels",
    "title": "5 - Iterative Search",
    "section": "BO in tidymodels",
    "text": "BO in tidymodels\nWeâ€™ll use a function called tune_bayes() that has very similar syntax to tune_grid().\n\nIt has an additional initial argument for the initial set of performance estimates and parameter combinations for the GP model."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#initial-grid-points",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#initial-grid-points",
    "title": "5 - Iterative Search",
    "section": "Initial grid points",
    "text": "Initial grid points\ninitial can be the results of another tune_*() function or an integer (in which case tune_grid() is used under to hood to make such an initial set of results).\n\nWeâ€™ll run the optimization more than once, so letâ€™s make an initial grid of results to serve as the substrate for the BO.\nI suggest at least the number of tuning parameters plus two as the initial grid for BO."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#an-initial-grid",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#an-initial-grid",
    "title": "5 - Iterative Search",
    "section": "An Initial Grid",
    "text": "An Initial Grid\n\nreg_metrics &lt;- metric_set(mae, rsq)\n\nset.seed(12)\ninit_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_grid(\n    resamples = hotel_rs,\n    grid = nrow(lgbm_param) + 2,\n    param_info = lgbm_param,\n    metrics = reg_metrics\n  )\n\nshow_best(init_res, metric = \"mae\") %&gt;% select(-.metric, -.estimator)\n#&gt; # A tibble: 5 Ã— 9\n#&gt;   min_n tree_depth learn_rate loss_reduction stop_iter  mean     n std_err .config             \n#&gt;   &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;     &lt;int&gt; &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1    16         12   0.0136         1.91e- 3         9  10.1    10   0.196 Preprocessor1_Model4\n#&gt; 2     9          4   0.0415         5.21e- 9        13  10.2    10   0.167 Preprocessor1_Model1\n#&gt; 3    25          8   0.00256        9.58e-10         7  14.1    10   0.278 Preprocessor1_Model7\n#&gt; 4    22          9   0.00154        5.77e- 6         5  19.3    10   0.326 Preprocessor1_Model5\n#&gt; 5    32          3   0.000144       3.02e+ 1        18  47.6    10   0.387 Preprocessor1_Model6"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#bo-using-tidymodels",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#bo-using-tidymodels",
    "title": "5 - Iterative Search",
    "section": "BO using tidymodels",
    "text": "BO using tidymodels\n\nctrl_bo &lt;- control_bayes(verbose_iter = TRUE) # &lt;- for demonstration\n\nset.seed(15)\nlgbm_bayes_res &lt;-\n  lgbm_wflow %&gt;%\n  tune_bayes(\n    resamples = hotel_rs,\n    initial = init_res,     # &lt;- initial results\n    iter = 20,\n    param_info = lgbm_param,\n    control = ctrl_bo,\n    metrics = reg_metrics\n  )\n#&gt; Optimizing mae using the expected improvement\n#&gt; \n#&gt; â”€â”€ Iteration 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=10.13 (@iter 0)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=32, tree_depth=12, learn_rate=0.0178, loss_reduction=1.03e-10, stop_iter=12\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â™¥ Newest results:    mae=10.08 (+/-0.175)\n#&gt; \n#&gt; â”€â”€ Iteration 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=10.08 (@iter 1)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=15, tree_depth=14, learn_rate=0.0977, loss_reduction=0.00535, stop_iter=4\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â™¥ Newest results:    mae=9.719 (+/-0.187)\n#&gt; \n#&gt; â”€â”€ Iteration 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.719 (@iter 2)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=38, tree_depth=1, learn_rate=0.1, loss_reduction=0.0809, stop_iter=10\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=15.45 (+/-0.253)\n#&gt; \n#&gt; â”€â”€ Iteration 4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.719 (@iter 2)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=32, tree_depth=1, learn_rate=0.00833, loss_reduction=1.31e-06, stop_iter=8\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=19.44 (+/-0.33)\n#&gt; \n#&gt; â”€â”€ Iteration 5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.719 (@iter 2)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=18, tree_depth=8, learn_rate=0.0495, loss_reduction=1.4e-06, stop_iter=5\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.757 (+/-0.146)\n#&gt; \n#&gt; â”€â”€ Iteration 6 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.719 (@iter 2)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=3, tree_depth=14, learn_rate=0.0319, loss_reduction=4.02e-09, stop_iter=17\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.76 (+/-0.163)\n#&gt; \n#&gt; â”€â”€ Iteration 7 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.719 (@iter 2)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=6, tree_depth=8, learn_rate=0.0883, loss_reduction=1.94e-08, stop_iter=4\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â™¥ Newest results:    mae=9.712 (+/-0.17)\n#&gt; \n#&gt; â”€â”€ Iteration 8 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.712 (@iter 7)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=6, tree_depth=8, learn_rate=0.025, loss_reduction=7.82e-05, stop_iter=19\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.838 (+/-0.17)\n#&gt; \n#&gt; â”€â”€ Iteration 9 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.712 (@iter 7)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=32, tree_depth=6, learn_rate=0.0737, loss_reduction=2.15e-07, stop_iter=8\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=10.06 (+/-0.2)\n#&gt; \n#&gt; â”€â”€ Iteration 10 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.712 (@iter 7)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=5, tree_depth=11, learn_rate=0.0451, loss_reduction=3.45e-10, stop_iter=7\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â™¥ Newest results:    mae=9.637 (+/-0.156)\n#&gt; \n#&gt; â”€â”€ Iteration 11 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.637 (@iter 10)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=2, tree_depth=7, learn_rate=0.0372, loss_reduction=2.44e-09, stop_iter=11\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.761 (+/-0.171)\n#&gt; \n#&gt; â”€â”€ Iteration 12 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.637 (@iter 10)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=26, tree_depth=15, learn_rate=0.00626, loss_reduction=0.00554, stop_iter=16\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=10.79 (+/-0.198)\n#&gt; \n#&gt; â”€â”€ Iteration 13 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.637 (@iter 10)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=29, tree_depth=10, learn_rate=0.0996, loss_reduction=4.5e-05, stop_iter=16\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.838 (+/-0.169)\n#&gt; \n#&gt; â”€â”€ Iteration 14 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.637 (@iter 10)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=12, tree_depth=13, learn_rate=0.085, loss_reduction=2.16, stop_iter=9\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.795 (+/-0.16)\n#&gt; \n#&gt; â”€â”€ Iteration 15 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.637 (@iter 10)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=4, tree_depth=9, learn_rate=0.0418, loss_reduction=0.00293, stop_iter=7\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.75 (+/-0.168)\n#&gt; \n#&gt; â”€â”€ Iteration 16 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.637 (@iter 10)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=6, tree_depth=15, learn_rate=0.0703, loss_reduction=5.15e-10, stop_iter=13\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.672 (+/-0.134)\n#&gt; \n#&gt; â”€â”€ Iteration 17 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.637 (@iter 10)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=27, tree_depth=15, learn_rate=0.0956, loss_reduction=3.74e-10, stop_iter=17\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.861 (+/-0.197)\n#&gt; \n#&gt; â”€â”€ Iteration 18 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.637 (@iter 10)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=2, tree_depth=11, learn_rate=0.0871, loss_reduction=0.00196, stop_iter=18\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â™¥ Newest results:    mae=9.601 (+/-0.147)\n#&gt; \n#&gt; â”€â”€ Iteration 19 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.601 (@iter 18)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=2, tree_depth=12, learn_rate=0.0991, loss_reduction=8.45e-06, stop_iter=14\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.61 (+/-0.17)\n#&gt; \n#&gt; â”€â”€ Iteration 20 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; i Current best:      mae=9.601 (@iter 18)\n#&gt; i Gaussian process model\n#&gt; âœ“ Gaussian process model\n#&gt; i Generating 5000 candidates\n#&gt; i Predicted candidates\n#&gt; i min_n=4, tree_depth=15, learn_rate=0.0206, loss_reduction=1.46e-06, stop_iter=15\n#&gt; i Estimating performance\n#&gt; âœ“ Estimating performance\n#&gt; â“§ Newest results:    mae=9.881 (+/-0.177)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#best-results",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#best-results",
    "title": "5 - Iterative Search",
    "section": "Best results",
    "text": "Best results\n\nshow_best(lgbm_bayes_res, metric = \"mae\") %&gt;% select(-.metric, -.estimator)\n#&gt; # A tibble: 5 Ã— 10\n#&gt;   min_n tree_depth learn_rate loss_reduction stop_iter  mean     n std_err .config .iter\n#&gt;   &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;     &lt;int&gt; &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n#&gt; 1     2         11     0.0871       1.96e- 3        18  9.60    10   0.147 Iter18     18\n#&gt; 2     2         12     0.0991       8.45e- 6        14  9.61    10   0.170 Iter19     19\n#&gt; 3     5         11     0.0451       3.45e-10         7  9.64    10   0.156 Iter10     10\n#&gt; 4     6         15     0.0703       5.15e-10        13  9.67    10   0.134 Iter16     16\n#&gt; 5     6          8     0.0883       1.94e- 8         4  9.71    10   0.170 Iter7       7"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#plotting-bo-results",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#plotting-bo-results",
    "title": "5 - Iterative Search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(lgbm_bayes_res, metric = \"mae\")"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#plotting-bo-results-1",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#plotting-bo-results-1",
    "title": "5 - Iterative Search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"parameters\")"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#plotting-bo-results-2",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#plotting-bo-results-2",
    "title": "5 - Iterative Search",
    "section": "Plotting BO Results",
    "text": "Plotting BO Results\n\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"performance\")"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#enhance",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#enhance",
    "title": "5 - Iterative Search",
    "section": "ENHANCE",
    "text": "ENHANCE\n\nautoplot(lgbm_bayes_res, metric = \"mae\", type = \"performance\") +\n  ylim(c(9, 14))"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#your-turn-1",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#your-turn-1",
    "title": "5 - Iterative Search",
    "section": "Your turn",
    "text": "Your turn\nLetâ€™s try a different acquisition function: conf_bound(kappa).\nWeâ€™ll use the objective argument to set it.\nChoose your own kappa value:\n\nLarger values will explore the space more.\nâ€œLargeâ€ values are usually less than one.\n\nBonus points: Before the optimization is done, press &lt;esc&gt; and see what happens.\n\n\n\nâˆ’+\n10:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#notes",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#notes",
    "title": "5 - Iterative Search",
    "section": "Notes",
    "text": "Notes\n\nStopping tune_bayes() will return the current results.\nParallel processing can still be used to more efficiently measure each candidate point.\nThere are a lot of other iterative methods that you can use.\nThe finetune package also has functions for simulated annealing search."
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#finalizing-the-model",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#finalizing-the-model",
    "title": "5 - Iterative Search",
    "section": "Finalizing the Model",
    "text": "Finalizing the Model\nLetâ€™s say that weâ€™ve tried a lot of different models and we like our lightgbm model the most.\nWhat do we do now?\n\nFinalize the workflow by choosing the values for the tuning parameters.\nFit the model on the entire training set.\nVerify performance using the test set.\nDocument and publish the model(?)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#locking-down-the-tuning-parameters",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#locking-down-the-tuning-parameters",
    "title": "5 - Iterative Search",
    "section": "Locking Down the Tuning Parameters",
    "text": "Locking Down the Tuning Parameters\nWe can take the results of the Bayesian optimization and accept the best results:\n\nbest_param &lt;- select_best(lgbm_bayes_res, metric = \"mae\")\nfinal_wflow &lt;- \n  lgbm_wflow %&gt;% \n  finalize_workflow(best_param)\nfinal_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: boost_tree()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; avg_price_per_room ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Boosted Tree Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt;   min_n = 2\n#&gt;   tree_depth = 11\n#&gt;   learn_rate = 0.0871075826616985\n#&gt;   loss_reduction = 0.00195652467829182\n#&gt;   stop_iter = 18\n#&gt; \n#&gt; Engine-Specific Arguments:\n#&gt;   num_threads = 1\n#&gt; \n#&gt; Computational engine: lightgbm"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#the-final-fit",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#the-final-fit",
    "title": "5 - Iterative Search",
    "section": "The Final Fit",
    "text": "The Final Fit\nWe can use individual functions:\nfinal_fit &lt;- final_wflow %&gt;% fit(data = hotel_train)\n\n# then predict() or augment() \n# then compute metrics\n\nRemember that there is also a convenience function to do all of this:\n\nset.seed(3893)\nfinal_res &lt;- final_wflow %&gt;% last_fit(hotel_split, metrics = reg_metrics)\nfinal_res\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits              id               .metrics         .notes           .predictions         .workflow \n#&gt;   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;           &lt;list&gt;               &lt;list&gt;    \n#&gt; 1 &lt;split [3749/1251]&gt; train/test split &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1,251 Ã— 4]&gt; &lt;workflow&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/advanced-05-iterative.html#test-set-results",
    "href": "archive/2024-08-posit-conf/advanced-05-iterative.html#test-set-results",
    "title": "5 - Iterative Search",
    "section": "Test Set Results",
    "text": "Test Set Results\n\n\n\nfinal_res %&gt;% \n  collect_predictions() %&gt;% \n  cal_plot_regression(\n    truth = avg_price_per_room, \n    estimate = .pred)\n\nTest set performance:\n\nfinal_res %&gt;% collect_metrics()\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard       9.60  Preprocessor1_Model1\n#&gt; 2 rsq     standard       0.949 Preprocessor1_Model1\n\n\n\n\n\n\n\n\n\n\n\n\nRecall that resampling predicted the MAE to be 9.601."
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#section",
    "href": "archive/2024-08-posit-conf/annotations.html#section",
    "title": "Annotations",
    "section": "ğŸ‘€",
    "text": "ğŸ‘€\nThis page contains annotations for selected slides.\nThereâ€™s a lot that we want to tell you. We donâ€™t want people to have to frantically scribble down things that we say that are not on the slides.\nWeâ€™ve added sections to this document with longer explanations and links to other resources."
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#data-splitting-and-spending",
    "href": "archive/2024-08-posit-conf/annotations.html#data-splitting-and-spending",
    "title": "Annotations",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nMore about the initial data split can be found in Chapter 3 of Applied Machine Learning for Tabular Data (AML4TD).\nIn particular, a three-way split into training, validation, and testing set can be done via\n\nset.seed(123)\ninitial_validation_split(forested, prop = c(0.6, 0.2))\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;4264/1421/1422/7107&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#what-is-set.seed",
    "href": "archive/2024-08-posit-conf/annotations.html#what-is-set.seed",
    "title": "Annotations",
    "section": "What is set.seed()?",
    "text": "What is set.seed()?\nWhat does set.seed() do?\nWeâ€™ll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random).\nThink of PRN as a box that takes a starting value (the â€œseedâ€) that produces random numbers using that starting value as an input into its process.\nIf we know a seed value, we can reproduce our â€œrandomâ€ numbers. To use a different set of random numbers, choose a different seed value.\nFor example:\n\nset.seed(1)\nrunif(3)\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\n# Get a new set of random numbers:\nset.seed(2)\nrunif(3)\n#&gt; [1] 0.1848823 0.7023740 0.5733263\n\n# We can reproduce the old ones with the same seed\nset.seed(1)\nrunif(3)\n#&gt; [1] 0.2655087 0.3721239 0.5728534\n\nIf we donâ€™t set the seed, R uses the clock time and the process ID to create a seed. This isnâ€™t reproducible.\nSince we want our code to be reproducible, we set the seeds before random numbers are used.\nIn theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same and we donâ€™t get reproducible results.\nThe value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to â€œspread the randomness aroundâ€. It is basically:\n\ncat(paste0(\"set.seed(\", sample.int(10000, 5), \")\", collapse = \"\\n\"))\n#&gt; set.seed(9725)\n#&gt; set.seed(8462)\n#&gt; set.seed(4050)\n#&gt; set.seed(8789)\n#&gt; set.seed(1301)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#what-is-wrong-with-this",
    "href": "archive/2024-08-posit-conf/annotations.html#what-is-wrong-with-this",
    "title": "Annotations",
    "section": "What is wrong with this?",
    "text": "What is wrong with this?\nIf we treat the preprocessing as a separate task, it raises the risk that we might accidentally overfit to the data at hand.\nFor example, someone might estimate something from the entire data set (such as the principle components) and treat that data as if it were known (and not estimated). Depending on the what was done with the data, consequences in doing that could be:\n\nYour performance metrics are slightly-to-moderately optimistic (e.g.Â you might think your accuracy is 85% when it is actually 75%)\nA consequential component of the analysis is not right and the model just doesnâ€™t work.\n\nThe big issue here is that you wonâ€™t be able to figure this out until you get a new piece of data, such as the test set.\nA really good example of this is in â€˜Selection bias in gene extraction on the basis of microarray gene-expression dataâ€™. The authors re-analyze a previous publication and show that the original researchers did not include feature selection in the workflow. Because of that, their performance statistics were extremely optimistic. In one case, they could do the original analysis on complete noise and still achieve zero errors.\nGenerally speaking, this problem is referred to as data leakage. Some other references:\n\nOverfitting to Predictors and External Validation\nAre We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning\nNavigating the pitfalls of applying machine learning in genomics\nA review of feature selection techniques in bioinformatics\nOn Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation"
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#brier-score",
    "href": "archive/2024-08-posit-conf/annotations.html#brier-score",
    "title": "Annotations",
    "section": "Brier score",
    "text": "Brier score\nThe Brier score measures how close a model probability estimate is to its best possible value (i.e., zero or one).\nIn the best case, the model is perfect, and every prediction equals 0.0 or 1.0 (depending on the true class). In this case, the Brier score is zero.\nWhen the model is uninformative and there are two classes, the worst-case values range from 0.25 to about 0.50. Imagine that the model predicts the same noninformative prediction of 50% (basically â€œÂ¯\\(ãƒ„)/Â¯â€). In that case, every prediction is either \\((0.00 - 0.50)^2\\) or \\((1.00 - 0.50)^2\\). The average of those is 0.25.\nThere are many different ways a model can be bad though, and some of these will produce Brier scores between 0.25 and 0.50."
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#dangers-of-overfitting",
    "href": "archive/2024-08-posit-conf/annotations.html#dangers-of-overfitting",
    "title": "Annotations",
    "section": "Dangers of overfitting",
    "text": "Dangers of overfitting\nSee the â€œOverfittingâ€ chapter of AML4TD for more information."
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#where-are-the-fitted-models",
    "href": "archive/2024-08-posit-conf/annotations.html#where-are-the-fitted-models",
    "title": "Annotations",
    "section": "Where are the fitted models?",
    "text": "Where are the fitted models?\nThe primary purpose of resampling is to estimate model performance. The models are almost never needed again.\nAlso, if the data set is large, the model object may require a lot of memory to save so, by default, we donâ€™t keep them.\nFor more advanced use cases, you can extract and save them. See:\n\nhttps://www.tmwr.org/resampling.html#extract\nhttps://www.tidymodels.org/learn/models/coefficients/ (an example)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#resampling-strategy",
    "href": "archive/2024-08-posit-conf/annotations.html#resampling-strategy",
    "title": "Annotations",
    "section": "Resampling Strategy",
    "text": "Resampling Strategy\nThese data have a time component, and while not a typical time series data set, we have the option to use a time series resampling method.\nAn example is shown in the extra slides â€œCase Study on Transportationâ€.\nConsider the agent data. Cross-validation may not group all of an agentâ€™s data into the analysis or assessment sets. In this case, our analysis data might have future data that is later than the agentâ€™s data in the assessment set."
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#different-types-of-grids",
    "href": "archive/2024-08-posit-conf/annotations.html#different-types-of-grids",
    "title": "Annotations",
    "section": "Different types of grids",
    "text": "Different types of grids\nMore on space-filling designs in Chapters 4 and 5 of Surrogates: Gaussian process modeling, design, and optimization for the applied sciences.\nIn the next version of tune (version 1.3.0) an improved set of space-filling designs will be the first choice if you ask for an automated grid."
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#update-parameter-ranges",
    "href": "archive/2024-08-posit-conf/annotations.html#update-parameter-ranges",
    "title": "Annotations",
    "section": "Update parameter ranges",
    "text": "Update parameter ranges\nIn about 90% of the cases, the dials function that you use to update the parameter range has the same name as the argument. For example, if you were to update the mtry parameter in a random forests model, the code would look like\n\nparameter_object %&gt;% \n  update(mtry = mtry(c(1, 100)))\n\nThere are some cases where the parameter function, or its associated values, are different from the argument name.\nFor example, with step_spline_naturall(), we might want to tune the deg_free argument (for the degrees of freedom of a spline function. ). In this case, the argument name is deg_free but we update it with spline_degree().\ndeg_free represents the general concept of degrees of freedom and could be associated with many different things. For example, if we ever had an argument that was the number of degrees of freedom for a \\(t\\) distribution, we would call that argument deg_free.\nFor splines, we probably want a wider range for the degrees of freedom. We made a specialized function called spline_degree() to be used in these cases.\nHow can you tell when this happens? There is a helper function called tunable() and that gives information on how we make the default ranges for parameters. There is a column in these objects names call_info:\n\nlibrary(tidymodels)\nns_tunable &lt;- \n  recipe(mpg ~ ., data = mtcars) %&gt;% \n  step_spline_natural(dis, deg_free = tune()) %&gt;% \n  tunable()\n\nns_tunable\n#&gt; # A tibble: 1 Ã— 5\n#&gt;   name     call_info        source component           component_id        \n#&gt;   &lt;chr&gt;    &lt;list&gt;           &lt;chr&gt;  &lt;chr&gt;               &lt;chr&gt;               \n#&gt; 1 deg_free &lt;named list [3]&gt; recipe step_spline_natural spline_natural_P1Tjg\nns_tunable$call_info\n#&gt; [[1]]\n#&gt; [[1]]$pkg\n#&gt; [1] \"dials\"\n#&gt; \n#&gt; [[1]]$fun\n#&gt; [1] \"spline_degree\"\n#&gt; \n#&gt; [[1]]$range\n#&gt; [1]  2 15"
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#early-stopping-for-boosted-trees",
    "href": "archive/2024-08-posit-conf/annotations.html#early-stopping-for-boosted-trees",
    "title": "Annotations",
    "section": "Early stopping for boosted trees",
    "text": "Early stopping for boosted trees\nWhen deciding on the number of boosting iterations, there are two main strategies:\n\nDirectly tune it (trees = tune())\nSet it to one value and tune the number of early stopping iterations (trees = 500, stop_iter = tune()).\n\nEarly stopping is when we monitor the performance of the model. If the model doesnâ€™t make any improvements for stop_iter iterations, training stops.\nHereâ€™s an example where, after eleven iterations, performance starts to get worse.\n\n\n\n\n\n\n\n\n\nThis is likely due to over-fitting so we stop the model at eleven boosting iterations.\nEarly stopping usually has good results and takes far less time.\nWe could an engine argument called validation here. Thatâ€™s not an argument to any function in the lightgbm package.\nbonsai has its own wrapper around (lightgbm::lgb.train()) called bonsai::train_lightgbm(). We use that here and it has a validation argument.\nHow would you know that? There are a few different ways:\n\nLook at the documentation in ?boost_tree and click on the lightgbm entry in the engine list.\nCheck out the pkgdown reference website https://parsnip.tidymodels.org/reference/index.html\nRun the translate() function on the parsnip specification object.\n\nThe first two options are best since they tell you a lot more about the particularities of each model engine (there are a lot for lightgbm)."
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#gaussian-processes-and-optimization",
    "href": "archive/2024-08-posit-conf/annotations.html#gaussian-processes-and-optimization",
    "title": "Annotations",
    "section": "Gaussian Processes and Optimization",
    "text": "Gaussian Processes and Optimization\nSome other references for GPâ€™s:\n\nChapter 5 of Surrogates: Gaussian process modeling, design, and optimization for the applied sciences\nBayesian Optimization, Chapter 3 (pdf)\nGaussian Processes for Machine Learning (pdf)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#acquisition-functions",
    "href": "archive/2024-08-posit-conf/annotations.html#acquisition-functions",
    "title": "Annotations",
    "section": "Acquisition Functions",
    "text": "Acquisition Functions\nMore references:\n\nChapter 7 of Surrogates: Gaussian process modeling, design, and optimization for the applied sciences\nBayesian Optimization, Chapter 6 (pdf)\nGaussian Processes for Machine Learning"
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#per-agent-statistics",
    "href": "archive/2024-08-posit-conf/annotations.html#per-agent-statistics",
    "title": "Annotations",
    "section": "Per-agent statistics",
    "text": "Per-agent statistics\nThe effect encoding method essentially takes the effect of a variable, like agent, and makes a data column for that effect. In our example, affect of the agent on the ADR is quantified by a model and then added as a data column to be used in the model.\nSuppose agent Max has a single reservation in the data and it had an ADR of â‚¬200. If we used a naive estimate for Maxâ€™s effect, the model is being told that Max should always produce an effect of â‚¬200. Thatâ€™s a very poor estimate since it is from a single data point.\nContrast this with seasoned agent Davis, who has taken 250 reservations with an average ADR of â‚¬100. Davisâ€™s mean is more predictive because it is estimated with better data (i.e., more total reservations). Partial pooling leverages the entire data set and can borrow strength from all of the agents. It is a common tool in Bayesian estimation and non-Bayesian mixed models. If a agentâ€™s data is of good quality, the partial pooling effect estimate is closer to the raw mean. Maxâ€™s data is not great and is â€œshrunkâ€ towards the center of the overall average. Since there is so little known about Maxâ€™s reservation history, this is a better effect estimate (until more data is available for him).\nThe Stan documentation has a pretty good vignette on this: https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html\nAlso, Bayes Rules! has a nice section on this: https://www.bayesrulesbook.com/chapter-15.html\nSince this example has a numeric outcome, partial pooling is very similar to the Jamesâ€“Stein estimator: https://en.wikipedia.org/wiki/Jamesâ€“Stein_estimator"
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#agent-effects",
    "href": "archive/2024-08-posit-conf/annotations.html#agent-effects",
    "title": "Annotations",
    "section": "Agent effects",
    "text": "Agent effects\nEffect encoding might result in a somewhat circular argument: the column is more likely to be important to the model since it is the output of a separate model. The risk here is that we might over-fit the effect to the data. For this reason, it is super important to make sure that we verify that we arenâ€™t overfitting by checking with resampling (or a validation set).\nPartial pooling somewhat lowers the risk of overfitting since it tends to correct for agents with small sample sizes. It canâ€™t correct for improper data usage or data leakage though."
  },
  {
    "objectID": "archive/2024-08-posit-conf/annotations.html#a-recipe---handle-correlations",
    "href": "archive/2024-08-posit-conf/annotations.html#a-recipe---handle-correlations",
    "title": "Annotations",
    "section": "A recipe - handle correlations",
    "text": "A recipe - handle correlations\nIn this code chunk, Whatâ€™s the story with !!stations?\n\nchi_pca_rec &lt;- \n  chi_rec %&gt;% \n  step_normalize(all_of(!!stations)) %&gt;% \n  step_pca(all_of(!!stations), num_comp = tune())\n\nstations is a vector of names of 20 columns that we want to use in the steps. If the list were shorter, we could type them in (e.g., c(\"col1\", \"col2\") etc.).\nThe vector lives in our global workspace, and if we are in parallel, the worker processes might not have access to stations. The !! (frequently said as â€œbang bangâ€) inserts the actual contents of the vector into the all_of() calls so that it looks like you just typed it in.\nThis means that the parallel process workers have a copy of the data in their reach and the code will run without error."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#chicago-l-train-data",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#chicago-l-train-data",
    "title": "Case Study on Transportation",
    "section": "Chicago L-Train data",
    "text": "Chicago L-Train data\nSeveral years worth of pre-pandemic data were assembled to try to predict the daily number of people entering the Clark and Lake elevated (â€œLâ€) train station in Chicago.\nMore information:\n\nSeveral Chapters in Feature Engineering and Selection.\n\nStart with Section 4.1\nSee Section 1.3\n\nVideo: The Global Pandemic Ruined My Favorite Data Set"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#predictors",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#predictors",
    "title": "Case Study on Transportation",
    "section": "Predictors",
    "text": "Predictors\n\nthe 14-day lagged ridership at this and other stations (units: thousands of rides/day)\nweather data\nhome/away game schedules for Chicago teams\nthe date\n\nThe data are in modeldata. See ?Chicago."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#l-train-locations",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#l-train-locations",
    "title": "Case Study on Transportation",
    "section": "L Train Locations",
    "text": "L Train Locations"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#your-turn-explore-the-data",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#your-turn-explore-the-data",
    "title": "Case Study on Transportation",
    "section": "Your turn: Explore the Data",
    "text": "Your turn: Explore the Data\nTake a look at these data for a few minutes and see if you can find any interesting characteristics in the predictors or the outcome.\n\nlibrary(tidymodels)\nlibrary(rules)\ndata(\"Chicago\")\ndim(Chicago)\n#&gt; [1] 5698   50\nstations\n#&gt;  [1] \"Austin\"           \"Quincy_Wells\"     \"Belmont\"          \"Archer_35th\"     \n#&gt;  [5] \"Oak_Park\"         \"Western\"          \"Clark_Lake\"       \"Clinton\"         \n#&gt;  [9] \"Merchandise_Mart\" \"Irving_Park\"      \"Washington_Wells\" \"Harlem\"          \n#&gt; [13] \"Monroe\"           \"Polk\"             \"Ashland\"          \"Kedzie\"          \n#&gt; [17] \"Addison\"          \"Jefferson_Park\"   \"Montrose\"         \"California\"\n\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#splitting-with-chicago-data",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#splitting-with-chicago-data",
    "title": "Case Study on Transportation",
    "section": "Splitting with Chicago data ",
    "text": "Splitting with Chicago data \nLetâ€™s put the last two weeks of data into the test set. initial_time_split() can be used for this purpose:\n\ndata(Chicago)\n\nchi_split &lt;- initial_time_split(Chicago, prop = 1 - (14/nrow(Chicago)))\nchi_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;5684/14/5698&gt;\n\nchi_train &lt;- training(chi_split)\nchi_test  &lt;- testing(chi_split)\n\n## training\nnrow(chi_train)\n#&gt; [1] 5684\n \n## testing\nnrow(chi_test)\n#&gt; [1] 14"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#time-series-resampling",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#time-series-resampling",
    "title": "Case Study on Transportation",
    "section": "Time series resampling",
    "text": "Time series resampling\nOur Chicago data is over time. Regular cross-validation, which uses random sampling, may not be the best idea.\nWe can emulate our training/test split by making similar resamples.\n\nFold 1: Take the first X years of data as the analysis set, the next 2 weeks as the assessment set.\nFold 2: Take the first X years + 2 weeks of data as the analysis set, the next 2 weeks as the assessment set.\nand so on"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#rolling-forecast-origin-resampling",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#rolling-forecast-origin-resampling",
    "title": "Case Study on Transportation",
    "section": "Rolling forecast origin resampling",
    "text": "Rolling forecast origin resampling\n\n\nThis image shows overlapping assessment sets. We will use non-overlapping data but it could be done wither way."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#times-series-resampling",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#times-series-resampling",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n\n\n\n\n  )\n\nUse the date column to find the date data."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#times-series-resampling-1",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#times-series-resampling-1",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n\n\n\n  )\n\nOur units will be weeks."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#times-series-resampling-2",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#times-series-resampling-2",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15  \n    \n    \n  )\n\nEvery analysis set has 15 years of data"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#times-series-resampling-3",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#times-series-resampling-3",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n\n  )\n\nEvery assessment set has 2 weeks of data"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#times-series-resampling-4",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#times-series-resampling-4",
    "title": "Case Study on Transportation",
    "section": "Times series resampling ",
    "text": "Times series resampling \n\nchi_rs &lt;-\n  chi_train %&gt;%\n  sliding_period(\n    index = \"date\",  \n    period = \"week\",\n    lookback = 52 * 15,\n    assess_stop = 2,\n    step = 2 \n  )\n\nIncrement by 2 weeks so that there are no overlapping assessment sets.\n\nchi_rs$splits[[1]] %&gt;% assessment() %&gt;% pluck(\"date\") %&gt;% range()\n#&gt; [1] \"2016-01-07\" \"2016-01-20\"\nchi_rs$splits[[2]] %&gt;% assessment() %&gt;% pluck(\"date\") %&gt;% range()\n#&gt; [1] \"2016-01-21\" \"2016-02-03\""
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#our-resampling-object",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#our-resampling-object",
    "title": "Case Study on Transportation",
    "section": "Our resampling object ",
    "text": "Our resampling object \n\n\n\nchi_rs\n#&gt; # Sliding period resampling \n#&gt; # A tibble: 16 Ã— 2\n#&gt;    splits            id     \n#&gt;    &lt;list&gt;            &lt;chr&gt;  \n#&gt;  1 &lt;split [5463/14]&gt; Slice01\n#&gt;  2 &lt;split [5467/14]&gt; Slice02\n#&gt;  3 &lt;split [5467/14]&gt; Slice03\n#&gt;  4 &lt;split [5467/14]&gt; Slice04\n#&gt;  5 &lt;split [5467/14]&gt; Slice05\n#&gt;  6 &lt;split [5467/14]&gt; Slice06\n#&gt;  7 &lt;split [5467/14]&gt; Slice07\n#&gt;  8 &lt;split [5467/14]&gt; Slice08\n#&gt;  9 &lt;split [5467/14]&gt; Slice09\n#&gt; 10 &lt;split [5467/14]&gt; Slice10\n#&gt; 11 &lt;split [5467/14]&gt; Slice11\n#&gt; 12 &lt;split [5467/14]&gt; Slice12\n#&gt; 13 &lt;split [5467/14]&gt; Slice13\n#&gt; 14 &lt;split [5467/14]&gt; Slice14\n#&gt; 15 &lt;split [5467/14]&gt; Slice15\n#&gt; 16 &lt;split [5467/11]&gt; Slice16\n\n\n\n\nWe will fit 16 models on 16 slightly different analysis sets.\nEach will produce a separate performance metrics.\nWe will average the 16 metrics to get the resampling estimate of that statistic."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#feature-engineering-with-recipes",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#feature-engineering-with-recipes",
    "title": "Case Study on Transportation",
    "section": "Feature engineering with recipes ",
    "text": "Feature engineering with recipes \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train)\n\nBased on the formula, the function assigns columns to roles of â€œoutcomeâ€ or â€œpredictorâ€"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#a-recipe",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#a-recipe",
    "title": "Case Study on Transportation",
    "section": "A recipe",
    "text": "A recipe\n\nsummary(chi_rec)\n#&gt; # A tibble: 50 Ã— 4\n#&gt;    variable         type      role      source  \n#&gt;    &lt;chr&gt;            &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 Austin           &lt;chr [2]&gt; predictor original\n#&gt;  2 Quincy_Wells     &lt;chr [2]&gt; predictor original\n#&gt;  3 Belmont          &lt;chr [2]&gt; predictor original\n#&gt;  4 Archer_35th      &lt;chr [2]&gt; predictor original\n#&gt;  5 Oak_Park         &lt;chr [2]&gt; predictor original\n#&gt;  6 Western          &lt;chr [2]&gt; predictor original\n#&gt;  7 Clark_Lake       &lt;chr [2]&gt; predictor original\n#&gt;  8 Clinton          &lt;chr [2]&gt; predictor original\n#&gt;  9 Merchandise_Mart &lt;chr [2]&gt; predictor original\n#&gt; 10 Irving_Park      &lt;chr [2]&gt; predictor original\n#&gt; # â„¹ 40 more rows"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#a-recipe---work-with-dates",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#a-recipe---work-with-dates",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) \n\nThis creates three new columns in the data based on the date. Note that the day-of-the-week column is a factor."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#a-recipe---work-with-dates-1",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#a-recipe---work-with-dates-1",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %&gt;% \n  step_holiday(date) \n\nAdd indicators for major holidays. Specific holidays, especially those non-USA, can also be generated.\nAt this point, we donâ€™t need date anymore. Instead of deleting it (there is a step for that) we will change its role to be an identification variable.\n\nWe might want to change the role (instead of removing the column) because it will stay in the data set (even when resampled) and might be useful for diagnosing issues."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#a-recipe---work-with-dates-2",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#a-recipe---work-with-dates-2",
    "title": "Case Study on Transportation",
    "section": "A recipe - work with dates ",
    "text": "A recipe - work with dates \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %&gt;% \n  step_holiday(date) %&gt;% \n  update_role(date, new_role = \"id\") %&gt;%\n  update_role_requirements(role = \"id\", bake = TRUE)\n\ndate is still in the data set but tidymodels knows not to treat it as an analysis column.\nupdate_role_requirements() is needed to make sure that this column is required when making new data points. The help page has a good discussion about the nuances."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#a-recipe---remove-constant-columns",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#a-recipe---remove-constant-columns",
    "title": "Case Study on Transportation",
    "section": "A recipe - remove constant columns ",
    "text": "A recipe - remove constant columns \n\nchi_rec &lt;- \n  recipe(ridership ~ ., data = chi_train) %&gt;% \n  step_date(date, features = c(\"dow\", \"month\", \"year\")) %&gt;% \n  step_holiday(date) %&gt;% \n  update_role(date, new_role = \"id\") %&gt;%\n  update_role_requirements(role = \"id\", bake = TRUE) %&gt;% \n  step_zv(all_nominal_predictors())"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#a-recipe---handle-correlations",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#a-recipe---handle-correlations",
    "title": "Case Study on Transportation",
    "section": "A recipe - handle correlations ",
    "text": "A recipe - handle correlations \nThe station columns have a very high degree of correlation.\nWe might want to decorrelated them with principle component analysis to help the model fits go more easily.\nThe vector stations contains all station names and can be used to identify all the relevant columns.\n\nchi_pca_rec &lt;- \n  chi_rec %&gt;% \n  step_normalize(all_of(!!stations)) %&gt;% \n  step_pca(all_of(!!stations), num_comp = tune())\n\nWeâ€™ll tune the number of PCA components for (default) values of one to four."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#make-some-models",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#make-some-models",
    "title": "Case Study on Transportation",
    "section": "Make some models     ",
    "text": "Make some models     \nLetâ€™s try three models. The first one requires the rules package (loaded earlier).\n\ncb_spec &lt;- cubist_rules(committees = 25, neighbors = tune())\nmars_spec &lt;- mars(prod_degree = tune()) %&gt;% set_mode(\"regression\")\nlm_spec &lt;- linear_reg()\n\nchi_set &lt;- \n  workflow_set(\n    list(pca = chi_pca_rec, basic = chi_rec), \n    list(cubist = cb_spec, mars = mars_spec, lm = lm_spec)\n  ) %&gt;% \n  # Evaluate models using mean absolute errors\n  option_add(metrics = metric_set(mae))\n\n\nBriefly talk about Cubist being a (sort of) boosted rule-based model and MARS being a nonlinear regression model. Both incorporate feature selection nicely."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#process-them-on-the-resamples",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#process-them-on-the-resamples",
    "title": "Case Study on Transportation",
    "section": "Process them on the resamples",
    "text": "Process them on the resamples\n\n# Set up some objects for stacking ensembles (in a few slides)\ngrid_ctrl &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\nchi_res &lt;- \n  chi_set %&gt;% \n  workflow_map(\n    resamples = chi_rs,\n    grid = 10,\n    control = grid_ctrl,\n    verbose = TRUE,\n    seed = 12\n  )"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#how-do-the-results-look",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#how-do-the-results-look",
    "title": "Case Study on Transportation",
    "section": "How do the results look?",
    "text": "How do the results look?\n\nrank_results(chi_res)\n#&gt; # A tibble: 31 Ã— 9\n#&gt;    wflow_id     .config              .metric  mean std_err     n preprocessor model   rank\n#&gt;    &lt;chr&gt;        &lt;chr&gt;                &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;  &lt;int&gt;\n#&gt;  1 pca_cubist   Preprocessor1_Model1 mae     0.806   0.104    16 recipe       cubisâ€¦     1\n#&gt;  2 pca_cubist   Preprocessor3_Model3 mae     0.990   0.114    16 recipe       cubisâ€¦     2\n#&gt;  3 pca_cubist   Preprocessor4_Model2 mae     0.990   0.121    16 recipe       cubisâ€¦     3\n#&gt;  4 pca_cubist   Preprocessor4_Model1 mae     1.00    0.127    16 recipe       cubisâ€¦     4\n#&gt;  5 pca_cubist   Preprocessor3_Model2 mae     1.00    0.119    16 recipe       cubisâ€¦     5\n#&gt;  6 pca_cubist   Preprocessor2_Model2 mae     1.02    0.118    16 recipe       cubisâ€¦     6\n#&gt;  7 pca_cubist   Preprocessor1_Model3 mae     1.05    0.131    16 recipe       cubisâ€¦     7\n#&gt;  8 basic_cubist Preprocessor1_Model8 mae     1.07    0.115    16 recipe       cubisâ€¦     8\n#&gt;  9 basic_cubist Preprocessor1_Model7 mae     1.07    0.112    16 recipe       cubisâ€¦     9\n#&gt; 10 basic_cubist Preprocessor1_Model6 mae     1.07    0.114    16 recipe       cubisâ€¦    10\n#&gt; # â„¹ 21 more rows"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#plot-the-results",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#plot-the-results",
    "title": "Case Study on Transportation",
    "section": "Plot the results  ",
    "text": "Plot the results  \n\nautoplot(chi_res)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#pull-out-specific-results",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#pull-out-specific-results",
    "title": "Case Study on Transportation",
    "section": "Pull out specific results  ",
    "text": "Pull out specific results  \nWe can also pull out the specific tuning results and look at them:\n\nchi_res %&gt;% \n  extract_workflow_set_result(\"pca_cubist\") %&gt;% \n  autoplot()"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit? \nModel stacks generate predictions that are informed by several models."
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-1",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-1",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-2",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-2",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-3",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-3",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-4",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-4",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-5",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#why-choose-just-one-final_fit-5",
    "title": "Case Study on Transportation",
    "section": "Why choose just one final_fit? ",
    "text": "Why choose just one final_fit?"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#building-a-model-stack",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#building-a-model-stack",
    "title": "Case Study on Transportation",
    "section": "Building a model stack ",
    "text": "Building a model stack \n\nlibrary(stacks)\n\n\nDefine candidate members\nInitialize a data stack object\nAdd candidate ensemble members to the data stack\nEvaluate how to combine their predictions\nFit candidate ensemble members with non-zero stacking coefficients\nPredict on new data!"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#start-the-stack-and-add-members",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#start-the-stack-and-add-members",
    "title": "Case Study on Transportation",
    "section": "Start the stack and add members ",
    "text": "Start the stack and add members \nCollect all of the resampling results for all model configurations.\n\nchi_stack &lt;- \n  stacks() %&gt;% \n  add_candidates(chi_res)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#estimate-weights-for-each-candidate",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#estimate-weights-for-each-candidate",
    "title": "Case Study on Transportation",
    "section": "Estimate weights for each candidate ",
    "text": "Estimate weights for each candidate \nWhich configurations should be retained? Uses a penalized linear model:\n\nset.seed(122)\nchi_stack_res &lt;- blend_predictions(chi_stack)\n\nchi_stack_res\n#&gt; # A tibble: 4 Ã— 3\n#&gt;   member           type         weight\n#&gt;   &lt;chr&gt;            &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 pca_cubist_1_1   cubist_rules  0.410\n#&gt; 2 basic_cubist_1_4 cubist_rules  0.266\n#&gt; 3 pca_cubist_3_2   cubist_rules  0.193\n#&gt; 4 pca_lm_4_1       linear_reg    0.171"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#how-did-it-do",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#how-did-it-do",
    "title": "Case Study on Transportation",
    "section": "How did it do?  ",
    "text": "How did it do?  \nThe overall results of the penalized model:\n\nautoplot(chi_stack_res)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#what-does-it-use",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#what-does-it-use",
    "title": "Case Study on Transportation",
    "section": "What does it use?  ",
    "text": "What does it use?  \n\nautoplot(chi_stack_res, type = \"weights\")"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#fit-the-required-candidate-models",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#fit-the-required-candidate-models",
    "title": "Case Study on Transportation",
    "section": "Fit the required candidate models",
    "text": "Fit the required candidate models\nFor each model we retain in the stack, we need their model fit on the entire training set.\n\nchi_stack_res &lt;- fit_members(chi_stack_res)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#the-test-set-best-cubist-model",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#the-test-set-best-cubist-model",
    "title": "Case Study on Transportation",
    "section": "The test set: best Cubist model  ",
    "text": "The test set: best Cubist model  \nWe can pull out the results and the workflow to fit the single best cubist model.\n\nbest_cubist &lt;- \n  chi_res %&gt;% \n  extract_workflow_set_result(\"pca_cubist\") %&gt;% \n  select_best()\n\ncubist_res &lt;- \n  chi_res %&gt;% \n  extract_workflow(\"pca_cubist\") %&gt;% \n  finalize_workflow(best_cubist) %&gt;% \n  last_fit(split = chi_split, metrics = metric_set(mae))"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#the-test-set-stack-ensemble",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#the-test-set-stack-ensemble",
    "title": "Case Study on Transportation",
    "section": "The test set: stack ensemble",
    "text": "The test set: stack ensemble\nWe donâ€™t have last_fit() for stacks (yet) so we manually make predictions.\n\nstack_pred &lt;- \n  predict(chi_stack_res, chi_test) %&gt;% \n  bind_cols(chi_test)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#compare-the-results",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#compare-the-results",
    "title": "Case Study on Transportation",
    "section": "Compare the results  ",
    "text": "Compare the results  \nSingle best versus the stack:\n\ncollect_metrics(cubist_res)\n#&gt; # A tibble: 1 Ã— 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 mae     standard       0.670 Preprocessor1_Model1\n\nstack_pred %&gt;% mae(ridership, .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mae     standard       0.687"
  },
  {
    "objectID": "archive/2024-08-posit-conf/extras-transit-case-study.html#plot-the-test-set",
    "href": "archive/2024-08-posit-conf/extras-transit-case-study.html#plot-the-test-set",
    "title": "Case Study on Transportation",
    "section": "Plot the test set  ",
    "text": "Plot the test set  \n\n\nlibrary(probably)\ncubist_res %&gt;% \n  collect_predictions() %&gt;% \n  ggplot(aes(ridership, .pred)) + \n  geom_point(alpha = 1 / 2) + \n  geom_abline(lty = 2, col = \"green\") + \n  coord_obs_pred()"
  },
  {
    "objectID": "archive/2024-08-posit-conf/index.html",
    "href": "archive/2024-08-posit-conf/index.html",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels offered at posit::conf 2024. The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. This website hosts the materials for both the Introduction to tidymodels and Advanced tidymodels courses.\nIntroduction to tidymodels will teach you core tidymodels packages and their uses: data splitting/resampling with rsample, model fitting with parsnip, measuring model performance with yardstick, and model optimization using the tune package. Time permitting, youâ€™ll be introduced to basic pre-processing with recipes. Youâ€™ll learn tidymodels syntax as well as the process of predictive modeling for tabular data.\nAdvanced tidymodels will teach you about model optimization using the tune and finetune packages, including racing and iterative methods. Youâ€™ll be able to do more sophisticated feature engineering with recipes. Time permitting, model ensembles via stacking will be introduced. This course is focused on the analysis of tabular data and does not include deep learning methods."
  },
  {
    "objectID": "archive/2024-08-posit-conf/index.html#welcome",
    "href": "archive/2024-08-posit-conf/index.html#welcome",
    "title": "Machine learning with tidymodels",
    "section": "",
    "text": "These are the materials for workshops on tidymodels offered at posit::conf 2024. The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles. This website hosts the materials for both the Introduction to tidymodels and Advanced tidymodels courses.\nIntroduction to tidymodels will teach you core tidymodels packages and their uses: data splitting/resampling with rsample, model fitting with parsnip, measuring model performance with yardstick, and model optimization using the tune package. Time permitting, youâ€™ll be introduced to basic pre-processing with recipes. Youâ€™ll learn tidymodels syntax as well as the process of predictive modeling for tabular data.\nAdvanced tidymodels will teach you about model optimization using the tune and finetune packages, including racing and iterative methods. Youâ€™ll be able to do more sophisticated feature engineering with recipes. Time permitting, model ensembles via stacking will be introduced. This course is focused on the analysis of tabular data and does not include deep learning methods."
  },
  {
    "objectID": "archive/2024-08-posit-conf/index.html#is-this-workshop-for-me",
    "href": "archive/2024-08-posit-conf/index.html#is-this-workshop-for-me",
    "title": "Machine learning with tidymodels",
    "section": "Is this workshop for me? ",
    "text": "Is this workshop for me? \nDepending on your background, one of Introduction to tidymodels or Advanced tidymodels might serve you better than the other.\n\nIntroduction to tidymodels\nThis workshop is for you if you:\n\nare comfortable using tidyverse packages to read data into R, transform and reshape data, and make a variety of graphs, and\nhave had some exposure to basic statistical concepts such as linear models, residuals, etc.\n\nIntermediate or expert familiarity with modeling or machine learning is not required. Interested students who have intermediate or expert familiarity with modeling or machine learning may be interested in the Advanced tidymodels workshop.\n\n\nAdvanced tidymodels\nThis workshop is for you if you:\n\nhave the prerequisite skills listed for the Introduction to tidymodels workshops,\nhave used tidymodels packages like recipes, rsample, and parsnip, and\nhave some experience with evaluating statistical models using resampling techniques like v-fold cross-validation or the bootstrap.\n\nParticipants who are new to tidymodels or machine learning will benefit from taking the Introduction to tidymodels workshop before joining this one. Participants who have completed the â€œIntroduction to tidymodelsâ€ workshop will be well-prepared for this course."
  },
  {
    "objectID": "archive/2024-08-posit-conf/index.html#preparation",
    "href": "archive/2024-08-posit-conf/index.html#preparation",
    "title": "Machine learning with tidymodels",
    "section": "Preparation",
    "text": "Preparation\nThe process to set up your computer for either workshop will look the same. Please join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2024.04.0), available at https://posit.co/download/rstudio-desktop/\nFor all of the slides, the following R packages can be installed from the R console:\n\n\n# Install the packages for the workshop\npkgs &lt;- \n  c(\"bonsai\", \"Cubist\", \"doParallel\", \"earth\", \"embed\", \"finetune\", \n    \"forested\", \"lightgbm\", \"lme4\", \"parallelly\", \"plumber\", \"probably\", \n    \"ranger\", \"rpart\", \"rpart.plot\", \"rules\", \"splines2\", \"stacks\", \n    \"text2vec\", \"textrecipes\", \"tidymodels\", \"vetiver\")\n\ninstall.packages(pkgs)\n\nFor the advanced course, you should install the newest version of the dials package (version 1.3.0). To check this, you can run:\n\nrlang::check_installed(\"dials\", version = \"1.3.0\")\n\nIf youâ€™re a Windows user and encounter an error message during installation noting a missing Rtools installation, install Rtools using the installer linked here."
  },
  {
    "objectID": "archive/2024-08-posit-conf/index.html#slides",
    "href": "archive/2024-08-posit-conf/index.html#slides",
    "title": "Machine learning with tidymodels",
    "section": "Slides",
    "text": "Slides\nThese slides are designed to use with live teaching and are published for workshop participantsâ€™ convenience. They are not meant as standalone learning materials. For that, we recommend tidymodels.org and Tidy Modeling with R.\n\nIntroduction to tidymodels\n\n01: Introduction\n02: Your data budget\n03: What makes a model?\n04: Evaluating models\n05: Tuning models\n06: Wrapping up\n\n\n\nAdvanced tidymodels\n\n01: Introduction\n02: Feature engineering using recipes\n03: Tuning hyperparameters (grid search)\n04: Grid search via racing\n05: Iterative search\n06: Wrapping up\n\n\n\nExtra content (time permitting)\n\nIntro: Using workflowsets\nIntro: Using recipes\nAdvanced: Transit case study (includes stacking)\nAdvanced: Effect encoding\nAdvanced: Model deployment\n\nThereâ€™s also a page for slide annotations; these are extra notes for selected slides."
  },
  {
    "objectID": "archive/2024-08-posit-conf/index.html#code",
    "href": "archive/2024-08-posit-conf/index.html#code",
    "title": "Machine learning with tidymodels",
    "section": "Code",
    "text": "Code\nQuarto files for working along are available on GitHub. (Donâ€™t worry if you havenâ€™t used Quarto before; it will feel familiar to R Markdown users.)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/index.html#past-workshops",
    "href": "archive/2024-08-posit-conf/index.html#past-workshops",
    "title": "Machine learning with tidymodels",
    "section": "Past workshops",
    "text": "Past workshops\n\nEnglish\n\nSeptember 2023 at posit::conf()\nJuly 2023 at the New York R Conference\nAugust 2022 in Reykjavik\nJuly 2022 at rstudio::conf()\n\n\n\nSpanish\n\nMarch 2024 at conectaR"
  },
  {
    "objectID": "archive/2024-08-posit-conf/index.html#acknowledgments",
    "href": "archive/2024-08-posit-conf/index.html#acknowledgments",
    "title": "Machine learning with tidymodels",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis website, including the slides, is made with Quarto. Please submit an issue on the GitHub repo for this workshop if you find something that could be fixed or improved."
  },
  {
    "objectID": "archive/2024-08-posit-conf/index.html#reuse-and-licensing",
    "href": "archive/2024-08-posit-conf/index.html#reuse-and-licensing",
    "title": "Machine learning with tidymodels",
    "section": "Reuse and licensing",
    "text": "Reuse and licensing\n\nUnless otherwise noted (i.e.Â not an original creation and reused from another source), these educational materials are licensed under Creative Commons Attribution CC BY-SA 4.0."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-on-forests-in-washington",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-on-forests-in-washington",
    "title": "2 - Your data budget",
    "section": "Data on forests in Washington",
    "text": "Data on forests in Washington\n\n\n\nThe U.S. Forest Service maintains ML models to predict whether a plot of land is â€œforested.â€\nThis classification is important for all sorts of research, legislation, and land management purposes.\nPlots are typically remeasured every 10 years and this dataset contains the most recent measurement per plot.\nType ?forested to learn more about this dataset, including references.\n\n\n\n\n\nCredit: https://www.svgrepo.com/svg/251793/forest-mountain"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-on-forests-in-washington-1",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-on-forests-in-washington-1",
    "title": "2 - Your data budget",
    "section": "Data on forests in Washington",
    "text": "Data on forests in Washington\n\n\n\nN = 7,107 plots of land, one from each of 7,107 6000-acre hexagons in WA.\nA nominal outcome, forested, with levels \"Yes\" and \"No\", measured â€œon-the-ground.â€\n18 remotely-sensed and easily-accessible predictors:\n\nnumeric variables based on weather and topography.\nnominal variables based on classifications from other governmental orgs.\n\n\n\n\n\n\nCredit: https://www.svgrepo.com/svg/67614/forest\n\n\n\nThose nominal variables are classifications similar to â€œforestedâ€ but from other agencies. e.g.Â land_type is from the European Space Agency, and is a remotely-sensed 3-class distribution based on predictions for how the land is used."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#checklist-for-predictors",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#checklist-for-predictors",
    "title": "2 - Your data budget",
    "section": "Checklist for predictors",
    "text": "Checklist for predictors\n\nIs it ethical to use this variable? (Or even legal?)\nWill this variable be available at prediction time?\nDoes this variable contribute to explainability?\n\n\n\nre: ethics â€“ what issues might arise from releasing the true lat and lon? In reality, these lat and lon are slightly jittered to help ensure trust with landowners who allow surveyers to come take measurements."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-on-forests-in-washington-2",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-on-forests-in-washington-2",
    "title": "2 - Your data budget",
    "section": "Data on forests in Washington",
    "text": "Data on forests in Washington\n\nlibrary(tidymodels)\nlibrary(forested)\n\nforested\n#&gt; # A tibble: 7,107 Ã— 19\n#&gt;    forested  year elevation eastness northness roughness tree_no_tree dew_temp\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt;\n#&gt;  1 Yes       2005       881       90        43        63 Tree             0.04\n#&gt;  2 Yes       2005       113      -25        96        30 Tree             6.4 \n#&gt;  3 No        2005       164      -84        53        13 Tree             6.06\n#&gt;  4 Yes       2005       299       93        34         6 No tree          4.43\n#&gt;  5 Yes       2005       806       47       -88        35 Tree             1.06\n#&gt;  6 Yes       2005       736      -27       -96        53 Tree             1.35\n#&gt;  7 Yes       2005       636      -48        87         3 No tree          1.42\n#&gt;  8 Yes       2005       224      -65       -75         9 Tree             6.39\n#&gt;  9 Yes       2005        52      -62        78        42 Tree             6.5 \n#&gt; 10 Yes       2005      2240      -67       -74        99 No tree         -5.63\n#&gt; # â„¹ 7,097 more rows\n#&gt; # â„¹ 11 more variables: precip_annual &lt;dbl&gt;, temp_annual_mean &lt;dbl&gt;,\n#&gt; #   temp_annual_min &lt;dbl&gt;, temp_annual_max &lt;dbl&gt;, temp_january_min &lt;dbl&gt;,\n#&gt; #   vapor_min &lt;dbl&gt;, vapor_max &lt;dbl&gt;, canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;,\n#&gt; #   land_type &lt;fct&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-splitting-and-spending",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-splitting-and-spending",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\nFor machine learning, we typically split data into training and test sets:\n\n\nThe training set is used to estimate model parameters.\nThe test set is used to find an independent assessment of model performance.\n\n\n\nDo not ğŸš« use the test set during training."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-splitting-and-spending-1",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-splitting-and-spending-1",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-splitting-and-spending-2",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-splitting-and-spending-2",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending",
    "text": "Data splitting and spending\n\nSpending too much data in training prevents us from computing a good assessment of predictive performance.\n\n\n\nSpending too much data in testing prevents us from computing a good estimate of model parameters."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#your-turn",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#your-turn",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nWhen is a good time to split your data?\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#the-initial-split",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#the-initial-split",
    "title": "2 - Your data budget",
    "section": "The initial split ",
    "text": "The initial split \n\nset.seed(123)\nforested_split &lt;- initial_split(forested)\nforested_split\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;5330/1777/7107&gt;\n\n\nHow much data in training vs testing? This function uses a good default, but this depends on your specific goal/data"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#what-is-set.seed",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#what-is-set.seed",
    "title": "2 - Your data budget",
    "section": "What is set.seed()?",
    "text": "What is set.seed()?\nTo create that split of the data, R generates â€œpseudo-randomâ€ numbers: while they are made to behave like random numbers, their generation is deterministic given a â€œseedâ€.\nThis allows us to reproduce results by setting that seed.\nWhich seed you pick doesnâ€™t matter, as long as you donâ€™t try a bunch of seeds and pick the one that gives you the best performance."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#accessing-the-data",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#accessing-the-data",
    "title": "2 - Your data budget",
    "section": "Accessing the data ",
    "text": "Accessing the data \n\nforested_train &lt;- training(forested_split)\nforested_test &lt;- testing(forested_split)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#the-training-set",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#the-training-set",
    "title": "2 - Your data budget",
    "section": "The training set",
    "text": "The training set\n\nforested_train\n#&gt; # A tibble: 5,330 Ã— 19\n#&gt;    forested  year elevation eastness northness roughness tree_no_tree dew_temp\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt;\n#&gt;  1 No        2016       464       -5       -99         7 No tree          1.71\n#&gt;  2 Yes       2016       166       92        37         7 Tree             6   \n#&gt;  3 No        2016       644      -85       -52        24 No tree          0.67\n#&gt;  4 Yes       2014      1285        4        99        79 Tree             1.91\n#&gt;  5 Yes       2013       822       87        48        68 Tree             1.95\n#&gt;  6 Yes       2017         3        6       -99         5 Tree             7.93\n#&gt;  7 Yes       2014      2041      -95        28        49 Tree            -4.22\n#&gt;  8 Yes       2015      1009       -8        99        72 Tree             1.72\n#&gt;  9 No        2017       436      -98        19        10 No tree          1.8 \n#&gt; 10 No        2018       775       63        76       103 No tree          0.62\n#&gt; # â„¹ 5,320 more rows\n#&gt; # â„¹ 11 more variables: precip_annual &lt;dbl&gt;, temp_annual_mean &lt;dbl&gt;,\n#&gt; #   temp_annual_min &lt;dbl&gt;, temp_annual_max &lt;dbl&gt;, temp_january_min &lt;dbl&gt;,\n#&gt; #   vapor_min &lt;dbl&gt;, vapor_max &lt;dbl&gt;, canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;,\n#&gt; #   land_type &lt;fct&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#the-test-set",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#the-test-set",
    "title": "2 - Your data budget",
    "section": "The test set ",
    "text": "The test set \nğŸ™ˆ\n\nThere are 1777 rows and 19 columns in the test set."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#your-turn-1",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#your-turn-1",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nSplit your data so 20% is held out for the test set.\nTry out different values in set.seed() to see how the results change.\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-splitting-and-spending-3",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#data-splitting-and-spending-3",
    "title": "2 - Your data budget",
    "section": "Data splitting and spending ",
    "text": "Data splitting and spending \n\nset.seed(123)\nforested_split &lt;- initial_split(forested, prop = 0.8)\nforested_train &lt;- training(forested_split)\nforested_test &lt;- testing(forested_split)\n\nnrow(forested_train)\n#&gt; [1] 5685\nnrow(forested_test)\n#&gt; [1] 1422"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#your-turn-2",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#your-turn-2",
    "title": "2 - Your data budget",
    "section": "Your turn",
    "text": "Your turn\n\nExplore the forested_train data on your own!\n\nWhatâ€™s the distribution of the outcome, forested?\nWhatâ€™s the distribution of numeric variables like precip_annual?\nHow does the distribution of forested differ across the categorical variables?\n\n\n\n\nâˆ’+\n08:00\n\n\n\n\nMake a plot or summary and then share with neighbor"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#section-1",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#section-1",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train %&gt;% \n  ggplot(aes(x = forested)) +\n  geom_bar()"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#section-2",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#section-2",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train %&gt;% \n  ggplot(aes(x = forested, fill = tree_no_tree)) +\n  geom_bar()"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#section-3",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#section-3",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train %&gt;% \n  ggplot(aes(x = precip_annual, fill = forested, group = forested)) +\n  geom_histogram(position = \"identity\", alpha = .7)"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#section-4",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#section-4",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train %&gt;% \n  ggplot(aes(x = precip_annual, fill = forested, group = forested)) +\n  geom_histogram(position = \"fill\")"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#section-5",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#section-5",
    "title": "2 - Your data budget",
    "section": "",
    "text": "forested_train %&gt;% \n  ggplot(aes(x = lon, y = lat, col = forested)) +\n  geom_point()"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-02-data-budget.html#the-whole-game---status-update",
    "href": "archive/2024-08-posit-conf/intro-02-data-budget.html#the-whole-game---status-update",
    "title": "2 - Your data budget",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#looking-at-predictions",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#looking-at-predictions",
    "title": "4 - Evaluating models",
    "section": "Looking at predictions",
    "text": "Looking at predictions\n\naugment(forested_fit, new_data = forested_train)\n#&gt; # A tibble: 5,685 Ã— 22\n#&gt;    .pred_class .pred_Yes .pred_No forested  year elevation eastness northness\n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 No             0.0114   0.989  No        2016       464       -5       -99\n#&gt;  2 Yes            0.636    0.364  Yes       2016       166       92        37\n#&gt;  3 No             0.0114   0.989  No        2016       644      -85       -52\n#&gt;  4 Yes            0.977    0.0226 Yes       2014      1285        4        99\n#&gt;  5 Yes            0.977    0.0226 Yes       2013       822       87        48\n#&gt;  6 Yes            0.808    0.192  Yes       2017         3        6       -99\n#&gt;  7 Yes            0.977    0.0226 Yes       2014      2041      -95        28\n#&gt;  8 Yes            0.977    0.0226 Yes       2015      1009       -8        99\n#&gt;  9 No             0.0114   0.989  No        2017       436      -98        19\n#&gt; 10 No             0.0114   0.989  No        2018       775       63        76\n#&gt; # â„¹ 5,675 more rows\n#&gt; # â„¹ 14 more variables: roughness &lt;dbl&gt;, tree_no_tree &lt;fct&gt;, dew_temp &lt;dbl&gt;,\n#&gt; #   precip_annual &lt;dbl&gt;, temp_annual_mean &lt;dbl&gt;, temp_annual_min &lt;dbl&gt;,\n#&gt; #   temp_annual_max &lt;dbl&gt;, temp_january_min &lt;dbl&gt;, vapor_min &lt;dbl&gt;,\n#&gt; #   vapor_max &lt;dbl&gt;, canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, land_type &lt;fct&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#confusion-matrix",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#confusion-matrix",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#confusion-matrix-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#confusion-matrix-1",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix \n\naugment(forested_fit, new_data = forested_train) %&gt;%\n  conf_mat(truth = forested, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction  Yes   No\n#&gt;        Yes 2991  176\n#&gt;        No   144 2374"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#confusion-matrix-2",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#confusion-matrix-2",
    "title": "4 - Evaluating models",
    "section": "Confusion matrix ",
    "text": "Confusion matrix \n\naugment(forested_fit, new_data = forested_train) %&gt;%\n  conf_mat(truth = forested, estimate = .pred_class) %&gt;%\n  autoplot(type = \"heatmap\")"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(forested_fit, new_data = forested_train) %&gt;%\n  accuracy(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.944\n\n\n\n\n\nThere used to be a slide here calling out the pitfalls of accuracy when classes are imbalanced."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(forested_fit, new_data = forested_train) %&gt;%\n  sensitivity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.954"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-2",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \n\n\n\naugment(forested_fit, new_data = forested_train) %&gt;%\n  sensitivity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity binary         0.954\n\n\n\naugment(forested_fit, new_data = forested_train) %&gt;%\n  specificity(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 specificity binary         0.931"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-3",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \nWe can use metric_set() to combine multiple calculations into one\n\nforested_metrics &lt;- metric_set(accuracy, specificity, sensitivity)\n\naugment(forested_fit, new_data = forested_train) %&gt;%\n  forested_metrics(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy    binary         0.944\n#&gt; 2 specificity binary         0.931\n#&gt; 3 sensitivity binary         0.954"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-4",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#metrics-for-model-performance-4",
    "title": "4 - Evaluating models",
    "section": "Metrics for model performance ",
    "text": "Metrics for model performance \nMetrics and metric sets work with grouped data frames!\n\naugment(forested_fit, new_data = forested_train) %&gt;%\n  group_by(tree_no_tree) %&gt;%\n  accuracy(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   tree_no_tree .metric  .estimator .estimate\n#&gt;   &lt;fct&gt;        &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 Tree         accuracy binary         0.946\n#&gt; 2 No tree      accuracy binary         0.941"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#your-turn",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#your-turn",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nApply the forested_metrics metric set to augment()\noutput grouped by tree_no_tree.\nDo any metrics differ substantially between groups?\n\n\n\nâˆ’+\n05:00\n\n\n\n\nThe specificity for \"Tree\" is a good bit lower than it is for \"No tree\".\nSpecificity is the proportion of negatives that are correctly identified as negatives. â€œNegativeâ€ is the non-event level of the outcome, i.e.Â â€œnon-forested.â€ So, when this index classifies the plot as having a tree, the model does not do well at correctly identifying the plot as non-forested when it is indeed non-forested."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#two-class-data",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#two-class-data",
    "title": "4 - Evaluating models",
    "section": "Two class data",
    "text": "Two class data\nThese metrics assume that we know the threshold for converting â€œsoftâ€ probability predictions into â€œhardâ€ class predictions.\n\nIs a 50% threshold good?\nWhat happens if we say that we need to be 80% sure to declare an event?\n\nsensitivity â¬‡ï¸, specificity â¬†ï¸\n\n\n\nWhat happens for a 20% threshold?\n\nsensitivity â¬†ï¸, specificity â¬‡ï¸"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#varying-the-threshold",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#varying-the-threshold",
    "title": "4 - Evaluating models",
    "section": "Varying the threshold",
    "text": "Varying the threshold"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#roc-curves",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#roc-curves",
    "title": "4 - Evaluating models",
    "section": "ROC curves",
    "text": "ROC curves\n\n\nFor an ROC (receiver operator characteristic) curve, we plot\n\nthe false positive rate (1 - specificity) on the x-axis\nthe true positive rate (sensitivity) on the y-axis\n\nwith sensitivity and specificity calculated at all possible thresholds.\n\n\n\n\n\n\n\n\n\n\n\n\nROC curves are insensitive to class imbalance."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#roc-curves-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#roc-curves-1",
    "title": "4 - Evaluating models",
    "section": "ROC curves",
    "text": "ROC curves\n\n\nWe can use the area under the ROC curve as a classification metric:\n\nROC AUC = 1 ğŸ’¯\nROC AUC = 1/2 ğŸ˜¢"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#roc-curves-2",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#roc-curves-2",
    "title": "4 - Evaluating models",
    "section": "ROC curves ",
    "text": "ROC curves \n\n# Assumes _first_ factor level is event; there are options to change that\naugment(forested_fit, new_data = forested_train) %&gt;% \n  roc_curve(truth = forested, .pred_Yes) %&gt;%\n  slice(1, 20, 50)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .threshold specificity sensitivity\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1   -Inf           0           1    \n#&gt; 2      0.235       0.885       0.972\n#&gt; 3      0.909       0.969       0.826\n\naugment(forested_fit, new_data = forested_train) %&gt;% \n  roc_auc(truth = forested, .pred_Yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.975"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#roc-curve-plot",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#roc-curve-plot",
    "title": "4 - Evaluating models",
    "section": "ROC curve plot ",
    "text": "ROC curve plot \n\n\naugment(forested_fit, \n        new_data = forested_train) %&gt;% \n  roc_curve(truth = forested, \n            .pred_Yes) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#your-turn-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#your-turn-1",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCompute and plot an ROC curve for your current model.\nWhat data are being used for this ROC curve plot?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#brier-score",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#brier-score",
    "title": "4 - Evaluating models",
    "section": "Brier score",
    "text": "Brier score\nWhat if we donâ€™t turn predicted probabilities into class predictions?\n\nThe Brier score is analogous to the mean squared error in regression models:\n\\[\nBrier_{class} = \\frac{1}{N}\\sum_{i=1}^N\\sum_{k=1}^C (y_{ik} - \\hat{p}_{ik})^2\n\\]"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#brier-score-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#brier-score-1",
    "title": "4 - Evaluating models",
    "section": "Brier score",
    "text": "Brier score\n\naugment(forested_fit, new_data = forested_train) %&gt;% \n  brier_class(truth = forested, .pred_Yes) \n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary        0.0469\n\n\nSmaller values are better, for binary classification the â€œbad model thresholdâ€ is about 0.25."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#separation-vs-calibration",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#separation-vs-calibration",
    "title": "4 - Evaluating models",
    "section": "Separation vs calibration",
    "text": "Separation vs calibration\n\n\nThe ROC captures separation.\n\n\n\n\n\n\n\n\n\n\nThe Brier score captures calibration.\n\n\n\n\n\n\n\n\n\n\n\n\nGood separation: the densities donâ€™t overlap.\nGood calibration: the calibration line follows the diagonal.\n\nCalibration plot: We bin observations according to predicted probability. In the bin for 20%-30% predicted prob, we should see an event rate of ~25% if the model is well-calibrated."
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-1",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting",
    "text": "Dangers of overfitting"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-2",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-2",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸",
    "text": "Dangers of overfitting âš ï¸"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-3",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-3",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\nforested_fit %&gt;%\n  augment(forested_train)\n#&gt; # A tibble: 5,685 Ã— 22\n#&gt;    .pred_class .pred_Yes .pred_No forested  year elevation eastness northness\n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 No             0.0114   0.989  No        2016       464       -5       -99\n#&gt;  2 Yes            0.636    0.364  Yes       2016       166       92        37\n#&gt;  3 No             0.0114   0.989  No        2016       644      -85       -52\n#&gt;  4 Yes            0.977    0.0226 Yes       2014      1285        4        99\n#&gt;  5 Yes            0.977    0.0226 Yes       2013       822       87        48\n#&gt;  6 Yes            0.808    0.192  Yes       2017         3        6       -99\n#&gt;  7 Yes            0.977    0.0226 Yes       2014      2041      -95        28\n#&gt;  8 Yes            0.977    0.0226 Yes       2015      1009       -8        99\n#&gt;  9 No             0.0114   0.989  No        2017       436      -98        19\n#&gt; 10 No             0.0114   0.989  No        2018       775       63        76\n#&gt; # â„¹ 5,675 more rows\n#&gt; # â„¹ 14 more variables: roughness &lt;dbl&gt;, tree_no_tree &lt;fct&gt;, dew_temp &lt;dbl&gt;,\n#&gt; #   precip_annual &lt;dbl&gt;, temp_annual_mean &lt;dbl&gt;, temp_annual_min &lt;dbl&gt;,\n#&gt; #   temp_annual_max &lt;dbl&gt;, temp_january_min &lt;dbl&gt;, vapor_min &lt;dbl&gt;,\n#&gt; #   vapor_max &lt;dbl&gt;, canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, land_type &lt;fct&gt;\n\nWe call this â€œresubstitutionâ€ or â€œrepredicting the training setâ€"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-4",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-4",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\nforested_fit %&gt;%\n  augment(forested_train) %&gt;%\n  accuracy(forested, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.944\n\nWe call this a â€œresubstitution estimateâ€"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-5",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-5",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\nforested_fit %&gt;%\n  augment(forested_train) %&gt;%\n  accuracy(forested, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.944"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-6",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-6",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\nforested_fit %&gt;%\n  augment(forested_train) %&gt;%\n  accuracy(forested, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.944\n\n\n\nforested_fit %&gt;%\n  augment(forested_test) %&gt;%\n  accuracy(forested, .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.886\n\n\n\nâš ï¸ Remember that weâ€™re demonstrating overfitting\n\n\nâš ï¸ Donâ€™t use the test set until the end of your modeling analysis"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#your-turn-2",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#your-turn-2",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse augment() and a metric function to compute a classification metric like brier_class().\nCompute the metrics for both training and testing data to demonstrate overfitting!\nNotice the evidence of overfitting! âš ï¸\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-7",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#dangers-of-overfitting-7",
    "title": "4 - Evaluating models",
    "section": "Dangers of overfitting âš ï¸ ",
    "text": "Dangers of overfitting âš ï¸ \n\n\n\nforested_fit %&gt;%\n  augment(forested_train) %&gt;%\n  brier_class(forested, .pred_Yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary        0.0469\n\n\n\nforested_fit %&gt;%\n  augment(forested_test) %&gt;%\n  brier_class(forested, .pred_Yes)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 brier_class binary        0.0888\n\n\n\nWhat if we want to compare more models?\n\n\nAnd/or more model configurations?\n\n\nAnd we want to understand if these are important differences?"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#cross-validation",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#cross-validation",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#cross-validation-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#cross-validation-1",
    "title": "4 - Evaluating models",
    "section": "Cross-validation",
    "text": "Cross-validation"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#your-turn-3",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#your-turn-3",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nIf we use 10 folds, what percent of the training data\n\nends up in analysis\nends up in assessment\n\nfor each fold?\n\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#cross-validation-2",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#cross-validation-2",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(forested_train) # v = 10 is default\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [5116/569]&gt; Fold01\n#&gt;  2 &lt;split [5116/569]&gt; Fold02\n#&gt;  3 &lt;split [5116/569]&gt; Fold03\n#&gt;  4 &lt;split [5116/569]&gt; Fold04\n#&gt;  5 &lt;split [5116/569]&gt; Fold05\n#&gt;  6 &lt;split [5117/568]&gt; Fold06\n#&gt;  7 &lt;split [5117/568]&gt; Fold07\n#&gt;  8 &lt;split [5117/568]&gt; Fold08\n#&gt;  9 &lt;split [5117/568]&gt; Fold09\n#&gt; 10 &lt;split [5117/568]&gt; Fold10"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#cross-validation-3",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#cross-validation-3",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWhat is in this?\n\nforested_folds &lt;- vfold_cv(forested_train)\nforested_folds$splits[1:3]\n#&gt; [[1]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;5116/569/5685&gt;\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;5116/569/5685&gt;\n#&gt; \n#&gt; [[3]]\n#&gt; &lt;Analysis/Assess/Total&gt;\n#&gt; &lt;5116/569/5685&gt;\n\n\nTalk about a list column, storing non-atomic types in dataframe"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#cross-validation-4",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#cross-validation-4",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \n\nvfold_cv(forested_train, v = 5)\n#&gt; #  5-fold cross-validation \n#&gt; # A tibble: 5 Ã— 2\n#&gt;   splits              id   \n#&gt;   &lt;list&gt;              &lt;chr&gt;\n#&gt; 1 &lt;split [4548/1137]&gt; Fold1\n#&gt; 2 &lt;split [4548/1137]&gt; Fold2\n#&gt; 3 &lt;split [4548/1137]&gt; Fold3\n#&gt; 4 &lt;split [4548/1137]&gt; Fold4\n#&gt; 5 &lt;split [4548/1137]&gt; Fold5"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#cross-validation-5",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#cross-validation-5",
    "title": "4 - Evaluating models",
    "section": "Cross-validation ",
    "text": "Cross-validation \nWeâ€™ll use this setup:\n\nset.seed(123)\nforested_folds &lt;- vfold_cv(forested_train, v = 10)\nforested_folds\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [5116/569]&gt; Fold01\n#&gt;  2 &lt;split [5116/569]&gt; Fold02\n#&gt;  3 &lt;split [5116/569]&gt; Fold03\n#&gt;  4 &lt;split [5116/569]&gt; Fold04\n#&gt;  5 &lt;split [5116/569]&gt; Fold05\n#&gt;  6 &lt;split [5117/568]&gt; Fold06\n#&gt;  7 &lt;split [5117/568]&gt; Fold07\n#&gt;  8 &lt;split [5117/568]&gt; Fold08\n#&gt;  9 &lt;split [5117/568]&gt; Fold09\n#&gt; 10 &lt;split [5117/568]&gt; Fold10\n\n\nSet the seed when creating resamples"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#fit-our-model-to-the-resamples",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#fit-our-model-to-the-resamples",
    "title": "4 - Evaluating models",
    "section": "Fit our model to the resamples",
    "text": "Fit our model to the resamples\n\nforested_res &lt;- fit_resamples(forested_wflow, forested_folds)\nforested_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 4\n#&gt;    splits             id     .metrics         .notes          \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n#&gt;  1 &lt;split [5116/569]&gt; Fold01 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  2 &lt;split [5116/569]&gt; Fold02 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  3 &lt;split [5116/569]&gt; Fold03 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  4 &lt;split [5116/569]&gt; Fold04 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  5 &lt;split [5116/569]&gt; Fold05 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  6 &lt;split [5117/568]&gt; Fold06 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  7 &lt;split [5117/568]&gt; Fold07 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  8 &lt;split [5117/568]&gt; Fold08 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt;  9 &lt;split [5117/568]&gt; Fold09 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\n#&gt; 10 &lt;split [5117/568]&gt; Fold10 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nforested_res %&gt;%\n  collect_metrics()\n#&gt; # A tibble: 3 Ã— 6\n#&gt;   .metric     .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.894     10 0.00562 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.0817    10 0.00434 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.951     10 0.00378 Preprocessor1_Model1\n\n\ncollect_metrics() is one of a suite of collect_*() functions that can be used to work with columns of tuning results. Most columns in a tuning result prefixed with . have a corresponding collect_*() function with options for common summaries.\n\n\nWe can reliably measure performance using only the training data ğŸ‰"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#comparing-metrics",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#comparing-metrics",
    "title": "4 - Evaluating models",
    "section": "Comparing metrics ",
    "text": "Comparing metrics \nHow do the metrics from resampling compare to the metrics from training and testing?\n\n\n\nforested_res %&gt;%\n  collect_metrics() %&gt;% \n  select(.metric, mean, n)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric       mean     n\n#&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 accuracy    0.894     10\n#&gt; 2 brier_class 0.0817    10\n#&gt; 3 roc_auc     0.951     10\n\n\nThe ROC AUC previously was\n\n0.97 for the training set\n0.95 for test set\n\n\n\nRemember that:\nâš ï¸ the training set gives you overly optimistic metrics\nâš ï¸ the test set is precious"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-1",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nctrl_forested &lt;- control_resamples(save_pred = TRUE)\nforested_res &lt;- fit_resamples(forested_wflow, forested_folds, control = ctrl_forested)\n\nforested_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [5116/569]&gt; Fold01 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [5116/569]&gt; Fold02 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [5116/569]&gt; Fold03 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [5116/569]&gt; Fold04 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [5116/569]&gt; Fold05 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [5117/568]&gt; Fold06 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [5117/568]&gt; Fold07 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [5117/568]&gt; Fold08 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [5117/568]&gt; Fold09 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [5117/568]&gt; Fold10 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-2",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-2",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\n# Save the assessment set results\nforested_preds &lt;- collect_predictions(forested_res)\nforested_preds\n#&gt; # A tibble: 5,685 Ã— 7\n#&gt;    .pred_class .pred_Yes .pred_No id      .row forested .config             \n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt; &lt;fct&gt;    &lt;chr&gt;               \n#&gt;  1 Yes           0.5       0.5    Fold01     2 Yes      Preprocessor1_Model1\n#&gt;  2 Yes           0.982     0.0178 Fold01     5 Yes      Preprocessor1_Model1\n#&gt;  3 No            0.00790   0.992  Fold01     9 No       Preprocessor1_Model1\n#&gt;  4 No            0.4       0.6    Fold01    14 No       Preprocessor1_Model1\n#&gt;  5 Yes           0.870     0.130  Fold01    18 Yes      Preprocessor1_Model1\n#&gt;  6 Yes           0.982     0.0178 Fold01    59 Yes      Preprocessor1_Model1\n#&gt;  7 No            0.00790   0.992  Fold01    67 No       Preprocessor1_Model1\n#&gt;  8 Yes           0.982     0.0178 Fold01    89 Yes      Preprocessor1_Model1\n#&gt;  9 No            0.00790   0.992  Fold01    94 No       Preprocessor1_Model1\n#&gt; 10 Yes           0.982     0.0178 Fold01   111 Yes      Preprocessor1_Model1\n#&gt; # â„¹ 5,675 more rows"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-3",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-3",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nforested_preds %&gt;% \n  group_by(id) %&gt;%\n  forested_metrics(truth = forested, estimate = .pred_class)\n#&gt; # A tibble: 30 Ã— 4\n#&gt;    id     .metric  .estimator .estimate\n#&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt;  1 Fold01 accuracy binary         0.896\n#&gt;  2 Fold02 accuracy binary         0.859\n#&gt;  3 Fold03 accuracy binary         0.868\n#&gt;  4 Fold04 accuracy binary         0.921\n#&gt;  5 Fold05 accuracy binary         0.900\n#&gt;  6 Fold06 accuracy binary         0.891\n#&gt;  7 Fold07 accuracy binary         0.896\n#&gt;  8 Fold08 accuracy binary         0.903\n#&gt;  9 Fold09 accuracy binary         0.896\n#&gt; 10 Fold10 accuracy binary         0.905\n#&gt; # â„¹ 20 more rows"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#where-are-the-fitted-models",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#where-are-the-fitted-models",
    "title": "4 - Evaluating models",
    "section": "Where are the fitted models? ",
    "text": "Where are the fitted models? \n\nforested_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [5116/569]&gt; Fold01 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [5116/569]&gt; Fold02 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [5116/569]&gt; Fold03 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [5116/569]&gt; Fold04 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [5116/569]&gt; Fold05 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [5117/568]&gt; Fold06 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [5117/568]&gt; Fold07 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [5117/568]&gt; Fold08 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [5117/568]&gt; Fold09 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [5117/568]&gt; Fold10 &lt;tibble [3 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;\n\n\nğŸ—‘ï¸"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#bootstrapping",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#bootstrapping",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#bootstrapping-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#bootstrapping-1",
    "title": "4 - Evaluating models",
    "section": "Bootstrapping ",
    "text": "Bootstrapping \n\nset.seed(3214)\nbootstraps(forested_train)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 25 Ã— 2\n#&gt;    splits              id         \n#&gt;    &lt;list&gt;              &lt;chr&gt;      \n#&gt;  1 &lt;split [5685/2075]&gt; Bootstrap01\n#&gt;  2 &lt;split [5685/2093]&gt; Bootstrap02\n#&gt;  3 &lt;split [5685/2129]&gt; Bootstrap03\n#&gt;  4 &lt;split [5685/2093]&gt; Bootstrap04\n#&gt;  5 &lt;split [5685/2111]&gt; Bootstrap05\n#&gt;  6 &lt;split [5685/2105]&gt; Bootstrap06\n#&gt;  7 &lt;split [5685/2139]&gt; Bootstrap07\n#&gt;  8 &lt;split [5685/2079]&gt; Bootstrap08\n#&gt;  9 &lt;split [5685/2113]&gt; Bootstrap09\n#&gt; 10 &lt;split [5685/2101]&gt; Bootstrap10\n#&gt; # â„¹ 15 more rows"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#the-whole-game---status-update",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#the-whole-game---status-update",
    "title": "4 - Evaluating models",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#your-turn-4",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#your-turn-4",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nCreate:\n\nMonte Carlo Cross-Validation sets\nvalidation set\n\n(use the reference guide to find the functions)\nDonâ€™t forget to set a seed when you resample!\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#monte-carlo-cross-validation",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#monte-carlo-cross-validation",
    "title": "4 - Evaluating models",
    "section": "Monte Carlo Cross-Validation ",
    "text": "Monte Carlo Cross-Validation \n\nset.seed(322)\nmc_cv(forested_train, times = 10)\n#&gt; # Monte Carlo cross-validation (0.75/0.25) with 10 resamples  \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits              id        \n#&gt;    &lt;list&gt;              &lt;chr&gt;     \n#&gt;  1 &lt;split [4263/1422]&gt; Resample01\n#&gt;  2 &lt;split [4263/1422]&gt; Resample02\n#&gt;  3 &lt;split [4263/1422]&gt; Resample03\n#&gt;  4 &lt;split [4263/1422]&gt; Resample04\n#&gt;  5 &lt;split [4263/1422]&gt; Resample05\n#&gt;  6 &lt;split [4263/1422]&gt; Resample06\n#&gt;  7 &lt;split [4263/1422]&gt; Resample07\n#&gt;  8 &lt;split [4263/1422]&gt; Resample08\n#&gt;  9 &lt;split [4263/1422]&gt; Resample09\n#&gt; 10 &lt;split [4263/1422]&gt; Resample10"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#validation-set",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#validation-set",
    "title": "4 - Evaluating models",
    "section": "Validation set ",
    "text": "Validation set \n\nset.seed(853)\nforested_val_split &lt;- initial_validation_split(forested)\nvalidation_set(forested_val_split)\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   splits              id        \n#&gt;   &lt;list&gt;              &lt;chr&gt;     \n#&gt; 1 &lt;split [4264/1421]&gt; validation\n\n\nA validation set is just another type of resample"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#random-forest-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#random-forest-1",
    "title": "4 - Evaluating models",
    "section": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ",
    "text": "Random forest ğŸŒ³ğŸŒ²ğŸŒ´ğŸŒµğŸŒ³ğŸŒ³ğŸŒ´ğŸŒ²ğŸŒµğŸŒ´ğŸŒ³ğŸŒµ\n\nEnsemble many decision tree models\nAll the trees vote! ğŸ—³ï¸\nBootstrap aggregating + random predictor sampling\n\n\n\nOften works well without tuning hyperparameters (more on this later!), as long as there are enough trees"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#create-a-random-forest-model",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#create-a-random-forest-model",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_spec &lt;- rand_forest(trees = 1000, mode = \"classification\")\nrf_spec\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#create-a-random-forest-model-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#create-a-random-forest-model-1",
    "title": "4 - Evaluating models",
    "section": "Create a random forest model ",
    "text": "Create a random forest model \n\nrf_wflow &lt;- workflow(forested ~ ., rf_spec)\nrf_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Random Forest Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt; \n#&gt; Computational engine: ranger"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#your-turn-5",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#your-turn-5",
    "title": "4 - Evaluating models",
    "section": "Your turn",
    "text": "Your turn\n\nUse fit_resamples() and rf_wflow to:\n\nkeep predictions\ncompute metrics\n\n\n\n\nâˆ’+\n08:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-4",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#evaluating-model-performance-4",
    "title": "4 - Evaluating models",
    "section": "Evaluating model performance ",
    "text": "Evaluating model performance \n\nctrl_forested &lt;- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\n\nset.seed(2)\nrf_res &lt;- fit_resamples(rf_wflow, forested_folds, control = ctrl_forested)\ncollect_metrics(rf_res)\n#&gt; # A tibble: 3 Ã— 6\n#&gt;   .metric     .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary     0.918     10 0.00585 Preprocessor1_Model1\n#&gt; 2 brier_class binary     0.0618    10 0.00337 Preprocessor1_Model1\n#&gt; 3 roc_auc     binary     0.972     10 0.00309 Preprocessor1_Model1"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#the-whole-game---status-update-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#the-whole-game---status-update-1",
    "title": "4 - Evaluating models",
    "section": "The whole game - status update",
    "text": "The whole game - status update"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#the-final-fit",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#the-final-fit",
    "title": "4 - Evaluating models",
    "section": "The final fit ",
    "text": "The final fit \nSuppose that we are happy with our random forest model.\nLetâ€™s fit the model on the training set and verify our performance using the test set.\n\nWeâ€™ve shown you fit() and predict() (+ augment()) but there is a shortcut:\n\n# forested_split has train + test info\nfinal_fit &lt;- last_fit(rf_wflow, forested_split) \n\nfinal_fit\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits              id               .metrics .notes   .predictions .workflow \n#&gt;   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n#&gt; 1 &lt;split [5685/1422]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#what-is-in-final_fit",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#what-is-in-final_fit",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_metrics(final_fit)\n#&gt; # A tibble: 3 Ã— 4\n#&gt;   .metric     .estimator .estimate .config             \n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 accuracy    binary        0.911  Preprocessor1_Model1\n#&gt; 2 roc_auc     binary        0.970  Preprocessor1_Model1\n#&gt; 3 brier_class binary        0.0652 Preprocessor1_Model1\n\n\nThese are metrics computed with the test set"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#what-is-in-final_fit-1",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#what-is-in-final_fit-1",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\ncollect_predictions(final_fit)\n#&gt; # A tibble: 1,422 Ã— 7\n#&gt;    .pred_class .pred_Yes .pred_No id                .row forested .config       \n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;fct&gt;    &lt;chr&gt;         \n#&gt;  1 Yes             0.822   0.178  train/test split     3 No       Preprocessor1â€¦\n#&gt;  2 Yes             0.707   0.293  train/test split     4 Yes      Preprocessor1â€¦\n#&gt;  3 No              0.270   0.730  train/test split     7 Yes      Preprocessor1â€¦\n#&gt;  4 Yes             0.568   0.432  train/test split     8 Yes      Preprocessor1â€¦\n#&gt;  5 Yes             0.554   0.446  train/test split    10 Yes      Preprocessor1â€¦\n#&gt;  6 Yes             0.970   0.0297 train/test split    11 Yes      Preprocessor1â€¦\n#&gt;  7 Yes             0.963   0.0367 train/test split    12 Yes      Preprocessor1â€¦\n#&gt;  8 Yes             0.947   0.0528 train/test split    14 Yes      Preprocessor1â€¦\n#&gt;  9 Yes             0.943   0.0573 train/test split    15 Yes      Preprocessor1â€¦\n#&gt; 10 Yes             0.977   0.0227 train/test split    19 Yes      Preprocessor1â€¦\n#&gt; # â„¹ 1,412 more rows"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#what-is-in-final_fit-2",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#what-is-in-final_fit-2",
    "title": "4 - Evaluating models",
    "section": "What is in final_fit? ",
    "text": "What is in final_fit? \n\nextract_workflow(final_fit)\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: rand_forest()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; forested ~ .\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Ranger result\n#&gt; \n#&gt; Call:\n#&gt;  ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n#&gt; \n#&gt; Type:                             Probability estimation \n#&gt; Number of trees:                  1000 \n#&gt; Sample size:                      5685 \n#&gt; Number of independent variables:  18 \n#&gt; Mtry:                             4 \n#&gt; Target node size:                 10 \n#&gt; Variable importance mode:         none \n#&gt; Splitrule:                        gini \n#&gt; OOB prediction error (Brier s.):  0.06153207\n\n\nUse this for prediction on new data, like for deploying"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#the-whole-game",
    "href": "archive/2024-08-posit-conf/intro-04-evaluating-models.html#the-whole-game",
    "title": "4 - Evaluating models",
    "section": "The whole game",
    "text": "The whole game"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-06-wrapping-up.html#your-turn",
    "href": "archive/2024-08-posit-conf/intro-06-wrapping-up.html#your-turn",
    "title": "6 - Wrapping up",
    "section": "Your turn",
    "text": "Your turn\n\nWhat is one thing you learned that surprised you?\nWhat is one thing you learned that you plan to use?\n\n\n\nâˆ’+\n05:00"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-06-wrapping-up.html#resources-to-keep-learning",
    "href": "archive/2024-08-posit-conf/intro-06-wrapping-up.html#resources-to-keep-learning",
    "title": "6 - Wrapping up",
    "section": "Resources to keep learning",
    "text": "Resources to keep learning\n\n\nhttps://www.tidymodels.org/\n\n\n\n\nhttps://www.tmwr.org/\n\n\n\n\nhttp://www.feat.engineering/\n\n\n\n\nhttps://smltar.com/\n\n\n\nFollow us on Mastodon and at the tidyverse blog for updates!"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-workflowsets.html#how-can-we-compare-multiple-model-workflows-at-once",
    "href": "archive/2024-08-posit-conf/intro-extra-workflowsets.html#how-can-we-compare-multiple-model-workflows-at-once",
    "title": "Extras - workflowsets",
    "section": "How can we compare multiple model workflows at once?",
    "text": "How can we compare multiple model workflows at once?"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-workflowsets.html#evaluate-a-workflow-set",
    "href": "archive/2024-08-posit-conf/intro-extra-workflowsets.html#evaluate-a-workflow-set",
    "title": "Extras - workflowsets",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nwf_set &lt;- workflow_set(list(forested ~ .), list(tree_spec, rf_spec))\nwf_set\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result    \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-workflowsets.html#evaluate-a-workflow-set-1",
    "href": "archive/2024-08-posit-conf/intro-extra-workflowsets.html#evaluate-a-workflow-set-1",
    "title": "Extras - workflowsets",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nwf_set_fit &lt;- wf_set %&gt;%\n  workflow_map(\"fit_resamples\", resamples = forested_folds)\nwf_set_fit\n#&gt; # A workflow set/tibble: 2 Ã— 4\n#&gt;   wflow_id              info             option    result   \n#&gt;   &lt;chr&gt;                 &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n#&gt; 1 formula_decision_tree &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n#&gt; 2 formula_rand_forest   &lt;tibble [1 Ã— 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-workflowsets.html#evaluate-a-workflow-set-2",
    "href": "archive/2024-08-posit-conf/intro-extra-workflowsets.html#evaluate-a-workflow-set-2",
    "title": "Extras - workflowsets",
    "section": "Evaluate a workflow set",
    "text": "Evaluate a workflow set\n\nwf_set_fit %&gt;%\n  rank_results()\n#&gt; # A tibble: 6 Ã— 9\n#&gt;   wflow_id         .config .metric   mean std_err     n preprocessor model  rank\n#&gt;   &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n#&gt; 1 formula_rand_foâ€¦ Preproâ€¦ accuraâ€¦ 0.919  0.00525    10 formula      randâ€¦     1\n#&gt; 2 formula_rand_foâ€¦ Preproâ€¦ brier_â€¦ 0.0617 0.00338    10 formula      randâ€¦     1\n#&gt; 3 formula_rand_foâ€¦ Preproâ€¦ roc_auc 0.972  0.00310    10 formula      randâ€¦     1\n#&gt; 4 formula_decisioâ€¦ Preproâ€¦ accuraâ€¦ 0.894  0.00562    10 formula      deciâ€¦     2\n#&gt; 5 formula_decisioâ€¦ Preproâ€¦ brier_â€¦ 0.0817 0.00434    10 formula      deciâ€¦     2\n#&gt; 6 formula_decisioâ€¦ Preproâ€¦ roc_auc 0.951  0.00378    10 formula      deciâ€¦     2\n\nThe first metric of the metric set is used for ranking. Use rank_metric to change that.\n\nLots more available with workflow sets, like collect_metrics(), autoplot() methods, and more!"
  },
  {
    "objectID": "archive/2024-08-posit-conf/intro-extra-workflowsets.html#your-turn",
    "href": "archive/2024-08-posit-conf/intro-extra-workflowsets.html#your-turn",
    "title": "Extras - workflowsets",
    "section": "Your turn",
    "text": "Your turn\n\nWhen do you think a workflow set would be useful?\nDiscuss with your neighbors!\n\n\n\nâˆ’+\n03:00"
  },
  {
    "objectID": "slides/intro-06-wrapping-up.html",
    "href": "slides/intro-06-wrapping-up.html",
    "title": "6 - Wrapping up",
    "section": "",
    "text": "We made it!"
  },
  {
    "objectID": "slides/extras-iterative-search.html",
    "href": "slides/extras-iterative-search.html",
    "title": "Extras - Iterative search",
    "section": "",
    "text": "library(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.4.1 â”€â”€\n#&gt; âœ” broom        1.0.9          âœ” rsample      1.3.1     \n#&gt; âœ” dials        1.4.2          âœ” tailor       0.1.0.9000\n#&gt; âœ” dplyr        1.1.4          âœ” tidyr        1.3.1     \n#&gt; âœ” infer        1.0.9          âœ” tune         2.0.0     \n#&gt; âœ” modeldata    1.5.1          âœ” workflows    1.3.0     \n#&gt; âœ” parsnip      1.3.3          âœ” workflowsets 1.1.1     \n#&gt; âœ” purrr        1.1.0          âœ” yardstick    1.3.2     \n#&gt; âœ” recipes      1.3.1\n#&gt; â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\n#&gt; âœ– purrr::discard() masks scales::discard()\n#&gt; âœ– dplyr::filter()  masks stats::filter()\n#&gt; âœ– dplyr::lag()     masks stats::lag()\n#&gt; âœ– recipes::step()  masks stats::step()\nlibrary(important)\nlibrary(probably)\n#&gt; \n#&gt; Attaching package: 'probably'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.factor, as.ordered\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\nmirai::daemons(parallel::detectCores())"
  },
  {
    "objectID": "slides/advanced-01-introduction.html",
    "href": "slides/advanced-01-introduction.html",
    "title": "1 - Introduction",
    "section": "",
    "text": "Welcome!"
  },
  {
    "objectID": "slides/intro-01-introduction.html",
    "href": "slides/intro-01-introduction.html",
    "title": "1 - Introduction",
    "section": "",
    "text": "Welcome!"
  },
  {
    "objectID": "slides/annotations.html#neural-network-tuning",
    "href": "slides/annotations.html#neural-network-tuning",
    "title": "Annotations",
    "section": "Neural network tuning",
    "text": "Neural network tuning\nThere might be some warnings that\n\nâ€œEarly stopping occurred at epoch 3 due to numerical overflow of the loss function.â€\n\ntorch can be very aggressive about moving in the direction of the gradient. In some cases, it moves to a space where the gradient is not well-behaved (e.g., flat in multiple directions, saddle point, etc).\nThis can cause abnormally large parameter values with magnitudes larger than double precision variables can hold.\nIn this case, brulee stops the optimization and returns the parameters from the last best iteration."
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html",
    "href": "slides/intro-04-evaluating-models.html",
    "title": "4 - Evaluating models",
    "section": "",
    "text": "#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.4.1 â”€â”€\n#&gt; âœ” broom        1.0.9          âœ” rsample      1.3.1     \n#&gt; âœ” dials        1.4.2          âœ” tailor       0.1.0.9000\n#&gt; âœ” dplyr        1.1.4          âœ” tidyr        1.3.1     \n#&gt; âœ” infer        1.0.9          âœ” tune         2.0.0     \n#&gt; âœ” modeldata    1.5.1          âœ” workflows    1.3.0     \n#&gt; âœ” parsnip      1.3.3          âœ” workflowsets 1.1.1     \n#&gt; âœ” purrr        1.1.0          âœ” yardstick    1.3.2     \n#&gt; âœ” recipes      1.3.1\n#&gt; â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\n#&gt; âœ– purrr::discard() masks scales::discard()\n#&gt; âœ– dplyr::filter()  masks stats::filter()\n#&gt; âœ– dplyr::lag()     masks stats::lag()\n#&gt; âœ– recipes::step()  masks stats::step()\n\n\naugment(forested_fit, new_data = forested_train)\n#&gt; # A tibble: 8,749 Ã— 22\n#&gt;    .pred_class .pred_Yes .pred_No forested  year elevation eastness roughness\n#&gt;    &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 Yes             0.931   0.0690 Yes       1997        66       82        10\n#&gt;  2 Yes             0.983   0.0172 No        1997       284      -99        58\n#&gt;  3 Yes             0.960   0.0401 Yes       2022       130       86        15\n#&gt;  4 Yes             0.870   0.130  Yes       2021       202      -55         3\n#&gt;  5 Yes             0.823   0.177  Yes       1995        75      -89         1\n#&gt;  6 Yes             0.758   0.242  No        1995       110      -53         5\n#&gt;  7 Yes             0.823   0.177  Yes       2022       111       73        12\n#&gt;  8 No              0.467   0.533  Yes       1997       230       96        14\n#&gt;  9 Yes             0.983   0.0172 Yes       2002       160      -88        13\n#&gt; 10 Yes             0.871   0.129  Yes       2020        39        9         6\n#&gt; # â„¹ 8,739 more rows\n#&gt; # â„¹ 14 more variables: tree_no_tree &lt;fct&gt;, dew_temp &lt;dbl&gt;, precip_annual &lt;dbl&gt;,\n#&gt; #   temp_annual_mean &lt;dbl&gt;, temp_annual_min &lt;dbl&gt;, temp_annual_max &lt;dbl&gt;,\n#&gt; #   temp_january_min &lt;dbl&gt;, vapor_min &lt;dbl&gt;, vapor_max &lt;dbl&gt;,\n#&gt; #   canopy_cover &lt;dbl&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, land_type &lt;fct&gt;, county &lt;fct&gt;"
  },
  {
    "objectID": "slides/intro-04-evaluating-models.html#brier-score-formula",
    "href": "slides/intro-04-evaluating-models.html#brier-score-formula",
    "title": "4 - Evaluating models",
    "section": "Brier score formula",
    "text": "Brier score formula\nWhat if we donâ€™t turn predicted probabilities into class predictions?\n\nThe Brier score is analogous to the mean squared error in regression models:\n\\[\nBrier_{class} = \\frac{1}{NC}\\sum_{i=1}^N\\sum_{k=1}^C (y_{ik} - \\hat{p}_{ik})^2\n\\]"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html",
    "href": "slides/advanced-02-model-tuning.html",
    "title": "2 - Model optimization by tuning",
    "section": "",
    "text": "library(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.4.1 â”€â”€\n#&gt; âœ” broom        1.0.9          âœ” rsample      1.3.1     \n#&gt; âœ” dials        1.4.2          âœ” tailor       0.1.0.9000\n#&gt; âœ” dplyr        1.1.4          âœ” tidyr        1.3.1     \n#&gt; âœ” infer        1.0.9          âœ” tune         2.0.0     \n#&gt; âœ” modeldata    1.5.1          âœ” workflows    1.3.0     \n#&gt; âœ” parsnip      1.3.3          âœ” workflowsets 1.1.1     \n#&gt; âœ” purrr        1.1.0          âœ” yardstick    1.3.2     \n#&gt; âœ” recipes      1.3.1\n#&gt; â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\n#&gt; âœ– purrr::discard() masks scales::discard()\n#&gt; âœ– dplyr::filter()  masks stats::filter()\n#&gt; âœ– dplyr::lag()     masks stats::lag()\n#&gt; âœ– recipes::step()  masks stats::step()\nlibrary(probably)\n#&gt; \n#&gt; Attaching package: 'probably'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.factor, as.ordered\nlibrary(desirability2)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\n\n# check torch:\nif (torch::torch_is_installed()) {\n  library(torch)\n}\n\n# Load our example data for this section\n\"https://raw.githubusercontent.com/tidymodels/\" |&gt; \n  paste0(\"workshops/main/slides/class_data.RData\") |&gt; \n  url() |&gt; \n  load()"
  },
  {
    "objectID": "slides/advanced-02-model-tuning.html#neural-network-tuning",
    "href": "slides/advanced-02-model-tuning.html#neural-network-tuning",
    "title": "2 - Model optimization by tuning",
    "section": "Neural network tuning   ",
    "text": "Neural network tuning   \n\nctrl &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(12)\nnnet_res &lt;-\n  nnet_wflow |&gt;\n  tune_grid(\n    resamples = sim_rs,\n    grid = 25,\n    # The options below are not required by default\n    param_info = nnet_param, \n    control = ctrl,\n    metrics = cls_mtr\n  )\n\n\n\nâœ‹ maybe donâ€™t run this just yetâ€¦\n\n\ntune_grid() is representative of tuning function syntax\nsimilar to fit_resamples()"
  },
  {
    "objectID": "slides/advanced-08-wrapping-up.html",
    "href": "slides/advanced-08-wrapping-up.html",
    "title": "8 - Wrapping up",
    "section": "",
    "text": "We made it!"
  }
]