---
title: "4 - Feature engineering: dummies and embeddings"
subtitle: "Getting More Out of Feature Engineering and Tuning for Machine Learning"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

```{r setup}
#| include: false
#| file: setup.R
```

## Getting set up `r hexes("tidymodels", "embed", "recipes")`

```{r}
#| label: user-startup

library(tidymodels)
library(embed)
library(extrasteps)

tidymodels_prefer()
theme_set(theme_bw())
options(pillar.advice = FALSE, pillar.min_title_chars = Inf)

# Load our example data for this section
"https://github.com/tidymodels/workshops/raw/refs/heads/2025-GMOFETML/slides/leaf_data.RData" |> 
  url() |> 
  load()
```

# Leaf data {background-color="#5568af"}

## Leaf data set

Slightly modified version of [modeldata::leaf_id_flavia](https://modeldata.tidymodels.org/reference/leaf_id_flavia.html).

```{r}
#| label: glimse-leaf-data
glimpse(leaf_data)
```

## Leaf data set

![](images/Samples-from-Flavia-and-Swedish-datasets.png){fig-align="center"}

::: footer
Lakshika, J. P., & Talagala, T. S. (2021). Computer-aided interpretable features for leaf image classification. arXiv preprint arXiv:2106.08077.
:::

## Leaf data set

![](images/image-processing-workflow.png){fig-align="center"}

::: footer
Lakshika, J. P., & Talagala, T. S. (2021). Computer-aided interpretable features for leaf image classification. arXiv preprint arXiv:2106.08077.
:::

## Leaf data set

![](images/leaf-features.png){fig-align="center"}

::: footer
Lakshika, J. P., & Talagala, T. S. (2021). Computer-aided interpretable features for leaf image classification. arXiv preprint arXiv:2106.08077.
:::

## Your turn

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Load and explore the leaf data*

*Make time to look at the `edge` column and think about how one would encode it into numerics*

```{r}
#| label: countdown-explore-leaves
#| echo: false
countdown(minutes = 5, id = "explore-leaves")
```

## Leaf edges

::: {.columns}
::: {.column}

- Low number of unique levels
- Some overlap

:::
::: {.column}

```{r}
#| label: leaf-count-edge
leaf_data |>
  count(edge)
```

:::
:::

# Advanced Dummies {background-color="#cdd629"}

## Leaf edges dummies

(technically one-hot encoding)

::: {style="font-size:0.9em;"}

```{r}
#| label: leaf-edges-dummies
#| echo: false
recipe(~edge, data = leaf_data) |>
  step_dummy(edge, one_hot = TRUE) |>
  prep() |>
  bake(new_data = NULL) |>
  slice(100, 200, 300, 400, 500, 600) |>
  rename_with(\(x) stringr::str_remove(x, "edge_")) |>
  knitr::kable()
```

:::

## Dummy variables

::: {.columns}
::: {.column}

### Pros

- Commonly used
- Easy interpretation
- Will rarely lead to a decrease in performance

:::
::: {.column}

### Cons

- Can create many columns
- Needs clean levels
- Not likely the most efficient representation

:::
:::

## Overlapping

::: {.columns}
::: {.column}

Some of the labels

- `lobed`
- `smooth`
- `toothed`
- `lobed, smooth`
- `lobed, toothed`

:::
::: {.column}

If you want to find a "toothed" leaf, which level do you pick?

- `toothed`
- `lobed, toothed`
- `toothed` and `lobed, toothed`


::: fragment
We can let `lobed, toothed` be counted for `lobed` and `toothed`
:::

:::
:::

## Advanced Dummies

This is basically a poor man's Natural Language Processing.

<br>

::: {style="text-align:center;"}
`tokenization -> counting`
:::

<br>

We think it is a frequent enough case that it is considered its own method.

We have 2 variants: "extraction" and "multi choice"

## Advanced Dummies - Extraction

Works on a singular column, using a regular expression to extract the items we want to count.

Done using regular expressions, either by specifying `sep` to split the string by, or by using `pattern` to extract the items.

Allows for 0 to many items in each string.

Implemented as [`step_dummy_extract()`](https://recipes.tidymodels.org/reference/step_dummy_extract.html).

::: {.absolute bottom=0 right=0}
[FEAZ](https://feaz-book.com/categorical-multi-dummy)
:::

## Advanced Dummies - Extraction

::: {.columns}
::: {.column width="35%"}

### Input

```{r}
#| label: unique-leaf-edge-values
#| echo: false
#| comment: ""
values <- leaf_data |>
  count(edge) |>
  pull(edge)
values <- paste0("\"", values, "\"")

cat(values, sep = "\n")
```

<br>

Using 

`sep = ", "`

:::
::: {.column width="65%"}

### Result

::: {style="font-size:0.9em;"}

```{r}
#| label: leaf-dummy-extract
#| echo: false
leaf_data |>
  count(edge) |>
  select(-n) |>
  recipe() |>
  step_dummy_extract(edge, sep = ", ") |>
  prep() |>
  bake(NULL) |>
  rename_with(\(x) stringr::str_remove(x, "edge_")) |>
  select(-other) |>
  knitr::kable()
```

:::

:::
:::

## Advanced Dummies - Extraction

::: {.columns}
::: {.column width="50%"}

### Input

```{r}
#| label: tate-medium-sample
#| echo: false
#| comment: ""
medium_examples <- tibble(
 medium = tate_text$medium[c(2, 5, 7, 9)]
)

medium_examples <- medium_examples |>
 tibble::add_row(medium = "Oil paint, ink on canvas")

values <- medium_examples |>
  pull(medium)
values <- paste0("\"", values, "\"")

cat(values, sep = "\n")
```

<br>

Using

`sep = ", "`

:::
::: {.column width="50%"}

### Result

```{r}
#| label: tate-medium-dummy-extract
#| echo: false
medium_examples |>
  recipe() |>
  step_dummy_extract(medium, sep = ", ") |>
  prep() |>
  bake(NULL) |>
  rename_with(\(x) stringr::str_remove(x, "medium_")) |>
  select(-other) |>
  knitr::kable()
```

:::
:::

## Advanced Dummies - Extraction

::: {.columns}
::: {.column width="50%"}

### Input

```{r}
#| label: tate-medium-sample-2
#| echo: false
#| comment: ""
medium_examples <- tibble(
 medium = tate_text$medium[c(2, 5, 7, 9)]
)

medium_examples <- medium_examples |>
 tibble::add_row(medium = "Oil paint, ink on canvas")

values <- medium_examples |>
  pull(medium)
values <- paste0("\"", values, "\"")

cat(values, sep = "\n")
```

<br>

Using

`sep = "(, )|( and )|( on )"`

:::
::: {.column width="50%"}

### Result

```{r}
#| label: better-tate-medium-dummy-extract
#| echo: false
medium_examples |>
  recipe() |>
  step_dummy_extract(medium, sep = "(, )|( and )|( on )") |>
  prep() |>
  bake(NULL) |>
  rename_with(\(x) stringr::str_remove(x, "medium_")) |>
  select(-other) |>
  knitr::kable()
```

:::
:::

## Advanced Dummies - Extraction

::: {.columns}
::: {.column width="50%"}

### Input

```{r}
#| echo: false
#| comment: ""
color_examples <- tibble(
 colors = c(
    "['red', 'blue']",
    "['red', 'blue', 'white']",
    "['blue', 'blue', 'blue']"
 )
)
values <- color_examples |>
  pull(colors)
values <- paste0("\"", values, "\"")

cat(values, sep = "\n")
```

<br>

Using

`pattern = "(?<=')[^',]+(?=')"`

:::
::: {.column width="50%"}

### Result

```{r}
#| label: colors-dummy-extract
#| echo: false
color_examples |>
  recipe() |>
  step_dummy_extract(colors, pattern = "(?<=')[^',]+(?=')") |>
  prep() |>
  bake(NULL) |>
  rename_with(\(x) stringr::str_remove(x, "colors_")) |>
  select(-other) |>
  knitr::kable()
```

:::
:::

## Advanced Dummies - Multi Choice

Works on multiple columns, counting items across columns.

It can be seen as joining multiple applications of dummy variables together.

Allows for 0 to many items.

Implemented as [`step_dummy_multi_choice()`](https://recipes.tidymodels.org/reference/step_dummy_multi_choice.html).

::: {.absolute bottom=0 right=0}
[FEAZ](https://feaz-book.com/categorical-multi-dummy)
:::

## Advanced Dummies - Multi Choice

::: {.columns}
::: {.column width="45%"}

### Input

```{r}
#| label: lang-sample
#| echo: false
#| comment: ""
values <- tribble(
  ~lang_1,    ~lang_2,   ~lang_3,
  "English",  NA,        "Italian",
  "Spanish",  "French",  "French",
  NA,         NA,        NA,
  "English",  NA,        NA,
)
knitr::kable(values)
```

:::

::: {.column width="55%"}

### Result

```{r}
#| label: lang-dummy-multi-choice
#| echo: false
recipe(~., data = values) |>
  step_dummy_multi_choice(starts_with("lang")) |>
  prep() |>
  bake(NULL) |>
  rename_with(\(x) stringr::str_remove(x, "lang_1_")) |>
  knitr::kable()
```

:::
:::

## Othering

Both [`step_dummy_extract()`](https://recipes.tidymodels.org/reference/step_dummy_extract.html) and [`step_dummy_multi_choice()`](https://recipes.tidymodels.org/reference/step_dummy_multi_choice.html) contain a `threshold` argument.

This is used to combine infrequent levels together.

We have a step [`step_other()`](https://recipes.tidymodels.org/reference/step_dummy_other.html) that does this for nominal variables, but it doesn't work in these cases, as it has to happen after the extraction/combination.

## Othering

`threshold = 0` produces 1217 columns.

```{r}
#| label: medium-dummy-extract-threshold-0
#| echo: false
recipe(~ medium, data = tate_text) |>
  step_dummy_extract(medium, sep = "(, )|( and )|( on )", threshold = 0) |>
  prep() |>
  bake(NULL) |>
  rename_with(\(x) stringr::str_remove(x, "medium_")) |>
  slice(1:6) |>
  knitr::kable()
```

## Othering

`threshold = 0.05` produces 11 columns.

```{r}
#| label: medium-dummy-extract-threshold-0.05
#| echo: false
recipe(~ medium, data = tate_text) |>
  step_dummy_extract(medium, sep = "(, )|( and )|( on )", threshold = 0.05) |>
  prep() |>
  bake(NULL) |>
  rename_with(\(x) stringr::str_remove(x, "medium_")) |>
  slice(1:6) |>
  knitr::kable()
```

## Your turn

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Figure out whether `step_dummy_extract()` or `step_dummy_multi_choice()` is most appropriate on the `edge` variable in `leaf_data` and apply it*

```{r}
#| label: countdown-dummy-leaves
#| echo: false
countdown(minutes = 3, id = "dummy-leaves")
```

# Dimensionality Reduction {background-color="#f8c9dd"}

## Dimensionality Reduction

### What is it?

Techniques that remove or alter features in order to have fewer but more informative features.

### Why do we do it?

- redundant information
- ineffective representation
- computational speed

## Redundant Information

A feature with no information could be included.

```{r}
#| label: count-leaf-outlying-contour
leaf_data |>
  count(outlying_contour)
```

## Redundant Information

Or duplicated or functionally equivalent features

```{r}
#| label: area-vs-equivalent-diameter
leaf_data |>
  ggplot(aes(area, equivalent_diameter)) +
  geom_point()
```

## Ineffective Representation

While keeping our models in mind, we want to make sure the data is well-suited

- Correlated data
  - hard for some models
- lat/lon compared to distance/angle
  - hard for most models

## Ineffective Representation

```{r}
#| label: mean-red-val-vs-mean-blue-val
#| echo: false
leaf_data |>
  ggplot(aes(mean_red_val, mean_blue_val)) +
  geom_point()
```

## Ineffective Representation

```{r}
#| label: physiological-length-vs-diameter
#| echo: false
leaf_data |>
  ggplot(aes(physiological_length, diameter)) +
  geom_point()
```

## Computational Speed

Depending on what method we are using and how the data is affected by it, we could see a large reduction in features. This, in turn, leads to a smaller model that is faster to train on.

Only exploration and trial and error can determine whether you should use dimensionality reduction techniques. Knowing which methods do what helps you determine what to try.

## Dimensionality Reduction Method

- Zero Variance removal
- PCA
  - Truncated PCA
  - Sparse PCA
- NNMF
- UMAP
- Isomap

## Restrictions

All the methods shown today will not be able to handle

- missing data
- Non-numeric data

## Why not t-SNE?

One of the main requirements for a feature engineering method is that you can reapply the trained transformation done on the training data set to the testing data set.

This is not possible with t-SNE as it is an iterative method that shifts observations in the lower-dimensional space based on their distances to points in the higher-dimensional space.

It doesn't create a mapping that can be reused.

::: footer
Visualizing Data using t-SNE, Laurens van der Maaten and Geoffrey Hinton, Journal of Machine Learning Research, 2008.
:::

## PCA

**P**rincipal **C**ompoment **A**nalysis is a linear combination of the original data such that most of the variation is captured in the first variable, then the second, then the third, and so on.

::: {.absolute bottom=0 right=0}
[FEAZ](https://feaz-book.com/too-many-pca), [FES](http://www.feat.engineering/numeric-many-to-many)
:::

## PCA Algorithm

The first principal component of a set of features $X_1, X_2, ..., X_p$ is the normalized linear combination of the features.

$$
Z_1 = \phi_{11} X_1 + \phi_{21} X_2 + ... + \phi_{p1}X_p
$$

that has the largest variance under the constraint that $\sum_{j=1}^p  \phi_{j1}^2 = 1$.

we refer to $\phi_{11}, ..., \phi_{p1}$ as the loadings of the first principal component.

And think of them as the loading vector $\phi_1$.

## PCA Algorithm 

since we have $z_{i1} = \phi_{11} x_{i1} + \phi_{21} x_{i2} + ... + \phi_{p1}x_{ip}$, then we can write

$$
\underset{\phi_{11}, ..., \phi_{p1}}{\text{maximize}} \left\{ \dfrac{1}{n} \sum^n_{i=j} z_{i1} ^2 \right\} \quad \text{subject to} \quad \sum_{j=1}^p  \phi_{j1}^2 = 1
$$

We are, in essence, maximizing the sample variance of the $n$ values of $z_{i1}$.

We refer to $z_{11}, ..., z_{n1}$ as the scores of the first principal component.

## PCA Algorithm

Luckily, this can be solved using techniques from Linear Algebra, more specifically, it can be solved using an eigen decomposition.

One of the main strengths of PCA is that you don't need to use optimization to get the results without approximations.

## PCA Algorithm

Once the first principal component is calculated, we can calculate the second principal component. 

We find the second principal component $Z_2$ as a linear combination of $X_1, ..., X_p$ that has the maximal variance out of the linear combinations that are uncorrelated with $Z_1$

this is the same as saying that $\phi_2$ should be orthogonal to the direction $\phi_1$

## How is that a dimensionality reduction method?

By itself, it isn't, as it rotates all the features in the feature space.

It becomes a dimensionality reduction method if we only calculate some of the principal components.

This is typically done by retaining a specific number of components or as a threshold on the variance explained.

## 4 different percent variance plots

```{r}
#| label: pca-cumulative-percent-variance
#| echo: false
#| fig-alt: |
#|   Faceted bar chart. 4 bar charts in a 2 by 2 grid. Each one represents
#|   a different data set which has had PCA applied to it. 
set.seed(1234)

data_cumpervar <- function(data) {
data |>
  select(where(is.numeric)) |>
  recipe(~.) |>
  step_normalize(all_predictors()) |>
  step_pca(all_predictors()) |>
  prep() |> 
  tidy(2, type = "variance") |>
  filter(terms == "percent variance")
}
library(scales)

bind_rows(
data_cumpervar(mtcars),
data_cumpervar(ames),
data_cumpervar(Chicago),
data_cumpervar(concrete)
) |>
  ggplot(aes(component, value)) +
  geom_col() +
  facet_wrap(~id, scales = "free_x") +
  labs(
 y = "Percent variance"
 ) +
  theme_minimal() +
  theme(strip.text = element_blank()) +
  scale_x_continuous(breaks = c(5, seq(0, 100, by = 10)))
```

## Applying PCA with recipes  `r hexes("recipes")`

Either use the `num_comp` argument.

```{r}
#| label: step-pca-num-comp
rec <- recipe(mpg ~ ., data = mtcars) |>
  step_normalize(all_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = 5)
```

<br>

or using the `threshold` argument

```{r}
#| label: step-pca-threshold
rec <- recipe(mpg ~ ., data = mtcars) |>
  step_normalize(all_predictors()) |>
  step_pca(all_numeric_predictors(), threshold = 0.8)
```

## Your turn

![](images/parsnip-flagger.jpg){.absolute top="0" right="0" width="150" height="150"}

*Apply PCA using `step_pca()` to `leaf_data` data set.*

*Experiment with different values of `num_comp` and or `threshold`.*

```{r}
#| label: countdown-pca-leaf-data
#| echo: false
countdown(minutes = 5, id = "pca-leaf_data")
```

## PCA Pros and Cons

::: {.columns}
::: {.column}

### Pros

- Fast
- Reliable
- Exact results (up to sign changes)

:::
::: {.column}

### Cons

- Computational time is linear in the number of columns
- Can be quite hard to interpret

:::
:::

## Truncated PCA

> Computational time is linear in the number of columns

By default, [`step_pca()`](https://recipes.tidymodels.org/reference/step_pca.html) calculates all the loading vectors. And then subset them down to what you need.

Instead, we can use a different implementation that only calculates what you need. This is what we call truncated PCA.

::: {.absolute bottom=0 right=0}
[FEAZ](https://feaz-book.com/too-many-pca-variants)
:::

## Truncated PCA with recipes `r hexes("embed", "recipes")`

Can only be done using `num_comp`

```{r}
#| label: step-pca-truncated
library(embed)

rec <- recipe(mpg ~ ., data = mtcars) |>
  step_normalize(all_predictors()) |>
  step_pca_truncated(all_numeric_predictors(), num_comp = 5)
```

## Sparse PCA

> Can be quite hard to interpret

Every component is a linear combination of **all predictors**.

$$
\begin{alignat}{4}
PC1 &= cyl \cdot -0.021 &&+ disp \cdot -0.85 &&+ hp \cdot -0.52\\
PC2 &= cyl \cdot 0.013 &&+ disp \cdot -0.52 &&+ hp \cdot 0.85\\
PC3 &= cyl \cdot -0.12 &&+ disp \cdot 0.016 &&+ hp \cdot 0.081\\
PC4 &= cyl \cdot -0.22 &&+ disp \cdot -0.0061 &&+ hp \cdot 0.033\\
PC5 &= cyl \cdot 0.73 &&+ disp \cdot -0.014 &&+ hp \cdot 0.0016
\end{alignat}
$$

If we could force some of the loadings to be 0, it would reduce things a lot.

::: {.absolute bottom=0 right=0}
[FEAZ](https://feaz-book.com/too-many-pca-variants)
:::

## Sparse PCA with recipes `r hexes("embed", "recipes")`

Can only be done using `num_comp`.

The `predictor_prop` argument is used to determine how many zeroes in the loadings.

```{r}
#| label: step-pca-sparse
library(embed)

rec <- recipe(mpg ~ ., data = mtcars) |>
  step_normalize(all_predictors()) |>
  step_pca_sparse(all_numeric_predictors(), num_comp = 5, predictor_prop = 0.8)
```

## Sparse PCA with recipes `r hexes("embed", "recipes")`

`predictor_prop = 0.8`

```{r}
#| label: step-pca-sparse-predictor-prop-0.8
recipe(mpg ~ ., data = mtcars) |>
  step_normalize(all_predictors()) |>
  step_pca_sparse(all_numeric_predictors(), num_comp = 4, predictor_prop = 0.8) |>
  prep() |>
  tidy(number = 2) |>
  pivot_wider(names_from = component, values_from = value) |>
  select(-id)
```

## Sparse PCA with recipes `r hexes("embed", "recipes")`

`predictor_prop = 0.2`

```{r}
#| label: step-pca-sparse-predictor-prop-0.2
recipe(mpg ~ ., data = mtcars) |>
  step_normalize(all_predictors()) |>
  step_pca_sparse(all_numeric_predictors(), num_comp = 4, predictor_prop = 0.2) |>
  prep() |>
  tidy(number = 2) |>
  pivot_wider(names_from = component, values_from = value) |>
  select(-id)
```

## NNMF

**N**on-**N**egative **M**atrix **F**actorization is conceptually similar to PCA, but it has different objectives. 

PCA aims to generate uncorrelated components that maximize the variances. One component at a time.

NNMF, on the other hand, simultaneously optimizes all the components under the constraint that all the loadings are non-negative. While the data is also non-negative.

::: {.absolute bottom=0 right=0}
[FEAZ](https://feaz-book.com/too-many-nmf) [FES](http://www.feat.engineering/numeric-many-to-many)
:::

## NNMF restriction

The data has to be non-negative, i.e., 0 or higher.

This might feel like a pretty big restriction. And that is not wrong. But a lot of data sets end up being naturally non-negative.

Or could at least be turned into non-negative ones with transformations. As long as you don't scale them below 0.

It makes for much easier interpretations as the loadings don't cancel each other out like they do for PCA.

## NNMF Pros and Cons

::: {.columns}
::: {.column}

- More interpretable results
- Pulls out better structures

:::
::: {.column}

- Data must be non-negative
- Computationally expensive
- Training depends on the seed

:::
:::

## NNMF with recipes `r hexes("embed", "recipes")`

```{r}
#| label: step-nnmf-sparse
set.seed(1234)

recipe(mpg ~ ., data = mtcars) |>
  step_nnmf_sparse(all_numeric_predictors(), num_comp = 2) |>
  prep() |>
  tidy(number = 1) |>
  pivot_wider(names_from = component, values_from = value) |>
  select(-id)
```

## UMAP

**U**niform **M**anifold **A**pproximation and **P**rojection is another method that takes high-dimensional data and transforms it into a lower-dimensional space.

Runs relatively fast and is popular in visualizations.

## UMAP Algorithm

Rough algorithm

1. Use spectral embedding to embed points in a low-dimensional space
2. Calculate similarity scores between points based on the original data set
3. Randomly samples a pair of points based on their similarity scores
4. Flips a coin to decide which of the pair of points to give to the other one
5. Randomly picks a non-neighbor point to move away from
6. Moves the selected point towards its neighbor and away from its non-neighbor
7. Repeat 3-6

## UMAP parameters

::: {.columns}
::: {.column width=25%}

- `n_neighbors`

:::
::: {.column width=75%}

Determines how many points are considered neighbors. A point counts as its own neighbor.
Lower values lead to a local view.

:::
:::

::: {.columns}
::: {.column width=25%}

- `min_dist`

:::
::: {.column width=75%}

Determines how close points are allowed to be to each other in the low-dimensional space.

:::
:::

::: {.columns}
::: {.column width=25%}

- `metric`

:::
::: {.column width=75%}

How distances are calculated in the input data: `euclidean`, `manhattan`, `jaccard`, etc.

:::
:::

## UMAP hesitancy 

Due to the flexibility of how this method works, it is almost always possible to generate graphs that appear to have insights in them.

## Which was created with random data?

```{r}
#| label: umap-unknown
#| echo: false
set.seed(2)
data_random <- rnorm(1000 * 100) |>
  matrix(ncol = 1000) |>
  as_tibble()

umap_2 <- recipe(~., data_random) |>
  step_umap(all_predictors(), neighbors = 2) |>
  prep() |>
  bake(NULL)

umap_3 <- recipe(~., data_random) |>
  step_umap(all_predictors(), neighbors = 3) |>
  prep() |>
  bake(NULL)

umap_5 <- recipe(~., data_random) |>
  step_umap(all_predictors(), neighbors = 5) |>
  prep() |>
  bake(NULL)

umap_10 <- recipe(~., data_random) |>
  step_umap(all_predictors(), neighbors = 10) |>
  prep() |>
  bake(NULL)
  
umap_all <- bind_rows(
  umap_2 |> mutate(type = "A", neighbors = 2),
  umap_3 |> mutate(type = "B", neighbors = 3),
  umap_5 |> mutate(type = "C", neighbors = 5),
  umap_10 |> mutate(type = "D", neighbors = 10)
)

umap_all |>
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point() +
  facet_wrap(~type, scales = "free")
```

## All of them! Different `n_neighbors`

```{r}
#| label: umap-known
#| echo: false
umap_all |>
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point() +
  facet_wrap(~neighbors, scales = "free")
```

## umap with recipes `r hexes("embed", "recipes")`

```{r}
#| label: step-umap
library(embed)
set.seed(1234)

recipe(mpg ~ ., data = mtcars) |>
  step_umap(all_numeric_predictors()) |>
  prep() |>
  bake(NULL)
```

## Isomap

Isometric mapping is a non-linear dimensionality reduction method. 

This is another method that uses distances between points to produce graphs of neighboring points. Where this method is different than other methods is that it uses geodesic distances as opposed to straight-line distances.

The geodesic distance is the sum of edge weights along the shortest path between two points.

The eigenvectors of the deodesic distance metric are then used to represent the new coordinates.

## Isomap Algorithm

A very high-level description of the Isomap algorithm is given below.

1. Find the neighbors for each point
2. Construct the neighborhood graph, using Euclidean distance as edge length
3. Calculate the shortest path between each pair of points
4. Use Multidimensional scaling to compute a lower-dimensional embedding

## Isomap Pros and Cons

::: {.columns}
::: {.column}

### Pros

- Captures non-linear effects
- Captures long-range structure, not just local structure
- No parameters to set other than neighbors

:::
::: {.column}

### Cons

- Computationally expensive
- Assumes a single connected manifold

:::
:::

## isomap with recipes `r hexes("recipes")`

```{r}
#| label: step-isomap
library(embed)
set.seed(1234)

recipe(mpg ~ ., data = mtcars) |>
  step_isomap(all_numeric_predictors(), neighbors = 10) |>
  prep() |>
  bake(NULL)
```
