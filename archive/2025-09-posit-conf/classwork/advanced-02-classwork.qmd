---
title: "2 - Model optimization by tuning - Classwork"
subtitle: "Getting More Out of Feature Engineering and Tuning for Machine Learning"
editor_options: 
  chunk_output_type: console
---

We recommend restarting R between each slide deck!

## Setup

Setup from deck 1

```{r}
library(tidymodels)
library(probably)
library(desirability2)

# Max's usual settings:
tidymodels_prefer()
theme_set(theme_bw())
options(
  pillar.advice = FALSE,
  pillar.min_title_chars = Inf
)

# check torch:
if (torch::torch_is_installed()) {
  library(torch)
}

# Load our example data for this section
"https://raw.githubusercontent.com/tidymodels/" |>
  paste0("workshops/main/slides/class_data.RData") |>
  url() |>
  load()

set.seed(429)
sim_split <- initial_split(class_data, prop = 0.75, strata = class)
sim_train <- training(sim_split)
sim_test <- testing(sim_split)

set.seed(523)
sim_rs <- vfold_cv(sim_train, v = 10, strata = class)
```

## A single model

```{r}
library(brulee)

nnet_ex_spec <-
  mlp(hidden_units = 20, penalty = 0.01, learn_rate = 0.005, epochs = 100) |>
  set_engine("brulee", class_weights = 3, stop_iter = 10) |>
  set_mode("classification")

rec <-
  recipe(class ~ ., data = sim_train) |>
  step_normalize(all_numeric_predictors())

nnet_ex_wflow <- workflow(rec, nnet_ex_spec)

# Fit on the first fold's 90% analysis set
set.seed(147)
nnet_ex_fit <- fit(nnet_ex_wflow, data = analysis(sim_rs$splits[[1]]))
```

Did it converge?

```{r}
nnet_ex_fit |>
  # pull out the brulee fit:
  extract_fit_engine() |>
  autoplot()
```

Did it work?

```{r}
assessment_data <- assessment(sim_rs$splits[[1]])
cls_mtr <- metric_set(brier_class, roc_auc, sensitivity, specificity)
holdout_pred <- augment(nnet_ex_fit, assessment_data)

# Performance metrics
holdout_pred |> cls_mtr(class, estimate = .pred_class, .pred_event)
```

## Calibration plots

```{r}
holdout_pred |>
  cal_plot_windowed(
    truth = class,
    estimate = .pred_event,
    window_size = 0.2,
    step_size = 0.025
  )
```

## Optimize tuning parameters

Optimizing the neural network

```{r}
nnet_spec <-
  mlp(
    hidden_units = tune(),
    penalty = tune(),
    learn_rate = tune(),
    epochs = 100,
    activation = tune()
  ) |>
  set_engine("brulee", class_weights = tune(), stop_iter = 10) |>
  set_mode("classification")

nnet_wflow <- workflow(rec, nnet_spec)
nnet_wflow

nnet_param <-
  nnet_wflow |> 
  extract_parameter_set_dials() |> 
  update(class_weights = class_weights(c(1, 50)))
nnet_param
```

Setting up parallel processing

```{r}
cores <- parallelly::availableCores(logical = FALSE)

# either future or mirai
library(mirai)
daemons(cores)
```

```{r}
ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)

set.seed(12)
nnet_res <-
  nnet_wflow |>
  tune_grid(
    resamples = sim_rs,
    grid = 25,
    # The options below are not required by default
    param_info = nnet_param,
    control = ctrl,
    metrics = cls_mtr
  )

nnet_res

# grid results
autoplot(nnet_res)

# Brier results
autoplot(nnet_res, metric = "brier_class") +
  facet_grid(. ~ name, scale = "free_x") +
  lims(y = c(0.04, 0.22)) # no tanh

# ROC curve results
autoplot(nnet_res, metric = "roc_auc") +
  facet_grid(. ~ name, scale = "free_x") +
  lims(y = c(0.83, 0.97)) # no tanh

# Sensitivity/Specificity results
autoplot(nnet_res, metric = c("sensitivity", "specificity"))
```

## Choosing Tuning Parameters

```{r}
collect_metrics(nnet_res) |>
  relocate(.metric, mean)

# or the results per fold
collect_metrics(nnet_res, summarize = FALSE) |>
  relocate(.metric, .estimate)
```

Choose a parameter combination

```{r}
# _show_ best based on the Brier score
show_best(nnet_res, metric = "brier_class") |>
  relocate(.metric, mean)

# then _select_ best parameter combination based on the Brier score
nnet_best <- select_best(nnet_res, metric = "brier_class")
nnet_best
```

Checking (approximate) calibration

```{r}
nnet_holdout_pred <-
  nnet_res |>
  collect_predictions(
    parameters = nnet_best
  )

nnet_holdout_pred |>
  cal_plot_windowed(
    truth = class,
    estimate = .pred_event,
    window_size = 0.2,
    step_size = 0.025,
  )
```

## Multimetric optimization 

```{r}
show_best_desirability(
  nnet_res,
  maximize(sensitivity),
  minimize(brier_class),
  constrain(specificity, low = 0.8, high = 1.0)
) |>
  relocate(class_weights, sensitivity, specificity, brier_class, .d_overall)
```

However...

```{r}
more_sens <-
  select_best_desirability(
    nnet_res,
    maximize(sensitivity),
    minimize(brier_class),
    constrain(specificity, low = 0.8, high = 1.0)
  )

nnet_res |>
  collect_predictions(
    parameters = more_sens
  ) |>
  cal_plot_windowed(
    truth = class,
    estimate = .pred_event,
    window_size = 0.2,
    step_size = 0.025,
  )
```

## Extracting results

If we want to know about the resampled workflow, we can write a function that can return information from `tune_grid()`. For example, this one can save the optimization process results: 

```{r}
extract_iter_hist <- function(wflow) {
  require(tidymodels)
  require(tibble)
  wflow |>
    extract_fit_engine() |>
    pluck("loss") |>
    as_tibble_col("loss") |>
    mutate(epoch = row_number() - 1)
}

set.seed(398)
mlp_brier_fit <-
  nnet_res |>
  fit_best(metric = "brier_class")

mlp_brier_fit |>
  extract_iter_hist() |>
  slice(1:5)
```

## Your turn

1. Read the docs for [`control_grid()`](https://tune.tidymodels.org/reference/control_grid.html), specifically the `extract` option.
2. Create a control object that extracts the iteration history.
3. Re-run `tune_grid()` with the same grid (or fewer grid points) and use the `control` option with the extraction function. 
4. Afterward use [`collect_extracts()`](https://tune.tidymodels.org/reference/collect_predictions.html) to get the results. 
5. Plot the iteration history for one or more grid points, coloring by the resample `id`. 

Parallel processing will make this go more quickly. 

```{r}
# Your code here!

```
