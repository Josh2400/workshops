{
  "hash": "3c65988e6c176a4da163a9fbc218017d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Annotations\"\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n    fig.path: \"figures/\"\n---\n\n\n\n\n\n<hr size=\"5\">\n\n# Introduction 1 - Introduction\n\n## ðŸ‘€\n\nThis page contains _annotations_ for selected slides. \n\nThere's a lot that we want to tell you. We don't want people to have to frantically scribble down things that we say that are not on the slides. \n\nWe've added sections to this document with longer explanations and links to other resources. \n\n<hr size=\"5\">\n\n# Introduction 2 - Data Budget\n\n## Data splitting and spending\n\nMore about the initial data split can be found in [Chapter 3](https://aml4td.org/chapters/initial-data-splitting.html) of _Applied Machine Learning for Tabular Data_ (AML4TD).\n\nIn particular, a three-way split into training, validation, and testing set can be done via\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ninitial_validation_split(forested, prop = c(0.6, 0.2))\n#> <Training/Validation/Testing/Total>\n#> <4264/1421/1422/7107>\n```\n:::\n\n\n\n## What is `set.seed()`? \n\nWhat does `set.seed()` do? \n\nWeâ€™ll use pseudo-random numbers (PRN) to partition the data into training and testing. PRN are numbers that emulate truly random numbers (but really are not truly random). \n\nThink of PRN as a box that takes a starting value (the \"seed\") that produces random numbers using that starting value as an input into its process. \n\nIf we know a seed value, we can reproduce our \"random\" numbers. To use a different set of random numbers, choose a different seed value. \n\nFor example: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nrunif(3)\n#> [1] 0.2655087 0.3721239 0.5728534\n\n# Get a new set of random numbers:\nset.seed(2)\nrunif(3)\n#> [1] 0.1848823 0.7023740 0.5733263\n\n# We can reproduce the old ones with the same seed\nset.seed(1)\nrunif(3)\n#> [1] 0.2655087 0.3721239 0.5728534\n```\n:::\n\n\n\nIf we _donâ€™t_ set the seed, R uses the clock time and the process ID to create a seed. This isnâ€™t reproducible. \n\nSince we want our code to be reproducible, we set the seeds before random numbers are used. \n\nIn theory, you can set the seed once at the start of a script. However, if we do interactive data analysis, we might unwittingly use random numbers while coding. In that case, the stream is not the same, and we donâ€™t get reproducible results. \n\nThe value of the seed is an integer and really has no meaning. Max has a script to generate random integers to use as seeds to \"spread the randomness around\". It is basically:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(paste0(\"set.seed(\", sample.int(10000, 5), \")\", collapse = \"\\n\"))\n#> set.seed(9725)\n#> set.seed(8462)\n#> set.seed(4050)\n#> set.seed(8789)\n#> set.seed(1301)\n```\n:::\n\n\n\n<hr size=\"5\">\n\n# Introduction 3 - What Makes A Model?\n\n## What is wrong with this? \n\nIf we treat the preprocessing as a separate task, it raises the risk that we might accidentally overfit to the data at hand. \n\nFor example, someone might estimate something from the entire data set (such as the principal components) and treat that data as if it were known (and not estimated). Depending on what was done with the data, the consequences of doing that could be:\n\n* Your performance metrics are slightly-to-moderately optimistic (e.g., you might think your accuracy is 85% when it is actually 75%)\n* A consequential component of the analysis is not right, and the model just doesnâ€™t work. \n\nThe big issue here is that you wonâ€™t be able to figure this out until you get a new piece of data, such as the test set. \n\nA really good example of this is in [â€˜Selection bias in gene extraction on the basis of microarray gene-expression dataâ€™](https://pubmed.ncbi.nlm.nih.gov/11983868/). The authors re-analyze a previous publication and show that the original researchers did not include feature selection in the workflow. Because of that, their performance statistics were extremely optimistic. In one case, they could do the original analysis on complete noise and still achieve zero errors. \n\nGenerally speaking, this problem is referred to as [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)). Some other references: \n\n * [Overfitting to Predictors and External Validation](https://bookdown.org/max/FES/selection-overfitting.html)\n * [Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/757b505cfd34c64c85ca5b5690ee5293-Abstract-round2.html)\n * [Navigating the pitfalls of applying machine learning in genomics](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Navigating+the+pitfalls+of+applying+machine+learning+in+genomics&btnG=)\n * [A review of feature selection techniques in bioinformatics](https://academic.oup.com/bioinformatics/article/23/19/2507/185254)\n * [On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation](https://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf)\n\n## Understand your model\n\nOn the next slide, we've abbreviated the code necessary to make this plot. If you want to replicate the plot exactly, you'll need:\n\n```r\nlibrary(rpart.plot)\nsplit_fun <- function(x, labs, digits, varlen, faclen) {\n  for (i in 1:length(labs)) {\n    if (grepl(\",\", labs[i])) {\n      parts <- strsplit(labs[i], \",\")[[1]]\n      parts <- trimws(parts)\n      if (length(parts) > 5) {\n        parts <- c(parts[1:5], \"...\")\n      }\n      labs[i] <- paste(parts, collapse = \", \")\n    }\n    labs[i] <- paste(strwrap(labs[i], width = 15), collapse = \"\\n\")\n  }\n  labs\n}\n\ntree_fit |>\n  extract_fit_engine() |>\n  rpart.plot(\n    roundint = FALSE,\n    type = 3,\n    clip.right.labs = FALSE,\n    split.fun = split_fun\n  )\n```\n\n<hr size=\"5\">\n\n# Introduction 4 - Evaluating Models\n\n## Brier score \n\nThe Brier score measures how close a model probability estimate is to its best possible value (i.e., zero or one). \n\nIn the best case, the model is perfect, and every prediction equals 0.0 or 1.0 (depending on the true class). In this case, the Brier score is zero. \n\nWhen the model is uninformative and there are two classes, the worst-case values range from 0.25 to about 0.50. Imagine that the model predicts the same noninformative prediction of 50% (basically \"Â¯\\\\_(ãƒ„)_/Â¯\").  In that case, every prediction is either $(0.00 - 0.50)^2$ or $(1.00 - 0.50)^2$. The average of those is 0.25.\n\nThere are many different ways a model can be bad, though, and some of these will produce Brier scores between 0.25 and 0.50.\n\n## Where are the fitted models?\n\nThe primary purpose of resampling is to estimate model performance. The models are almost never needed again. \n\nAlso, if the data set is large, the model object may require a lot of memory to save so, by default, we don't keep them. \n\nFor more advanced use cases, you can extract and save them. See:\n\n * <https://www.tmwr.org/resampling.html#extract>\n * <https://www.tidymodels.org/learn/models/coefficients/> (an example)\n\n<hr size=\"5\">\n\n# Advanced 2 - Model optimization by tuning\n\n## Different types of grids\n\nMore on space-filling designs in Chapters [4](https://bookdown.org/rbg/surrogates/chap4.html)  and [5](https://bookdown.org/rbg/surrogates/chap5.html) of _Surrogates: Gaussian process modeling, design, and optimization for the applied sciences_.\n\nThese designs also scale well as the number of tuning parameters grows. They are designed to fill the predictor space efficiently.  \n\n## Extract and update parameters\n\nIn about 90% of the cases, the dials function that you use to update the parameter range has the same name as the argument. For example, if you were to update the `mtry` parameter in a random forest model, the code would look like\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparameter_object |> \n  update(mtry = mtry(c(1, 100)))\n```\n:::\n\n\n\nIn some cases, the parameter function or its associated values differ from the argument name. \n\nFor example, with `step_spline_naturall()`, we might want to tune the `deg_free` argument (for the degrees of freedom of a spline function). In this case, the argument name is `deg_free`, but we update it with `spline_degree()`. \n\n`deg_free` represents the general concept of degrees of freedom and could be associated with many different things. For example, if we ever had an argument that was the number of degrees of freedom for a $t$ distribution, we would call that argument `deg_free`. \n\nWe probably want a wider range of degrees of freedom for splines. To this end, we created a specialized function called `spline_degree()`. \n\nHow can you tell when this happens? There is a helper function called `tunable()`, and that gives information on how we make the default ranges for parameters. There is a column in these objects names `call_info`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nns_tunable <- \n  recipe(mpg ~ ., data = mtcars) |> \n  step_spline_natural(dis, deg_free = tune()) |> \n  tunable()\n\nns_tunable\n#> # A tibble: 1 Ã— 5\n#>   name     call_info        source component           component_id        \n#>   <chr>    <list>           <chr>  <chr>               <chr>               \n#> 1 deg_free <named list [3]> recipe step_spline_natural spline_natural_P1Tjg\nns_tunable$call_info\n#> [[1]]\n#> [[1]]$pkg\n#> [1] \"dials\"\n#> \n#> [[1]]$fun\n#> [1] \"spline_degree\"\n#> \n#> [[1]]$range\n#> [1]  2 15\n```\n:::\n\n\n\n## Neural network tuning\n\nThere might be some warnings that \n\n> \"Early stopping occurred at epoch 3 due to numerical overflow of the loss function.\"\n\ntorch can be very aggressive about moving in the direction of the gradient. In some cases, it moves to a space where the gradient is not well-behaved (e.g., flat in multiple directions, saddle point, etc). \n\nThis can cause abnormally large parameter values with magnitudes larger than double precision variables can hold. \n\nIn this case, brulee stops the optimization and returns the parameters from the last best iteration. \n\n## Running in parallel\n\nWe usually leave one or two cores available to work with when we run in parallel. \n\nIf you are using an HPC system, be careful to use only the cores allocated to you. \n\nAlso, memory is duplicated _for each worker_. If your main R session is using 1GB of memory, using 10 workers requires 11GB. \n\nComparing these methods: \n\n - They usually perform about the same. \n - future is more mature and is geared towards users and developers. \n - mirai contains its own queuing system and can be more efficient in allocating work. \n\n<hr size=\"5\">\n\n# Advanced 4 - Feature Engineering: dummies and embeddings\n\n## isomap with recipes\n\nThe results from Isomap, unlike UMAP, cannot be distorted in the same way. Here, the number of neighbors changes between the charts. Each of the charts is an application of Isomap on the same completely random data. The points are in different locations, but none of the charts appear to show any patterns. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](figures/isomap-unknown-1.svg)\n:::\n:::\n\n\n\n# Advanced Extras - Effect Encodings\n\n## Per-agent statistics\n\nThe effect encoding method essentially takes the effect of a variable, like an agent, and creates a data column for that effect. In our example, the agent's effect on the ADR is quantified by a model and then added as a data column to be used in the model. \n\nSuppose agent Max has a single reservation in the data, with an ADR of â‚¬200. If we use a naive estimate for Maxâ€™s effect, the model is told that Max should always produce an effect of â‚¬200. Thatâ€™s a very poor estimate since it is from a single data point. \n\nContrast this with seasoned agent Davis, who has taken 250 reservations with an average ADR of â‚¬100. Davisâ€™s mean is more predictive because it is estimated with better data (i.e., more total reservations). \nPartial pooling leverages the entire data set and can borrow strength from all of the agents. It is a common tool in Bayesian estimation and non-Bayesian mixed models. If an agentâ€™s data is of good quality, the partial pooling effect estimate is closer to the raw mean. Maxâ€™s data is not great and is \"shrunk\" towards the center of the overall average. Since there is so little known about Maxâ€™s reservation history, this is a better effect estimate (until more data is available for him). \n\nThe Stan documentation has a pretty good vignette on this:  <https://cran.r-project.org/web/packages/rstanarm/vignettes/pooling.html>\n\nAlso, _Bayes Rules!_ has a nice section on this: <https://www.bayesrulesbook.com/chapter-15.html>\n\nSince this example has a numeric outcome, partial pooling is very similar to the Jamesâ€“Stein estimator: <https://en.wikipedia.org/wiki/Jamesâ€“Stein_estimator>\n\n## Agent effects\n\nEffect encoding might result in a somewhat circular argument: the column is more likely to be important to the model since it is the output of a separate model. The risk here is that we might overfit the effect to the data. For this reason, it is super important to verify that we arenâ€™t overfitting by checking with resampling (or a validation set). \n\nPartial pooling somewhat lowers the risk of overfitting since it tends to correct for agents with small sample sizes. However, it canâ€™t correct for improper data usage or data leakage. \n\n# Advanced Extras - Case Study on Transportation\n\n## A recipe - handle correlations\n\nIn this code chunk, what's the story with `!!stations`? \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchi_pca_rec <- \n  chi_rec |> \n  step_normalize(all_of(!!stations)) |> \n  step_pca(all_of(!!stations), num_comp = tune())\n```\n:::\n\n\n\n`stations` is a vector of names of 20 columns that we want to use in the steps. If the list were shorter, we could type them in (e.g., `c(\"col1\", \"col2\")`, etc.). \n\nThe vector lives in our global workspace, and if we are in parallel, the worker processes might not have access to `stations`. The `!!` (frequently said as \"bang bang\") inserts the actual contents of the vector into the `all_of()` calls so that it looks like you just typed it in. \n\nThis means that the parallel process workers have a copy of the data in their reach, and the code will run without error. \n\n\n# Advanced extras - Iterative Search\n\n## Gaussian Processes and Optimization\n\nSome other references for GP's: \n\n- Chapter 5 of [_Surrogates: Gaussian process modeling, design, and optimization for the applied sciences_](https://bookdown.org/rbg/surrogates/chap5.html)\n- _Bayesian Optimization_, Chapter 3 [(pdf)](https://github.com/bayesoptbook/bayesoptbook.github.io/tree/master/book)\n- _Gaussian Processes for Machine Learning_ [(pdf)](https://gaussianprocess.org/gpml/chapters/)\n\n## Acquisition Functions\n\nMore references:\n\n- Chapter 7 of [_Surrogates: Gaussian process modeling, design, and optimization for the applied sciences_](https://bookdown.org/rbg/surrogates/chap7.html)\n- _Bayesian Optimization_, Chapter 6 [(pdf)](https://github.com/bayesoptbook/bayesoptbook.github.io/tree/master/book)\n- _Gaussian Processes for Machine Learning_\n\n",
    "supporting": [
      "figures"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}