---
title: "1 - Introduction"
subtitle: "Getting More Out of Feature Engineering and Tuning for Machine Learning"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

```{r setup}
#| include: false
#| file: setup.R
```

::: r-fit-text
Welcome!
:::

::: columns
::: {.column width="50%"}

<center>

### <i class="fa fa-wifi"></i>

Wi-Fi network name

`Posit Conf 2025`

</center>

:::

::: {.column width="50%"}

<center>

### <i class="fa fa-key"></i>

Wi-Fi password

`conf2025`

</center>

:::
:::

## Venue information

-   There are gender neutral bathrooms located on floor LL2, next to Chicago A

-   A meditation/prayer room is located on floor LL2 in Chicago A

-   A lactation room is located on floor LL2 in Chicago B

## Workshop policies

-   Please review the posit::conf code of conduct, which applies to all workshops: <https://posit.co/code-of-conduct>

-   CoC site has info on how to report a problem (in person, email, phone)

-   Please do not photograph people wearing red lanyards


## Who are you?

-   You can use the magrittr `%>%` or base R `|>` pipe

-   You are familiar with functions from dplyr, tidyr, ggplot2

-   You have exposure to basic statistical concepts

-   You do need intermediate familiarity with modeling or ML

-   You have used some tidymodels packages
 
-   You have some experience with evaluating statistical models using resampling techniques 

## Who are tidymodels?

-   Simon Couch
-   Hannah Frick
-   Emil Hvitfeldt
-   Max Kuhn

. . .

\+ our TA today, Edgar Ruiz!

. . . 

Many thanks to Davis Vaughan, Julia Silge, David Robinson, Julie Jung, Alison Hill, and DesirÃ©e De Leon for their role in creating these materials!

## Asking for help

. . .

ðŸŸª "I'm stuck and need help!"

. . .

ðŸŸ© "I finished the exercise"


## Discord `r fontawesome::fa("discord")`

-   [pos.it/conf-event-portal](http://pos.it/conf-event-portal) (login)
-   Click on "Join Discord, the virtual networking platform!"
-   Browse Channels -> `#workshop-feat-eng-tune`

## `r emo::ji("eyes")` {.annotation}

![](images/pointing.svg){.absolute top="0" right="0"}

## `r emo::ji("eyes")`

![](images/pointing_down.png){.absolute top="120" left="200" width="40%" }

::: footer
<span style="color:#CA225E;font-size:1.2rem;"><https://workshops.tidymodels.org></span>
:::

## Tentative plan for this workshop


- Model optimization by tuning
  - Grid search
  - Racing
- Feature engineering with recipes
- Postprocessing  
- Feature selection

##  {.center}

### Introduce yourself to your neighbors ðŸ‘‹

## Getting the materials

<br>

### If you are using Posit Cloud: 

<i class="fa fa-cloud"></i> Log in to Posit Cloud (free): TODO-ADD-LATER

<br> 

### If you are working locally: 

```r
# local download
usethis::use_course("tidymodels/workshops", destdir = "some_path")

# or fork via
usethis::create_from_github("tidymodels/workshops", fork = TRUE)
```

## Let's install some packages

If you are using your own laptop instead of Posit Cloud:

```{r load-pkgs}
#| eval: false

# Install the packages for the workshop
pkgs <- 
  c("almanac", "betacal", "bonsai", "brulee", "C50", "Cubist", "desirability2", 
    "dimRed", "earth", "embed", "extrasteps", "finetune", "igraph", 
    "important", "irlba", "kknn", "lightgbm", "lme4", "mirai", "parallelly", 
    "plumber", "probably", "RANN", "rpart", "RSpectra", "rules", 
    "splines2", "stacks", "text2vec", "textrecipes", "tidymodels", 
    "uwot", "vetiver")

install.packages(pkgs)
```

Also, you should make sure that you have installed the newest version of a few packages. To check this, you can run: 

```{r}
#| label: check-versions
#| eval: false
#| echo: true
rlang::check_installed("tidymodels", version = "1.4.1")
rlang::check_installed("embed", version = "1.2.0")
```

# Let's get started!  {background-color="#B00000FF"}

## Load tidymodels `r hexes("tidymodels")`

We're here to learn more about how to use the more advanced bits of tidymodels for supervised learning. Let's load the meta-package: 

```{r}
#| label: load-tm
#| message: true
library(tidymodels)
```

## Resolve naming conflicts `r hexes("tidymodels")`

You might want to run this function to avoid function name conflicts: 

<br> 

```{r}
#| label: tm-conflicts
tidymodels_prefer()
```

<br> 

To get more details, use the `quiet = FALSE` option. 

## Data sets

For illustration, we'll use a few different data sets today: 

::: incremental
 - `class_data`: a simulated set of data with a 1:10 class imbalance. Two classes, 20 predictors, and 2,000 data points. 
 - `leaf_data`: a real data set to identify plant species from their leaves. Thirty-two levels, 53 predictors, and 1,907 data points. 
 - `hotel_data`: a real data set for predicting the average cost per night. Numeric outcome, 27 predictors, and 15,402 data points. 
:::

. . .

<br> 

Let's get warmed up with the first data set. 

## Imbalanced data

These data can be loaded from the GitHub repo: 

<br>

```{r}
#| label: load-class-data

"https://raw.githubusercontent.com/tidymodels/" |> 
  paste0("workshops/main/slides/class_data.RData") |> 
  url() |> 
  load()
```

<br> 

The outcome column is `class` with levels `"event"` and `"no_event"`. Predictors are `"predictor_01"` to `"predictor_30"`.



## Your turn {transition="slide-in"}

Let's warm up by taking 8 minutes to explore the data. 

<br>

We'll ask you to tell us something about these data that might be interesting for modeling. 

```{r ex-class-data}
#| echo: false
countdown::countdown(minutes = 8, id = "class-data")
```

# A quick review of tidymodels {background-color="#D04E59FF"}

## Data splitting `r hexes("rsample")`

One of our first tasks is to split our data into (at a minimum) a training set and a testing set. The rsample package has numerous functions for this, prefixed by `initial_`. 
<br>

Let's create a 3:1 split of the simulated data and use a stratified random sample (by class):

```{r}
#| label: sim-split
#| code-line-numbers: "|1|2-3|7-8|"
set.seed(429)
sim_split <- initial_split(class_data, prop = 0.75, strata = class)
sim_split

sim_train <- training(sim_split)
sim_test  <- testing(sim_split)
```

::: {.absolute bottom=0 right=0}
[AML4TD](https://aml4td.org/chapters/initial-data-splitting.html), [TMwR](https://www.tmwr.org/splitting)
:::

## Data splitting `r hexes(c("ggplot2", "rsample"))`

:::: {.columns}

::: {.column width="50%"}
```{r}
#| label: train-freq
#| fig-width: 6
#| fig-height: 4
sim_train |> 
  ggplot(aes(class)) + 
  geom_bar()
```
:::

::: {.column width="50%"}
```{r}
#| label: test-freq
#| fig-width: 6
#| fig-height: 4
sim_test |> 
  ggplot(aes(class)) + 
  geom_bar()
```
:::

::::

## Resampling 

:::: {.columns}

::: {.column width="45%"}
We'll want to get accurate estimates of model performance. 

Let's use a resampling method to make multiple versions of our data (using 10-fold cross-validation).

<br> 

For large amounts of data, a validation set is also a good alternative. 
:::

::: {.column width="55%"}

![](images/10-Fold-CV.svg){fig-align='center' width=90%}
:::

::::

::: {.absolute bottom=0 right=0}
[AML4TD](https://aml4td.org/chapters/resampling.html), [TMwR](https://www.tmwr.org/resampling)
:::

## Resampling `r hexes("rsample")`

```{r}
#| label: sim-folds
#| code-line-numbers: "1|2|"
set.seed(523)
sim_rs <- vfold_cv(sim_train, v = 10, strata = class)
sim_rs
```


## Resampled data sets `r hexes("rsample")`

```{r}
#| label: folds

model_data_1 <- sim_rs |> get_rsplit(1) |> analysis()
model_data_1 |> count(class)

perf_data_1 <- sim_rs |> get_rsplit(1) |> assessment()
perf_data_1 |> count(class)
```

## Models via parsnip `r hexes("parsnip")`

Let's fit a simple decision tree to the data: 

```{r}
#| label: sim-tree-fit
#| code-line-numbers: "|1-2|5|"
# Specify what you want
tree_spec <- decision_tree(mode = "classification")

# Then train:
tree_fit <- tree_spec |> fit(class ~ ., data = model_data_1)  
tree_fit
```  

## Predicting... `r hexes("parsnip")`


```{r}
#| label: sim-tree-pred
predict(tree_fit, new_data = head(perf_data_1, 4))

predict(tree_fit, new_data = head(perf_data_1, 4), type = "prob")
```  

## Augmenting... `r hexes(c("broom", "parsnip"))`

```{r}
#| label: sim-tree-aug
tree_pred <- augment(tree_fit, new_data = perf_data_1)
tree_pred |> slice(1:5)
```  

## Performance metrics `r hexes("yardstick")`

There are _many_ yardstick metrics<sup>*</sup> for [class predictions](https://yardstick.tidymodels.org/reference/index.html#classification-metrics) and [probability estimates](https://yardstick.tidymodels.org/reference/index.html#class-probability-metrics). 

Let's make a collection of metrics and then evaluate our model.

```{r}
#| label: sim-metrics
#| code-line-numbers: "|1|2|"
cls_metrics <- metric_set(brier_class, roc_auc, sensitivity, specificity)
tree_pred |> cls_metrics(truth = class, estimate = .pred_class, .pred_event)
```

::: {.absolute bottom=0 left=0}
<sup>*</sup> ... and metrics for [regression](https://yardstick.tidymodels.org/reference/index.html#regression-metrics) and others. 
:::

::: {.absolute bottom=0 right=0}
[AML4TD](https://aml4td.org/chapters/cls-metrics.html)
:::


## Recipes and workflows `r hexes(c("recipes", "workflows"))`

Recipes are preprocessors that perform sequential operations on the preductors. 

. . .

<br> 

For example, to center and scale our predictors: 

```{r}
#| label: sim-recipes
#| code-line-numbers: "|2|3|"

rec <- 
  recipe(class ~ ., data = sim_train) |> 
  step_normalize(all_numeric_predictors())
```

. . .

<br> 

A model, a recipe, and other objects can be added to a _workflow_ to have a single object for the whole modeling sequence: 

```{r}
#| label: sim-wflow
tree_wflow <- workflow(rec, tree_spec)
```


## Your turn {transition="slide-in"}

Fit a different type of decision tree, this time:

- Using the `C5.0` engine
- Change the minimum number of samples required for splitting to 10. 

<br>

Did performance change much? 

<br>

```{r}
#| label: change-engine
#| echo: false
countdown::countdown(minutes = 5, id = "change-engine")
```


## Our versions

```{r pkg-list, echo = FALSE}
pkgs <- 
  c("almanac", "betacal", "bonsai", "brulee", "C50", "Cubist", "desirability2", 
    "dimRed", "earth", "embed", "extrasteps", "finetune", "igraph", 
    "important", "irlba", "kknn", "lightgbm", "lme4", "mirai", "parallelly", 
    "plumber", "probably", "RANN", "rpart", "RSpectra", "rules", 
    "splines2", "stacks", "text2vec", "textrecipes", "tidymodels", 
    "uwot", "vetiver")
loaded <- purrr::map(pkgs, ~ library(.x, character.only = TRUE))
excl <- c("iterators", "emo", "countdown", "stats", "graphics", 
          "grDevices", "utils", "datasets", "methods", "base", "forcats", 
          "infer", "foreach", "Matrix", "R6", "parallel", "devtools", "usethis")
loaded <- loaded[[length(loaded)]]
loaded <- loaded[!(loaded %in% excl)]
pkgs <- 
  sessioninfo::package_info(loaded, dependencies = FALSE) |> 
  select(-date)
df <- tibble::tibble(
  package = pkgs$package,
  version = pkgs$ondiskversion
)

ids <- split(
  seq_len(nrow(df)), 
  ceiling(seq_len(nrow(df)) / ceiling(nrow(df) / 4))
)

column1 <- df |>
  dplyr::slice(ids[[1]])

column2 <- df |>
  dplyr::slice(ids[[2]])

column3 <- df |>
  dplyr::slice(ids[[3]])

column4 <- df |>
  dplyr::slice(ids[[4]])

quarto_info <- paste0("Quarto (", system("quarto --version", intern = TRUE), ")")
```

`r R.version.string`, `r quarto_info`

::: {.columns style="font-size:0.5em; font-family: monospace"}
::: {.column width="25%"}
```{r}
#| label: column1
#| echo: false
knitr::kable(column1)
```
:::

::: {.column width="25%"}
```{r}
#| label: column2
#| echo: false
knitr::kable(column2)
```
:::

::: {.column width="25%"}
```{r}
#| label: column3
#| echo: false
knitr::kable(column3)
```
:::

::: {.column width="25%"}
```{r}
#| label: column4
#| echo: false
knitr::kable(column4)
```
:::
:::

