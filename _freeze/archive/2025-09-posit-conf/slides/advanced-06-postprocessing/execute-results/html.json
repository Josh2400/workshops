{
  "hash": "6011ddfec81e84929e105d0a12b42a9b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"6 - Postprocessing\"\nsubtitle: \"Getting More Out of Feature Engineering and Tuning for Machine Learning\"\nformat:\n  revealjs: \n    slide-number: true\n    footer: <https://workshops.tidymodels.org>\n    include-before-body: header.html\n    include-after-body: footer-annotations.html\n    theme: [default, tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk: \n    echo: true\n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n\n\n\n## Startup!  ![](hexes/tidymodels.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tailor.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/probably.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(desirability2)\nlibrary(probably)\nlibrary(mirai)\n\n# check torch:\nif (torch::torch_is_installed()) {\n  library(torch)\n}\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\ndaemons(parallel::detectCores())\n```\n:::\n\n\n\n## More startup!   ![](hexes/rsample.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load our example data for this section\n\"https://raw.githubusercontent.com/tidymodels/\" |> \n  paste0(\"workshops/main/slides/class_data.RData\") |> \n  url() |> \n  load()\n\nset.seed(429)\nsim_split <- initial_split(class_data, prop = 0.75, strata = class)\nsim_train <- training(sim_split)\nsim_test  <- testing(sim_split)\n\nset.seed(523)\nsim_rs <- vfold_cv(sim_train, v = 10, strata = class)\n```\n:::\n\n\n\n## Our neural network model ![](hexes/recipes.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/parsnip.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/brulee.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <- \n  recipe(class ~ ., data = sim_train) |> \n  step_normalize(all_numeric_predictors())\n\nnnet_spec <- \n  mlp(hidden_units = tune(), penalty = tune(), learn_rate = tune(), \n      epochs = 100, activation = tune()) |> \n  # Remove the class_weights argument\n  set_engine(\"brulee\", stop_iter = 10) |> \n  set_mode(\"classification\")\n  \nnnet_wflow <- workflow(rec, nnet_spec)\n\nnnet_param <- \n  nnet_wflow |> \n  extract_parameter_set_dials()   \n```\n:::\n\n\n\n# What is postprocessing?  {background-color=\"#28A8B8FF\"}\n\n## Adjusting model predictions\n\nHow can we modify our predictions? Some examples: \n\n - Fixing calibration issues<sup>*</sup>.\n - Limit the range of predictions.\n - Alternative cutoffs for binary data.\n - Declining to predict.\n\n. . .\n\n<sup>*</sup> Requires further estimation (and data).\n\n<br>\n\nLet's first consider the easiest case: alternative cutoffs.\n\n## Alternative thresholds\n\nInstead of up-weighting the samples in the minority (via `class_weights`), we can try to fit the best model and then define what it means to be an \"event.\" \n\n<br> \n\nInstead of using a 50% threshold, we might lower the level of evidence needed to call a prediction an event. \n\n<br> \n\nHow do we tune the threshold? \n\n## Tailors ![](hexes/tailor.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nThe tailor package is similar to recipes but specifies how to adjust predictions. \n\nA simple example: \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2|3|\"}\nthrsh_tlr <-\n  tailor() |>\n  adjust_probability_threshold(threshold = 1 / 3)\n\nthrsh_tlr\n```\n:::\n\n\n\n - Like a recipe, this initial call doesn't do anything but declare intent. \n\n - Unlike a recipe, it does not need the data (i.e., predictions) at this point. \n   - Relevant prediction columns are selected when `fit()` is used (next slide). \n\n## Manual use of a tailor  ![](hexes/tailor.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\nThere is a `fit()` method that requires data and the names of the prediction columns: \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|5|14|15|16-17|\"}\nthree_rows <- \n  tribble(\n     ~ class, ~ .pred_class, ~.pred_event, ~.pred_nonevent,\n     \"event\",       \"event\",          0.6,             0.4,\n     \"event\",    \"nonevent\",          0.4,             0.6, \n  \"nonevent\",    \"nonevent\",          0.1,             0.9  \n  ) |> \n  mutate(across(where(is.character), factor))\n\nthrsh_fit <-\n  thrsh_tlr |>\n  fit(\n    three_rows,\n    outcome = class,\n    estimate = .pred_class,\n    .pred_event:.pred_nonevent  # No argument name and order matches factor levels\n  )\n```\n:::\n\n\n\n## Manual use of a tailor  ![](hexes/tailor.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n`predict()` applies the adjustments: \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|3|8|\"}\nthrsh_fit\n\npredict(thrsh_fit, three_rows)\n#> # A tibble: 3 × 4\n#>   class    .pred_class .pred_event .pred_nonevent\n#>   <fct>    <fct>             <dbl>          <dbl>\n#> 1 event    event               0.6            0.4\n#> 2 event    event               0.4            0.6\n#> 3 nonevent nonevent            0.1            0.9\n```\n:::\n\n\n\n<br> \n\n## tailors within workflows  ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tailor.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\nIn practice, we would add the tailor to a workflow to make it easier to use: \n\n<br>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnnet_wflow <- workflow(rec, nnet_spec, thrsh_tlr)\n```\n:::\n\n\n\n<br>\n\n- We don't have to set the names of the outcome or prediction columns (yet). \n- `fit()` and `predict()` happen automatically. \n\n\n## Current adjustments\n\n- [`adjust_equivocal_zone()`](https://tailor.tidymodels.org/reference/adjust_equivocal_zone.html): decline to predict.\n - [`adjust_numeric_calibration()`](https://tailor.tidymodels.org/reference/adjust_numeric_calibration.html): try to readjust predictions to be consistent with numeric predictions.\n - [`adjust_numeric_range()`](https://tailor.tidymodels.org/reference/adjust_numeric_range.html): restrict the range of predictions.\n - [`adjust_predictions_custom()`](https://tailor.tidymodels.org/reference/adjust_predictions_custom.html): similar to `dplyr::mutate()`.\n - [`adjust_probability_calibration()`](https://tailor.tidymodels.org/reference/adjust_probability_calibration.html): try to readjust predictions to be consistent with probability predictions.\n - [`adjust_probability_threshold()`](https://tailor.tidymodels.org/reference/adjust_probability_threshold.html): custom rule for hard-class predictions from probabilities. \n\n## Some notes\n\n- Adjustment order matters; tailor will error early if the ordering rules are violated. \n\n- Adjustments that change class probabilities also affect hard class predictions. \n\n- Adjustments happen before performance estimation. \n   - Undoing something like a log transformation is a bad idea here. \n   \n- We have more calibration methods in mind. \n\n## Your turn {transition=\"slide-in\"}\n\n- Discuss with those around you what the \"ordering rules\" could be.\n\n<br>\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"ordering-rules\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n\n\n## More Notes\n\n- When estimation is required, the data considerations become more complex. \n\n- Most arguments can be tuned. \n\n- For grid search, we use a conditional execution algorithm that avoids redundant retraining of the preprocessor or model. \n\n# Back to our neural network  {background-color=\"#90D0C8FF\"}\n\n## Tuning the probability threshold   ![](hexes/workflows.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"} ![](hexes/tailor.png){.absolute top=-20 right=128 width=\"64\" height=\"74.24\"} ![](hexes/dials.png){.absolute top=-20 right=192 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|3|5|10|\"}\nthrsh_tlr <-\n  tailor() |>\n  adjust_probability_threshold(threshold = tune())\n\nnnet_thrsh_wflow <- workflow(rec, nnet_spec, thrsh_tlr)\n  \nnnet_thrsh_param <- \n  nnet_thrsh_wflow |> \n  extract_parameter_set_dials() |> \n  update(threshold = threshold(c(0.001, 0.5)))\n```\n:::\n\n\n\n## Tuning the probability threshold    ![](hexes/yardstick.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/tune.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\nNearly the same code as before: \n\n<br>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)\ncls_mtr <- metric_set(brier_class, roc_auc, sensitivity, specificity)\n\nset.seed(12)\nnnet_thrsh_res <-\n  nnet_thrsh_wflow |>\n  tune_grid(\n    resamples = sim_rs,\n    grid = 25,\n    param_info = nnet_thrsh_param, \n    control = ctrl,\n    metrics = cls_mtr\n  )\n```\n:::\n\n\n\n\n## Grid results ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(nnet_thrsh_res)\n```\n\n::: {.cell-output-display}\n![](advanced-06-postprocessing_files/figure-revealjs/sim-nnet-threshold-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## Grid results\n\n - `tanh` activation is doing much better. \n - `threshold` should not (and does not) affect the Brier or ROC metrics. \n - We can achieve low Brier scores.\n - We could run another grid with values <2% for a better threshold estimate.\n\n## Multimetric optimization ![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1|3|4|5|11|\"}\nnnet_thrsh_res |>\n  show_best_desirability(\n    maximize(sensitivity),\n    minimize(brier_class),\n    constrain(specificity, low = 0.8, high = 1.0)\n  ) |>\n  relocate(threshold, sensitivity, specificity, brier_class, .d_overall)\n#> # A tibble: 5 × 14\n#>   threshold sensitivity specificity brier_class .d_overall hidden_units  penalty\n#>       <dbl>       <dbl>       <dbl>       <dbl>      <dbl>        <int>    <dbl>\n#> 1     0.250       0.853       0.947      0.0422      0.940           28 2.61e-10\n#> 2     0.292       0.817       0.953      0.0403      0.937           38 1   e- 5\n#> 3     0.105       0.882       0.899      0.0470      0.923           48 1.78e- 9\n#> 4     0.209       0.833       0.933      0.0473      0.904            8 1   e-10\n#> 5     0.126       0.852       0.901      0.0522      0.882           42 3.16e- 3\n#> # ℹ 7 more variables: activation <chr>, learn_rate <dbl>, .config <chr>,\n#> #   roc_auc <dbl>, .d_max_sensitivity <dbl>, .d_min_brier_class <dbl>,\n#> #   .d_box_specificity <dbl>\n```\n:::\n\n\n\n## Calibration![](hexes/tune.png){.absolute top=-20 right=0 width=\"64\" height=\"74.24\"} ![](hexes/probably.png){.absolute top=-20 right=64 width=\"64\" height=\"74.24\"}\n\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nmore_sens <-\n  nnet_thrsh_res |>\n  select_best_desirability(\n    maximize(sensitivity),\n    minimize(brier_class),\n    constrain(specificity, low = 0.8, high = 1.0)\n  )\n\nnnet_thrsh_res |>\n  collect_predictions(\n    parameters = more_sens\n  ) |>\n  cal_plot_windowed(\n    truth = class,\n    estimate = .pred_event,\n    window_size = 0.2,\n    step_size = 0.025,\n  )\n```\n\n::: {.cell-output-display}\n![](advanced-06-postprocessing_files/figure-revealjs/nnet-cal-plot-sens-1.svg){width=90%}\n:::\n:::\n\n\n\n## Thoughts about these results\n\nThe calibration issue in the previous plot shows that some very likely non-events will have underestimated probabilities.\n\n- That may not matter if we are very focused on events. \n- Thresholding does not affect calibration. \n- We might be able to:\n    - Further tune the neural network to solve the issue and/or \n    - Add a calibration postprocessor \n\n## Thoughts about the approach\n\nLet's say that we pick a threshold of 2%. Our explanation to the user/stakeholder would be \n   \n   > \"As long as the model is at least 2% sure it is an event, we will call it an event\". \n   \nIt may be challenging to convince someone that this is the best option. \n\n<br> \n\nThat said, this is probably a better approach than cost-sensitive learning. \n\n# Fitting a postprocessor {background-color=\"#B0D0B8FF\"}\n\n## Data to train the adjustments\n\nIf an adjustment requires data, where do we get it from?\n\n::: incremental\n- Fitting a calibration model to the training set re-predictions would be bad. \n\n- Also, we don't want to touch the validation or test sets. \n:::\n\n. . .\n\n<br>\n\nWe need another data set.\n\n## Data sources\n\nTwo possibilities: \n\n1. Shave some data off the training set to create a _calibration set_. \n\t- During resampling, we can do the same to the analysis set.\n\t- The \"shaving\" process emulates the original sampling method. \n\t- There are fewer data points for training the preprocessor and primary model. \n\t\n2. Use a static calibration set outside our training/validation/testing splits. \n\nCurrently, we have implemented the first method. \n\n## Example: 3-fold CV\n\n![](images/3CV-split.svg){fig-align='center'}\n\n## Example: 3-fold CV _internal split_\n\n\n![](images/3CV-internal-split.svg){fig-align='center'}\n\n\n## Breakdown for the class imbalance data\n\n<br>\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|Data        |Strategy       | event| no_event|\n|:-----------|:--------------|-----:|--------:|\n|Original    |All            |   225|     1775|\n|Training    |No Calibration |   168|     1331|\n|Analysis    |No Calibration |   151|     1197|\n|Training    |Calibration    |   126|      998|\n|Analysis    |Calibration    |   135|     1077|\n|Calibration |Calibration    |    16|      120|\n\n\n:::\n:::\n\n\n\n## Calibration\n\nCalibration models, in essence, try to predict the true class using the model predictions. _Symbolically_: \n\n- For regression models: `outcome ~ .pred`\n- For binary classifiers: `class ~ .pred_class_1`\n- For multiclass: `class ~ .pred_class_1 + .pred_class_2 + ...`\n\nEach calibration method works slightly differently. \n\nFor example, in regression, a (generalized) linear model is fit, and the residuals are added to new predictions. \n\n## Calibration expectations\n\nKeep expectations low. For these methods to work: \n\n - The systematic issue will need to be large, or at least not subtle.\n - A large calibration set is needed to work effectively. \n \nThe [example in ALM4TD](https://aml4td.org/chapters/cls-metrics.html#sec-cls-calibration) is an illustrative example and details. \n\nIn many cases, trying a different model or tuning parameters would be better. \n\n<br>\n\nYou can tune the calibration method, one of which is _no calibration_.\n \n\n## Your turn {transition=\"slide-in\"}\n\n- Based on previous results, choose and fix a specific activation type (i.e., no `tune()`).\n- Add a calibrator to your tailor with a `method = tune()` value. \n- Run another grid search\n\nDoes it help with this data set?\n\n<br>\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"class-data\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">10</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}