---
title: "Extras - Iterative search"
subtitle: "Getting More Out of Feature Engineering and Tuning for Machine Learning"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
    fig.path: "figures/"
---

```{r}
#| label: setup
#| include: false
#| file: setup.R
```

```{r}
#| label: quiet-start
#| include: false
load("bayes_opt_calcs.RData")
```

## Startup!  `r hexes(c("tidymodels", "probably"))`

```{r}
#| label: user-startup
library(tidymodels)
library(important)
library(probably)

tidymodels_prefer()
theme_set(theme_bw())
options(pillar.advice = FALSE, pillar.min_title_chars = Inf)
mirai::daemons(parallel::detectCores())
```

## More startup! `r hexes(c("rsample"))`

```{r}
#| label: user-startup-2
# Load our example data for this section
"https://raw.githubusercontent.com/tidymodels/" |> 
  paste0("workshops/main/slides/class_data.RData") |> 
  url() |> 
  load()

set.seed(429)
sim_split <- initial_split(class_data, prop = 0.75, strata = class)
sim_train <- training(sim_split)
sim_test  <- testing(sim_split)

set.seed(523)
sim_rs <- vfold_cv(sim_train, v = 10, strata = class)
```

## Neural network `r hexes("tune", "recipes")`

From the seventh advanced slide deck:

```{r}
#| label: nnet-start
nnet_spec <-
  mlp(hidden_units = tune(), penalty = tune(), learn_rate = tune(),
      epochs = 100, activation = tune()
  ) |>
  set_engine("brulee", stop_iter = 10) |>
  set_mode("classification")

rec <- 
  recipe(class ~ ., data = sim_train) |> 
  step_normalize(all_numeric_predictors())
  
thrsh_tlr <-
  tailor() |>
  adjust_probability_threshold(threshold = tune()) 
  
nnet_wflow <- workflow(rec, nnet_spec, thrsh_tlr)

nnet_param <-
  nnet_wflow |> 
  extract_parameter_set_dials() |> 
  update(threshold = threshold(c(0.0001, 0.1)))
  
cls_mtr <- metric_set(brier_class, roc_auc, sensitivity, specificity)
```

## Iterative Search

Instead of pre-defining a grid of candidate points, we can model our current results to predict what the next candidate point should be. 

<br>

Suppose that we are only tuning the learning rate in our neural network. 

<br>

We could do something like: 

```r
brier_model <- lm(brier ~ learn_rate, data = resample_results)
```

and use this to predict and rank new learning rate candidates. 


## Iterative Search

A linear model probably isn't the best choice though (more in a minute). 

To illustrate the process, we resampled a large grid of learning rate values for our data to show what the relationship is between error and learning rate. 

Now suppose that we used a grid of three points in the parameter range for learning rate...


## A Large Grid


```{r}
#| label: grid-large
#| echo: false
#| out-width: 50%
#| fig-align: center
#| fig-width: 6
#| fig-height: 4.25

grid_points |>
  ggplot(aes(learn_rate, mean)) +
  geom_line(alpha = 1 / 2, col = "#0b84a5", linewidth = 1.5) +
  scale_x_log10() +
  labs(y = "Error (resampled)", x = "Learning Rate")
```


## A Three Point Grid

```{r}
#| label: grid-large-sampled
#| echo: false
#| out-width: 50%
#| fig-align: center
#| fig-width: 6
#| fig-height: 4.25

grid_points |>
  ggplot(aes(learn_rate, mean)) +
  geom_line(alpha = 1 / 10, col = "#0b84a5", linewidth = 1.5) +
  geom_point(data = init_points) +
  scale_x_log10() +
  labs(y = "Error (resampled)", x = "Learning Rate")
```

## Gaussian Processes and Optimization  {.annotation}

We can make a "meta-model" with a small set of historical performance results. 

[Gaussian Processes](https://gaussianprocess.org/gpml/) (GP) models are a good choice to model performance. 

- It is a Bayesian model so we are using **Bayesian Optimization (BO)**.
- For regression, we can assume that our data are multivariate normal. 
- We also define a _covariance_ function for the variance relationship between data points. A common one is:

$$\operatorname{cov}(\boldsymbol{x}_i, \boldsymbol{x}_j) = \exp\left(-\frac{1}{2}|\boldsymbol{x}_i - \boldsymbol{x}_j|^2\right) + \sigma^2_{ij}$$


:::notes
GPs are good because 

- they are flexible regression models (in the sense that splines are flexible). 
- we need to get mean and variance predictions (and they are Bayesian)
- their variability is based on spatial distances.

Some people use random forests (with conformal variance estimates) or other methods but GPs are most popular.
:::


## Predicting Candidates

The GP model can take candidate tuning parameter combinations as inputs and make predictions for performance (e.g. Brier, ROC AUC, RMSE, etc.)

 - The _mean_ performance
 - The _variance_ of performance 
 
The variance is mostly driven by spatial variability (the previous equation). 

The predicted variance is zero at locations of actual data points and becomes very high when far away from any observed data. 


## Your turn {transition="slide-in"}

:::: {.columns}

::: {.column width="50%"}

*Your GP makes predictions on two new candidate tuning parameters.*  

*We want to minimize error.* 

*Which should we choose?*

:::

::: {.column width="50%"}
```{r}
#| label: two-candidates
#| echo: false
#| out-width: 100%
#| fig-width: 5
#| fig-height: 5

set.seed(28383)
num_points <- 5000
exerc_data <- 
  tibble(Error = c(rnorm(num_points, 10, 2), rnorm(num_points, 13, 1 / 2)),
         `Choose:` = rep(paste("candidate", 1:2), each = num_points))

exerc_data |> 
  ggplot(aes(Error, col = `Choose:`)) + 
  geom_line(stat = "density", adjust = 1.25, trim = TRUE, linewidth = 1) +
  geom_vline(mapping = aes(xintercept = 14)) +
  theme(legend.position = "top") +
  annotate("text", x = 15, y = .65, label = "Current\nbest")
```
:::

::::

```{r}
#| echo: false
countdown::countdown(minutes = 3, id = "mean-var-trade", left = "0")
```


## GP Fit (ribbon is mean +/- 1SD)

```{r}
#| label: gp-iter-0
#| echo: false
#| out-width: 50%
#| fig-align: center
#| fig-width: 6
#| fig-height: 4.25

scaled_obj |>
  filter(.iter == 0) |> 
  ggplot(aes(learn_rate, .mean)) +
  geom_line(data = grid_points, aes(y = mean), alpha = 1 / 10, 
            col = "#0b84a5", linewidth = 1.5) +
  geom_line() +
  geom_point(data = bayes_points |> filter(.iter == 0), aes(y = mean)) +
  geom_ribbon(aes(ymin = .mean - .sd, ymax = .mean + .sd),
              alpha = 1 / 7) +
  scale_x_log10() +
  labs(y = "Error", x = "Learning Rate")
```


## Choosing New Candidates

This isn't a very good fit but we can still use it.

How can we use the outputs to choose the next point to measure?

<br> 

[_Acquisition functions_](https://ekamperi.github.io/machine%20learning/2021/06/11/acquisition-functions.html) take the predicted mean and variance and use them to balance: 

 - _exploration_:  new candidates should explore new areas.
 - _exploitation_: new candidates must stay near existing values. 

Exploration focuses on the variance, exploitation is about the mean. 

## Acquisition Functions

We'll use an acquisition function to select a new candidate.

The most popular method appears to be _expected improvement_ ([EI](https://arxiv.org/pdf/1911.12809.pdf)) above the current best results. 
 
  - Zero at existing data points. 
  - The _expected_ improvement is integrated over all possible improvement ("expected" in the probability sense). 

We would probably pick the point with the largest EI as the next point. 

(There are other functions beyond EI.)

## Expected Improvement

```{r}
#| label: gp-iter-0-ei
#| echo: false
#| out-width: 50%
#| fig-align: center
#| fig-width: 6
#| fig-height: 4.25

scaled_obj |>
  filter(.iter == 0) |> 
  ggplot(aes(learn_rate, scaled)) +
  geom_point(data = bayes_points |> filter(.iter == 0), aes(y = zero)) +
  geom_line(alpha = 1 / 2, col = "#D95F02", linewidth = 1) +
  scale_x_log10()  +
  labs(y = "Expected Improvement", x = "Learning Rate")
```

## Iteration

Once we pick the candidate point, we measure performance for it (e.g. resampling). 

<br> 

Another GP is fit, EI is recomputed, and so on. 

<br> 

We stop when we have completed the allowed number of iterations _or_ if we don't see any improvement after a pre-set number of attempts. 


## GP Fit with four points

```{r}
#| label: gp-iter-1
#| echo: false
#| out-width: 50%
#| fig-align: center
#| fig-width: 6
#| fig-height: 4.25

scaled_obj |>
  filter(.iter == 1) |> 
  ggplot(aes(learn_rate, .mean)) +
  geom_line(data = grid_points, aes(y = mean), alpha = 1 / 4, 
            col = "#0b84a5", linewidth = 1.5) +
  geom_line() +
  geom_point(data = bayes_points |> filter(.iter == 1), aes(y = mean)) +
  geom_ribbon(aes(ymin = .mean - .sd, ymax = .mean + .sd),
              alpha = 1 / 7) +
  scale_x_log10() +
  labs(y = "Error", x = "Learning Rate")
```


## Expected Improvement

```{r}
#| label: gp-iter-1-ei
#| echo: false
#| out-width: 50%
#| fig-align: center
#| fig-width: 6
#| fig-height: 4.25

scaled_obj |>
  filter(.iter == 1) |> 
  ggplot(aes(learn_rate, scaled)) +
  geom_point(data = bayes_points |> filter(.iter == 1), aes(y = zero)) +
  geom_line(alpha = 1 / 2, col = "#D95F02", linewidth = 1) +
  scale_x_log10()  +
  labs(y = "Expected Improvement", x = "Learning Rate")
```

::: {.absolute bottom=0 right=0}
[AML4TD](https://aml4td.org/chapters/iterative-search.html#fig-bayes-opt), [TMwR](https://www.tmwr.org/iterative-search#bayesian-optimization)
:::


## BO in tidymodels

We'll use a function called `tune_bayes()` that has very similar syntax to `tune_grid()`. 

<br> 

It has an additional `initial` argument for the initial set of performance estimates and parameter combinations for the GP model. 

## Initial grid points

`initial` can be the results of another `tune_*()` function or an integer (in which case `tune_grid()` is used under to hood to make such an initial set of results).
 
 - We'll run the optimization more than once, so let's make an initial grid of results to serve as the substrate for the BO. 

 - I suggest at least the number of tuning parameters plus two as the initial grid for BO. 

## Qualitative parameters

- What about non-numeric tuning parameters such as `activation`?

- _Currently_, tidymodels converts these to dummy indicators and uses those in the GP. This is not unusual but also not great. 
	- Our initial grid should include more points; one for each _level_ of the qualitative parameter.
	- In our case, the activation function is preset to use `r length(nnet_param$object[nnet_param$id == "activation"][[1]]$values)` possible values. 

- An upcoming version of tune will use a different R package to fit the GP that uses _factor_ or _Gower kernels_. This will avoid making indicators and require fewer initial points.


## An Initial Grid

```{r} 
#| label: nnet-bo-initial
set.seed(12)
init_res <-
  nnet_wflow |>
  tune_grid(
    resamples = sim_rs,
    grid = nrow(nnet_param) + 6, # for activation values + 1 extra
    param_info = nnet_param,
    metrics = cls_mtr
  )

show_best(init_res, metric = "brier_class", n = 3) |> select(-.metric, -.estimator)
```

## BO using tidymodels

```{r} 
#| label: nnet-bo
#| message: true
#| code-line-numbers: "1|6,8-11|"

ctrl_bo <- control_bayes(verbose_iter = TRUE, no_improve = Inf) 

set.seed(125)
nnet_bayes_res <-
  nnet_wflow |>
  tune_bayes(
    resamples = sim_rs,
    initial = init_res,     # <- initial results
    iter = 25,
    control = ctrl_bo,
    param_info = nnet_param,
    metrics = cls_mtr
  )
```

## Best results
```{r} 
#| label: nnet-bo-best

show_best(nnet_bayes_res, metric = "brier_class") |> select(-.metric, -.estimator)
```


## Plotting BO Results

```{r}
#| label: autoplot-marginals
#| echo: true
#| out-width: 50%
#| fig-align: center
#| fig-width: 10
#| fig-height: 4.25

autoplot(nnet_bayes_res, metric = "brier_class")
```


## Plotting BO Results

```{r}
#| label: autoplot-param
#| echo: true
#| out-width: 50%
#| fig-align: center
#| fig-width: 10
#| fig-height: 4.25

autoplot(nnet_bayes_res, metric = "brier_class", type = "parameters")
```


## Plotting BO Results

```{r}
#| label: autoplot-perf
#| echo: true
#| out-width: 50%
#| fig-align: center
#| fig-width: 6
#| fig-height: 4.25

autoplot(nnet_bayes_res, metric = "brier_class", type = "performance")
```

## Your turn {transition="slide-in"}

*Let's try a different acquisition function: `conf_bound(kappa)`.*

*We'll use the `objective` argument to set it.*

*Choose your own `kappa` value:*

 - *Larger values will explore the space more.* 
 - *"Large" values are usually less than one.*

**Bonus points**: Before the optimization is done, press `<esc>` and see what happens.

```{r}
#| echo: false
countdown::countdown(minutes = 10, id = "conf-bound")
```

## Notes

- Stopping `tune_bayes()` will return the current results. 

- Parallel processing can still be used to more efficiently measure each candidate point. 

- There are [a lot of other iterative methods](https://github.com/topepo/Optimization-Methods-for-Tuning-Predictive-Models) that you can use. 

- The finetune package also has functions for [simulated annealing](https://www.tmwr.org/iterative-search.html#simulated-annealing) search. 

