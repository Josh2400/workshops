---
title: "3 - Racing"
subtitle: "Getting More Out of Feature Engineering and Tuning for Machine Learning"
format:
  revealjs: 
    slide-number: true
    footer: <https://workshops.tidymodels.org>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

```{r setup}
#| include: false
#| file: setup.R
```

```{r}
#| label: start-quiet
#| include: false
load("lgbm_times.RData")
```

## Startup!  `r hexes(c("bonsai", "finetune", "tidymodels"))`

```{r}
#| label: user-startup
library(tidymodels)
library(finetune)
library(bonsai)

tidymodels_prefer()
theme_set(theme_bw())
options(pillar.advice = FALSE, pillar.min_title_chars = Inf)
```

## More startup!   `r hexes(c("rsample"))`

```{r}
#| label: user-startup-2
# Load our example data for this section
"https://raw.githubusercontent.com/tidymodels/" |> 
  paste0("workshops/main/slides/class_data.RData") |> 
  url() |> 
  load()

set.seed(429)
sim_split <- initial_split(class_data, prop = 0.75, strata = class)
sim_train <- training(sim_split)
sim_test  <- testing(sim_split)

set.seed(523)
sim_rs <- vfold_cv(sim_train, v = 10, strata = class)
```

## First, a shameless promotion

```{r}
#| label: shameless
#| echo: false
#| fig-align: "center"
#| out-width: 50%
knitr::include_graphics("images/finetune-toot.png")
```

# Efficient Grid Search  {background-color="#5E9546FF"}

## Making Grid Search More Efficient

Previously, we evaluated 250 models (25 candidates times 10 resamples).

We can make this go faster using parallel processing. 

<br> 

Also, for some models, we can _fit_ far fewer models than the number being evaluated. 
 
 * For example, with boosted trees, a model with `X` trees can often predict on candidates with fewer than `X` trees (i.e., no retraining). 
 
These strategies can lead to enormous speed-ups. 

## Model Racing 

[_Racing_](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=+Hoeffding+racing) is an old tool that we can use to go even faster. 

1. Evaluate all of the candidate models, but only for a few resamples. 
1. Determine which candidates have a low probability of being selected (_cough_, _cough_, `tanh` activation, _cough_).
1. Eliminate poor candidates.
1. Repeat with next resample (until no more resamples remain).

This can result in fitting a small number of models. 

It is *not* an iterative search; it is an adaptive grid search. 

::: {.absolute bottom=0 right=0}
[TMwR](https://www.tmwr.org/grid-search#racing), [TMwR example](https://www.tmwr.org/workflow-sets#racing-example), [AML4TD](https://aml4td.org/chapters/grid-search.html#sec-racing)
:::


## Discarding Candidates

How do we eliminate tuning parameter combinations? 

There are a few methods to do so. We'll use one based on analysis of variance (ANOVA). 

_However_... there is typically a large resampling effect in the results. 

## Resampling Results (Non-Racing)

:::: {.columns}

::: {.column width="50%"}


```{r}
#| label: race-data-comp
#| echo: false
#| out-width: 60%
#| fig-align: center
#| fig-width: 5
#| fig-height: 5

# Simulate some resamples for candidate models
num_resamples <- 10
race_example <- crossing(model = paste("candidate", 1:2), resample = 1:num_resamples)
candidates <- tibble(model = paste("candidate", 1:2), mean = c(4, 2))
set.seed(937)
resamples <- tibble(resample = 1:num_resamples, effect = rnorm(num_resamples, sd = 2))
race_example <-
  race_example |>
  full_join(candidates, by = "model") |>
  full_join(resamples, by = "resample") |>
  mutate(
    error = mean + effect + rnorm(n(), sd = 1) + 2,
    resample = format(resample)
  ) |>
  select(-mean, -effect)


race_table <- 
  race_example |>
  mutate(model = gsub(" ", "_", model)) |>
  pivot_wider(id_cols = c(resample), names_from = model, values_from = error)

race_cor <- cor(race_table[, -1], method = "spearman")[1,2]

get_ci <- function(ind) {
  dat <- race_table[1:ind,]
  t_test <- t.test(dat$candidate_1, dat$candidate_2, paired = TRUE)
  t_test <- tidy(t_test, conf)
  t_test |> 
    select(difference = estimate, lower = conf.low, upper = conf.high) |> 
    mutate(`number of resamples` = ind)
}

race_ci <- map_dfr(2:10, get_ci) 

```

Here are some realistic (but simulated) examples of two candidate models. 

An error estimate is measured for each of 10 resamples. 

 - The lines connect resamples. 

There is usually a significant resample-to-resample effect (rank corr: `r round(race_cor, 2)`). 

:::

::: {.column width="50%"}


```{r}
#| label: race-data
#| echo: false
#| out-width: 100%
#| fig-align: center
#| fig-width: 5
#| fig-height: 5
race_example |>
  ggplot(aes(x = model, y = error, group = resample, col = factor(resample))) +
  geom_point(show.legend = FALSE) +
  geom_line(alpha = 1 / 2, show.legend = FALSE) +
  labs(x = NULL, y = "model error")
```

:::

::::


## Are Candidates Different?

One way to evaluate these models is to do a paired t-test
 
 - or a t-test on their differences matched by resamples

With $n = 10$ resamples, the confidence interval for the difference in the model error is (`r signif(race_ci$lower[9], 2)`, `r signif(race_ci$upper[9], 2)`), indicating that candidate number 2 has a smaller error. 

## Evaluating Differences in Candidates

:::: {.columns}

::: {.column width="40%"}
What if we were to have compared the candidates while we sequentially evaluated each resample? 

`r emo::ji("point_right")`

<be>

One candidate shows superiority when `r which.max(race_ci$lower > 0) + 1` resamples have been evaluated.

:::

::: {.column width="60%"}
```{r}
#| label: race-ci
#| echo: false
#| out-width: 100%
#| fig-align: center
#| fig-width: 6
#| fig-height: 4.25

race_ci |> 
  ggplot(aes(`number of resamples`, difference)) + 
  geom_point() +
  geom_hline(yintercept = 0, col = "red", lty = 2) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 1 / 10, alpha = 0.5) +
  labs(y = "error difference from best") + 
  scale_x_continuous(breaks = pretty_breaks())
```
:::

::::


## Interim Analysis of Results

One version of racing uses a _mixed model ANOVA_ to construct one-sided confidence intervals for each candidate versus the current best. 

Any candidates whose bound does not include zero are discarded.  [Here](https://www.tmwr.org/race_results.mp4) is an animation.

The resamples are analyzed in a random order (so set the seed).

<br>

[Kuhn (2014)](https://arxiv.org/abs/1405.6974) has examples and simulations to show that the method works. 

The [finetune](https://finetune.tidymodels.org/) package has functions `tune_race_anova()` and `tune_race_win_loss()`. 

# Boosted Trees  {background-color="#70B7E1FF"}


## Boosted Trees

These are popular ensemble methods that build a _sequence_ of tree models. 

<br>

Each tree uses the results of the previous tree to better predict samples, especially those that have been poorly predicted. 

<br>

Each tree in the ensemble is saved, and new samples are predicted using a weighted average of its votes. 

<br>

We'll focus on the popular lightgbm implementation. 

## Boosted Tree Tuning Parameters

Some _possible_ parameters: 

* `mtry`: The number of predictors randomly sampled at each split (in $[1, ncol(x)]$ or $(0, 1]$).
* `trees`: The number of trees ($[1, \infty]$, but usually up to thousands).
* `min_n`: The number of samples needed to further split ($[1, n]$).
* `learn_rate`: The rate that each tree adapts from previous iterations ($(0, \infty]$, usual maximum is 0.1).
* `stop_iter`: The number of iterations of boosting where _no improvement_ was shown before stopping ($[1, trees]$).

## Boosted Tree Tuning Parameters

TBH, it is usually not difficult to optimize these models. 

<br>

:::: {.columns}

::: {.column width="50%"}
Often, there are multiple _candidate_ tuning parameter regions with very good results. 

For example: `r emo::ji("point_right")`

<br>

To demonstrate, we'll look at optimizing five of the tuning parameters.
:::

::: {.column width="50%"}
![](images/small_init.svg){fig-align="center" width=80%}
:::

::::


## Boosted Tree Tuning Parameters `r hexes(c("workflows", "parsnip", "bonsai"))`

We'll need to load the bonsai package. This has the information needed to use lightgbm

```{r}
#| label: boost-spec
#| code-line-numbers: "|5-9|12-14|16-17|"
library(bonsai)

lgbm_spec <-
  boost_tree(
    trees = tune(),
    learn_rate = tune(),
    mtry = tune(),
    min_n = tune(),
    stop_iter = tune()
  ) |>
  set_mode("classification") |>
  # Turn off within-tree parallel processing; it's faster to run 
  # the resamples/configurations in parallel
  set_engine("lightgbm", num_threads = 1) 

# No preprocessing required:
lgbm_wflow <- workflow(class ~ ., lgbm_spec)
```

# Racing our boosted trees  {background-color="#3381A8FF"}

## Racing `r hexes(c("tune", "bonsai", "finetune"))`

```{r } 
#| label: lgb-grid-race
#| cache: true
#| code-line-numbers: "|3-4|6-7|12|14-15|"
library(finetune)

# Set this to true to demo
ctrl <- control_race(verbose_elim = FALSE)

# Optimizes on the first metric in the set
cls_mtr <- metric_set(brier_class, roc_auc, sensitivity, specificity)

mirai::daemons(parallel::detectCores() - 1)

set.seed(321)
lgbm_res <-
  lgbm_wflow |>
  tune_race_anova(              # <- very similar syntax to tune_grid()
    resamples = sim_rs,
    # Let's use a larger grid
    grid = 50,
    control = ctrl,
    metrics = cls_mtr
  )
```

## Racing Results `r hexes("tune")`

```{r}
#| label: best-race
#| code-line-numbers: "|1|5|"
show_best(lgbm_res, metric = "brier_class")
```

<br> 

. . .

Times using 10 cores: sequential: `r ceiling(lgbm_times["sequential"])`s, parallel: `r ceiling(lgbm_times["parallel"])`s, and parallel racing: `r ceiling(lgbm_times["racing"])`s. 

<br> 

. . .

Parallel was `r round(lgbm_times["sequential"]/lgbm_times["parallel"], 1)`-fold faster, and racing in parallel was `r round(lgbm_times["sequential"]/lgbm_times["racing"], 1)`-fold faster.

## Racing Results `r hexes("finetune")`

:::: {.columns}

::: {.column width="50%"}
Only `r sum(map_int(lgbm_res$.metrics, ~ nrow(.x) / 2))` models were fit (out of `r nrow(lgbm_res) * 50`). 

`select_best()` never considers candidate models that did not get to the end of the race. 

There is a helper function to see how candidate models were removed from consideration. 

:::

::: {.column width="50%"}


```{r}
#| label: plot-race
#| out-width: 100%
#| fig-align: center
#| fig-width: 5
#| fig-height: 3.8

plot_race(lgbm_res)
```

:::

::::


## Your turn {transition="slide-in"}

- *Run `tune_race_anova()` with a different seed and/or a different metric.*
- *Did you get the same or similar results?*


```{r}
#| label: racing-repeat
#| echo: false
countdown::countdown(minutes = 8, id = "racing-repeat")
```

